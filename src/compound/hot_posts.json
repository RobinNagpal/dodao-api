[
  {
    "number_of_comments": 11,
    "postid": "92d44896-966d-4b3c-981f-d796b0c598a9",
    "posturl": "https://www.comp.xyz/t/the-receiving-address-is-a-tokens-contract-address/2707",
    "combinedcontent": "Hi everybody, i’m trying to withdraw Cdai and Cusdt from compound finance since i really need that money ( even with the high eth fees ) and i have this message\nThe receiving address is a tokens contract address\nSo i cannot withdraw and i don’t get it …can you please help ?\\nYou might be confusing the “receiving address” with the address of the contract you are interacting with.   You can’t withdraw supplied assets from Compound to another address, only to the address that holds to ctokens.\\nWell how can i confuse since i don’t do anything other than clicking withdraw from compound and it opens a the windrawal window with that message on the back …\\n\n1638635592693791×1186 379 KB\n\\nNobody can help on this issue ?\\nThis seems to be an issue with the wallet you are using. Maybe try connecting your Trezor to metamask?\\nCan you show the details info? We can decipher that to know what is actually being sent.\\nI made a picture of the instant , there’s nothing more to share i’m sorry …\n\n16385509983121920×2560 700 KB\n\\nNo, like rn, you have “Transaction” selected, and to the right, you can select “Details.”\\nHi Getty ,\nGot it now …here’s the picture …\n\n16388885220981000×1333 203 KB\n\\nThis is the correct hex data for a withdrawal from cUSDT. There is a bug with the wallet software you are using. Try using your Trezor with metamask.\\nI tried too but unfortunately, Compound connects to metamask but when i tried to withdraw, it doesn’t pop up to confirm the withdrawal …I completly stuck on that problem…"
  },
  {
    "number_of_comments": 16,
    "postid": "63e36353-1202-430d-bbcd-01c628659a72",
    "posturl": "https://www.comp.xyz/t/governance-weekly-recap/3576",
    "combinedcontent": "Hi all - duncand from Boardroom 7 here \nPart of our team’s mission is to increase informed participation in governance, and as a project supporting Compound governance we thought it would be useful to provide a weekly recap of recent governance activity in the Compound community. We feel a governance-focused summary will be useful for delegates and voters due to the often rapid pace of governance news and updates. When appropriate we’ll also note a few specific areas for action on timely subjects.\nWithin our weekly recap, we’ll provide a look back at:\n\nProposals\nGovernance changes\nImpactful operating suggestions, and\nCritical governance-related discussions across Discourse, Twitter, and Discord\n\nThe first recap will be posted today. We’d love to hear your feedback and any suggestions you have!\\n\nSeptember 9, 2022\n\nSummary\nIt’s been an eventful few weeks. The big news: Compound III was announced on August 25; the first deployment allows for the borrowing of USDC using ETH, WBTC, LINK, UNI, and COMP as collateral. The unexpected news: there were some difficulties with the Oracle upgrade. The v3 Oracle, which updated the price feed for Compound v2 and which was implemented 12 days ago with the passing of the Compound Oracle Upgrade proposal 3, contained an error in the code that caused the cETH market to be “temporarily frozen.” The response was fast, as you’ll see in the first two proposals described below.\n\nProposals\n\n“Oracle Update 1” (119). Once the oracle error was discovered, @GFXlabs quickly proposed  reverting the oracle from v3 to v2. This proposal passed and “the cETH v2 markets are fully functional again.” Because of the seven-day governance process, however, there remained a risk that sudden liquidations could occur during this period due to price fluctuations.\n“Unpause Seize 2” (121). Before the above proposal succeeded, a proactive proposal was made by @Arr00 (working with OpenZeppelin) to allow for liquidations to be paused “to ensure that any users or downstream protocols have ample time to rebalance their positions once the price feed for cETH is restored.” As written, the proposal was meant to make sure that Compound’s Pause Guardian (which is designed to pause liquidations if certain conditions are met) can be unpaused: “This ensures that a pause action will only last ~23 hours as opposed to the usual 7-day governance process.” Since the Pause Guardian was not triggered the proposal was canceled.\n“Risk Parameter Updates for 1 Collateral Asset 2” (118). Gauntlet’s risk parameter update, part of their regular recommendations, was canceled pending a fix to the Oracle.\n“cETH Risk Mitigation” (122). Among other things, this proposal by @monet-supply sets a borrow cap and updates the interest-rate model in Compound v2 ahead of the Ethereum merge, which “has the potential to cause disruption to ETH lending markets.” The proposal passed with 100% voting in favor.\n“Risk Parameter Changes for FEI 2” (123). This proposal by Gauntlet pauses FEI borrowing and sets a high reserve factor (99%) as the “first step in winding down the FEI market,” which will be deprecated due to Tribe DAO’s entering a “terminal state.” The proposal passed with 100% voting in favor.\n“Initialize Compound III COMP Distribution 3” (124). Prior to this proposal, COMP distribution — which enables Compound community governance — was provided only for users of Compound v2. This proposal initiates COMP distribution to users of Compound III by re-allocating 14.17% from v2 markets (total distribution has not changed), according to the parameters specified in the proposal. This “empowers users of the new deployment, and encourages the migration of usage from Compound v2.” The proposal passed with 100% voting in favor.\n\n Meanwhile, Gauntlet is seeking community feedback through two Snapshot votes:\n\n\nThe first (which ends on September 9) aims for discussion and consensus on reserve factors in Compound v2.\nThe second (which ends on September 14) seeks consensus on rate-of-change for collateral factors, particularly with respect to “risk-off” liquidations.\n\n\nIn the Forums\n\n\nDelegate platforms: @pennblockchain has updated their thread 2 to voice their opinions on the latest proposals.\n\nRenew Gauntlet’s twelve-month engagement? There’s positive sentiment in response to @pauljlei’s proposal.\n\nMove the governance forum? There’s also positive sentiment 1 around @commonwealth.im’s proposal to move from Discourse to Commonwealth.\n\n\nOn Twitter\n\n\nCompound Grants 2.0: The community-run program is looking for Domain Allocators 1.\n\nGauntlet presents its Compound case-study 3: how it helped generate at least $5.15m in additional interest.\n\n\nIn Discord\n\nThe governance-feed channel tracks proposal votes and stages.\n\n\nDeveloper Community Calls\n\nCalls are held every two weeks. A recording of he last one can be found here 1.\n\n Join the next call on September 21 (4:30PM GMT), in Discord.\nQuick Gov Links: Governance Overview 2 | Governance Docs | Compound on Boardroom\\n\nWeek of September 12, 2022\n\nSummary\nThe Merge was — happily! — successful, with no signs of disturbance to markets on Compound, thanks in large part to careful preparation. In other, big (non governance-related) news, Compound Treasury announced borrowing for institutions.\n\nProposals\nGauntlet <> Compound Renewal (125 2): This proposal from Gauntlet seeks to renew their twelve-month engagement with Compound. Gauntlet has been working with Compound in various ways for three years; their current engagement runs through September 27, 2022. Gauntlet describes their role as follows: “continuous market risk management to maximize capital efficiency while minimizing the risk of insolvency and liquidations to create long-term sustainable growth.” New for this engagement: Coverage of the recently initiated Compound III, and a refund model should Gauntlet’s optimizations incur losses.\n\n“Our ultimate goal is to protect the protocol - we stand behind our work and want the community to have confidence in our recommendations.”\n\n Vote here 2, beginning tomorrow.\nTUSD market update proposal. @Joyce from TrueUSD has submitted for discussion a proposal for adding TUSD as a collateral asset on Compound. TUSD is an ERC-20 that describes itself as “the first independently-verified digital asset redeemable 1-for-1 for US Dollars.”\nResults of community feedback on Gauntlet’s two Snapshot polls: 75% voted in favor of increasing reserve factors 1 by 0.025 for USDT and DAI while nearly 100% voted in favor of reducing each collateral asset’s collateral factor 1 by up to 5%.\n\nIn the Forums\nPost-mortem of the cETH price-feed incident. @cylon from OpenZeppelin (a security advisor for Compound DAO) breaks down what happened and provides suggestions for going forward — which include exploring “how proposal upgrades could be rolled back more quickly for a limited time following an upgrade’s execution without going through 7-day governance.”\nNormalizing ETH market post-merge. @pauljlei from Gauntlet initiates discussion on this topic, since proposal 122 mitigated cETH market risk prior to the merge. @monet-supply suggests shifting to a “jump rate” model.\nThere are some follow-up questions for Questbook’s delegated domain allocation proposal to the Compound Grants program.\n\nOn Twitter\n@getty_hill wonders whether it’s time to push for Compound Autonomous Proposals again, given how much the DeFi space has changed.\n@blockworks_ publishes an op-ed by John Morrow of Gauntlet pointing to their new advisory standard with DAOs (see proposal above): “if you are wrong, you don’t get paid.”\n\nIn Discord\nLeighton asks 2 “when do we cut all COMP V2 Distribution??? V3 COMP Distribution method is so superior.”\nMatto suggests “streamling the [COMP] distribution adjustments dynamically.”\n\nDeveloper Community Calls\n\nCalls are held every two weeks. A recording of he last one can be found here.\n\n Join the next call on September 21 (4:30PM GMT), in Discord.\nQuick Gov Links: Governance Overview | Governance Docs | Compound on Boardroom 1\\n\nWeek of September 19, 2022\n\nSummary\nWelcome to the latest governance recap from Boardroom, covering all of last week’s important governance related content. We hope it proves useful to the community.\n\nProposals\nCompound III Supply Caps (127 2). Noting that WBTC, ETH, and COMP are utilizing most of their supply caps in Compound III — which has been operating for more than a month ”without bugs, losses, or disruptions” — @rleshner writes that “Now is a prudent time to begin carefully raising collateral supply caps, so that the protocol can continue expanding past its trial phase.” The specific increases suggested are outlined in the proposal.\nOpenZeppelin Security Partnership - 2022 Q4 Adjustment (126). Last December OpenZeppelin was selected to provide “continuous audit, security advisory, and auditing services” to Compound. Each quarter they submit a proposal to update their service fee, which this round as in the last is a $1m lump-sum payment in COMP.\nThe Gauntlet <> Compound Renewal (125), summarized in last week’s recap, was passed, and was executed on September 25.\n Vote here!\n\nIn the Forums\nOpenZeppelin security updates for August & September are detailed here by @cylon. It includes a link to their Q4 compensation proposal, which follows the structure of their Q3 proposal.\nUpdate cComp parameters? @monet-supply suggests 1 that “it may be safe to increase borrowing caps to allow for more utilization and greater cCOMP yields.”\nDune dashboard and educational materials for Compound III. @andrewhong5297 proposes a Dune dashboard “to build out more user-level analysis, as well as a writeup and youtube video explaining the technicals of the protocol (making it more accessible for developers and analysts to understand).” Here’s the poll.\nNormalizing ETH market post merge. The discussion continues; Gauntlet “Gauntlet supports >0% borrow rate at 0% utilization.”\nDeprecate FEI market. The first part of this proposal was executed on September 10; @OneTrueKirk wonders if it’s time to submit a proposal to execute the second part, which sets the “FEI reserve factor to 100% (which allows for liquidation of outstanding borrows).”\nTUSD market update proposal. This was noted in last week’s recap — and discussion continues, with an update from a16z, and a presentation during the community call.\n1independence, which aims to “distribute life-changing opportunities to people,” hopes to discuss a partnership with Compound.\nInitialize Compound III (USDC on Polygon PoS). @hamzahkhan proposes initializing Compound III on Polygon, an EVM compatible chain, “with USDC as the borrowable asset…[and seeding] the market with 500k USDC.” While there is support expressed in the comments, a few concerns are noted, and a suggestion made to split the proposal into smaller parts so that governance can address each portion adequately.\nInvestigate market manipulation risk in ZRK and other tokens? Volt Protocol wonders whether we should return to the topic, given the GMX oracle attack.\n\nDeveloper Community Calls\nThe last call took place on September 21, and included discussions led by Gauntlet, OpenZeppelin, and the TUSD team. The recording can be found here.\n Join the next call on October 5 (4:30PM GMT), in Discord.\nQuick Gov Links: Governance Overview | Governance Docs | Compound on Boardroom\\n\nWeek of September 26, 2022\n\nSummary\nQuorum seems to have been harder to reach lately. What are the causes?\n\nProposals\nCompound III Supply Caps (128). This proposal did not reach quorum in its first go-round (127 1); Gauntlet re-published the proposal.\n Vote now, if you haven’t already!\nOpenZeppelin Security Partnership - 2022 Q4 Adjustment (126) passed with just about 100% voting in favor and was executed on September 30.\n\nIn the Forums\nRisk Parameter Updates for the week. Gauntlet recommended no changes to the parameters. See the post for a full presentation and analysis.\nDeprecate FEI market. Is it time for the next stage 1, which would set the reserve factor to 99% or 100%?\nAdd New Zealand dollar stablecoin NZDS to Compound 1?\nAdd market: 1INCH has gotten some support.\n\nOn Twitter\nFollow Compound Governance, a new account that posts all proposals and governance updates.\nGauntlet celebrates the passing of the “Gauntlet <> Compound Renewal” proposal, breaks down the voting, and reminds us of their new “insolvency refund.” “Contributors need to be accountable to the DAOs they serve.”\n\nIn Discord\nReaching quorum has been a challenge lately 3. Should “abstain” votes be counted towards quorum?\nA reminder that your COMP needs to be delegated 1 to an address (your own or a delegate’s) in order to have voting power.\nIs deploying the new implementation of cUSDCv3 “scary 2”?\n\nDeveloper Community Calls\n Join the next call on October 5 (4:30PM GMT), in Discord. (A recording of the last one can be found here.)\nQuick Gov Links: Governance Overview | Governance Docs 1 | Compound Governance on Twitter | Governance Feed in Discord | Compound on Boardroom\\n\nWeek of October 3, 2022\n\nSummary\nSee below for important details about the next iteration of the Compound Grants Program, and read the latest “Compound Digest,” a monthly newsletter 1 “highlighting the best of the Compound ecosystem.”\n\nProposals\nRisk Parameter Changes for FEI: Part 2 (129 1). This proposal pauses cFEI minting and sets the reserve factor to 100%. It follows from a previous proposal, and is discussed fully here.\nCompound III Supply Caps (128). This proposal succeeded with nearly 100% voting “For,” and was executed on October 7.\nProposal To Improve Governance Discussion and voting processes with Commonwealth. This proposal 3 on Snapshot, initiated by Blockchain at Berkeley based on the thread 1 started by @commonwealth.im, received nearly 100% support in favor of migrating from Discourse to Commonwealth.\n\nIn the Forums\nThe next iteration of the Compound Grants Program is still forming, led by @harsha and Questbook. There’s an update identifying domains and domain allocators, as well as one concerning legal compliance.\nUpdate cCOMP Parameters (Borrow Cap and Interest Rate Model). This proposal, put forward by @monet-supply, could move on-chain soon, pending some research by Gauntlet.\nLiquidation event handling in Compound III: The absorb function is missing an event log when a liquidated user is left with a positive account balance. @jared says that adding an event log “will improve the user experience on Etherscan and blockchain explorers, and make analytics easier.”\n\nOn Twitter\nInterested in earning “up to 2 years worth of Compound lending fees, instantly and upfront — all in one ethereum transaction”? asks @ZacharyDash from Flashstake.\n\nIn Discord\nCheck in on the #governance-feed channel to track proposal progress.\n\nDeveloper Community Calls\nCalls are held every two weeks. Here’s a recording of the last one from October 5, which included discussions led by Gauntlet, Open Zeppelin, Questbook, DeFi Saver, and Adrastia.\n Join the next call on October 19 (4:30PM GMT), in Discord.\nQuick Gov Links: Governance Overview | Governance Docs | Compound on Boardroom\\n\nWeek of October 10, 2022\n\nSummary\nWe may have a few proposals forthcoming; see forum discussions noted below. This Wednesday’s community call 1 should also be interesting, with participants from Gauntlet, Open Zeppelin, and Compound Labs, among others. But as @adam reminds us, “this call belongs to the community, and as a community member you are empowered to take this call in any direction you think best benefits the community.”\n\nProposals\nRisk Parameter Changes for FEI: Part 2 (129) succeeded with nearly 100% voting “for,” and was executed on October 16.\n\nIn the Forums\nAdjust stablecoin rate parameters? @monet-supply points out that stablecoin interest rate models haven’t changed much since the introduction of Compound v2, and proposes updating the rate parameters.\nOn-chain credit scores. @ccb from RociFi suggests using their credit score model to increase Compound’s capital efficiency.\nRisk parameter udpates. @pauljlei, on behalf of Gauntlet, recommends “no changes to Compound II’s current parameterization” for the week.\nMarket manipulation risk. After the recent Mango markets manipulation @monet-supply is working on a proposal to address this — and help reduce risk to Compound. @OneTrueKirk has some ideas, and Gauntlet is running analyses.\ncComp paremeters. After further discussion, a proposal may be forthcoming that adjusts the cComp borrow cap only.\nBiLira (TRYB) stablecoin. @BiLira may be submitting a proposal 3 to on-board TRYB, “the only and 1:1 Turkish Lira-backed stablecoin,” on Compound III as a borrowable asset only (i.e. not as a collateral asset).\n\nOn Twitter\nCan credit models work in DeFi 1?\nSenator Hickenlooper writes to Gary Gensler about establishing rules for Crypto.\n\nIn Discord\nInteresting, active discussion about changing the cComp parameters (see above).\nCompound governance is “relatively unique in that proposals are the code changes being voted on.”\n\nDeveloper Community Calls\n\nCalls are held every two weeks. A recording of he last one can be found here.\n\n Join the next call on October 19 (4:30PM GMT), in Discord. See the agenda 1.\nQuick Gov Links: Governance Overview | Governance Docs | Compound on Boardroom\\n\nWeek of October 17, 2022\n\nSummary\nA couple of important proposals were put forward this week, and there’s a discussion happening now about creating another governance structure: Compound Improvement Proposals 1. See the highlighted note below.\n\nProposals\nPause Supply for Compound Assets (131 1). This proposal, presented “out of an abundance of caution,” hopes to mitigate any Mango-Markets style attacks on Compound v2 by pausing the supply for ZRX, BAT, MKR, and YFI assets. The proposal looks set to pass with nearly 100% voting “for.”\nAdjust cCOMP and cUNI Parameters (130). The v2 cComp market has recently seen very high utilization — reaching the COMP borrow cap. Changing the borrow cap from 90,750 to 150,000 COMP and switching the rate model to the one used by cUNI, as this proposal suggests, should help the market attain “a more suitable equilibrium.” The proposal passed with nearly 100% voting “for” and is now queued for execution.\n\nIn the Forums\nCompound Improvement Proposals. @cylon from Open Zeppelin puts forward 1 “a foundational CIP-1 to define the process for CIPs and the role they can play in the Compound ecosystem going forward.”\n Join the community call to discuss this: October 26, 9:30AM PST / 12:30PM EST / 4:30PM GMT\nTUSD doesn’t have a fallback oracle specified 1? Is that ok? asks @devenmatthews.\nAnother patch: @jared describes how another opportunity for improvement to the protocol was discovered while implementing a patch to fix liquidation event handling.\nDelegate platform updates have been posted by FranklinDAO (@pennblockchain).\n\nOn Twitter\nBitcoin and Ether are both worth less than they were ~5 years ago. “Keep thinking about it” says @rleshner.\n\nDeveloper Community Calls\n\nCalls are held every two weeks. Here’s a recording of the last one from October 19.\n\n Join the next call on November 2 (4:30PM GMT), in Discord.\nQuick Gov Links: Governance Overview | Governance Docs | Compound on Boardroom\\n\nWeek of October 24, 2022\n\nSummary\nThere’s a new governance process for Compound being proposed: Compound Improvement Proposals. The inaugural CIP-1 is available for review, and another discussion is scheduled 1 for Wednesday.\n\nProposals\nRisk Parameter Updates 2022-10-26 (133 1). Gauntlet proposes one parameter change for one asset (SUSHI) on V2. See the presentation here.\nLiquidation Event Handling and Collateral Reserves (132 1) passed with nearly 100% voting “for” and was executed on October 31.\nAdjust cCOMP and cUNI Parameters (130) was executed on October 24.\nPause Supply for Compound Assets (131) was executed on October 26.\n\nIn the Forums\nCompound Improvement Proposals. @cylon from OpenZeppelin has made updates and revisions 2 to CIP-1: Check out the doc here.\n Join the community call 1 to discuss this: Wednesday, November 2, 9:30AM PST / 12:30PM EST / 4:30PM GMT\nInitialize Compound III (WETH on Ethereum). “With the Compound III codebase having proven itself thus far,” @jared writes, it’s time to consider 1 launching a second market. Gauntlet is analyzing.\nTUSD’s fallback oracle. @CL_Michael helpfully answers 1 questions about this.\n\nOn Twitter\nAll tweets will be AI generated from now on, unless otherwise stated, says @rleshner.\n\nIn Discord\nThere’s no Snapshot for Compound — yet. If approved, the new CIP process might change that.\n\nDeveloper Community Calls\n\nCalls are held every two weeks, on Wednesdays. A recording of he last one can be found here.\n\n Join the next call on November 2 (4:30PM GMT), in Discord 1.\nQuick Gov Links: Governance Overview | Governance Docs | Compound on Boardroom 1\\n\nWeek of October 31, 2022\n\nSummary\nCIP-1 is following its own proposed process and is now in the fourteen-day “final call” stage before it will go to Snapshot for a vote. Here’s @cylon’s latest update 1, follow the discussion. And check out the latest draft of CIP-1, if you haven’t already.\n\nProposals\nRisk Parameter Updates for One Collateral Asset (133 1) passed with over 90% voting “for” and was executed on November 6.\n\nIn the Forums\nCompound grants. @harsha provides an update on Questbook’s delegated domain allocation: it looks like a proposal is in the works for next week.\nInitialize Compound III (WETH on Ethereum). Discussion continues, specifically about the Chainlink/Coinbase price feed.\nSUSHI risk. The parameter update proposed by Gauntlet (which passed 1) was in part due to observation of specific SUSHI supplier; that supplier has since decreased their borrowing.\n\nOn Twitter\nCompound Thesis featured Gauntlet’s Tarun Chitra 1.\n\nIn Discord\nMore SUSHI. Maybe we should enable it in Compound III 1, with appropriate parameters in place, and wind down SUSHI on v2?\n\nDeveloper Community Calls\n\nCalls are held every two weeks. Here’s a recording 1 of the last one.\n\n Join the next call on November 16  (16:30PM UTC), in Discord.\nQuick Gov Links: Governance Overview | Governance Docs | Compound on Boardroom\\n\nWeek of November 7, 2022\n\nSummary\nA reminder that CIP-1 is likely to go up for voting this week, on Snapshot. But keep your eye on this thread 2 for any updates and adjustments.\nIt’s a good time to be involved with governance in DeFi.\n\nProposals\nNone active at the moment.\n\nIn the Forums\nMarket downturn. Gauntlet’s @pauljlei provides analyses: “no new insolvencies” as of November 10.\nRisk parameter updates. Gauntlet recommends no updates this week, “given current market volatility.”\nSUSHI supplier: @ccb from RociFi presents more analysis of a specific supplier / borrower, assigning a credit score.\n\nOn Twitter\nPost-FTX sentiment check 4, from @rleshner.\n\nIn Discord\nHow long does it take for a proposal to go from “pending” to “active”?\nSubscribe to #governance-feed to be notified when proposals are created.\n\nDeveloper Community Calls.\n Join the next call 1 on November 16 (4:30PM GMT), in Discord.\nQuick Gov Links: Governance Overview | Governance Docs | Compound on Boardroom\\n\nWeek of November 14, 2022\n\nSummary\nCIP-1 is live on Snapshot 3 — check it out if you haven’t already.\n\nProposals\nCIP-1. This inaugural Compound Improvement Proposal “describes standards, processes, and enhancements intended to improve the Compound Protocol.” The new CIP process is designed to be the new default mechanism to “improve Compound.” Among other things, it increases access to Compound governance by removing the requirement of holding 25k COMP to make a proposal: anyone from the community can use it to suggest improvements, develop opportunities to contribute, or request funding.\n Read more and vote here 3.\n\nIn the Forums\nCompound Grants 2.0. Questbook’s proposal 1 will be live on November 28. (Note that several grants 1 have come forward in the forums already.)\nDelegate platform update 1 from @pennblockchain.\nYear-in-review. OpenZeppelin looks back on their security partnership — and then considers what lies ahead.\nHow to encourage wETH supply on Compound III? Gauntlet asks, and presents three options 1.\n\nDeveloper Community Calls\n Join the next call on November 30 (4:30PM GMT), in Discord. Here’s a recording of last week’s call.\nQuick Gov Links: Governance Overview | Governance Docs | Compound on Boardroom\\n\nWeek of November 21, 2022\n\nSummary\nThe initiating Compound Improvement Proposal (CIP-1) passed, with nearly 100% (or 827 wallets) voting in favor. @cylon will be setting up the new CIP repo soon.\n\nProposals\nBorrow Caps for Compound V2 Assets (135). This proposal seeks to adjust borrow caps on 10 Compound V2 assets, including COMP, LINK, UNI, and WBTC. There’s a good discussion with Gauntlet about it here 1.\n\nIn the Forums\nCompound III — WETH on Ethereum. Gauntlet has provided an initial analysis 1.\nCompound grants 2.0. As the program takes steps forward, @VonNeumann2022 asks a few questions — which are answered.\n\nOn Twitter\nThere’s a Compound governance account 1, in case you didn’t know.\n\nIn Discord\nAny interest in more extensive delegate dashboards?\n\nDeveloper Community Calls\n Join the next call on November 30 (4:30PM GMT), in Discord.\nQuick Gov Links: Governance Overview | Governance Docs | Compound on Boardroom\\nHi there, I’m new here…I just have a question to be eligibe for any airdrop and also for vote can I kee my COMP tokens on BSC too or only on ETH? Also need to be staked? Thank you in advance.\\n\nWeek of November 28, 2022\n\nSummary\nCompound Grants 2.0 is officially going forward — which is great news. Otherwise it’s been another week of several discussions and actions related to risk.\n\nProposals\nIncrease WETH Supply Cap in cUSDv3 (137). Because the WETH supply cap has been reached in cUSDCv3, this proposal aimed to double the cap to 150,000. It passed on December 5 with 100% voting “For” and is queued for execution.\nCGP 2.0 by Questbook (136). This proposal — which revives and restructures the Compound Grants program — passed with nearly 100% voting “For” and was executed on December 4.\nBorrow Caps for V2 Assets (135). This proposal passed with nearly 100% voting “For,” and was executed on November 30.\n\nIn the Forums\nProtocol maintenance. @rleshner lists some 3 overlooked items in v2.\nRisk Council. @VonNeumann2022 suggests creating one to more quickly be able to make (limited) risk-parameter changes. Aave is considering something similar.\nRisk parameter updates have been posted by Gauntlet, suggesting the adjustment of one parameter for one asset (COMP) in V3.\nGitHub repository for Compound Improvement Proposals — @cylon has created the initial setup.\nDelegate platform update 1 from FranklinDAO (@pennblockchain).\nCommunity multisig. @arr00 announces 2 that Compound Labs has stepped down from the multsig and will be replaced by Gauntlet.\n\nOn Twitter\nVitalik on governance rights and token value.\nHappy birthday beacon chain!\n\nIn Discord\nWhat makes a CIP? There’s some discussion here 2.\n\nDeveloper Community Calls\nA recording of last week’s call can be found here.\n Join the next call on December 14 (16:30PM UTC), in Discord.\nQuick Gov Links: Governance Overview | Governance Docs | Compound on Boardroom\\n\nWeek of December 5, 2022\n\nSummary\nThere are two proposals up for voting, and a few brewing in the forums. Also, @jayson introduced 1 Compound Extensions this week — a new feature for “optional add-ons, built by community developers, that enhance the Compound experience.” And the latest Compound Digest 2 is out.\nThere’s a Compound Developer Community Call coming up on Wednesday — see details below.\n\nProposals\nOpenZeppelin Security Partnership - 2023 Q1 Compensation (139 1). OpenZeppelin proposes that their security partnership with Compound be renewed on the same terms, but on a quarterly basis.\nReserve Maintenance (138 1). This proposal comprises “a series of minor maintenance actions from the list of Protocol Maintenance Tasks, to withdraw the reserves of deprecated markets and residual cToken balances to the governance-controlled Timelock, and migrate deprecated assets (SAI and REP).”\n\nIn the Forums\nCompound V2’s oracle. @CL_Michael from Chainlink proposes to upgrade the oracle to UAV3, with one main change from the last proposal.\nCompound V3 risk dashboard. @pauljlei announces 1 Gauntlet’s new dashboard 1, which will “help the community understand our methodology and recommendations…. [and] communicate how these recommendations mitigate risk and increase capital efficiency for the protocol.”\nIncrease UNI supply cap. Gauntlet recommends increasing UNI’s supply cap in the USDC comet market to 2.3m, to support “organic growth” without creating “outsized market risk.”\nV2 risk parameter updates. Gauntlet advises no changes this week.\n\nOn Twitter\nA thread on Compound Extensions 1.\nUnderstanding lending protocol market risk, a thread by Gauntlet hitting the points in their Medium post.\n\nDeveloper Community Calls\n Join the next call on December 14 (17:30PM UTC), in Discord.\nQuick Gov Links: Governance Overview 1 | Governance Docs | Compound on Boardroom\\n\nWeek of December 12, 2022\n\nSummary\nThree proposals have passed as we head into the holiday season. It’s been a time of very positive developments for Compound.\nThe next Community Call (see below) won’t be until January 11, 2023.\n\nProposals\nRisk Parameter Updates for USDC Comet Market (140). This proposal to increase the supply cap for UNI on the USDC comet market (from 1.25m to 2.3m) and to decrease the COMP liquidation factor on the same market (from 93% to 88%) passed with nearly 100% voting “For” and was executed on December 19.\nOpenZeppelin Security Partnership - 2023 Q1 Compensation (139). This proposal passed with 100% voting “For” and was executed on December 17.\nReserve Maintenance (138). This proposal passed with nearly 100% voting “For” and was executed on December 16.\n\nIn the Forums\nCompound DSR proposal. @MakerGrowth informs us that Maker 1 has recently reactivated the DAI Savings Rate (1% variable yield) and proposes “the reactivation of the DSR in Compound so users can have direct access to this low risk base DeFi yield.”\nLINK borrow cap. @pauljlei alerts us that LINK is reaching its borrow cap on Compound v2. Gauntlet will soon follow up with recommendations.\nCompound extensions. Discussion continues 2 about this exciting development.\n\nOn Twitter\nCompound audit highlights from OpenZeppelin 1.\n\nIn Discord\nCompound Grants v2 planning to go live in mid January 2023, according to Questbook 1.\n\nDeveloper Community Calls\n The next call will be on January 11, 2023 (4:30PM GMT), in Discord 2. A recording of last week’s call can be found here 1.\nQuick Gov Links: Governance Overview 2 | Governance Docs 1 | Compound on Boardroom"
  },
  {
    "number_of_comments": 23,
    "postid": "6e91e51d-5101-427b-901f-3db27263d112",
    "posturl": "https://www.comp.xyz/t/trueusd-listing-proposal/219",
    "combinedcontent": "Hello - I’m Rafael, CEO of TrustToken, and our team would like to propose listing TrueUSD on Compound.\nWe believe TUSD would be a valuable addition to Compound as it has reliable live price feeds, strong community interest, and sufficient market liquidity.\nDown the road, other TrueCurrencies, such as TCAD, TAUD, TGBP, and THKD, could also add value to the Compound platform by allowing users to on-ramp via their native currencies or speculate on Forex with leverage.\n917×194 33.6 KB\nWhy TrueCurrencies?\nPopularity: TUSD has been listed on AAVE since 2019, currently ranks 3rd in market size among stablecoins on AAVE, and makes up 13.6% of AAVE’s total market size.\nLargest AAVE markets at time of writing, sorted by Total Borrowed:\n1600×1042 91.7 KB\nIf TrueUSD performed similarly on Compound, we expect that it would become Compound’s third-largest market in terms of total borrow (after DAI and USDC).\nTransparency: TUSD is the only stablecoin with live attestations 6 to corroborate TUSD out in the market with dollars held in escrow. These attestations are facilitated by top accounting firm Armanino, LLP.\nReliability: Based on DeFi Score 9, TUSD on AAVE has the highest trust rating of any fiat-backed stablecoin currently listed on the platform.\nLiquidity: TUSD is at over $280M in market cap at the time of writing, ranking as the 4th largest stablecoin after USDT, USDC, and DAI. It enjoys deep liquidity on both centralized and decentralized exchanges.\nDeFi Adoption: The MakerDAO community accepted our TUSD collateral application 7 and is in the process of green-lighting TAUD, TGBP, and TCAD.\nStatus:\n\nTUSD Proposal Poll: (99.99% voted “Yes”)\n\nTGBP Greenlight Poll: (100% voted “Yes”)\n\nTAUD Greenlight Poll: (100% voted “Yes”)\n\nTCAD Greenlight Poll: (100% voted “Yes”)\n\n\nAccess: TUSD is widely available through both 1st party and 3rd party fiat-to-crypto on-ramps, creating a new entry point for both individual and institutional Compound users. You can mint and redeem it right here 6.\nDifferentiators\nTrueCurrency holders can rest assured that their tokens are actually redeemable 1:1 for Fiat by checking our real-time attestation of reserves 8.\nWe are working to bring this data on-chain in the coming months through partnerships with Oracle providers.\nBecause TAUD, TCAD, TGBP, and THKD are not pegged to the US dollar, Compound users in those respective jurisdictions would be able to participate in DeFi without taking on Forex risk by leveraging TrueCurrencies and borrowing from Compound against their native fiat currency.\nLegal Info\nTrueCoin, LLC, manages the smart contracts of TrueCurrencies. It is a wholly-owned subsidiary of TrustLabs, Inc, which is a Delaware, USA registered company.\nIndependent, third-party banks and trust companies in the United States and Hong Kong manage the fiat collateral backing TUSD in escrow accounts held for the benefit of all TUSD holders.\nTrueCurrencies are fiat-backed stablecoins that are fully-collateralized (1:1 backed) with fiat held by banks and trust companies.\nWeaknesses\nTAUD, TCAD, TGBP, and THKD have lower liquidity when compared to TUSD. However, for each of our stablecoins the Compound community is open to onboarding, TrustToken can commit to providing $1mm of liquidity at the time of listing to seed the market.\nFeedback\nWe think there is a strong partnership here and would love to work with the Compound community to address any questions or concerns.\\nWell let’s get more reporters in the open oracle that include tusd. From my views, adding more assets is not the priority. Are you in some way affiliated with forex? You mention it a lot.\\nHey @TennisBowling - in addition to being the creators of TrueUSD, we’ve also created TrueAUD, TrueCAD, TrueGBP, and TrueHKD. Each of these is a stablecoin that’s backed by the corresponding fiat currency 1-for-1.\nWe think that, at some point, listing these currencies in addition to TrueUSD could add novel use-cases for Compound.\\nCompound community members- would love to hear if you’d support this listing, and if not, what your concerns would be.\\nThanks for proposing this to the community @rafael.cosman. The community has not yet added any tokens since it has been given control through governance. I believe the community will be adding a token in the coming month and bringing up TUSD after it has been done will likely allow for more traction.\nAs for TUSD in specific, I don’t really see how it adds much utility supporting it. From short research, it seems like the the far majority of TUSD usage is due to the yCurve pool. Total Uniswap liquidity is at $30k. The Compound community is very security oriented and I don’t think that the benefit added from supporting TUSD now counteracts the downside of adding another point of centrality to Compound.\\nHey @arr00, we do believe TrueUSD would have strong demand on Compound and would be one of the most popular assets. As I mentioned earlier, TrueUSD makes up a significant amount of AAVE’s total market size (increased since my original post from 13.6% to now being 14.3%) and is the third most borrowed asset on the platform.\nWe think similarly that if TrueUSD were listed on Compound it would be one of the most demanded assets, likely ranking third (after DAI and USDC) in terms of total borrow.\nThe other TrueCurrencies (TAUD, TGBP, TCAD, and THKD) are smaller products than TrueUSD, and would likely have less total demand, but would allow Compound users to directly on-ramp via many different fiat currencies which is not possible today.\nOn centralization- unlike most other stablecoins, the TrueUSD smart contract is owned by a special security management smart contract which requires additional signatures for larger mints. This protects token holders and DeFi protocols such as Compound. TrueUSD is also the only stablecoins to have real-time audits 1, adding a further layer of protection for DeFi protocols such as Compound. Finally, we’re now working on getting the balances backing TrueUSD reported on-chain by oracles, and we’ll have more updates on this project soon.\nUniswap liquidity- we know it’s low and it’s something we’re working on, should have some updates on this soon.\\n@rafael.cosman to @arr00’s point above, he was pointing out that the majority of TUSD comes from the Curve “yCurve pool”, which currently has 268M of TUSD (supplied to Aave); and some basic block exploration shows that the majority of TUSD borrowed is for the same yCurve pool, which has been yielding 100%+.\n\nMost of the pitch above is talking about other platforms – what are the benefits and risks to COMP holders of adding it to the platform? Do you have data on usage overlap with the existing users? Would it add anything new?\\n@rleshner- the benefit to $COMP holders is diversification beyond the 2 assets that drive the overwhelming majority of Compound usage today, as well as significant increased usage of the platform, something that, right now, our stablecoins are the assets most likely to provide.\nCompound is one of the DeFi projects that I personally am most excited about. It’s got great technology, great design, and a widely respected token model. I think it’s going to be successful long-term, but to get there, it’s going to need to have more than 2 assets with significant usage. Let’s look at the numbers. Compound today lists 9 assets:\nimage1008×74 7.5 KB\nimage1006×74 6.35 KB\nimage1006×74 8.56 KB\nimage1004×74 8.1 KB\nimage1008×73 7.28 KB\nThese are 5 of Compound’s 9 assets, and their total borrow, combined, is only 17.4mm. This constitutes about 1.8% of Compound’s total borrow. Their supply interest rates range from 0.00% to a low 1.11%.\nimage1004×71 6.48 KB\nimage1006×74 6.79 KB\nThe next two assets have a little more usage, together totaling 48.6mm in total borrow. This is about 5% of Compound’s total borrow.\nimage1002×76 8.56 KB\nimage1006×76 7.77 KB\nThese two assets (both stablecoins) make up 93% of Compound’s total borrow. It’s a significant risk to the platform to have only two assets making up such a large percentage of total borrow.\nBecause 7/9 assets on Compound, a platform for lending and borrowing crypto, have limited demand today for lending or borrowing, just two assets make up 93% of Compound’s total borrow (as well as 69% of its total supply).\nJust to be clear, that means that all 7 other assets listed on Compound, all combined, make up <7% of the total borrow.\n\nGiven that both of the highly-demanded assets are stablecoins, the hypothesis I’d propose is that, by\nfar, what people are interested in lending and borrowing, today, are stablecoins.\nCould this just be an anomaly because users are borrowing and lending Dai to farm $COMP? Unlikely, given that AAVE also has 92% of its total borrow from stablecoins 1, even though it lists different products.\nBut unlike Compound, their usage is more diversified: their top two assets only constitute 58.6% of their total borrow, and we’re working with them to list additional non-USD stablecoins that can diversify their platform further.\n\nLooking at the usage data from both lending platforms suggests that while not all stablecoins make big markets, all big markets, today, are stablecoins.\nHow this collaboration can benefit $COMP holders\nWe’re an issuer that’s bringing to the table 5 highly trustworthy stablecoins, both USD and non-USD. If the Compound community is interested in diversifying and creating new markets that get significant usage, we think these products are the best bet. There isn’t any other partner where you can get this potential for growth and diversification, while only taking on the risk of a single issuer.\nRight now, Compound’s listings don’t correspond to what people (and smart contracts) actually want to lend and borrow: stablecoins. For example, while Augur, WBTC, and BAT are excellent projects, it’s no surprise that they receive tepid usage on Compound given how much usage they get elsewhere. As a comparable, these three products are all also listed on AAVE and, all combined, make up <1% of AAVE’s total borrow. Compound’s situation could be radically different but that’s not what the data suggests.\nWhen it comes to stablecoins, TrueCurrencies meet or exceed the level of trust of existing stablecoins on the platform.\n\nBased on DeFi Score 2, TUSD on AAVE has the highest trust rating of any fiat-backed stablecoin currently listed on the platform, higher than DAI, USDC, and USDT, all of which are already listed on Compound. Why?\nTrueCurrencies are the only stablecoins with 24/7 live audits 1 to prove each TrueCurrency is always fully backed. These audits are facilitated by top accounting firm Armanino, LLP.\nWe’re now working with oracles to become the only stablecoins to live report collateralization on-chain.\nUnlike some other stablecoins, purchasing and redeeming TrueCurrencies is easy. Just go here 3.\n\nWe think this is a natural partnership and are very ready to invest in making it successful for both projects. Thank you for engaging with us, we’re looking forward to hearing your thoughts on this.\\n@rleshner- to directly answer your questions:\n\n\n\n rleshner:\n\nthe majority of TUSD comes from the Curve “yCurve pool”, which currently has 268M of TUSD (supplied to Aave); and some basic block exploration shows that the majority of TUSD borrowed is for the same yCurve pool\n\n\nMy understanding is that the yCurve pool puts TUSD into yEarn 1 which uses a variety of strategies to pursue yield. Right now it primarily puts TUSD into AAVE but it could put a significant amount of TUSD into Compound were that possible. We see smart contracts such as yEarn as fully legitimate users of platforms such as Compound- they are ultimately just another gateway by which end-users access Compound.\n\n\n\n rleshner:\n\nDo you have data on usage overlap with the existing users?\n\n\nThis is difficult to acquire given that most existing compound users are anonymous Ethereum addresses, and listing TrueCurrencies would bring many new users to the platform. I’ve provided above what I think is the strongest evidence that TrueCurrencies would be demanded by Compound users, which is a pattern the data is clearly showing: stablecoins = demanded for lending and borrowing. Everything else = basically not.\\nTUSD would have better yield on Compound than on Aave, so all the funds from the yearn pool will likely come to Compound to farm COMP. This will increase TVL. Risk isn’t impacted much because most of the TUSD will be borrowed against TUSD. Same as with DAI now.\nThe value to Compound users is increased liquidity of TUSD, if that’s what you’d like to borrow (Personally I trust it a lot more than USDT).\nThe value to COMP holders is higher TVL looks good and might pump the price as well as more funds in reserves that may be distributed in a distant future to holders of COMP.\\nWhy would TUSD have better yields on Compound than on Aave?\\nListing TUSD on Compound seems like a great way to diversify the collateral types and increase TVL through borrowing. Looking at the diversity in collateral posted by @rafael.cosman I think it makes sense to have an alternative to USDT and USDC. TUSD has real-time attestation of escrowed funds here: https://real-time-attest.trustexplorer.io/token/TrueUSD 1\nAs for Liquidity, TUSD already has a huge amount of liquidity on CEXs (usually ~100M volume per day) and through Curve or 1Inch exchange also has a good amount of DEX liquidity. I think seeing a bump in liquidity on Uniswap would help justify listing and support decentralized use of the asset.\\n\n\n\n compoundbart:\n\nThe value to Compound users is increased liquidity of TUSD, if that’s what you’d like to borrow (Personally I trust it a lot more than USDT).\nThe value to COMP holders is higher TVL looks good and might pump the price as well as more funds in reserves that may be distributed in a distant future to holders of COMP.\n\n\nAgreed. Based on the metrics above, listing another stablecoin may increase TVL by something like 10-25% while listing another volatile crypto might increase TVL by only 0.1-5%.\\nSecurity update:\n\n  \n      twitter.com\n  \n  \n    \n\nRafael Cosman (RafaelCosman) 1\n\n \uD83D\uDD12 For added security we’ve moved all 5 #TrueCurrency smart contracts to be owned by the 2/3 Gnosis Safe at https://t.co/Xaid46hE8o.\n\nThis adds additional security for users- @gnosisSafe is the most trusted multi-sig because it’s time-tested, audited, and formally verified.\n\n\n  9:05 AM - 4 Sep 2020\n    \n      \n        \n       1\n    \n\n\n  \n  \n    \n    \n  \n  \n\n\nAlso, to make it easy for busy folks, here’s a bullet-point summary of the key reasons we think listing $TUSD adds value for $COMP holders:\n\n\nListing TUSD on Compound will benefit $COMP holders through increased TVL and usage\n\n\nUsage data suggests that on Compound, stablecoins are over 2x more popular than all other coins combined. The importance of this result is difficult to overstate.\n\n\nAs of Aug 26th, 93% of Compound’s total borrow (as well as 69% of its total supply) came from 2 assets, both stablecoins\n\n\nThis means the remaining 7 assets, all together, comprised 7% of total borrow\n\n\nListing TUSD will help diversify Compound, whose goal is to be a interest rate protocol for crypto, not for just 2 assets.\n\n\nThe overwhelming demand for stablecoins is not limited to compound-  measured at the same time, AAVE also had 92% of its total borrow from stablecoins\n\n\nTUSD is one of the largest stablecoins globally, after the 3 stablecoins already listed on Compound\n\n\nWhile USDC has trusted brand-names like Coinbase behind it, when you look at what fiat-backed stablecoins are actually doing to be transparent and build trust, TUSD comes out as #1 globally.\n\n\nFor example, TUSD is the only USD-backed stablecoin to ever have 24/7 real-time 3rd-party audits 1 and has the highest DeFi Score of any stablecoin listed on AAVE, higher than USDC, DAI, and USDT, all of which are already listed on Compound.\n\n\\nWith another point of centralization, maybe we could have it with the same interest rate model and a 0% collateral factor.\\n@TennisBowling if the Compound community feels that’s necessary, we understand. But given the ways in which TUSD matches or leads other stablecoins in terms of trust (such as the items mentioned above) we’d recommend both an interest rate model and collateral factor matching USDC.\nimage2074×1474 198 KB\\n\n\n\n arr00:\n\nTotal Uniswap liquidity is at $30k\n\n\nHey compound Community, we’re planning on submitting an autonomous proposal for listing TUSD soon.\nWe know Uniswap liquidity is one of the factors the community is considering when considering listing TrueCurrencies, so we want to share some progress that we’ve made there:\n\n  \n      twitter.com\n  \n  \n    \n\nRafael Cosman (RafaelCosman) 1\n\n Based on feedback from the @MakerDAO, @compoundfinance, and @AaveAave communities, we’ve been working to improve DEX liquidity for TrueCurrencies \uD83D\uDCC8\n\n\n  5:06 PM - 15 Sep 2020\n    \n      \n        \n       1\n    \n\n\n  \n  \n    \n    \n  \n  \n\n\n\n  \n      twitter.com\n  \n  \n    \n\nRafael Cosman (RafaelCosman) 2\n\n As of 4PM PT today, EACH of the following @UniswapProtocol markets has over $400k liquidity:\n\nTGBP<>TUSD: https://t.co/svt7BTn7cY\n\nTCAD<>TUSD: https://t.co/FqtVkC527h\n\nTAUD<>TUSD: https://t.co/e8Um90B9zZ\n\nWelcome to #cryptoforex. Happy trading \uD83D\uDE00\n\n\n  5:06 PM - 15 Sep 2020\n    \n      \n        \n       1\n    \n\n\n  \n  \n    \n    \n  \n  \n\n\nThis brings total TUSD liquidity on Uniswap up to $920k. More to come soon. Let us know which markets are most important for Compound when considering liquidations, and we can increase those.\\n\n\n\n rleshner:\n\nwhat are the benefits and risks to COMP holders of adding it to the platform? Do you have data on usage overlap with the existing users? Would it add anything new?\n\n\n@rleshner - the MakerDao community recently determined that TUSD does add value to their platform. Support was added ~6 days ago with a 50mm DAI debt ceiling and we just hit that ceiling, making TUSD the 4th most demanded collateral type on the platform. It’s difficult to know how much larger the real demand is until the debt ceiling is raised.\nGiven TUSD’s strong usage on Maker and AAVE, why do you think it wouldn’t see similar high demand on Compound? If you really think it wouldn’t, happy to place a 100 $COMP bet on the subject \\nPersonally, I think compound should support more stablecoins.\n2021 would be a great year for stablecoins as well, why not  just get more opportunities?\\nAgreed @DeFiNavy!\nUpdate for the thread: we just submitted an autonomous proposal to list $TUSD 42, would appreciate your support!\nPlease let us know if you have any questions or concerns about the proposal. @arr00 @rleshner thank you for your feedback and help with this.\\nhow do you actually trade trueusd.  coinbase wallet doesn’t support converting to/from trueusd.  So what’s the point as after i take out a loan in trueusd, i cant convert it to any other coin i need…i tried uniswap too it says the token is inactive?\\nTry 1inch: 1inch - DeFi / DEX aggregator on Ethereum & Binance Smart Chain 3\\nhi does anyone know why the trueusd compound distribution is 0?  trueusd does not get compounds from supplying/borrowing? thanks\\nThis wasn’t included in the initial governance proposal; the community can adjust the COMP Distribution and Collateral Factor for any market after it has launched."
  },
  {
    "number_of_comments": 17,
    "postid": "f89b1fc8-fa14-4d8f-a3eb-60ce89a84cee",
    "posturl": "https://www.comp.xyz/t/aera-pilot-for-compound-v2-reserves/4655",
    "combinedcontent": "This proposal is a follow up to the Temperature Check post 20 that was posted a few months ago about piloting Aera 8, an autonomous treasury management protocol.\nOverview\nAera is a solution for optimizing DAO funds autonomously and on-chain. For most DAOs, treasury funds (e.g. reserves, treasuries, safety modules, backstops) are not actively managed or adjusted based on market conditions. For DAOs, this can lead to an inability to maintain runway, cover liabilities, and benefit from growth in the market. Traditional institutions can allocate funds to more nimble managers who make day-to-day decisions, but DAOs face numerous challenges with this model including governance and creating strong incentive alignment with external managers.\nAera provides DAOs with a one-stop, non-custodial solution for managing treasury funds efficiently and transparently. The Aera protocol consists of vaults, which are constructed on a per-protocol basis and can hold a combination of stablecoins, native tokens, and other cryptocurrencies. The portfolio strategy of the vault is determined by each DAO and is highly customizable ranging from simply keeping fund proportions in line with liabilities, to complex strategies using Liquidity positions to provide POL. Vaults are automatically rebalanced by Vault Guardians who compete on-chain to propose the best combination of assets in the portfolio. This ensures that the vault strategy is met across a wide range of market scenarios and time horizons.\nInitial Pilot\nWe have chosen to bring Aera to market slowly through rigorous research and testing both internally and through small pilots. Aera has been audited by Spearbit and tested internally for 9 months. We launched out first pilot with Moonwell 3 in May. We built an Aera vault comprised of ETH and USDC ($250k total allocation, initially all in USDC) that allows the DAO to target a specific portfolio volatility level (15%) as a method of managing overall portfolio risk. This Aera vault is rebalanced on a daily basis to target the specified volatility level - you can view the live dashboard that tracks vault performance and transaction here 11 along with the updates we shared with the community to date here 1. As of today, the total value in the vault is $239k (down from $251k last month due to recent ETH volatility) with a distribution of 65% USDC, 35% ETH and Moonwell is planning to increase their allocation for the next phase of their pilot.\nMotivation\nThe Compound v2 market on Ethereum has accumulated over $45m of reserves, which are now primarily in the form of USD stablecoins ($22.2m DAI, $14.1m USDC, $3.7m USDT), WBTC ($3.1m), ETH ($1.3m), and BAT ($597k), 0x ($305k). These reserves are static and do not help the DAO future proof its reserve needs particularly because there is higher than necessary exposure to long tail assets such as BAT, and 0x. With the launch of Aera, there is now a purpose-built solution that simplifies and automates much of this optimization.\nA few key benefits for Compound:\n\nAera helps to minimize bureaucracy. The DAO doesn’t need to plan strategies, but rather just pick assets.\nThe Aera protocol continuously rebalances Compound’s fund portfolio based on market conditions.\nAera is non-custodial - the DAO is in control of it’s assets.\n\nImplementation\nWe will start with a small test pilot for Compound in late Q3 or early Q4 to build trust in the Aera protocol. Throughout the 3-month pilot period, we will share performance updates for these funds. Assuming the pilot goes well, the community can vote to increase the allocation size once the pilot concludes.\nPilot steps:\n\nCompound governance deposits $500k of BAT or 0x from reserves into an Aera Vault on mainnet. A governance proposal removes reserves from the protocol and transfers them to an Aera vault on mainnet.\nThe vault would then swap out of these assets into wstETH and USDC using an execution strategy that minimizes slippage and utilizes the best onchain liquidity source.\nGuardian continuously rebalances the vault based on market conditions and the portfolio strategy. For the pilot, the portfolio strategy will allow the DAO to target a specific portfolio volatility level (15%) as a method of managing overall portfolio risk to capture gains during periods of market strength and protect losses during downturns.\nCompound can view vault performance at any time through the public dashboard and have access to funds due to the non-custodial design of Aera.\n\nFAQs\n\nWhat does it mean to target a portfolio volatility level of 15%?\n\nVolatility is a measure of risk and gives a sense of the scale of day-to-day fluctuations. A volatility of 15% means there is a good chance (but not a guarantee) that the portfolio will move somewhere between 15% up or 15% down from the starting balance. For the pilot, the vault will be a two-asset vault consisting of wstETH and USDC. Over the past year, a strategy targeting a portfolio volatility of 15% would have resulted in wstETH allocations between 15% and 35% (with the remaining in USDC).\n\n\nWhat does this cost?\n\nFree during pilot phase (no fees paid to Vault Guardians). Cost structure will be considered in late 2024 during the scaled rollout phase.\n\n\nAre there any execution costs incurred by rebalancing?\n\nYes, the vault will incur execution costs as part of ongoing rebalancing activities. These execution costs will be minimized as much as possible by using the best on chain liquidity sources.\n\n\nWhat is a Guardian?\n\nGuardians are experienced risk analysts and can be institutions or individuals. Gauntlet will serve as the initial Guardian with all fees set to zero. There will not be other Guardians participating for the pilot. During the scaled rollout phase in 2024, a new version will launch with the ability to assign new Guardians and enable vault fees to promote Guardian specialization and competition.\n\n\nAre there docs and an app?\n\nYes, see docs.aera.finance 5\n\n\n\nNext Steps\nWould be great to hear the community’s feedback and happy to answer any questions - please comment below. If the community is ready to move forward, we will follow up with an onchain vote.\\nHello,\nI think this rebalancing technology is neat.\nWould you mind explaining what Compound gains from principal risk?\n\nto target a specific portfolio volatility level (15%) as a method of managing overall portfolio risk to capture gains during periods of market strength and protect losses during downturns.\n\nThe vault has incurred -4.8% principal loss since May as it continues purchasing the down asset. It’s a bear market, so this doesn’t come as a surprise. Potentially, the vault could be profitable next bull. In the same timeframe, GMX produced double-digital APR 2 for LPs on Arbitrum (representing $24.6m in gains 2).\n\nThe total value in the vault is $239k (down from $251k last month due to recent ETH volatility)\n\nIs Aera interested in developing yield-bearing vaults which could help Compound capture compound yield while not incurring principal risk (i.e. impermanent loss)? The goal being to preserve principal while providing a sustainable vehicle for growth. Simple exposure may not meet the mark in this case.\nPleased to hear any others thoughts.\nSincerely,\nCameron\\nThanks for the questions Cameron!\n\n\n\n cmrn:\n\nWould you mind explaining what Compound gains from principal risk?\n\n\nIn this pilot, we are proposing that Compound transfer excess ZRX and/or BAT. These tokens are already exposed to broader market movements and potential principal loss (in USD terms). Our plan is to swap out of these assets into wstETH and USDC where the ETH exposure provides the upside opportunity in a bull market. The vault will then be continuously rebalanced between wstETH and USDC depending on market conditions.\nAdditionally, just wanted to clarify that the vault is not exposed to impermanent loss - trades are only subject to execution costs from swapping between assets (referred to as “rebalancing costs”), which will be minimized by using an execution strategy that minimizes slippage and utilizes the best onchain liquidity source.\n\n\n\n cmrn:\n\nGMX produced double-digital APR  for LPs on Arbitrum (representing $24.6m in gains ).\n\n\nIf we understand the GMX corollary you are drawing correctly - providing liquidity on GMX is in theory possible; however, there are still risks 1 involved, and principal is not guaranteed. The APR that LPs are getting in GMX (which I believe you are referring to GLP holders) is from fee revenue from GMX traders. Our understanding is that GLP holders take the other side of the trade and get paid fees to compensate them for the risk they are taking. Slightly more technically, Aera is a non-custodial solution that ensures the vault owner is the DAO’s governance contract, which currently implies that we need to manage the funds on the same chain; Compound Governance is on ETH mainnet and GMX is on Arbitrum.\n\n\n\n cmrn:\n\nIs Aera interested in developing yield-bearing vaults which could help Compound capture compound yield while not incurring principal risk (i.e. impermanent loss)?\n\n\nYes - we recently added support for staking yields on ETH (via wstETH) and for generating yield in ERC4626-compatible pools. This is a bit trickier for Compound itself since reserves have historically been earmarked to pay contributors, and more importantly, to cover insolvencies. Putting reserves back into Compound would be putting the same funds that are used to cover insolvency in the same pool as other funds that are in theory at risk of insolvency and liquidation. This is why we didn’t initially propose this in the proposal, but if there is interest in this we are open to hearing from the community.\\n\n\n\n dtalwar:\n\njust wanted to clarify that the vault is not exposed to impermanent loss - trades are only subject to execution costs from swapping between assets (referred to as “rebalancing costs”),\n\n\nThanks for the swift response. In this case, i’m referring to IL as the balance of initially supplied assets (i.e. started around 177k USDC/40.1 WETH, today 155k USDC/51.6 WETH). If the vault mandate was adjusted to reverse course at some point in the future, that was how I’m thinking IL is erased.\nGreat to hear your yield-bearing vaults are coming along.\n\n\n\n dtalwar:\n\nOur plan is to swap out of these assets into wstETH and USDC…\n\n\nThis conversion would be a good move for the treasury.\nI’ve checked out your contracts on Etherscan. Very clean. What is Aera’s Github?\nERC-4626 vaults are vulnerable to inflation attacks. Are yours defended via virtual offset 2?\n\n\n\n dtalwar:\n\nproviding liquidity on GMX is in theory possible; however, there are still risks involved\n\n\nI understand 2/3 risks mentioned in this link are also exposed in Aera vaults (i.e. smart contract risk & token risk). Counterparty risk being the remaining, it may make sense in terms of risk vs reward by reviewing performance history of accepting such. In my opinion, there’s no shortcut to ‘real yield’.\nI like your idea of the governance contract being the vault owner. Makes sense.\nSincerely,\nCameron\\n\n\n\n cmrn:\n\nWhat is Aera’s Github?\n\n\n\n  \n      \n\n      GitHub\n  \n\n  \n    \n\nGitHub - aera-finance/aera-contracts-public\n\n  Contribute to aera-finance/aera-contracts-public development by creating an account on GitHub.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\n\n\n cmrn:\n\nERC-4626 vaults are vulnerable to inflation attacks. Are yours defended via virtual offset ?\n\n\nAera uses a 4626 wrapper that Aave has implemented for their vaults, audit reports have no mention of inflation bugs or outstanding issues: https://github.com/aave/Aave-Vault/tree/main/audits 2\\nAs I noted in my response to the Temperature Check post, Aera is an innovative solution for treasury risk management, and is worth piloting, initially on a limited basis.\nHowever, my general concern remains that most trading strategies (that require trading often) don’t work as expected during high-volatility regimes due to price slippage, lack of liquidity etc. Perhaps, strong metrics collected during the pilot can help address some of these concerns. However, a 3-month pilot is too short a window, especially given that the crypto markets are rather dull presently. I would like to see the pilot(s) or limited launches demonstrate resilience through one or more high-volatility regimes.\nSecondly, more pertinent to this post, I believe that using volatility as a target for rebalancing may not be an optimal strategy for preserving portfolio value for the following reason. Assets usually trade in a very narrow range (resulting in a low volatility) before exploding in either direction. This consolidating behavior is usually more pronounced preceding crashes. It may present a false sense of “calm” that the overall portfolio volatility is below the target. This situation actually calls for reducing the risk asset weight even further and bringing down the overall portfolio volatility. Increasing the risk asset weight (or acting slowly by adjusting the weights only gradually) to achieve a preset volatility target can be precisely the wrong thing to do. Further, during a price whipsaw, after volatility has already spiked, reducing the risk asset weight to reach down to the volatility target can make things worse. It’s similar to buying high and selling low.\nWe can see this general dynamic play out in the metrics provided for the Moonwell’s vault 4.\nThere was a whipsaw in ETH prices during the two week period shown in the table below. At the end of the period, ETH closed higher than at the beginning. However, during the same period, the vault has lost value.\n\n\n\n\nDate\nETH Closing Price ($)\nMoonwell Vault value ($)\n\n\n\n\n06/07/23\n1841\n252.3 K\n\n\n06/14/23\n1637\n246 K\n\n\n06/21/23\n1879\n248.6 K\n\n\n\nFor the above reason, I would suggest considering a different target (or object function), and not one based on historical volatility, which may convey a false sense of calm.\nAlternatively, Guardians can be given the leeway to use the volatility target in a range, so that they can use a lower threshold for rebalancing when they sense that the market is indicating a “calm before the storm”. A variation of the above is to use the community-set target as the higher threshold.\\nThanks for the thoughtful comments as always @RogerS.\n\n\n\n RogerS:\n\nmost trading strategies (that require trading often) don’t work as expected during high-volatility regimes due to price slippage, lack of liquidity etc\n\n\nWe constrain our trades to be less than 1% slippage. For this pilot, lack of liquidity is not a concern because once we have shifted the allocation into majors (USDC and wstETH) as the liquidity for both these assets is very strong. For the execution out of ZRX and BAT, we will minimize slippage and slowly execute out of those assets.\n\n\n\n RogerS:\n\nI believe that using volatility as a target for rebalancing may not be an optimal strategy for preserving portfolio value\n\n\nWe use exponential moving averages to smooth the volatility e.g. - if volatility was at 10% and then spiked to 50% and then quickly dropped back to 10%, we would only rebalance as if it went from 10% to 15% (these are not exact numbers, just illustrative).\\n\n\n\n dtalwar:\n\nWe use exponential moving averages to smooth the volatility e.g. - if volatility was at 10% and then spiked to 50% and then quickly dropped back to 10%, we would only rebalance as if it went from 10% to 15% (these are not exact numbers, just illustrative).\n\n\nThat only reinforces the concern that the design proposed won’t be responsive to  price swings. Such “slow” rebalancing is equivalent to a buy-and-hold strategy.\nFor example, if ETH annual volatility is 45%, and the community elects to have 15% volatility for the portfolio, then simply holding two-thirds of the portfolio value in USDC, one-third in ETH will have very similar results as the design being proposed. In fact, the Compound DAO could even save money by not paying fees related to AERA, assuming the pilot is adopted for full launch.\\nMy summary thoughts:\n\n\nAERA overall concept has a lot of promise. However, the proposed design for the pilot (using historical volatility or its EMA as the target for rebalancing) is very underwhelming.\n\n\nI believe that better targets can be constructed that take into account forecasted price moves (and not just rely on historical volatility, which is a bad predictor of future prices).\n\n\nAs it stands now, the pilot won’t be very effective in demonstrating the potential strengths and benefits of AERA.\n\n\nDue to the above reasons, I am a bit underwhelmed and will either abstain or vote “no”, if a proposal comes up for voting.\\nHi @RogerS, thank you for your feedback and thoughts.\n\n\n\n RogerS:\n\nThat only reinforces the concern that the design proposed won’t be responsive to price swings. Such “slow” rebalancing is equivalent to a buy-and-hold strategy.\n\n\nTo clarify, this pilot Aera vault is not designed to seek alpha. Any active trading strategy inherently carries with it significantly more risk than what we are proposing. For this pilot, we intend to keep the structure simple to understand for the community- a vault designed to be passively managed to offer both volatility targeting and optimized yield in a non-custodial manner. That said, Aera vaults are flexible by design and additional vault strategies can be incorporated in the future should the DAO have an appetite for such.\n\n\n\n RogerS:\n\nFor example, if ETH annual volatility is 45%, and the community elects to have 15% volatility for the portfolio, then simply holding two-thirds of the portfolio value in USDC, one-third in ETH will have very similar results as the design being proposed. In fact, the Compound DAO could even save money by not paying fees related to AERA, assuming the pilot is adopted for full launch.\n\n\nWhile this is correct with regard to ETH exposure, there are several reasons that make even this strategy difficult to implement for DAOs. In this example, if the price of ETH doubles, effective exposure becomes 50 ETH / 50 USDC. A governance vote would be required to rebalance back to the required 1:2 exposure. In order to maintain “constant” exposure, votes would be required after every major market move. Furthermore, if the volatility of ETH were to double, such that the community felt that 1:2 was too aggressive, a new vote would be required to determine a) the new exposure ratio and b) rebalancing to that new exposure.\nVolatility targeting instead simply allows you to say “buy and hold with this amount of proportionate risk.” No further action is required from the DAO, even when market conditions change. This eliminates the need for additional governance votes every time the market moves.\nIn this case, the only time a vote would be required would be to adjust the overall risk appetite.\n\n\n\n RogerS:\n\n\nAERA overall concept has a lot of promise. However, the proposed design for the pilot (using historical volatility or its EMA as the target for rebalancing) is very underwhelming.\nI believe that better targets can be constructed that take into account forecasted price moves (and not just rely on historical volatility, which is a bad predictor of future prices).\n\n\n\nWe are open to alternative targets as decided upon by the DAO. The current proposal’s volatility targeting objective is not trying to predict future prices but rather maintain a constant ETH exposure as measured by risk without requiring constant monitoring by the DAO.\n\n\n\n RogerS:\n\nAs it stands now, the pilot won’t be very effective in demonstrating the potential strengths and benefits of AERA.\n\n\nWe think this will be a good showcase of the initial capabilities of an Aera vault which are certain to expand as strategies and treasury requirements evolve. We are just scratching the surface of what Aera will eventually bring to the market. That’s one of the primary reasons this is a free pilot.\nA yield-optimized, risk-adjusted ETH-based strategy for Compound’s treasury could effectively demonstrate Aera’s initial capabilities with very limited downside. The DAO can withdraw its funds at any time if it is not satisfied with the initial results.\\nAs of now, we see this as no net negative for Compound with a substantial possible upside. This is relatively small in the grand realm of proposals and given they aren’t taking a “service fee,” we see no issues with trying this pilot service out.\\nA couple things of interest to us:\n\nStarting with assets that already introduces higher risk to carry and converting to more stable assets like wstETH and USDC feels like a good step, and in line with the objective of protecting the treasury. In Trading out of these risky assets they also aim to cap total trading size at 20k per day, aiming at reducing price impact and slippage.\nGauntlet is historically transparent and supports public view of Aera performance for us to judge.\nThis seems like a promising opportunity to test, and if successful enable greater participation of the compound community to contribute to the DAO through treasury management in the role of a merited Guardian.\n\nWe’re excited to see how the community interacts with this deployment if it is passed. It was great having a one on one discussion with the gauntlet team to go through prior case studies. Ultimately, feels like a free pilot with limited downsides that introduces something new.\\nWe agree with @PGov and @pennblockchain in that the trial carries very little downside while allowing the DAO to experiment with decentralized treasury management.\nThe selected assets and their conversion to wstETH and USDC is a net positive. We would’ve liked to have seen USDC be interest-bearing however, we understand that it wouldn’t make sense to supply it to Compound or a competitor. An alternative solution could be to substitute USDC for sDAI.\\nAs for selected assets it always had sense to convert to stables, regardless of pilot or not. If you do business in Turkey or Argentina, and accrue any sort of interest in respective currencies like lira or peso it makes zero sense to accumulate said currencies and call it a reserves. Because you just look at the chart against usd and it only goes up and to the right. Which makes it if you eventually need such currencies in some event you would pretty much guaranteed to have more of them when you convert back from usd in future. In very much similar way minor crypto tokens behave.\n\n\n\n WintermuteGovernance:\n\nwe understand that it wouldn’t make sense to supply it to Compound or a competitor.\n\n\nIt is very interesting why it doesn’t make sense to you. Aren’t current assets already exactly being supplied into Compound, not like they are sitting in some separate address and not in the same ppol from where users are borrowing from. It actually makes a lot of sense to to put at least part of the reserves in stables to Compound itself, as in such case they would actually bear interest, while at the same time contribute to available liquidity. And those are unrelated to pilot.\nAs for speculating with reserves, as Aera propose to do in that pilot, to be honest it is not in interest of protocol. There is never a lack of entities, who would like to play with somebody else money, where profits are shared, but risks are on the owner of money. If the product is so good that it works, than such manager of course can borrow any amount of the money, play with them themselves, and keep all the profits for themselves.\nAnd the reason why they don’t do exactly that, usually is because they don’t want to bear risks, The profits are theoretical, while risks are very real.\nThus personally i would be against of piloting speculations with reserves money, though  , of course, it might very well pass through governance regardless. Because entities voting on governance also deciding the fate of somebody elses money, not their own. So why not gamble with them: in worst case Compound reserves lose some money, in good case there could be some profits to reserves and they can picture themselves as contributors to success story.\nThough some ideas emerged from that discussion are quite meaningful. And better later than never. Those minor assets reserves from v2 pools are really a waste and probably could and should be converted into something more solid, so reserves could stay reserves, regardless of direction and scale of movement of crypto markets.\\nWe are in favor of this Aera pilot proposal. Establishing a clear objective for treasury/reserves management has been challenging. Pursuing growth can inadvertently lead to speculative activities, while aiming solely for stables might curtail the potential of capitalizing on market dynamics. Targeting a portfolio with a specific volatility seems like a balanced approach—easy to manage while maintaining trust.\nAfter the trial period, the DAO can assess the efficacy of this strategy and potentially propose new objectives, given the well-developed Aera infrastructure. We don’t see much risk here either as the custody of funds remains entirely under compound DAO’s discretion. And if the 3 month minimum trial period is not deemed to be enough, we can always circle back to increase the pilot’s duration—seems like a no-brainer if Gauntlet isn’t charging a performance or management fee. Furthermore, with Compound V2 in its deprecation phase, initiating this program with less active coins from the reserve seems apt. Otherwise, those assets would be liquidated in bulk later on anyway.\nThe DeFi space currently lacks a systematic solution for treasury/reserve management. The obvious method might involve entrusting funds to a third party, which goes against the principles of decentralization. In this backdrop, Aera emerges as a promising non-custodial solution for DAO fund management, ensuring control remains within the DAO governance framework.\\nWe appreciate the community’s feedback and discussion here. The governance proposal for the Aera pilot is now live 11 and will be available for voting in 1 day!\nThe proposal does the following:\n\nAccepts ownership of the vault, asset registry, and hooks module.\nRemoves ZRX and BAT from V2 reserves.\n\nSpecifically, ~$400k of ZRX and ~$100k of BAT. This ratio was chosen due to onchain liquidity conditions.\n\n\nDeposits into the Compound Aera vault.\nResumes the Aera vault to allow for rebalancing to take place by the vault guardian.\n\nAs a reminder:\n\nThis is a non-custodial vault, meaning the Compound Governor Bravo Timelock owns the vault. Thus, the DAO can initiate withdraw/deposit requests via a governance proposal at any time.\nGauntlet will serve as the vault guardian for this vault.\nThere are no fees associated with this pilot.\n\nPlease see additional details above and in the onchain proposal.\nNote, the Compound Governance page 1 seems to be throwing errors on fetching recent proposals, you can vote using Tally 11 or Boardroom\\nVoting is now live! Please use Tally  4 or Boardroom to vote as the compound governance UI is still down.\\nThe vote has passed and been executed! Thanks to everyone in the community for voting.\nThe vault can be seen at https://app.aera.finance/1/vault/v2/0x3D6eEf6A92b15361697698695334E98C5db91D6b 5\nWe will start a new thread for updates on the progress of the pilot."
  },
  {
    "number_of_comments": 10,
    "postid": "378a4bda-177b-4824-8582-ce39831cedfa",
    "posturl": "https://www.comp.xyz/t/comp-distribution-incentives/963",
    "combinedcontent": "We (Gauntlet) wanted to provide an update on our work to add better incentives for COMP distribution. First, we wanted to go into more detail on the reasoning behind our current proposal 22. Next, we describe our plan going forward to get this proposal out to Mainnet.\nWhy vesting?\nCOMP Rewards have driven usage of Compound (and honestly, of DeFi in general) since their inception last year.\nHowever, we could categorize this usage into two buckets:\n\nOrganic Demand\n\nPeople who already wanted to use the protocol, who now receive COMP rewards\n\n\nCOMP Farming\n\nPeople using the protocol just for the COMP rewards\n\nThese people often use circular borrows and immediately sell the COMP.\n\n\n\n\n\nFrom Robert’s announcement 7, the goals of COMP distribution are for users to “automatically receive governance rights, for free— in order to shape the future of the protocol”\nIn the case of farming, this goal is not met because these users clearly have no intention of shaping the future of the protocol, as they immediately surrender those rights to their exit liquidity. It can be hard to differentiate the nature of usage between the two categories above but from some simple analysis 9, it’s clear that farming is a considerable percentage of current usage. A second problem arises as well with farming - it creates a huge stream of COMP that is consistently dumped on exchanges, punishing protocol supporters who hold COMP.\nSo how can we distribute COMP to people who care about the future of the protocol? The vesting and cooldown proposals that Gauntlet has put together use a simple heuristic to determine if users have interest in the success and future of Compound - they are long COMP.\nNow, on a short timeframe (e.g. a cooldown or vesting period of < 30 days), the effects of any vesting/cooldown proposal would merely mitigate the second issue of COMP being dumped on exchanges. But as that timeframe increases, you force farmers to take on more and more COMP exposure.\n\nWhy wouldn’t this just delay when farmers dump all of the COMP?\n\nWith the simplest assumptions, it would, but it would also force the farmers to care about COMP price (as a proxy for Compound in general) for the length of the vesting/cooldown period\nOne thing to note that the cost incurred is commensurate to the user’s desire to be long COMP. If you want to hold all of your farming rewards as COMP, the cost is negligible.\n\nThis would ideally make farming more attractive to users who cared about the future of Compound, and should shift the user base in this direction. If/when these changes go through, we’ll see if this is the case.\n\n\n\n\nThere is clearly a balance here, because you also start to deteriorate the experience for organic demand users who now have to manage the cooldown period\n\nEven some organic users want to reduce exposure, which is reasonable to a limited extent even for the most ardent of Compound supporters\n\n\n\nNow, this is not the only way to reward people who care about the protocol with COMP, you could create a staking pool (as SNX, AAVE, and others have) or pay users in COMP to vote. However, neither of these would fix the current issues with distribution, though it would reward users who bought COMP from farmers and then participated, which would have its own benefits. Also just a reminder - COMP can now be paid directly to community contributors (after proposal CP030). Participants who drive value to COMP, who might rely on COMP rewards, can just get them directly via Governance.\nNext steps\nWe got great feedback on Discord as well as during the dev call, and we’ve put together a plan for addressing the most salient points here 10. Most notably, this new version will preserve the existing API much better. Going forward, our plan is to:\n\nIncorporate final dev feedback\nAudit the code with Quantstamp\nTesting on Kovan\nPropose adding the cooldown logic (ETA first week of February, though timelines are not finalized). Starting with a 0-length cooldown, the community can decide on what works over time.\n\nIf the proposal passes, we’ll then reach out to wallets and other partner applications as we work to turn on the functionality by setting a cooldown > 0.\nIf it doesn’t, we’re happy to respect the community’s decision here. Vesting was something that was first suggested on these very forums, and while we think the tradeoffs it presents are good for Compound, we understand how other community members might disagree. We’ve now produced two tested and working implementations, and those will be available to the community in the case minds change.\\nAs gas costs are high I’m in the habit of accruing COMP then supplying it in chunks. Would it be possible to accrue COMP to our supply and skip that step? I am 100% ok with my rewards being put on a cooldown, but I would prefer the above option.\nJD\\n@jmo, the work that you @tarun and the Gauntlet team have done in exploring / proving a number of vesting/cooldown approaches is fantastic and deserves recognition.\nLast week’s community call 4 walked through the cooldown codebase, and gathered lots of new feedback from the community. Everyone seems fully aligned with the goal of getting more COMP in the hands of loyal users - while dissuading purely exploitative farming.\nIn Discord, @lay2000lbs raised the issue that a cooldown will degrade / complicate the user experience (for normal users, farmers, and integrated products), which I agree with. Additional steps, delays, and flows is a “stick”, that hits farmers harder than organic users. While this will likely tilt usage towards organic users, it might still sting.\nExtending @johndoh 's idea a bit, I wanted to take a moment to explore the “carrot” path, to make the COMP Distribution as uplifting as possible.\n\nUpgrade: Distribute cCOMP\nInstead of distributing COMP, the claim() and grantCompInternal() functions can be upgraded to distribute cCOMP to users. This would convert the uncollected COMP, into COMP supplied to the protocol, earning interest and increasing borrowing capacity (if collateral is enabled). At any time, users can withdraw COMP from the protocol. If users are looking for liquidity, they can borrow any asset (like a stablecoin) instead.\nTo a user, the status quo (post-claim) switches from a decision on “what to do” with COMP (supply it, or sell it), to a decision on “whether to withdraw it”.\nThis can be paired with significantly reducing the gas costs of the claim() function (by moving compAccrued to the end of the function to run/distribute once, instead of looping over a transfer function multiple times).\nNote: this wasn’t possible when the Distribution first launched (cCOMP didn’t exist yet).\nUpgrade: Enable cCOMP voting\nThe Governor Bravo 3 (a modernized Governance framework) in development could be upgraded to allow cCOMP voting alongisde (or eventually, instead of) COMP voting.\nAll COMP earned by users would be able to immediately vote, further reducing the incentive to withdraw COMP from the protocol, and increasing participating in governance.\nNote: this might require modifying the cCOMP implementation with a balance checkpoint first.\n\nAfter releasing these upgrades, the community could monitor whether participation in the governance process has increased. If it hasn’t, the cCOMP contract could be updated with the Cooldown logic – locking users into a longer (but more useful) commitment.\nIdeas / thoughts / counterarguments are very welcomed.\\nI like this idea.\nI will say, after reading @jmo explanation I’m less opposed to the original implementation. I’m more neutral on it. It would be an interesting experiment that I personally don’t think would meaningful change behavior but I’m still curious to see the outcome.\nGoing back to Rob’s proposal I like the idea of transitioning the question from “what to do with COMP” to “Whether to withdraw it”. I also like reducing gas costs and enabling cCOMP to natively vote. The only thing I don’t like is that again, we may be overloading the function of the COMP market. If at the end of the day we’re trying to incentives holding COMP let’s create something that does exactly that, rather than hacking an interest rate market to sort of do it.\nI believe a key learning here is that current COMP distribution is an amazing way to incentives capital allocation and a very poor way to distribute governance to long term stakeholders. So perhaps we should re-frame COMP distribution as the capital incentive for the protocol and start fresh on how we want to distribute governance to long term stakeholders… I’ve heard rumors of @blck and @arr00 working on something that may be relevant here.\nAnyway, I’m starting to feel like a perpetual critic and I do appreciate the concrete nature of Rob’s suggestion. I think it meets two critical criteria in that 1) it’s an improvement on existing user experience and 2) it plausibly will reduce COMP farm and dumping somewhat. So it’s likely the best next step forward.\\n\n\n\n rleshner:\n\nUpgrade: Distribute cCOMP\nInstead of distributing COMP, the claim() and grantCompInternal() functions can be upgraded to distribute cCOMP to users. This would convert the uncollected COMP, into COMP supplied to the protocol, earning interest and increasing borrowing capacity (if collateral is enabled). At any time, users can withdraw COMP from the protocol. If users are looking for liquidity, they can borrow any asset (like a stablecoin) instead.\nTo a user, the status quo (post-claim) switches from a decision on “what to do” with COMP (supply it, or sell it), to a decision on “whether to withdraw it”.\n\n\nI’m getting slightly off topic, but maybe instead of incentivizing deposits to cCOMP (which has some possible negative externalities like more votes available to rent), we could create a staking insurance mechanism similar to stkAAVE and funnel liquidity incentives into this.\nUser would deposit COMP to staking module/market and receive stkCOMP. If there was a loss event to the protocol, a certain percentage of the value would be seized and sold on the market to recapitalize the system. And the cooldown logic could be applied here to disincentivize immediate dumping, along with incentives in the form of COMP issuance or redirecting some of the reserves earned by other markets to stakers.\\nRobert - Distributing cCOMP does add a “nudge” to encourage COMP holding, and while there are still UX trade-offs with any change, cCOMP distribution introduces considerably less friction than the cool down. We’re happy to work with the community to stage the cool down proposal after other initiatives.  We’ll finish the audit of the cool down and post the final version to the Compound github for the community to consider after landing these other initiatives.\nJD - this new order of operations for current community initiatives should address your concerns. If we add the cool down after Robert’s suggested changes, where all COMP rewards will be cCOMP, this will result in the requested behavior.\\nI’ve been developing a new method for COMP distribution for a while now and think we should come to some sort of consensus before I continue moving forward.\nWhat I have been developing is essentially a way for users to stake their COMP for a determined period of time with linear decaying weighting (like CRV staking). Dividends are then distributed to stakers based on their weighting. Initially, this would only distribute COMP, but it supports the addition of new assets, such as funneled reserve factors. Users maintain full delegation rights on their COMP, but are unable to remove it until their stake period ends. Dividends are continually claimable, but only if the account delegated to participates in governance (votes).\nI decided to go this route because I don’t believe that incentivizing cCOMP or restricting users’ ability to claim is the way to go. Giving cCOMP voting power essentially allows for an unbounded amount of votes only limited by the borrow cap. Cooldowns are a nice idea, but the fact that it would break contracts interacting with compound is a dealbreaker for me.\\nSo one thing to clarify, is that in the latest plan for the coolDown 6, it shouldn’t break other contracts using Compound. You just call claimComp() both to start the cooldown and remove your COMP.\\nHi there,\nI hope this one is not too much off topic:\nBeing an “ordinary user” who likes and supports the idea behind Comound/COMP but without in depth technical knowledge on the topic, I am just wondering why the fees to claim accrued COMP are so extremely high. This thread here suggests that there seems to be some sort of vesting in place and that this fee might come down with time; is this correct?\nOr is there now a threshold amount in place beyond which the cost to claim becomes resonable?\nSince there were some thoughts mentioned above on the motivation of the different types of users, let me quickly tell you about mine. Maybe others share the same.\nIt’s not my intention to sell the COMP immediately, but instead I would just like to see the COMP that have accrued end up in my wallet and made available again to the community; by that granting me voting rights (that I would delegate to one of the “professionals” around) and adding to generating more COMP interest.\\n\n\n\n jmo:\n\nPropose adding the cooldown logic (ETA first week of February, though timelines are not finalized).\n\n\nAny updates on timeline for this?\\nWe’re still waiting in the wings for any progress on the other proposed changes. We’ve started our audit with Quantstamp and should finish soon."
  },
  {
    "number_of_comments": 22,
    "postid": "22c9db22-97ef-4108-b68b-4cdc39804887",
    "posturl": "https://www.comp.xyz/t/governor-bravo-development/942",
    "combinedcontent": "For the past few weeks, Blurr and I have been working on preparing a Governor Bravo implementation based on community feedback over the past few months. The start of the conversation from Governor Bravo began in this post 96 and has inspired some of the changes we’ve made so far. I’ll summarize the changes made along with their inspiration, and include a tentative timeline. Any feedback or recommendations for changes are greatly appreciated!\n\nUpgradable Implementation - Like the Comptroller, Governance will now be upgradable to allow for incremental changes without breaking infrastructure built around it. The implementation is governed by Compound Governance.\nAbstain option on voting - This allows for users to abstain to votes on chain. It is a necessary feature for a future project involving COMP staking.\nVoting reason string - Users are able to send a string along with their vote explaining their voting reasoning. This is stored along with the vote receipt.\nSettable governance parameters (voting period, voting delay, proposal threshold) - Having these fields be settable by governance allows for changes to the governance process without the requiring new contracts to be deployed.\nAllowing proposer to cancel their own proposals - Currently, the only way for a proposal to be cancelled is for the proposer to lose delegations and fall below the proposal threshold. Allowing the proposer of the proposal to cancel their proposal at any point makes it possible for an erroneous proposal to be canceled if needed.\nRemoval of the Guardian - the guardian logic was used for the initial setup of Governance but is no longer needed. The logic around it has been removed.\nContinuous Proposal Id Count - I have added logic to GovernorBravo which will set the initial proposal ID to the number after the final GovernorAlpha proposal ID.\nAdmin Logic - the settable parameters and implementation are governed by the admin, which will be set to the compound timelock.\n\nWe are approaching completion of the GovernorBravo implementation with the above features. Over the next few days we hope to complete testing and refactoring the code. After, we get the code audited which may take a while due to the current crypto environment. Shortly after the audit comes back, assuming all is well, we will post an autonomous proposal with the changes.\nYou can follow along with the development effort here 177.\\nNice work arr00, these seem like excellent changes. After your external audit, would you mind starting an open bug bounty here for seven days before posting your autonomous proposal? It could help start the precedent that the Compound community itself has an incentivized opportunity to find bugs before all major code changes are put up for a vote.\\nI like this idea a lot—a great precedent to start.\\nThis sounds awesome, thank you and Blurr for taking the initiative on this. I look forward to learning more.\n\n\n\n arr00:\n\nAbstain option on voting - This allows for users to abstain to votes on chain. It is a necessary feature for a future project involving COMP staking.\n\n\nCan you share your vision for the project mentioned?\\nSure. COMP staking which allows for you to maintain your voting rights. COMP is distributed to stakers and you are only eligible for rewards if you participate in governance. I think an on-chain abstain should count as participation, but that can be debated later on. I just want the infrastructure to be in place.\nThere are many intricacies involved in the COMP staking project, but I’d prefer to leave those to another post when we are closer to completing the project.\\nThanks for opening great topic. I think that it would be very flexible governance structure if it has been implemented these factors.\nBut I’m not sure yet how to operate eth based Compound protocol after launching Compound Chain.\nif it needs to operate them separately, is it possible to identify Compound Chain specific proposal in Governor Bravo design?\\nCompound Chain is definitely a task for later down the road. Governor Bravo has an upgradable implementation which allows for future updates to be made easily.\\nGovernor Bravo is now active on the Kovan test-net. The new Governor for Compound on kovan is here 13.\\nHi @arr00 and Blurr. This is tremendous! I am very excited that you have taken the initiative to improve the existing protocol. I have an idea that might make governance participation a bit easier.\nBefore I say it, I want to express that this is something that can come at a later date, in a separate iteration of Governor contract changes. I think it is important to make protocol changes in small increments; a best practice if you will.\nMy idea is to enable governance voting for individuals that hold cCOMP in addition to COMP delegates. There are probably several ways to implement this. Perhaps changes would need to be made to cCOMP as well to make it possible? I’m not certain. To everyone reading this, not just Ar00, what are your thoughts on enabling voting with cCOMP?\\nHi Adam,\nI’ve been thinking of this as well and made this recent POC PR to Governor Bravo that might be of interest: [WIP] Add fractional voting to castVote by nijynot · Pull Request #2 · Arr00-Blurr/compound-protocol · GitHub 14\nMy take on it was to enable fractional voting, i.e. voting with less than the total amount of votes that an account has. This would make it possible for the accounts that hold the underlying COMP to vote accordingly to the ratio that cCOMP votes on. Also read Arr00’s reply in the PR - changing cCOMP to be delegateable would be another way to do this.\\nSorry for the late reply. I think something like this can come in the future, but I am not necessarily a fan of the idea. Throughout all other Compound markets, you generally sacrifice the utility of your asset for the economic aspect of being able to borrow on it. If you are do not lose anything when lending to cCOMP, then I don’t really see it as lending and neither will the market. I think it is ideal for COMP to have a functioning market and incentives COMP holding through other means.\\nGovernor Bravo development and auditing is finally complete! You can view the finalized PR here 25, and the audit will be published by Open Zeppelin in a few days.\nWhile I am quite confident that the code is sound, I will be hosting a community wide bug bounty for the next week in order to encourage community review prior to the proposal. Please reply to this post with any findings and if they are legitimate, you will be rewarded in COMP.\\nFantastic progress @arr00 \\nFantastic work @arr00!!!\nThe team at Tally 7 will be sure to take a look!\\nThe audit by Open Zeppelin is complete. Compound Governor Bravo Audit – OpenZeppelin blog 20\\nI understand and consider logical certain arguments for “utility trade-of” (i.e. COMP will become cCOMP - if holders will play with rationality). However, I feel that the current utility of the COMP token is underutilized. I am in favor of creating a COMP staking pool with a staking rewards less than 5%, COMP staked in that pool has a full voting right but with a timelock period between 1 - 3 months.\n\nWith timelock option we will be able to filter real long-term believers and be able to predict moving “native” funds inside protocol\nThe governance system would be separated from the COMP market (long-term users who believe and want to work on the project would be separated from recursive farmers)\nIt would be clearer if the project is moving towards decentralized management (difficult to predict in the current situation) - it would be easier to control the evolution into a truly decentralized system\n\nI’m not for fractional voting because I don’t think we would get clear metrics that show protocol health (share of particular interest groups on protocol, reactions after prop implementation, recursive farming discovery etc.).\nAll praise for @arr00, great job\\n\n\n\n dabar90:\n\nHowever, I feel that the current utility of the COMP token is underutilized.\n\n\nI would say utility of COMP token is intentionally crippled. Currently there are basically 2 things you can do with COMP tokens:\n\n\nvote with it (which realistically just for fun and not impact anything unless your holdings count in thousands. And even then it’s not really impactful on results of voting)\n\n\nYou can use it as collateral. Which would really be enough as a major usecase unless it wouldn’t be kind of messed also:\n\n\na) By putting it as collateral you, for some weird reason, loosing your voting power, even if relatively small one.\nb) You are forced to accept that your tokens might be lend, which you might not be fan of at all. And then you get arguments that: hey, they wanted to get some interest, so that’s a trade-off. Except there was no other choice to use it for collateral in other way.\nGuess what, i’d say pretty much nobody with COMP holding in hundreds give a slightest care about that 5% or so of interest you might get at cCOMP market, because you can very well borrow some USDC or DAI and get interest in 20-30% APY on that relatively easily.\nI’m not sure  there is that need of COMP borrowing market at all, as i read so many times concerns that “somebody could borrow COMP and somehow manipulate governance” Realistically i believe there are 2 major usecases for current COMP market:\n\nrecursive farming (if you were lucky to fit your borrow under CAP)\nshorting COMP . If you are COMP stable coin farmer, that’s a great idea to sell your future distributions of COMP at high price (when it’s high) and repay it back with future distributions.\n\nBut again we can leave gimped cCOMP market as it is. Solution is actually quite simple, and don’t even require some impressive coding skills and just goes down to creating another cCOMP token. We can call it cCOMP2, or gCOMP (governance COMP) or wCOMP (wrapped COMP) or whatever else to separate it. And implement it with borrowing side disabled completely. Than we can have all the collateral usage, no borrowing side risks, no governance manipulation risks with borrowed tokens. And it can actually allow user to fully retain his voting power (that might need some coding though) as all tokens stay there in pool unless redeemed back. It even could be non-transferrable, like only be able to redeem it back for COMP, but not transfer from one address to another. That could very well be a defalt way of storing COMP tokens rather than quite costly one by keeping it in your address. (the cost here is opportunity cost, you lock away your capital in holding token which otherwise could be used in generating profits.)\nAnd yeagh, i get it, COMP token is governance token, have no value. Except it’s not the case, it certanly DO have a market value.\\n\n\n\n Sirokko:\n\nThat could very well be a defalt way of storing COMP tokens rather than quite costly one by keeping it in your address. (the cost here is opportunity cost, you lock away your capital in holding token which otherwise could be used in generating profits.)\n\n\nActually, i have opportunity cost now in situation where I need to choose which utility to use. With given gas cost (waiting to accumulate amount of COMP) and voting significance ( illusion ) average user has nothing.\nWhat the difference between staking and gComp pool?\nAnd for lock away COMP vs generating profits - depend of user preference - so it’s subjective perspective.\nI just want to say that maybe will be better have more choices when come to COMP utility. If you think that 5% is low then go to dapp interface and watch. I was just bring average staking number, read xy%.\\n\n\n\n dabar90:\n\nWhat the difference between staking and gComp pool?\n\n\nStaking usually bring zero value. Like you just send it to contract, contract lock it away and this is it. It’s a bit different, for example when you stake LP tokens, as then you providing liquidity and getting fees from pool.\nMaking another pool without borrowing is different, because you are actually using it as collateral. It is true for cCOMP pool also, but in cCOMP pool you lose control over your voting power, which is main purpose of having COMP to begin with. And you can’t really preserve your voting power in cCOMP pool for a reason it have borrowing side, thus amount of COMP in pool is able to fluctuate. But thing is, it came with a “packaged deal”, nobody asked Supplier if he want his COMP tokens be lend at all. Maybe his only interest was to use it as collateral (which is big deal by itself, given current valuation). That’s my point.\nSurely, there are some users who want to lend their COMP. But i’m talking about much bigger userbase, who not interested in doing it at all. Quite opposite. They intend to hold it for extended period of time, and they want to use it for voting, and they want to use it as collateral. Pretty basic usecase imho.\nWhen you use your assets as a collateral in a bank, you don’t expect or allow bank to lend it. And you don’t expect to lose your voting power either, in case if your assets are shares in some company. Why it’s different for COMP i have no idea.\nSo, Compound does not provide a very basic tool for COMP holders, yet so many complains are there that COMP have not much use case. And concerns about governance manipulation via borrowed COMP.\\n@arr00 fantastic work, and congrats on the audit results. The Governor Alpha contract has become a standard in the Ethereum community, and Bravo will continue the trend \n@dabar90 @Sirokko these are great points, and an interesting conversation, but aren’t related to the development of this new voting contract. Let’s try to keep the conversation focused on the Bravo contract.\\nI have deployed the two Governor Bravo contracts to the following addresses:\n\nGovernor Bravo Delegator: 0xc0da02939e1441f497fd74f78ce7decb17b66529\n\nGovernor Bravo Delegate: 0xAAAaaAAAaaaa8FdB04F544F4EEe52939CddCe378\n\n\nI will create new post-deploy fork simulations using these addresses, then deploy a CAP later today. After deploying the CAP, I will create a new post with the proposal details and use a proposal tag.\\n@arr00 Thanks for this. We’re really excited to support Bravo at Tally 12 and would love to setup a call with you to talk more about the implementation (anyone could come to that call as well, just would like an open AMA about Bravo! Maybe a specific topic for the Developer calls?)\\nHey @dennison. I’d be happy to have a call with Tally to talk about Governor Bravo—just send me a PM on discord and we’ll make it happen."
  },
  {
    "number_of_comments": 35,
    "postid": "33113dd0-6159-4bcd-984e-20d3cec73cf4",
    "posturl": "https://forum.makerdao.com/t/blue-ad-recognition-submission/20915",
    "combinedcontent": "0xb6c09680d822f162449cdfb8248a7d3fc26ec9bf\n0xb18680092734394295d0591BB42f2bD3c184517e: Followed KISS AVC\n0x445CB6c63c502fDBebD1B273F6AEa1AaD691e0aa: Followed SovFi AVC\nCryptographically signed AD Recognition Submission Message from the Ethereum address controlling Delegate Contract 1: https://etherscan.io/verifySig/25260 2\nCryptographically signed AD Recognition Submission Message from the Ethereum address controlling Delegate Contract 2: https://etherscan.io/verifySig/19059 5\nCryptographically signed AD Recognition Submission message from Ecosystem Actor Ethereum Address, where this address is not one of the addresses controlling a Delegate Contract: https://etherscan.io/verifySig/19057 5\\n\n\n\n BLUE:\n\nFollowed Growth AVC\n\n\nThanks for following Growth AVC. Please follow the discussion on the forum and Discord\\nConfirming that these are all correct. We’ll have you up on the portal shortly.\nWelcome to Maker!\\nSummary of Votes on 31 May 2023\nAdd BlockTower Andromeda (RWA015) as a new RWA Vault Type - May 29, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal 3\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nGrowth\nVote: Yes. It is part of the endgame strategy and provides diversification to the management of RWA allocation. Strengthening the surplus buffer has aslo been a topic for a long time, this is a step in that direction.\nInteroperable smart contracts ensure a seamless integration of BlockTower Andromeda into MakerDAO’s system, with the proposed two-stage strategy offering flexibility and robustness. Stage one builds upon existing Real World Assets (RWAs) structures, and stage two introduces innovative conduit components to allow a selection of multiple stablecoins.\nThe integration does not come without its share of risks and challenges. Regulatory uncertainties, potential exchange platform difficulties, and specific issues related to U.S. Treasuries could pose significant hurdles. I refer to the risk assessment made by Steakhouse for more details. Successfully navigating these risks will be critical.\nThe BlockTower Andromeda vault will significantly bolster MakerDAO on its path toward the endgame transitioning, demonstrating its capability to balance growth yields and risk within the dynamic ecosystem. The associated risks requires careful evaluation and mitigation to ensure success.\nComposability\nVote: Yes. It is part of the endgame strategy and provides diversification to the management of RWA allocation. Strengthening the surplus buffer has aslo been a topic for a long time, this is a step in that direction.\nInteroperable smart contracts ensure a seamless integration of BlockTower Andromeda into MakerDAO’s system, with the proposed two-stage strategy offering flexibility and robustness. Stage one builds upon existing Real World Assets (RWAs) structures, and stage two introduces innovative conduit components to allow a selection of multiple stablecoins.\nThe integration does not come without its share of risks and challenges. Regulatory uncertainties, potential exchange platform difficulties, and specific issues related to U.S. Treasuries could pose significant hurdles. I refer to the risk assessment made by Steakhouse for more details. Successfully navigating these risks will be critical.\nThe BlockTower Andromeda vault will significantly bolster MakerDAO on its path toward the endgame transitioning, demonstrating its capability to balance growth yields and risk within the dynamic ecosystem. The associated risks requires careful evaluation and mitigation to ensure success.\nNon-Scope Defined Parameter Changes - May 29, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal 1\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nGrowth\nVoted Yes. A series of parameter and organizational changes has been proposed to better reflect current market conditions. These include changes to the Dai Savings Rate (DSR) and Stability Fee (SF) across various collaterals, potentially influencing the lending market dynamics. SES has been appointed as the new Responsible Facilitator, focusing on Stability Scope changes, while BA Labs remains an Ecosystem Actor as part of the Advisory Council.\nThe Stability Scope Bounded Mutable Alignment Artifact, currently limited to ETH and WSTETH vault types, needs to be expanded to include other collateral types. This step is vital to avoid collateral offboarding and potential disruption. Consistency of parameters across collateral types is addressed, with proposals aimed at unifying system behavior.\nDetailed changes in Stability Scope Parameter and Non-Scope Related proposals are offered. Consideration is given to important collateral like rETH, whose DC is proposed to increase, reflecting its potential within MakerDAO’s ecosystem. (and aligned with the Engame plan).\nThese proposed modifications signal a strategic change within MakerDAO, impacting its future functionality and positioning within the DeFi landscape.\nComposability\nVoted yes. A series of parameter and organizational changes has been proposed to better reflect current market conditions. These include changes to the Dai Savings Rate (DSR) and Stability Fee (SF) across various collaterals, potentially influencing the lending market dynamics. SES has been appointed as the new Responsible Facilitator, focusing on Stability Scope changes, while BA Labs remains an Ecosystem Actor as part of the Advisory Council.\nThe Stability Scope Bounded Mutable Alignment Artifact, currently limited to ETH and WSTETH vault types, needs to be expanded to include other collateral types. This step is vital to avoid collateral offboarding and potential disruption. Consistency of parameters across collateral types is addressed, with proposals aimed at unifying system behavior.\nDetailed changes in Stability Scope Parameter and Non-Scope Related proposals are offered. Consideration is given to important collateral like rETH, whose DC is proposed to increase, reflecting its potential within MakerDAO’s ecosystem. (and aligned with the Engame plan).\nThese proposed modifications signal a strategic change within MakerDAO, impacting its future functionality and positioning within the DeFi landscape.\nAdjust Spark Protocol D3M Parameters - May 29, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nGrowth\nVoteed yes. The proposal to lift the D3M debt ceiling to $20m presents an intriguing balancing act between risk and reward. Greater liquidity could stimulate economic activity and spur participation. However, the expanded exposure to market fluctuations presents higher risk. The proposition is a bold affirmation of the Lab’s confidence in their robust system and anticipation of heightened demand for D3M.\nComposability\nVoteed yes. The proposal to lift the D3M debt ceiling to $20m presents an intriguing balancing act between risk and reward. Greater liquidity could stimulate economic activity and spur participation. However, the expanded exposure to market fluctuations presents higher risk. The proposition is a bold affirmation of the Lab’s confidence in their robust system and anticipation of heightened demand for D3M.\nDeploy and Use Spark Protocol Executive Proxy - May 29, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal 1\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nGrowth\nVoted yes. The decision to deploy Spark Proxy and introduce an additional admin role reflects a strategic shift towards operational efficiency and resilience. Delegating collateral onboarding access to the Spark Proxy should streamline processes and quicken decision-making. The addition of an admin role diffuses authority, mitigating single point failure risks while staying true to decentralization principles.\nComposability\nVoted yes. The decision to deploy Spark Proxy and introduce an additional admin role reflects a strategic shift towards operational efficiency and resilience. Delegating collateral onboarding access to the Spark Proxy should streamline processes and quicken decision-making. The addition of an admin role diffuses authority, mitigating single point failure risks while staying true to decentralization principles.\nOnboard rETH to Spark Protocol - May 29, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nGrowth\nVoted yes. The proposal to onboard rETH to Spark Protocol shows Phoenix Labs’ calculated approach to managing risk in a volatile cryptocurrency environment. With parameters like the supply cap and liquidation threshold, the Labs display commitment to maintaining system stability. The E-Mode ETH category is a particularly novel solution, linking liquidation thresholds to relative movements between rETH and ETH rather than ETH/USD, thereby reducing market risk. This anticipates increased rETH exposure and exhibits Phoenix Labs’ strategic foresight.\nComposability\nVoted yes. The proposal to onboard rETH to Spark Protocol shows Phoenix Labs’ calculated approach to managing risk in a volatile cryptocurrency environment. With parameters like the supply cap and liquidation threshold, the Labs display commitment to maintaining system stability. The E-Mode ETH category is a particularly novel solution, linking liquidation thresholds to relative movements between rETH and ETH rather than ETH/USD, thereby reducing market risk. This anticipates increased rETH exposure and exhibits Phoenix Labs’ strategic foresight.\nAdjust Spark Protocol DAI Interest Rate Strategy to Match the DSR - May 29, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nGrowth\nVoted yes. In a bid to cultivate passive DSR holders, Phoenix Labs plans to align the DAI interest rate strategy with the Dai Savings Rate. This move is an astute response to attract and retain capital amidst a competitive Defi market. It signals Phoenix Labs’ adaptability to evolving macroeconomic trends within the Defi ecosystem.\nComposability\nVoted yes. In a bid to cultivate passive DSR holders, Phoenix Labs plans to align the DAI interest rate strategy with the Dai Savings Rate. This move is an astute response to attract and retain capital amidst a competitive Defi market. It signals Phoenix Labs’ adaptability to evolving macroeconomic trends within the Defi ecosystem.\nReduce PSM-USDP-A Debt Ceiling to 0 - May 29, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nGrowth\nVoted yes. MakerDAO has been advised to consider lowering the debt ceiling of the PSM-USDP-A from 500 million DAI to zero. This recommendation stems from two main concerns: the limited liquidity of USDP compared to other stablecoins like USDC and GUSD, and the lack of revenue generation from USDP exposure. Adopting the change would not instantaneously impact USDP exposure but would initiate a gradual outflow from the PSM. While Paxos, the issuer of USDP, has suggested a marketing fee scheme that could change the revenue equation, these talks are still in the early stages. The proposition comes from BA Labs, part of the Stability Scope Advisory Council, and hinges on the approval of the Stability Scope facilitator. Future efforts are expected to refine stablecoin allocation rules, mitigating discretionary management.\nComposability\nVoted yes. MakerDAO has been advised to consider lowering the debt ceiling of the PSM-USDP-A from 500 million DAI to zero. This recommendation stems from two main concerns: the limited liquidity of USDP compared to other stablecoins like USDC and GUSD, and the lack of revenue generation from USDP exposure. Adopting the change would not instantaneously impact USDP exposure but would initiate a gradual outflow from the PSM. While Paxos, the issuer of USDP, has suggested a marketing fee scheme that could change the revenue equation, these talks are still in the early stages. The proposition comes from BA Labs, part of the Stability Scope Advisory Council, and hinges on the approval of the Stability Scope facilitator. Future efforts are expected to refine stablecoin allocation rules, mitigating discretionary management.\\nSummary of Votes on 13/6/23\nGUSD PSM Parameter Adjustments - June 12, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal 1\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nGrowth\nVoted yes. The proposal is about reducing the Price Stability Module (PSM) Maximum Debt Ceiling for PSM-GUSD-A from 500 million to 110 million DAI and the Fee Out (tout) parameter to 0%. This aims to increase the DAI’s liquidity and utility value, as GUSD contributes less compared to USDC due to lower primary market liquidity and less usage by vault users.\nAdditionally, GUSD generates less revenue than USDC. Gemini provides 2% annual payments for GUSD, whereas Coinbase offers 2.6% for USDC. Moreover, forthcoming deployments, such as Andromeda, could yield 4-5% or more.\nThe reduction aligns with MIP104’s proposed revisions and paves the way for a better capital deployment into more profitable opportunities. However, this doesn’t immediately decrease GUSD exposure, but rather it’s expected to gradually flow out due to arbitrage opportunities and its use by Real World Asset entities.\nBA Labs, in its advisory capacity, endorses the reduction, subject to the approval of the Stability Scope facilitator. It also advises refining the Stability Scope for clear stablecoin allocation rules to decrease discretionary management, thereby leading to more efficient governance.\nComposability\nVoted yes. The proposal is about reducing the Price Stability Module (PSM) Maximum Debt Ceiling for PSM-GUSD-A from 500 million to 110 million DAI and the Fee Out (tout) parameter to 0%. This aims to increase the DAI’s liquidity and utility value, as GUSD contributes less compared to USDC due to lower primary market liquidity and less usage by vault users.\nAdditionally, GUSD generates less revenue than USDC. Gemini provides 2% annual payments for GUSD, whereas Coinbase offers 2.6% for USDC. Moreover, forthcoming deployments, such as Andromeda, could yield 4-5% or more.\nThe reduction aligns with MIP104’s proposed revisions and paves the way for a better capital deployment into more profitable opportunities. However, this doesn’t immediately decrease GUSD exposure, but rather it’s expected to gradually flow out due to arbitrage opportunities and its use by Real World Asset entities.\nBA Labs, in its advisory capacity, endorses the reduction, subject to the approval of the Stability Scope facilitator. It also advises refining the Stability Scope for clear stablecoin allocation rules to decrease discretionary management, thereby leading to more efficient governance.\\nBLUE votes\nSummary of Votes on 26/06/23\nRatification Poll for MIP Amendment Subproposal (MIP102c2-SP8) - June 12, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nGrowth\nVoted yes. The recent proposal presented brings about the following changes: it involves adjustments to the stability scope on collateral types, the accessibility scope on limiting access to the frontends.\nDelegate slots have been reduced. Chronicle Labs has been introduced with a dedicated budget, presumably to spur innovation, although its specific role remains unclear. A stablecoin recovery phrase is added. Additionally, the Ethereum Core Protocol development is now arranged into stages.\nThe proposal’s contentious aspects, including the reduction of delegate slots impacting AVC members and the financial streamlining consolidating budgets, warrant careful consideration. Moreover, the MKR budget expansion, though ambitious, raises concerns about fiscal prudence.\nComposability\nVoted yes. The recent proposal presented brings about the following changes: it involves adjustments to the stability scope on collateral types, the accessibility scope on limiting access to the frontends.\nDelegate slots have been reduced. Chronicle Labs has been introduced with a dedicated budget, presumably to spur innovation, although its specific role remains unclear. A stablecoin recovery phrase is added. Additionally, the Ethereum Core Protocol development is now arranged into stages.\nThe proposal’s contentious aspects, including the reduction of delegate slots impacting AVC members and the financial streamlining consolidating budgets, warrant careful consideration. Moreover, the MKR budget expansion, though ambitious, raises concerns about fiscal prudence.\nRatification Poll for MIP Amendment Subproposal (MIP102c2-SP9) - June 12, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nGrowth\nVoted yes. We disagree with the delegates saying the setup is a slippery slope. This might be based on our lacking deeper understanding of the Atlas. We expect all ecosystem actors to have some sort of internal rules or policies, if they also serve other customers. If it is a hinderance, it sounds like something that can be fixed one way or another. Maybe its simply because Immunify like other ecosystem actors should sell its services on a project basis instead of aiming for being integrated directly through the scopes.\nComposability\nVoted yes. We disagree with the delegates saying the setup is a slippery slope. This might be based on our lacking deeper understanding of the Atlas. We expect all ecosystem actors to have some sort of internal rules or policies, if they also serve other customers. If it is a hinderance, it sounds like something that can be fixed one way or another. Maybe its simply because Immunify like other ecosystem actors should sell its services on a project basis instead of aiming for being integrated directly through the scopes.\\nSummary of Votes on 27/06/23\nSmart Burn Engine Launch Parameters - June 26, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nGrowth\nVoted yes. The Smart Burn Engine introduces a significant evolution in MakerDAO’s system. It redeploys surplus Dai to augment MKR liquidity by acquiring Univ2 LP tokens instead of direct burning, with Dss Flappers replacing Surplus Auctions. Adjustments in system parameters like the Surplus Buffer Upper Limit are essential for market risk mitigation. Despite calculated market assumptions forming the basis of initial parameters, constant vigilance is necessary. Notably, sudden shifts in transaction costs or market liquidity present potential risks. With effective management, however, the Smart Burn Engine can notably generate value.\nComposability\nVoted yes. The Smart Burn Engine introduces a significant evolution in MakerDAO’s system. It redeploys surplus Dai to augment MKR liquidity by acquiring Univ2 LP tokens instead of direct burning, with Dss Flappers replacing Surplus Auctions. Adjustments in system parameters like the Surplus Buffer Upper Limit are essential for market risk mitigation. Despite calculated market assumptions forming the basis of initial parameters, constant vigilance is necessary. Notably, sudden shifts in transaction costs or market liquidity present potential risks. With effective management, however, the Smart Burn Engine can notably generate value.\\n\nBlockTower Andromeda Updates, GUSD PSM Parameter Changes, and Other Actions - June 28, 2023\nIn accordance with previous votes.\\nSummary of Votes on 07/06/23\nDAO Resolution to Approve Legal Representation - July 3, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nGrowth\nVoted yes. Appropriate questions have been posed and, to a certain degree, answered. Considering our time constraints, We believe we have no other option but to proceed with the current setup.\nComposability\nVoted yes. Appropriate questions have been posed and, to a certain degree, answered. Considering our time constraints, We believe we have no other option but to proceed with the current setup.\\nCryptographically signed AD Recognition Submission from ethereum address from the ethereum address controlling Delegate contract 2: https://etherscan.io/verifySig/21625 5\n@GovAlpha-Core-Unit\\nHi @BLUE  !\nThank you very much for following Sovereign Finance AVC! We are looking forward to working together.\n Here is a link  with all the dates of our upcoming meetings.\nIn case you missed something, here is our Notion  with useful information…\nNext subcommittee meeting’s topic will be MIP 106: Support Scope.\nHope to see you there!\\nBlockTower Andromeda Upgrade, Smart Burn Engine Deployment, Keeper Job Updates, Scope Defined Parameter Changes, Delegate Compensation, Ecosystem Actor and Core Unit Funding Updates, Spark Protocol Proxy Spell Execution - July 14, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposable and ReFi AVC: votes yes. Getting the smart burn engine going is a top priority. Nothing in the vote was unexpected.\\nDecrease the Harbor Trade Credit (RWA004-A) Debt Ceiling - July 17, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal 1\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposability\nVoted yes. The recent Harbor Trade transaction incident serves as a reminder of the complex dynamics of novel RWA deals and supporting structures. The proposal to lower the Debt Ceiling to zero prevent further exposure.\nSovfi\nVoted yes. The recent Harbor Trade transaction incident serves as a reminder of the complex dynamics of novel RWA deals and supporting structures. The proposal to lower the Debt Ceiling to zero prevent further exposure.\\nSummary of Votes on 25/07/23\nStability Scope Bootstrapping Edits - July 24, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposability\nVoted yes. MakerDAO’s proposed Enhanced Dai Savings Rate (EDSR) is a growth-oriented strategy aiming to boost Dai adoption and the DSR’s usage. The goal is to scale to 100 billion Dai in three years, inspired by tech industry strategies utilizing network effects. The proposal hinges on the “Endgame” plan involving gamification and tokenomics. The EDSR aims to prime the protocol for the Endgame Phase 1 launch, bolstered by the concept of SubDAO farming. Implementation safeguards include one-way activation, adjustability through executive votes, and potential deactivation based on community feedback, thus balancing growth with risk mitigation. The EDSR aligns with MakerDAO’s vision, providing critical data for refining future strategies.\nSovFi\nVoted yes. MakerDAO’s proposed Enhanced Dai Savings Rate (EDSR) is a growth-oriented strategy aiming to boost Dai adoption and the DSR’s usage. The goal is to scale to 100 billion Dai in three years, inspired by tech industry strategies utilizing network effects. The proposal hinges on the “Endgame” plan involving gamification and tokenomics. The EDSR aims to prime the protocol for the Endgame Phase 1 launch, bolstered by the concept of SubDAO farming. Implementation safeguards include one-way activation, adjustability through executive votes, and potential deactivation based on community feedback, thus balancing growth with risk mitigation. The EDSR aligns with MakerDAO’s vision, providing critical data for refining future strategies.\nNew Silver (RWA002-A) Restructuring and Parameter Changes - July 24, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposability\nVoted yes. New Silver’s special purpose vehicle, NS Pool LLC’s DROP tokens merge RWA with the crypto economy. While providing potential high yields, the tokens rely on the performance of underlying assets, carrying risks such as default or underperformance. Automated transactions via Tinlake Protocol highlight the importance of robust smart contracts, yet underscore vulnerabilities to malfunctions and cyber-attacks (among other risks highlighed in the risk and legal assessment). Embracing cryptos censorship resistance principle, NS Pool LLC’s operations must nonetheless adhere to local and federal laws, which could impose financial strain.\nSovFi\nVoted yes. New Silver’s special purpose vehicle, NS Pool LLC’s DROP tokens merge RWA with the crypto economy. While providing potential high yields, the tokens rely on the performance of underlying assets, carrying risks such as default or underperformance. Automated transactions via Tinlake Protocol highlight the importance of robust smart contracts, yet underscore vulnerabilities to malfunctions and cyber-attacks (among other risks highlighed in the risk and legal assessment). Embracing cryptos censorship resistance principle, NS Pool LLC’s operations must nonetheless adhere to local and federal laws, which could impose financial strain.\nAdjust Spark Protocol D3M Debt Ceiling - July 24, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposability\nVoted yes. The Spark Protocol proposes raising the DAI D3M Debt Ceiling from 20m to 200m DAI, increasing lending capacity while potentially elevating systemic risk in market downturns. Meanwhile, shifting the DAI interest strategy from tracking the DAI Savings Rate to the ETH-C stability fee aims for stability and aligns with MakerDAO community’s direction. This shift could inflate borrowing utilization rates due to yield farming opportunities, possibly distorting market demand representation. Recognizing these changes’ potential risks, adaptations in the ETH borrow rate slope or base rate parameter could maintain system equilibrium. These proactive responses highlight the criticality of dynamic protocol development in the evolving DeFi environment.\nSovFi\nVoted yes. The Spark Protocol proposes raising the DAI D3M Debt Ceiling from 20m to 200m DAI, increasing lending capacity while potentially elevating systemic risk in market downturns. Meanwhile, shifting the DAI interest strategy from tracking the DAI Savings Rate to the ETH-C stability fee aims for stability and aligns with MakerDAO community’s direction. This shift could inflate borrowing utilization rates due to yield farming opportunities, possibly distorting market demand representation. Recognizing these changes’ potential risks, adaptations in the ETH borrow rate slope or base rate parameter could maintain system equilibrium. These proactive responses highlight the criticality of dynamic protocol development in the evolving DeFi environment.\nSpark Protocol WETH and DAI Market Parameter Changes - July 24, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal 1\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposability\nVoted yes. Phoenix Labs proposed changes to Spark Protocol’s DAI and WETH markets, aimed at reducing financial risks and enhancing user experience. By setting the DAI Market Loan-to-Value (LTV) and Liquidation Threshold (LT) to 0.01%, Phoenix Labs aims to limit DAI as collateral, reducing the likelihood of full utilization and subsequent liquidation risks. For the WETH market, the decrease in Reserve Factor to 5% promises to boost the Annual Percentage Yield (APY) for suppliers while promoting healthier utilization rates. Although the proposed changes are validated by BA Labs, they caution about the possibility of “inorganic borrowing utilization” and a potential spike in staked ETH yields. While these changes epitomize the constant evolution of the DeFi landscape, they should be handled with careful community consensus and monitoring.\nSovFi\nVoted yes. Phoenix Labs proposed changes to Spark Protocol’s DAI and WETH markets, aimed at reducing financial risks and enhancing user experience. By setting the DAI Market Loan-to-Value (LTV) and Liquidation Threshold (LT) to 0.01%, Phoenix Labs aims to limit DAI as collateral, reducing the likelihood of full utilization and subsequent liquidation risks. For the WETH market, the decrease in Reserve Factor to 5% promises to boost the Annual Percentage Yield (APY) for suppliers while promoting healthier utilization rates. Although the proposed changes are validated by BA Labs, they caution about the possibility of “inorganic borrowing utilization” and a potential spike in staked ETH yields. While these changes epitomize the constant evolution of the DeFi landscape, they should be handled with careful community consensus and monitoring.\\nEnhanced Dai Savings Rate Activation, Spark Protocol Debt Ceiling Increase, RWA Vault Updates, AVC Member Compensation for Q2 2023, DAO Resolution for Monetalis Clydesdale, Launch Project Funding, Spark Proxy Spell Execution - August 2, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal 1\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposable and ReFi AVC: votes yes. Nothing in the vote was unexpected.\\nBLUE\nSummary of Votes on 8/8/23\nSmart Burn Engine Parameter Update - August 7, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposability\nVoted yes. The proposed changes to MakerDAO’s Smart Burn Engine include recalibrations to optimize liquidity, control slippage, and reduce costs. The growth rate in the Uniswap v2 DAI/MKR pool and alignment with MIP104 drive these modifications, with potential positive impacts on market stability, attack prevention, and transaction efficiency.\nSovFi\nVoted yes. The proposed changes to MakerDAO’s Smart Burn Engine include recalibrations to optimize liquidity, control slippage, and reduce costs. The growth rate in the Uniswap v2 DAI/MKR pool and alignment with MIP104 drive these modifications, with potential positive impacts on market stability, attack prevention, and transaction efficiency.\nNon-Scope Defined Parameter Changes - wstETH-B DC-IAM Changes - August 7, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposability\nVoted yes. The proposed changes to the WSTETH-A & B vaults include increasing the Target Available Debt for WSTETH-B and reducing the Ceiling Increase Cooldown, effectively doubling WSTETH-B’s daily DAI minting throughput to 90M. These changes enhance flexibility for vault users, balancing increased accessibility with potential market risks. WSTETH-B’s higher collateralization requirements act as a safeguard, and the adjustments align with the needs of the Enhanced DAI Savings Rate. The current ambiguity in MIP104’s language underscores the complexity of governance and risk management in the decentralized financial ecosystem. We will address this ambiguity on the AVC meetings focusing on the Stability scope in three weeks.\nSovFi\nVoted yes. The proposed changes to the WSTETH-A & B vaults include increasing the Target Available Debt for WSTETH-B and reducing the Ceiling Increase Cooldown, effectively doubling WSTETH-B’s daily DAI minting throughput to 90M. These changes enhance flexibility for vault users, balancing increased accessibility with potential market risks. WSTETH-B’s higher collateralization requirements act as a safeguard, and the adjustments align with the needs of the Enhanced DAI Savings Rate. The current ambiguity in MIP104’s language underscores the complexity of governance and risk management in the decentralized financial ecosystem. We will address this ambiguity on the AVC meetings focusing on the Stability scope in three weeks.\\nSummary of Votes on 15/08/23\nStability Scope Bootstrapping Edits (EDSR Changes) - August 14, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposability\nVoted yes. The rapid introduction of EDSR led to a significant spike in DAI supply, revealing the delicate balance between financial incentives and market demands. A key reaction was the rush of large capital inflows, enticed by high yields. This was coupled with the unexpected “borrow arbitrage” behavior, where whales exploited the rate differentials, overshadowing regular DAI users. Immediate adjustments included capping EDSR to 5%, revising utilization tiers, and tying crypto borrow rates to EDSR. While this curtailed the arbitrage window, it risked unsettling regular vault users and compelled Spark Protocol to incentivize via retroactive token farming. The strategic emphasis shifted to fostering genuine DAI demand, particularly in light of aggressive borrow arbitrage and potential fee repercussions. The importance of staked ETH borrowers and the lure of token farming in Spark Protocol were accentuated, showcasing the need to reward early and dedicated users. This episode exemplifies DeFi’s volatility and the crucialness of agile decision-making.\nSovFi\nVoted yes. The rapid introduction of EDSR led to a significant spike in DAI supply, revealing the delicate balance between financial incentives and market demands. A key reaction was the rush of large capital inflows, enticed by high yields. This was coupled with the unexpected “borrow arbitrage” behavior, where whales exploited the rate differentials, overshadowing regular DAI users. Immediate adjustments included capping EDSR to 5%, revising utilization tiers, and tying crypto borrow rates to EDSR. While this curtailed the arbitrage window, it risked unsettling regular vault users and compelled Spark Protocol to incentivize via retroactive token farming. The strategic emphasis shifted to fostering genuine DAI demand, particularly in light of aggressive borrow arbitrage and potential fee repercussions. The importance of staked ETH borrowers and the lure of token farming in Spark Protocol were accentuated, showcasing the need to reward early and dedicated users. This episode exemplifies DeFi’s volatility and the crucialness of agile decision-making.\\nSummary of Votes on 18/08/23\nEDSR Adjustment, Vault and Smart Burn Engine Parameter Updates, CRVV1ETHSTETH-A Offboarding, Delegate Compensation, and Other Changes - August 17, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposability and SovFi\nVoted yes. Aligned with the supported pools and the Atlas.\nNothing suspecious observed when reviewing it the spell.\\nSummary of Votes on 22/08/23\nReserve Governance Facilitator Appointment - August 21, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposability\nVoted on LeBateleur, VoteWizard and JanSky. DAOMasons did not receive our support, since they are doxxed and we interpret this as being non compliant since the facilitator alignment conserver role that becomes active when facilitatordaos come to life, must be anon.\nSovFi\nVoted on LeBateleur, VoteWizard and JanSky. DAOMasons did not receive our support, since they are doxxed and we interpret this as being non compliant since the facilitator alignment conserver role that becomes active when facilitatordaos come to life, must be anon.\nIncrease Spark Protocol wstETH Supply Cap - August 21, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposability\nVoted yes. The Spark Lend protocol contemplates doubling the wstETH market supply cap from 200,000 to 400,000 wstETH. This move addresses the saturated lending pool, offering more room for deposits and signaling protocol robustness. Safety mechanisms, such as a 7% liquidation penalty and a high liquidation threshold of 79.5%, are designed to deter risks. Additionally, the DSR mechanism offers a safety cushion against drastic market fluctuations. The broader market implications might mean a boost in protocol activity, with a potential shift in liquidity dynamics across other protocols. Notable endorsements from BA Labs, favor the cap increment.\nSovFi\nVoted yes. The Spark Lend protocol contemplates doubling the wstETH market supply cap from 200,000 to 400,000 wstETH. This move addresses the saturated lending pool, offering more room for deposits and signaling protocol robustness. Safety mechanisms, such as a 7% liquidation penalty and a high liquidation threshold of 79.5%, are designed to deter risks. Additionally, the DSR mechanism offers a safety cushion against drastic market fluctuations. The broader market implications might mean a boost in protocol activity, with a potential shift in liquidity dynamics across other protocols. Notable endorsements from BA Labs, favor the cap increment.\nSpark Protocol ETH Market Parameter Changes - August 21, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposability\nVoted yes. Spark Lend’s proposed tweaks to its ETH market focus on refining interest rate models, similar to Aave. The adjustments involve reducing variableRateSlope1 from 3% to 2.8%, increasing optimal utilization from 80% to 90%, and escalating variableRateSlope2 from 80% to 120%. These shifts aim to better market efficiency by offering enhanced supplier rates and safeguarding against high utilization levels. For instance, a higher optimal utilization can drive Spark’s ETH supply rate from 2.06% to 2.48%. However, considerations arise regarding market liquidity, especially with large wallets that might skew balance. A significant price drop might strain ETH liquidity.\nSovFi\nVoted yes. Spark Lend’s proposed tweaks to its ETH market focus on refining interest rate models, similar to Aave. The adjustments involve reducing variableRateSlope1 from 3% to 2.8%, increasing optimal utilization from 80% to 90%, and escalating variableRateSlope2 from 80% to 120%. These shifts aim to better market efficiency by offering enhanced supplier rates and safeguarding against high utilization levels. For instance, a higher optimal utilization can drive Spark’s ETH supply rate from 2.06% to 2.48%. However, considerations arise regarding market liquidity, especially with large wallets that might skew balance. A significant price drop might strain ETH liquidity.\nRatification Poll for MIP Amendment Subproposal (MIP102c2-SP11) - August 14, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposability\nVoted no. The MakerDAO subproposal strives to ensure the continuity of the MIP64 bug bounty program via the Scopes framework. However, based on the recent forum post by Jetstream, the Endgame launch project suggests a different approach. Despite MIP64’s merits, a more attuned, imminent solution suggests its redundancy. The core is clear: a resilient bug bounty program is pivotal, but its evolution must mirror MakerDAO’s broader aspirations.\nSovFi\nVoted no. The MakerDAO subproposal strives to ensure the continuity of the MIP64 bug bounty program via the Scopes framework. However, based on the recent forum post by Jetstream, the Endgame launch project suggests a different approach. Despite MIP64’s merits, a more attuned, imminent solution suggests its redundancy. The core is clear: a resilient bug bounty program is pivotal, but its evolution must mirror MakerDAO’s broader aspirations.\nRatification Poll for MIP Amendment Subproposals - Article 1 Edits (MIP102c2-SP12) - August 14, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposability\nVoted yes. The revision will modify Article 1 under the Governance, Support, and Protocol scope. This will provide a more detailed description of the necessary skill set for AC members, outline the assessment of these skills, and offer clarity to assist governance facilitators in onboarding them, covering aspects such as duration, budget, and contracts, etc.\nSovFi\nVoted yes. The revision will modify Article 1 under the Governance, Support, and Protocol scope. This will provide a more detailed description of the necessary skill set for AC members, outline the assessment of these skills, and offer clarity to assist governance facilitators in onboarding them, covering aspects such as duration, budget, and contracts, etc.\nRatification Poll for MIP Amendment Subproposal (MIP102c2-SP13) - August 14, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal 1\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposability\nVoted yes. The AD privacy requirements has been changed to AD OpSec requirements, amending the Stability Scope for Base Rate, DSR Spread, and ALM tiers, makes updates to new token denominations, changes the Accessibility scope by increasing the Launch budget (and resetting it), finally changes are made to the Support scope where Spark becomes a predetermined brand for SubDAO TWO. Amendments are aligned with the Endgame Plan, granting further clarification.\nSovFi\nVoted yes. The AD privacy requirements has been changed to AD OpSec requirements, amending the Stability Scope for Base Rate, DSR Spread, and ALM tiers, makes updates to new token denominations, changes the Accessibility scope by increasing the Launch budget (and resetting it), finally changes are made to the Support scope where Spark becomes a predetermined brand for SubDAO TWO. Amendments are aligned with the Endgame Plan, granting further clarification.\\nSummary of Votes on 30/08/23\nApprove DAO Resolution Pertaining to HV Bank (RWA009-A) - August 28, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposability\nVoted yes. The decision to approve the DAO Resolution to terminate future purchases and return available cash from the HVB Master Participation Trust aligns with MIP104, preparing for NewChain’s launch. This safeguards DAI’s stability and ensures the return of surplus DAO funds.\nSovFi\nVoted yes. The decision to approve the DAO Resolution to terminate future purchases and return available cash from the HVB Master Participation Trust aligns with MIP104, preparing for NewChain’s launch. This safeguards DAI’s stability and ensures the return of surplus DAO funds.\nDecrease Fortunafi (RWA005-A) Debt Ceiling - August 28, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposability\nVoted yes. The decision to cut Fortunafi’s debt ceiling to zero aligns with MIP104, preparing for NewChain’s launch.\nSovFi\nVoted yes. The decision to cut Fortunafi’s debt ceiling to zero aligns with MIP104, preparing for NewChain’s launch.\\nSummary of Votes on 31/08/23\n Management of ConsolFreight (RWA003-A) Default, ESM Authorization, Chainlog Updates, Budget Management, Spark Proxy Spell - August 30, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposability and SovFi\nVoted yes.\nAligned with the supported pools and the Atlas.\nNothing suspicious observed when reviewing the spell.\\nSummary of Votes on 05/09/23\nAdjust Spark Protocol D3M Parameters - September 4, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposability\nVoted yes. Increasing the Debt Ceiling from 200 million DAI to 400 million DAI caters to the maxed-out DAI borrow market, aiming for broader borrower inclusion. Present market conditions, especially the near-full utilization over 24 days, validate the urgency of these modifications.\nMeanwhile, extending the Ceiling Increase Cooldown from 8 to 12 hours is a strategic risk mitigation.\nSafeguards like the Direct Deposit Breaker offer governance a timely intervention during crises, mirroring practices at Maker Core. As SparkLend develops its risk model, it leans towards quantitative decision-making, promising empirical risk assessment, but we are not there yet. Overall, the proposed modifications resonate with Makers evolving plans towards the endgame.\nSovFi\nVoted yes. Voted yes. Increasing the Debt Ceiling from 200 million DAI to 400 million DAI caters to the maxed-out DAI borrow market, aiming for broader borrower inclusion. Present market conditions, especially the near-full utilization over 24 days, validate the urgency of these modifications.\nMeanwhile, extending the Ceiling Increase Cooldown from 8 to 12 hours is a strategic risk mitigation.\nSafeguards like the Direct Deposit Breaker offer governance a timely intervention during crises, mirroring practices at Maker Core. As SparkLend develops its risk model, it leans towards quantitative decision-making, promising empirical risk assessment, but we are not there yet. Overall, the proposed modifications resonate with Makers evolving plans towards the endgame.\nAdjust Spark Protocol DAI Interest Rate Strategy Borrow Spread - September 4, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposability\nVoted yes. Adjusting DAI’s borrow spread from 0% to 0.5% aims to deter over-leveraging in airdrop farming. This move safeguards DAI’s stability while ensuring airdrop recipients are genuinely engaged.\nSovFi\nVoted yes. Adjusting DAI’s borrow spread from 0% to 0.5% aims to deter over-leveraging in airdrop farming. This move safeguards DAI’s stability while ensuring airdrop recipients are genuinely engaged.\nAdjust Spark Protocol Flash Loan Fee - September 4, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposability\nVoted yes. The proposed 0% fee, while seemingly cutting potential revenue, could actually amplify adoption and transaction volume. The pivot promotes financial arbitrage democratization. However, the shift to a 0% model poses challenges, especially around smart contract integrity and precision. If adopted widely, this trend could pressurize other platforms to reevaluate their fees.\nSovFi\nVoted yes. The proposed 0% fee, while seemingly cutting potential revenue, could actually amplify adoption and transaction volume. The pivot promotes financial arbitrage democratization. However, the shift to a 0% model poses challenges, especially around smart contract integrity and precision. If adopted widely, this trend could pressurize other platforms to reevaluate their fees.\nJAT1 to JAT2 Asset Reallocation - September 4, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposability\nVoted yes. MakerDAO proposes reallocating assets from JAT1 to JAT2, marking a transition from diverse ETFs to a US treasury ladder system. Advantages include bi-weekly liquidity events and locked-in maturity payouts from the treasury, streamlining surplus predictions over six months. Associated costs, notably brokerage fees, could affect net yields over the year. Implementation details encompass liquidating JAT1, directing funds to JAT2, and dissolving JAT1.\nSovFi\nVoted yes. MakerDAO proposes reallocating assets from JAT1 to JAT2, marking a transition from diverse ETFs to a US treasury ladder system. Advantages include bi-weekly liquidity events and locked-in maturity payouts from the treasury, streamlining surplus predictions over six months. Associated costs, notably brokerage fees, could affect net yields over the year. Implementation details encompass liquidating JAT1, directing funds to JAT2, and dissolving JAT1.\\nSummary of Votes on 14/09/23\nStability Scope Parameter Changes, Spark Protocol D3M Parameter Changes, Set Fortunafi Debt Ceiling to Zero DAI, DAO Resolution for HV Bank, Delegate Compensation and Other Actions - September 13, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nMaker Governance - Governance Portal\nComposability and SovFi\nVoted yes.\nAligned with the supported pools and the Atlas.\nNothing suspicious observed when reviewing the spell.\\nSummary of Votes on 19/09/23\nActivate Gnosis Chain Instance of Spark Lend - September 18, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposability\nVoted yes. Setting up Spark on the Gnosis Chain requires a careful balance between having enough liquidity and managing risks properly. The plan focuses on starting with lower initial rates for selling off assets and constantly adjusting the LTV ratio while keeping a close eye on specific factors, such as changes in the ETH to USD market. Collaborating with others, like Agave, and involving the community in discussions are seen as important steps. However, there are concerns about possible liquidity issues and interruptions in the service which cast a shadow of doubt over the project. Despite the plan, the fact that the Gnosis Chain is still very new means that strategies will need to be flexible to maintain smooth operations and safety in a market that keeps changing.\nSovFi\nVoted yes. Setting up Spark on the Gnosis Chain requires a careful balance between having enough liquidity and managing risks properly. The plan focuses on starting with lower initial rates for selling off assets and constantly adjusting the LTV ratio while keeping a close eye on specific factors, such as changes in the ETH to USD market. Collaborating with others, like Agave, and involving the community in discussions are seen as important steps. However, there are concerns about possible liquidity issues and interruptions in the service which cast a shadow of doubt over the project. Despite the plan, the fact that the Gnosis Chain is still very new means that strategies will need to be flexible to maintain smooth operations and safety in a market that keeps changing.\nRatification Poll for MIP Amendment Subproposals - Article 1 Edits (MIP102c2-SP15) - September 11, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposability\nVoted yes. The revision will modify Article 1 under all five scopes. This will provide a more detailed description of the necessary skill set for AC members, outline the assessment of these skills, and offer clarity to assist governance facilitators in onboarding them, covering aspects such as duration, budget, and contracts, etc.\n(some of them supported back in August)\nSovfi\nVoted yes. The revision will modify Article 1 under all five scopes. This will provide a more detailed description of the necessary skill set for AC members, outline the assessment of these skills, and offer clarity to assist governance facilitators in onboarding them, covering aspects such as duration, budget, and contracts, etc.\n(some of them supported back in August)\nRatification Poll for MIP Amendment Subproposal (MIP102c2-SP16) - September 11, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal 1\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposability\nVoted yes. The proposal seeks to gear up for the Phase 1 of the Endgame by planning to update the Alignment Artifacts. Key updates include changing the base rate formula and decreasing the DSR spread to 0.25%, aiming to offer a better deal to Dai savers and to control speculative tendencies.\nThe shift in the ALM tiers and excluding crypto-backed exposures from ALM calculations is seen as a bid to smarten risk management. On the partnership front, a revenue-sharing agreement has been established between Spark Protocol and Aave, opening avenues for collaborative growth, although it needed a few tweaks to get things just right.\nAn essential update is the higher security standards set for facilitators, underlining MakerDAO’s commitment to ensuring a safe and trustworthy operational framework in the DeFi sector. These changes highlight MakerDAO’s strategic approach to secure and synergistic growth in the decentralized financial space as it moves towards a critical phase in its development.\nSovFi\nVoted yes. Voted yes. The proposal seeks to gear up for the Phase 1 of the Endgame by planning to update the Alignment Artifacts. Key updates include changing the base rate formula and decreasing the DSR spread to 0.25%, aiming to offer a better deal to Dai savers and to control speculative tendencies.\nThe shift in the ALM tiers and excluding crypto-backed exposures from ALM calculations is seen as a bid to smarten risk management. On the partnership front, a revenue-sharing agreement has been established between Spark Protocol and Aave, opening avenues for collaborative growth, although it needed a few tweaks to get things just right.\nAn essential update is the higher security standards set for facilitators, underlining MakerDAO’s commitment to ensuring a safe and trustworthy operational framework in the DeFi sector. These changes highlight MakerDAO’s strategic approach to secure and synergistic growth in the decentralized financial space as it moves towards a critical phase in its development.\nRatification Poll for MIP Amendment Subproposals (MIP102c2-SP4) - September 11, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposability\nVoted yes. Voted yes. The revised MIP104 details changes in Arranger’s reporting and fee structure. Stable assets, such as stablecoins, US government securities, or bank deposits, will have a lower fee cap of 10 basis points, compared to 15 for non-stable assets. The amendment dictates that Arrangers publish monthly reports and bi-annual stress tests that mimic historical financial crises to ensure robust risk management. Independent reviewers will validate these tests to maintain transparency and accuracy.\nSovFi\nVoted yes. Voted yes. The revised MIP104 details changes in Arranger’s reporting and fee structure. Stable assets, such as stablecoins, US government securities, or bank deposits, will have a lower fee cap of 10 basis points, compared to 15 for non-stable assets. The amendment dictates that Arrangers publish monthly reports and bi-annual stress tests that mimic historical financial crises to ensure robust risk management. Independent reviewers will validate these tests to maintain transparency and accuracy.\\nSummary of Votes on 27/09/23\nReconfiguring RWA Allocator Vaults - September 25, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nComposability\nVoted yes. The proposal to reactivate Clydesdale’s DC-IAM and adjust Andromeda and Clydesdale Vaults underscores their pivotal role in shaping the foundation for RWA collateral backing Dai. The adjustments play a critical role in balancing financial risk within the 78% limitation imposed by Stability Scope and ALM Tier 2 rules. The smart contracts are facilitating seamless execution of these technical configurations, while stablecoins’ symbiotic relationship with DeFi indicates a ripple effect in market dynamics.\nSovFi\nVoted yes. The proposal to reactivate Clydesdale’s DC-IAM and adjust Andromeda and Clydesdale Vaults underscores their pivotal role in shaping the foundation for RWA collateral backing Dai. The adjustments play a critical role in balancing financial risk within the 78% limitation imposed by Stability Scope and ALM Tier 2 rules. The smart contracts are facilitating seamless execution of these technical configurations, while stablecoins’ symbiotic relationship with DeFi indicates a ripple effect in market dynamics.\\nSevering Composable GSL\nAD Recognition Submission\n0xEdc6c08960F517F233E8b6f5A5A61b6B19834deF:Severed\nCryptographically signed AD Recognition Submission Message from the Ethereum address controlling Delegation Contract 1: Composable 5\n@votewizard , @JanSky Please see the established GSL to KISS as an edit to the main post.\n@LDF , @opensky , @stefdelev, collaborating with you over the last six months has been a true pleasure. Please do not take our decision to switch to KISS personally. We believe that our skills can be better utilized to deliver more value to the protocol and MKR holders through a GSL established with that AVC.\n@iammeeoh see you on Matrix\\nThanks for the heads up, @BLUE. I was already drafting a post on how to transition from one AVC to another, anticipating your change.\nThe process you followed is very similar to the one that I’ve sent to other ADs privately. I’ll update this change in the portal shortly\\nWe are looking forward to that post to analyse it. If necessary we can work on a proposal to improve the Governance Scope to establishes how the transition should be in these cases.\\nSummary of Votes on 02/10/23\nDAO Resolution for Monetalis Clydesdale, HV Bank On-chain RWA Agreement Update, Fortunafi Vault Change, Trigger Spark Protocol Proxy Spell - September 27, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nKISS and SovFi\n(no MKR delegated to KISS contract)\nVoted yes.\nAligned with the supported pools and the Atlas.\nNothing suspicious observed when reviewing the spell.\\nthank you for the very good collaboration. It was a pleasure\\nSummary of Votes on 4/10/23\n(No MKR delegated to KISS contract)\nNon-Scope Defined Parameter Changes - WBTC DC-IAM Changes - October 2, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nSovFi\nVoted yes. Alterations in the Enhanced Dai Savings Rate (EDSR) and both native and non-native Vault collateral parameters exemplify a meticulous navigation through incentivizing DAI stability and preempting economic perturbations within the ecosystem. Strategic evolutions, including the SF hikes for ETH-A/B and WBTC-A, the prudent Debt Ceiling Increase Amount (GAP) for WBTC, and the methodical offboarding of RETH-A, all harmonize towards an implicit commitment to fortifying platform robustness, mitigating systemic risk, while attentively considering user engagement and experience.\nKISS\nVoted yes. Alterations in the Enhanced Dai Savings Rate (EDSR) and both native and non-native Vault collateral parameters exemplify a meticulous navigation through incentivizing DAI stability and preempting economic perturbations within the ecosystem. Strategic evolutions, including the SF hikes for ETH-A/B and WBTC-A, the prudent Debt Ceiling Increase Amount (GAP) for WBTC, and the methodical offboarding of RETH-A, all harmonize towards an implicit commitment to fortifying platform robustness, mitigating systemic risk, while attentively considering user engagement and experience.\nIncrease rETH Supply Cap to 60k - October 2, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nSovFi\nVoted yes. The proposal to raise rETH supply cap on SparkLend Ethereum to 60k aims to enhance liquidity but may risk market oversaturation affecting rETH price. Noteworthy is the risk management strategy, Siloed Borrowing, to shield from misleading oracles. Past similar cap changes hint at improved liquidity and user engagement.  The technical setup, especially the Spark spell mechanism, is well-planned but not without typical DeFi smart contract risks.\nKISS\nVoted yes. The proposal to raise rETH supply cap on SparkLend Ethereum to 60k aims to enhance liquidity but may risk market oversaturation affecting rETH price. Noteworthy is the risk management strategy, Siloed Borrowing, to shield from misleading oracles. Past similar cap changes hint at improved liquidity and user engagement.  The technical setup, especially the Spark spell mechanism, is well-planned but not without typical DeFi smart contract risks.\nOnboard USDC to SparkLend Ethereum and Activate USD eMode for USDC and sDAI Markets - October 2, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nSovFi\nVoted yes. The introduction of USDC into Phoenix Labs’ SparkLend Ethereum platform intertwines a stable monetary value with decentralized finance, presenting a user-attuned, borrow-only market mechanism, while potentially impacting the Demand for DSR. This intersectionality of financial elements embodies a spectrum of financial risk and stability, navigating through historical echoes of stablecoin challenges and inviting a future tempered by governance-informed optimism. The activation of USD Efficiency Mode (eMode) for USDC and synthetic DAI (sDAI) markets, coupled with parameters like a 91% LTV and a 1% liquidation penalty, harmonizes with dynamic interest rate models and an optimal utilization rate set at 95%. In this blend, “Spark Spell” smart contracts enable automated adjustments in supply caps and asset onboardings, though the intricate nature of stablecoin markets and governance demands a prudently balanced approach to navigate the financial landscape, skillfully mitigating risks and capitalizing on decentralized financial advancements.\nKISS\nVoted yes. The introduction of USDC into Phoenix Labs’ SparkLend Ethereum platform intertwines a stable monetary value with decentralized finance, presenting a user-attuned, borrow-only market mechanism, while potentially impacting the Demand for DSR. This intersectionality of financial elements embodies a spectrum of financial risk and stability, navigating through historical echoes of stablecoin challenges and inviting a future tempered by governance-informed optimism. The activation of USD Efficiency Mode (eMode) for USDC and synthetic DAI (sDAI) markets, coupled with parameters like a 91% LTV and a 1% liquidation penalty, harmonizes with dynamic interest rate models and an optimal utilization rate set at 95%. In this blend, “Spark Spell” smart contracts enable automated adjustments in supply caps and asset onboardings, though the intricate nature of stablecoin markets and governance demands a prudently balanced approach to navigate the financial landscape, skillfully mitigating risks and capitalizing on decentralized financial advancements.\nOnboard USDT to SparkLend Ethereum and Activate USD eMode - October 2, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nSovFi\nVoted yes. Integrating USDT into SparkLend’s Ethereum framework seeks to strategically navigate through the multifaceted stablecoin functionality and its associated risks. The proposal adopts a borrow-only market approach and activates USD eMode, intending to boost DAI demand while also addressing non-DAI users. While the initiative involves exploring interest rate dynamics and DSR implications, it also encounters potential issues related to risk and return.\nKISS\nVoted yes. Integrating USDT into SparkLend’s Ethereum framework seeks to strategically navigate through the multifaceted stablecoin functionality and its associated risks. The proposal adopts a borrow-only market approach and activates USD eMode, intending to boost DAI demand while also addressing non-DAI users. While the initiative involves exploring interest rate dynamics and DSR implications, it also encounters potential issues related to risk and return.\\nSummary of Votes on 17/10/23\nUSDP-PSM incentives, rETH initial offboarding, RWA vaults reconfiguration, various parameter changes, AVC and AD compensation, Facilitator and Ecosystem Actor compensation, Spark proxy-spell - October 11, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nKISS and SovFi\nVoted yes. (no MKR delegated to KISS contract)\nAligned with the supported pools and the Atlas.\nNothing suspicious observed when reviewing the spell.\\nSummary of Votes on 7/11/23\nSpark Protocol-Aave Revenue Share Payment, DAO Resolution for HV Bank, Immunefi Security Core Unit MKR Vesting Transfer and Chainlog Housekeeping - November 1, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nKISS and SovFi\nVoted yes. (no MKR delegated to KISS contract)\nAligned with the Endgame plan, supported pools and the Atlas.\nNothing suspicious observed when reviewing the spell.\\nSummary of Votes on 9/11/23\nApprove DAO Resolution to Onboard Mars Foundation - November 6, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nKISS\nVoted yes (no MKR delegated to contract). The integration of the Mars Foundation signifies a strategic decision in the Endgame Plan, focusing on enhancing user accessibility and experience through the deployment of front-ends like Spark FE and Sakura FE. In additon its used for storing IP.\nStrictly defined boundaries and and human oversight in governing the foundation is essential for confidence.\nSovFi\nVoted yes. The integration of the Mars Foundation signifies a strategic decision in the Endgame Plan, focusing on enhancing user accessibility and experience through the deployment of front-ends like Spark FE and Sakura FE. In additon its used for storing IP.\nStrictly defined boundaries and and human oversight in governing the foundation is essential for confidence.\nSparkLend Gnosis Chain - Increase wstETH Supply Cap to 10,000 wstETH - November 6, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nKISS\nVoted yes (no MKR delegated to contract). Phoenix Labs’ proposal to increase wstETH supply cap on Gnosis Chain is a strategic response to market demands and protocol growth. The initial conservative cap, set for stability post-launch, has been frequently reached, indicating robust user interest and market health. Further risk analysis shows substantial liquidity growth in DEX and Balancer pools, suggesting a ready market for larger liquidations, essential for higher cap implementation. This proposal, while advancing Spark’s adoption on Gnosis Chain, must consider continuous market and risk monitoring, ensuring future cap adjustments align with evolving market dynamics. The proposal signifies a careful balance between demand-driven growth and risk management.\nSovFi\nVoted yes. Phoenix Labs’ proposal to increase wstETH supply cap on Gnosis Chain is a strategic response to market demands and protocol growth. The initial conservative cap, set for stability post-launch, has been frequently reached, indicating robust user interest and market health. Further risk analysis shows substantial liquidity growth in DEX and Balancer pools, suggesting a ready market for larger liquidations, essential for higher cap implementation. This proposal, while advancing Spark’s adoption on Gnosis Chain, must consider continuous market and risk monitoring, ensuring future cap adjustments align with evolving market dynamics.\nSparkLend Ethereum - Set DAI Market Maximum Loan-to-Value to Zero Percent - November 6, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nKISS\nVoted yes (no MKR delegated to contract). Phoenix Labs’ proposal to reduce the LTV of the DAI market on Ethereum’s SparkLend to 0% is aimed at mitigating financial risks and ensuring stability in the market. By eliminating the use of DAI as collateral, the proposal addresses potential liquidity issues and market saturation. This change signifies a shift towards more risk-averse strategies, focusing on market resilience. The implementation of this policy through smart contracts underscores the importance of automated compliance and trust in DeFi. While it restricts the utility of DAI as leverage, the presence of the sDAI market as an alternative collateral option provides flexibility.\nSovFi\nVoted yes. Phoenix Labs’ proposal to reduce the LTV of the DAI market on Ethereum’s SparkLend to 0% is aimed at mitigating financial risks and ensuring stability in the market. By eliminating the use of DAI as collateral, the proposal addresses potential liquidity issues and market saturation. This change signifies a shift towards more risk-averse strategies, focusing on market resilience. The implementation of this policy through smart contracts underscores the importance of automated compliance and trust in DeFi. While it restricts the utility of DAI as leverage, the presence of the sDAI market as an alternative collateral option provides flexibility.\nSparkLend Ethereum - Reactivate WBTC and Optimize Parameters for Current Market Conditions - November 6, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nKISS\nVoted yes (no MKR delegated to contract). The proposal to reactivate WBTC on SparkLend Ethereum by Phoenix Labs entails setting parameters like a 70% Maximum LTV and a 75% Liquidation Threshold to balance opportunity and risk. The WBTC market, known for its liquidity and role in lending protocols, introduces additional risks such as custody, misappropriation, and business continuity, all linked to its custodian, Bitgo. These risks necessitate stringent risk mitigation strategies, including conservative parameter settings and continuous market monitoring. The integration of WBTC impacts MakerDAO’s risk profile, particularly concerning its surplus buffer and the stability of DAI.\nSovFi\nVoted yes. The proposal to reactivate WBTC on SparkLend Ethereum by Phoenix Labs entails setting parameters like a 70% Maximum LTV and a 75% Liquidation Threshold to balance opportunity and risk. The WBTC market, known for its liquidity and role in lending protocols, introduces additional risks such as custody, misappropriation, and business continuity, all linked to its custodian, Bitgo. These risks necessitate stringent risk mitigation strategies, including conservative parameter settings and continuous market monitoring. The integration of WBTC impacts MakerDAO’s risk profile, particularly concerning its surplus buffer and the stability of DAI.\nSparkLend Ethereum - Adjust Spark Protocol D3M Maximum Debt Ceiling - November 6, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nKISS\nVoted yes (no MKR delegated to contract). Phoenix Labs’ proposal to increase the DAI D3M debt ceiling on SparkLend Ethereum from 400 million to 800 million DAI, during MakerDAO’s one-month governance break, reflects a strategic approach to maintaining protocol growth and stability. This adjustment aims to support Spark’s burgeoning user base and demand, evidenced by recent growth trends. The inclusion of new collateral assets like WBTC and RETH is anticipated to further drive user migration and acquisition, offering more competitive borrowing costs and collateralization requirements compared to Maker core vaults. Risk analysis of current borrowing positions on Spark shows a conservative profile with a strong collateral backing, indicating a well-hedged market against potential downturns. The proposal aligns with MakerDAO’s objectives of promoting stability in the DeFi ecosystem and highlights the importance of smart contracts in ensuring automated, transparent governance decisions. This proactive measure is pivotal in steering MakerDAO through anticipated growth, while maintaining its core principles of stability and decentralization.\nSovFi\nVoted yes. Phoenix Labs’ proposal to increase the DAI D3M debt ceiling on SparkLend Ethereum from 400 million to 800 million DAI, during MakerDAO’s one-month governance break, reflects a strategic approach to maintaining protocol growth and stability. This adjustment aims to support Spark’s burgeoning user base and demand, evidenced by recent growth trends. The inclusion of new collateral assets like WBTC and RETH is anticipated to further drive user migration and acquisition, offering more competitive borrowing costs and collateralization requirements compared to Maker core vaults. Risk analysis of current borrowing positions on Spark shows a conservative profile with a strong collateral backing, indicating a well-hedged market against potential downturns. The proposal aligns with MakerDAO’s objectives of promoting stability in the DeFi ecosystem and highlights the importance of smart contracts in ensuring automated, transparent governance decisions. This proactive measure is pivotal in steering MakerDAO through anticipated growth, while maintaining its core principles of stability and decentralization.\nSparkLend Ethereum - Increase rETH & wstETH Supply Caps - November 6, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nKISS\nVoted yes (no MKR delegated to contract). Phoenix Labs’ proposal to increase rETH and wstETH supply caps in MakerDAO’s ecosystem addresses user growth and market trends. The raise from 60,000 to 80,000 for rETH and 400,000 to 800,000 for wstETH aims to support leveraged staking strategies and liquidity.\nSovFi\nVoted yes. Voted yes (no MKR delegated to contract). Phoenix Labs’ proposal to increase rETH and wstETH supply caps in MakerDAO’s ecosystem addresses user growth and market trends. The raise from 60,000 to 80,000 for rETH and 400,000 to 800,000 for wstETH aims to support leveraged staking strategies and liquidity.\nSparkLend Ethereum & Gnosis Chain - Adjust ETH Market Interest Rate Models - November 6, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nKISS\nVoted yes (no MKR delegated to contract). Phoenix Labs’ proposal to modify ETH interest rates on SparkLend Ethereum and Gnosis Chain is a strategic move in response to the decline in staked ETH rates. Suggested adjustments - a 0% base rate, 3.2% optimal rate, and a 123.2% maximum rate - are designed to realign borrowing costs with market conditions, potentially boosting both borrower appeal and supplier revenue.  The proposal considers market equilibrium, weighing the benefits of high supply rates against the risks of high utilization and volatility. Additionally, it acknowledges the possible impacts on ETH market liquidity, especially during market fluctuations.\nSovFi\nVoted yes.  Phoenix Labs’ proposal to modify ETH interest rates on SparkLend Ethereum and Gnosis Chain is a strategic move in response to the decline in staked ETH rates. Suggested adjustments - a 0% base rate, 3.2% optimal rate, and a 123.2% maximum rate - are designed to realign borrowing costs with market conditions, potentially boosting both borrower appeal and supplier revenue.  The proposal considers market equilibrium, weighing the benefits of high supply rates against the risks of high utilization and volatility. Additionally, it acknowledges the possible impacts on ETH market liquidity, especially during market fluctuations.\\nSummary of Votes on 19/11/23\nSpark Proxy Spell, Increase Spark Lend Maximum Debt Ceiling, Launch Project Funding, Approve Updates to the HVBank Facility, Whistleblower Bounty, October 2023 Delegate Compensation - November 15, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nKISS and SovFi\nVoted yes. (no MKR delegated to KISS contract)\nAligned with the supported pools and the Atlas.\nNothing suspicious observed when reviewing the spell.\\nSummary of Votes on 23/11/23\nRatification Poll for MIP Amendment Subproposals - Article 1 Edits (MIP102c2-SP17) - November 13, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nKISS\n(no MKR delegated to this contract)\nWould have voted yes. As a delegate following KISS, we obviously support this.\nSovFi\nVoted yes. The proposal has SovFi’s support because assigning specific tasks to the Advisory Council will clarify their responsibilities.\nRatification Poll for MIP Amendment Subproposal (MIP102c2-SP18) - November 13, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nKISS\n(no MKR delegated to this contract)\nWould have voted no. This proposal is in conflict with the Endgame proposal, which is up for vote in the same monthly cycle. This proposal specifically states: “If other MIP102c2 amendments are being voted upon in the same monthly cycle as MIP102c2-SP18, then MIP102c2-SP18 overwrites any other amendments passed”. Consequently, this led to our decision to vote against it.\nSovFi\nVoted no. This proposal is in conflict with the Endgame proposal, which is up for vote in the same monthly cycle. This proposal specifically states: “If other MIP102c2 amendments are being voted upon in the same monthly cycle as MIP102c2-SP18, then MIP102c2-SP18 overwrites any other amendments passed”. Consequently, this led to our decision to vote against it.\nRatification Poll for MIP Amendment Subproposal (MIP102c2-SP20) - November 13, 2023\n\n  \n\n      vote.makerdao.com\n  \n\n  \n    \n\nMaker Governance - Governance Portal\n\n  The MakerDAO Governance Portal allows for anyone to view governance proposals, and also allows for MKR holders to vote.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nKISS\n(no MKR delegated to this contract)\nWould have voted yes. The proposal, which supports the Endgame, includes a shift in focus for the Alignment Conservers to ensure that Stage 2, the GAIT work, is completed.\nSovFi\nVoted yes. The proposal, which supports the Endgame, includes a shift in focus for the Alignment Conservers to ensure that Stage 2, the GAIT work, is completed."
  },
  {
    "number_of_comments": 22,
    "postid": "38ddb882-c9d4-4ef0-9932-463e78c4e9e8",
    "posturl": "https://www.comp.xyz/t/proposal-adjust-eth-interest-rate-model/3493",
    "combinedcontent": "\nOverview\nThis proposal adjusts parameters for the Compound ETH market. Specifically, this proposal would implement a borrow cap and update the interest rate model to a new jump rate model featuring a much higher maximum borrow rate to ensure continuous withdrawal liquidity for Compound ETH collateral.\n\nMotivation\nThe upcoming Ethereum merge upgrade is expected to be accompanied by at least one ETH fork that maintains POW consensus. The expectation of forks could have a significant impact on the defi space. See the following post on the Maker forum for an overview of potential impacts on the defi space.\n\n  \n      \n\n      The Maker Forum – 5 Aug 22\n  \n\n  \n    \n\nMerge Risks and Market Impacts 38\n\n  With the merge drawing closer and increased discussion about potential network fork(s), risk CU wanted to share an overview of the possible market effects and risks involved.   Background Ethereum is planning to merge the proof of stake (POS)...\n\n  \n    Reading time: 5 mins \uD83D\uDD51\n      Likes: 21 ❤\n  \n\n  \n\n  \n    \n    \n  \n\n  \n\n\nSpecially for Compound, the merge and prospect for fork(s) has the potential to significantly increase liquidity risk for cETH. The vast majority of non-ETH assets are likely to become worthless on fork chains, but forked ETH may retain some non-negligible share of value. In the first blocks after a fork, miners or other MEV operators will be able to extract all of the fork ETH value from certain defi liquidity pools (including Compound’s cETH as well as other lending or decentralized exchange pools) by supplying worthless stablecoins or other tokens. This incentivizes defi users to (1) withdraw ETH from pooled defi liquidity protocols, and (2) borrow any available ETH from lending pools before the fork, allowing them to retain the potential value of forked ETH tokens.\nIf the cETH market borrowing utilization grows excessively high, this can increase insolvency risk within the protocol by interfering with atomic liquidation transactions; liquidators would not be able to immediately withdraw ETH to repurchase assets they sold during liquidation. Compound’s utilization based interest rate models are designed to protect against this liquidity risk, but the dynamics of ETH fork airdrops (where significant value is distributed to ETH holders in a single block) could put the existing rate model under significant stress.\nThe proposed changes to interest rate model and borrow cap will penalize excessive utilization and borrowing in the cETH market. This should help reduce risk and maintain orderly liquidation mechanisms in the lead up to the merge.\n\nSpecification\nUpdate the cETH interest rate model to a new jump rate model with the following parameters:\n\nRate at 0% utilization: 2%\nOptimal utilization (kink point): 50%\nRate at optimal utilization: 50%\nRate at 100% utilization: 1000%\n\n(updated parameters based on feedback)\n\nRate at 0% utilization: 2%\nOptimal utilization (kink point): 80%\nRate at optimal utilization: 20%\nRate at 100% utilization: 1000%\n\nAn interest rate model contract will be deployed and submitted for community review before starting the on chain proposal.\nSet a borrow cap for the cETH market of 100,000 ETH.\n\nJustification of Parameter Changes\nWhy does the proposed new interest rate model include such high borrowing rates?\nThe proposed rate changes are benchmarked based on hypothetical delta neutral “fork farming” behaviors of borrowers and suppliers in the Compound cETH market. It is expected that any ETH remaining within the Compound money markets at the time of the merge and accompanying fork(s) will be unrecoverable to cToken owners due to MEV activity and widespread pool insolvency on the fork chain(s). Withdrawing or borrowing ETH before the merge provides an immediate gain based on the expected value of fork-ETH. Rational users are incentivized to borrow or withdraw ETH if the expected gain from receiving fork airdrop(s) exceeds borrowing costs or returns from supplying ETH to the market.\nWhile the current maximum borrow rate of ~14.5% is fairly high over long time periods, over shorter periods (days to weeks) it may not be sufficient to incentivize cETH market liquidity in light of potential fork windfalls. 14.5% annual rate corresponds to a weekly borrowing cost of only 0.26% (or daily cost of just 0.037%). With pre-market trading of POW-ETH (Poloniex spot 4 and BitMex futures 3), stETH discount 1, and futures basis indicating an expected fork value of roughly 2-4%, this level of borrowing cost is clearly insufficient to prevent the cETH market from reaching dangerous levels of utilization for prolonged periods.\nThe proposed maximum borrow rate of 1000% corresponds to a weekly cost 4.7% (daily cost of 0.66%), and corresponding supply rates at 100% utilization would be 800% per year (4.3% per week, 0.6% per day). These levels should be high enough to discourage full utilization of the cETH market (borrowings or withdrawals) until just before the fork block. Minimizing the expected amount of time when the cETH market is fully utilized will reduce risk of insolvent accounts or other market disruption.\nWhy is a relatively low borrow cap proposed?\nAs long as the total amount of ETH supplied remains above the proposed borrow cap, the cETH market will not become fully utilized and will avoid the greatest potential risks from disorderly liquidations. It is relatively simpler for users to open new cETH borrow positions vs existing cETH suppliers to withdraw collateral; new borrowings require as little as one single user with sufficient collateral assets, while reducing supplied assets involves action across many individual users, who may also have greater lock-in incentives due to outstanding collateralized loans. In summary, setting a borrow cap is likely to provide additional protection against cETH becoming excessively utilized due to naturally higher inertia of ETH suppliers vs borrowers.\nWhat impact will this have on current users?\nBased on the current market utilization rate of 3.3%, borrowing rate would increase from 2.7% to 5.2%, while supply rate would rise from 0.07% to 0.13%.\nGiven current total cETH supply of just over 500,000 ETH, the proposed 100,000 token borrow cap would limit utilization to roughly 20% of market liquidity (before accounting for any possible ETH withdrawals from Compound). This would correspond to a borrow rate of 21.2%, and supply rate of 3.4%.\n\nCOMP Payment\nIf this proposal is adopted, a one time payment of 100 COMP will be transferred from the Compound governance timelock to the MakerDAO pause proxy. This is intended to compensate for Block Analitica 13’s research and development costs in connection with this proposal; BA provides risk management consulting services to MakerDAO as the Maker risk core unit.\nTransfer 100 COMP to 0xBE8E3e3618f7474F8cB1d074A26afFef007E98FB 10.\n\nPlanned Future Changes\nAfter the merge is completed, a further proposal will be submitted to adjust cETH market parameters. This is expected to include an adjustment to interest rate model reducing rates across the utilization curve, as well as an increase in the cETH borrow cap (or full removal of borrow cap depending on community preferences). This will help normalize market activity once the immediate fork induced risks have passed.\n\nLicense and Disclaimer\nCopyright and related rights waived via CC0 4.\nThis proposal is provided for informational purposes only. This is not intended and should not be construed as financial, legal, regulatory, or tax advice. Proposal is provided on an “as is” basis without warranty of any kind. As a condition of accepting this proposal, protocol users, token holders, and other stakeholders disclaim any and all warranties, express or implied, including all implied warranties and conditions of merchantability, noninfringement, and fitness for a particular purpose. Stakeholders accept and implement the proposal solely at their own risk.\\nThanks for kicking off this process @monet-supply – its a great observation that the ETH interest rate model should be updated heading into the merge (and likely reverted afterwards). And thank you for suggesting the interest rate model and borrowing cap parameters.\nMy primary concern is that the proposed interest rate model is punitive in its goal to prevent ETH illiquidity; for the market to function, there needs to be some liquidity; 50% of the market (at 50% utilization) is more than enough for users to withdraw or borrow at will. The rapid slope of borrowing costs (at current utilization, and on the way to 50%), and the hyper-acceleration of borrowing costs after 50% utilization seems like it might be too aggressive.\nTwo ways that the parameters could be potentially improved:\n\nKeep borrowing costs unchanged for existing users, by lowering the base borrowing rate.\nRaise the kink to 80% utilization–while keeping the same 1000% maximum borrowing rate.\n\nWhat would be the ideal timing of a switch? A week before the merge? A day before? The community could help time the proposal (and a proposal to revert to a normal interest rate model) accordingly. Thank you for the model and effort \\nThanks for the feedback on this!\nI’m receptive to a higher target utilization / kink point up to 80% (vs initially proposed 50%), as the main risk here is that cETH becomes 100% utilized and even 80-90% utilization wouldn’t bring too high of safety concerns. Also understand the desire to not increase effective costs on existing borrowers (although it may be unavoidable that borrowing costs increase into the merge due to higher utilization, so users should be aware of this regardless).\nI think the alternative rate model below may do better at conforming to these goals:\n\nRate at 0% utilization: 2%\nOptimal utilization (kink point): 80%\nRate at optimal utilization: 20%\nRate at 100% utilization: 1000%\n\nThis would protect against dangerous utilization levels, while still allowing for economically viable borrowing at typical utilization levels. At current utilization rate (3.43%), the implied borrow cost would be 2.77%, which is just slightly higher than existing rate model’s borrowing cost of 2.71%.\nHowever, the borrow rate would increase with utilization levels somewhat faster than the current rate model; for example the new rate model would reach 11% at 50% utilization vs current rate model’s 8.31%. I think this is acceptable but happy to hear any contrary opinions \nOne other note on housekeeping, it looks like the existing rate models implicitly assume 15 second block times (2102400 blocks per year). This is why Compound rates aren’t currently round numbers (as realized block average closer to 12-13 seconds). With block times becoming a fixed 12 seconds after the merge, I think it would make sense to shift to 2628000 blocks per year formatting, which will make it easier for governance to set rates and lead to simpler UX (borrowing costs should ideally be integers at 0% utilization, kink point, and 100% utilization).\\nAll of that makes complete sense to me. Would love to see any other comments / ideas from the community as well, but this seems sound.\\nInteresting, but there’s one nuance. ETH market is a legacy one. Thus, borrow cap and kink point is really out of question. And we going to have much bigger potential mess with an attempt of rushed migration, which even if executed in timely manner before prospective merge would only have some funds migrated to a new market, thus splitting market liquidity and greatly increasing risks for both new and legacy eth markets. And i seriously doubt any user would appreciate that.\nIt would be nice to hear some input from risk guys on the topic, but i seriously doubt we are on the verge of 100% utilization. Frankly speaking i am not convinced we have a problem here. Indeed we might see some more utilization on eth market shortly before merge and might see some removal of eth liquidity, but as for the most users it’s not worth to bother i don’t believe we could hit even 70% utilization.\\nEven on old cToken models (the original contract) the interest rate model can be replaced—while the code of how the cToken itself works is immutable.\nCalling on OpenZeppelin to confirm that the new interest rate model contract works with cETH, but this should not be an issue as far as I understand \\nWell, if it is so, then introducing interest rate model with kink point to eth market would be a nice improvement regardless of merge.\nAnd bumping up interest rates at much more steep rate at higher utilization rate looks like a reasonable precaution to me, even if probability of achieving such utilization rate doesn’t seems to be high.\\n\n\n\n Sirokko:\n\nIt would be nice to hear some input from risk guys on the topic, but i seriously doubt we are on the verge of 100% utilization. Frankly speaking i am not convinced we have a problem here. Indeed we might see some more utilization on eth market shortly before merge and might see some removal of eth liquidity, but as for the most users it’s not worth to bother i don’t believe we could hit even 70% utilization.\n\n\nI hope this is how it turns out, but I’m fairly convinced that without any action we’ll see a prolonged period of unsafe utilization, potentially starting days or weeks before the expected merge date.\nWith current weekly cost capped at around 0.26%, and market currently indicating over 3% expected value of forks vs mainnet ETH, it is economically rational for users to supply USDC or other collateral assets and borrow ETH to hold in their wallet. Potential “fork-farmers” are incentivized to open borrowing positions early, before all cETH liquidity is exhausted.\nI made a simple sheet to estimate potential returns from fork-farming strategies: ETH Liquidity Risk - Google Sheets 7\n\n\n\n rleshner:\n\nCalling on OpenZeppelin to confirm that the new interest rate model contract works with cETH, but this should not be an issue as far as I understand \n\n\n@cylon would love OZ’s input on this! Also curious if borrow caps can be implemented for cETH market. Looks like that functionality is in the Comptroller implementation but not sure if there could be any compatibility issues with legacy ctoken.\\nThank you Robert, amazing feedback. We should stress out or set in the proposal that parameters to revert back to normality n blocks after merge.\\nThank you for opening this issue on the table.\nFor borrow caps, (if I remember correctly) it is managed by community multi-sig. (no governance voting) but not sure it is possible to set it for legacy cETH.\nBorrowing cap history (Jun, 2021) 5\nFor changing interest model proposal, I agree that it could reduce borrowing demands a little bit but won’t much helpful as long as expected profit is greater than its borrowing cost. So, I only agree that set borrow caps for ETH option without modifying interest model itself. and another reason is that it has no guarantee for its merge schedule. It can be canceled/delayed with various reasons. Then should we change its interest model again depending on merge schedule?\nand if it is not possible to avoid 100% utilization situation, I was wondering how to support Compound users who want to withdraw ETH under 100% utilization. The usage of reserve(for ETH) is not clearly described in the document but this would be one of solution to support them.\\nChanging the IRM in a way that isn’t punitive at modest utilization but protects against a run on the protocol’s ETH liquidity makes sense, so I support the idea to shift the kink out to 80% utilization.\nThe only part I’m skeptical of is the proposed rate at 100% utilization. 1000% annualized sounds extreme at first glance but is, in my view, generously modest on the daily-to-weekly timescale of the merge/fork. We can learn here from the experience of AMPL on Aave, where borrow costs north of 100,000% annualized aren’t enough to maintain liquidity for AMPL lenders during short-term periods of significant positive rebases. Perhaps the sheer size of ETH money markets makes the comparison moot, but the benefits of a much steeper increase post-kink seem, to me, to outweigh the costs if the goal is to successfully incentivize some residual liquidity in the ETH market. Why not make the rate at 100% utilization something on the scale of several percent daily instead of annualized?\\nThanks, @monet-supply, for the thoughtful proposal. Here we provide Gauntlet’s analysis of the impact of the proposed changes under different scenarios.\nGiven that a user can potentially benefit from the forked PoW ETH (ETHW) by borrowing ETH before the merge, we formulate a rational user’s decision logic as follows:\n\n692×74 6.3 KB\n\nA user will borrow ETH if the expected ETHW value exceeds the ETH borrowing interest and the transaction cost (price slippage for selling ETHW on a centralized exchange, gas fee).\nThe proposal updates the interest rate model to a jump rate model, accelerating the interest rate if the pool utilization exceeds the Kink. As seen from the above formula, the efficacy of the interest rate change also depends on the expected yield of fork farming (expected ETHW price / ETH price) and the duration of ETH borrowing (time between borrowing ETH and selling ETHW to USD on a centralized exchange).\nAs pointed out by @allthecolors, the effect of the PoW fork is similar to the effect of a positive Ampleforth rebase, where both create profit opportunities at a scheduled time. We conducted a similar analysis of the relationship between AMPL rebase and AMPL interest rate 4. Similar to the positive AMPL rebase, if ETHW retains non-negligible value, raising the interest rate cannot completely eliminate 100% pool utilization since the borrow duration can be short (within minutes). Even if the maximum borrow rate is 1000%, a daily borrowing cost of 0.66% is negligible if a user only borrows ETH for a few minutes. However, raising the interest rate can make ETHW fork farming less profitable, reducing the time of 100% utilization. The downside is that raising interest rates makes the market more unpredictable when the ETH pool utilization exceeds the Kink.\nIf the community’s goal is minimizing ETH illiquidity, it’s more effective to set a low borrow cap (like what @monet-supply suggested), and the community multi-sig can update the borrow cap when the Kink is lower than it, since\n\nThe acceleration of borrowing cost will not kick in if the borrow cap is less than Kink.\nPredicting the value of ETHW and the amount of ETH supply on Compound around the Merge is difficult.\nIncreasing interest rate won’t completely eliminate 100% pool utilization if ETHW retains non-negligible value.\nIncreasing interest rate makes the market more unpredictable when the ETH utilization is above Kink.\nThe governance process for updating interest rates is slow.\n\nConclusion\nWe support upgrading ETH’s interest rate model, but increasing ETH’s interest rate at 100% utilization doesn’t completely solve the ETH utilization problem. We support setting a borrow cap for the cETH market of 100,000 ETH, per Monet Supply’s suggestion.\\nResponding to @monet-supply and @rleshner’s requests here.\nOpenZeppelin did a technical review and can confirm that the interest rate model can be replaced on cETH. We can also confirm that borrow caps can be set in the comptroller for cETH and we don’t foresee any issues.\nIf for any reason you plan to use a new contract for the jump rate model rather than the ones already used by the protocol, we would, of course, like to audit it before a governance proposal is submitted to perform the changes.\\nThanks @monet-supply for creating this proposal, @pauljlei for providing Gauntlet’s analysis of the impact, and for everyone else’s comments.\nThe ETH market has been long due for an interest rate model upgrade. A mere 15% borrowing rate on ETH at 100% utilization isn’t much, and 100% utilization can be very problematic.\nThe updated parameters for the new interest rate model look great and in my opinion, will prevent 100% utilization for long periods. However, I do not think this interest rate model will be enough to prevent 100% utilization in the moments leading up to and following the merge. I think this model in addition to a low borrow cap is best.\\nSomething important to point out is that the rates we set are essentially APR whereas the protocol actually pays/charges APY and compounds each block. 1000% would be well over 20k% APY.\\nThanks all for your comments / analysis!\nResponding to various feedback received so far:\n\n\n\n dakeshi:\n\nFor changing interest model proposal, I agree that it could reduce borrowing demands a little bit but won’t much helpful as long as expected profit is greater than its borrowing cost. So, I only agree that set borrow caps for ETH option without modifying interest model itself. and another reason is that it has no guarantee for its merge schedule. It can be canceled/delayed with various reasons. Then should we change its interest model again depending on merge schedule?\n\n\nI agree that borrow cap may be the more effective measure to prevent against cETH utilization reaching 100%, but rate model change provides a bit of a second layer of defense against liquidity risk. If enough users withdraw their ETH to the lead up to the merge, the borrow cap might no longer be effective to prevent 100% utilization; if this happened the rate changes (which cannot be triggered in short time period due to governance voting and timelock delays) would help incentivize users to deposit ETH and help ease the liquidity crisis.\nI think the merge timeline seems to be solidifying, but if it was significantly delayed the rate changes could always be reversed.\n\n\n\n dakeshi:\n\nand if it is not possible to avoid 100% utilization situation, I was wondering how to support Compound users who want to withdraw ETH under 100% utilization. The usage of reserve(for ETH) is not clearly described in the document but this would be one of solution to support them.\n\n\nI set up a pool for cETH/ETH on Uniswap v3 4, and added liquidity from just above current price to roughly 8% discount. Somewhat surprisingly, this pool is already getting really high volume and has been earning double digit % yield. If liquidity grows deep enough, this could allow liquidators to sell cETH into the pool in the event cETH is fully utilized. Ability to purchase cETH at a discount greater than expected ETH fork value could incentives traders to refill ETH liquidity in the pool.\n\n\n\n allthecolors:\n\nThe only part I’m skeptical of is the proposed rate at 100% utilization. 1000% annualized sounds extreme at first glance but is, in my view, generously modest on the daily-to-weekly timescale of the merge/fork. We can learn here from the experience of AMPL on Aave, where borrow costs north of 100,000% annualized aren’t enough to maintain liquidity for AMPL lenders during short-term periods of significant positive rebases. Perhaps the sheer size of ETH money markets makes the comparison moot, but the benefits of a much steeper increase post-kink seem, to me, to outweigh the costs if the goal is to successfully incentivize some residual liquidity in the ETH market. Why not make the rate at 100% utilization something on the scale of several percent daily instead of annualized?\n\n\nWe could use a higher borrow rate at max utilization. For example, at 1,000,000% APR the daily borrowing cost would be ~2.55% (vs ~0.65% daily cost at 1,000% APR). If enough users withdrew ETH so the borrow cap was no longer higher than ETH supplied, this would help limit time period of 100% utilization. But, it would still be possible to see hours of full utilization, so I think this should still be treated as a backup rather than primary defense mechanism.\n\n\n\n arr00:\n\nSomething important to point out is that the rates we set are essentially APR whereas the protocol actually pays/charges APY and compounds each block. 1000% would be well over 20k% APY.\n\n\nThis in an important point. I wonder if it would be possible to work backwards from target annualized rates at zero, optimal, and max utilization to the specific per block interest rate values needed to achieve these target rates?\n\nAlso wanted to go some expected/possible outcomes resulting from this proposal.\nCurrently cETH has total deposits of roughly 495,000 ETH, and 3.1% borrow utilization. The proposed 100,000 ETH borrow cap would limit utilization to ~20%, which if fully used would result in a 6.5% borrow rate and 1.04% supply rate.\nAssuming 1 month until the merge takes place (and accompanying POW fork), ETH borrowers would face roughly 0.5% total borrow cost to hold their position open through the merge. Considering that market indications of ETH fork value are closer to 2-3%, it seems likely that the 100,000 ETH worth of borrowing capacity will be fully utilized after this limit is put in place due to expected profitability of borrowing ETH at these rates and holding through the merge.\nIt’s possible that some traders may even front-run the proposal’s execution, resulting in a total amount borrowed exceeding the 100,000 cap. However, total amount borrowed is unlikely to exceed 80% of total cETH supply due to extremely high cost of holding positions open for multiple days/weeks when above the new rate model’s kink point.\nIf total borrows reach very high levels as this proposal is being executed, Compound should have time to pursue an additional interest rate model change to encourage repayments back towards the 100,000 ETH borrow cap.\\nIsn’t 100k borrowing cap a bit too conservative? The idea of temporary setting it, as well as ramping up rate curve is not really about ripping users of their money, or their potential profits. It’s for the unlikely event of making protocol illiquid at a time of a merge if it suddenly will experience a huge borrow demand as well as potential supply withdraw.\nThus, something in range of 200-250k sounds more reasonable. As measures discussed here are not for preventing people from making money by using services, provided by protocol, but rather to ensure that those actions wouldn’t jeopardise overal stability of protocol and convenience of other users. I’d say while cap shouldn’t be too much agressive, it shouldn’t be too conservative either.\nIt’s not for a protocol to decide for users how they should act in anticipation of merge for their own benefit. As long as protocol can maintain uninterrupted service in a safe manner it shouldn’t be restricting users on their activity.\n20% of utilization is not great utilization for any market, 40-50% projected at a time of potential spike still sounds conservative enough to have enough headroom to navigate through merge in a safe way.\\n\n\n\n Sirokko:\n\nThus, something in range of 200-250k sounds more reasonable. As measures discussed here are not for preventing people from making money by using services, provided by protocol, but rather to ensure that those actions wouldn’t jeopardise overal stability of protocol and convenience of other users. I’d say while cap shouldn’t be too much agressive, it shouldn’t be too conservative either.\n\n\nThe issue I see is, for ETH users who are only trying to maximize their return on assets, the best option will nearly always be to withdraw from the market rather than trying to earn interest in cETH. Even if borrowers end up paying total interest equal to fork value, and utilization is at or near 100% during this time (which would be dangerous for long periods), they would still face 20% reserve factor.\nRather than catering to ETH yield seekers, I think Compound should focus on collateralized borrowers (supplying ETH to borrow eg stablecoins), who may be a more reliable source of supply side liquidity. I assume many of these users place greater emphasis on minimizing insolvency and liquidity risk vs potential to earn a bit of incremental yield on their cETH collateral. If Compound can ensure this, I would bet at least 100k ETH or more of deposits would remain through the merge, avoiding any periods of 100% utilization entirely.\nSetting a borrow cap of eg. 250,000 COMP would allow for greater yield for depositors and reserve growth for the protocol, but if it turns out that we underestimated the amount of users who actively withdraw in the lead up to the merge, it may not be possible to set a lower borrow cap retroactively or make additional adjustments to interest rate model to preserve liquidity.\nAlso, the borrow cap guardian (community multisig) could theoretically raise the borrow cap in the days before the merge if it becomes clear that liquidity risk is lower than initially feared.\\nI fully agree that Compound should cater to collateralized borrowers. Actually i don’t actually believe there is pretty much any meaningful amount of Suppliers of eth for yield purpose, as even 0.15% if include COMP is nothing impressive. It’s fairly easy to get single digits apr yield on eth, and even double digit, if that person can take a bit of volatility in amm, and able to set range for his liquidity appropriately.\nI also don’t personally expect high utilization at all, and while appreciate pointing to it, and accept the case as possible, still don’t think as higly probable. On the contrarary, i believe it’s quite unlikely, as potential fork is likely to be a train wreck and more talks than actual money to be made. So i am expecting most of supplied eth to be collateral, which is likely going to just sit through the merge without movement.\nThe motivation of me questioning if cap is too low, is primarily because the core concept of building decentralized protocols on blockchain is permissionless and 100% available service. If the service is going to be restricted by governance every time it deems apropriate, rather them dictated by market conditions we are setting bad precedent here.\nEven if merge is once-in-a-lifetime event, there eventually might be other concerns in future.\n\n\n\n monet-supply:\n\nAlso, the borrow cap guardian (community multisig) could theoretically raise the borrow cap in the days before the merge if it becomes clear that liquidity risk is lower than initially feared.\n\n\nThis is a solid argument. It’s hard to say though if multisig going to react in timely manner at all and provide liquidity to the market if there would be demand (which might be there even if for a short time)\nWhatever the CAP in the end is going to be decided by governance i would like to basically see 2 things:\nas less negative impact on current suppliers and borrowers as possible\nas much headroom for new borrowers as market need without putting at too much risk stability and liquidity of eth market through the merge.\nAbility for multisig to raise the cap if supply side remaines relatively stable and borrowing side will hit the cap more or less resolves most of concernes. Though technically the most efficient way for users to utilize their capital would be hours if not minutes before actual merge. So we might not really see any serious movement up to the very last moments.\nAlso the very fact of existence of borrow CAP does remove initiative for suppliers to withdraw prior to merge at all, even if they want to play the fork game. Since it wouldn’t be possible to borrow even half of supply because of CAP (which will also appear on the ethpow after activating on mainnet), Supplier then can withdraw eth on the forked chain AFTER the merge. His eth still going to be there in Compound contract and while potentially couldn’t be borrowed on forked chain due to the cap reached there, would be redeemable still.\\n\n\n\n Sirokko:\n\nThe motivation of me questioning if cap is too low, is primarily because the core concept of building decentralized protocols on blockchain is permissionless and 100% available service. If the service is going to be restricted by governance every time it deems apropriate, rather them dictated by market conditions we are setting bad precedent here.\n\n\nI agree with this, but I think it’s a balancing act between availability of borrowing ETH, vs availability of withdrawing deposited ETH or liquidating unsafe positions. Not allowing borrowing full amount of ETH is an inconvenience, but inability to withdraw or liquidate is a safety concern that puts the rest of the market (including non-ETH users) potentially at risk.\n\n\n\n Sirokko:\n\nSince it wouldn’t be possible to borrow even half of supply because of CAP (which will also appear on the ethpow after activating on mainnet), Supplier then can withdraw eth on the forked chain AFTER the merge. His eth still going to be there in Compound contract and while potentially couldn’t be borrowed on forked chain due to the cap reached there, would be redeemable still.\n\n\nAt least based on currently stated ETHPOW plans, the cETH contract will be “frozen” manually by their client implementation and it will not be possible to withdraw funds after the fork. Maybe after imposing a borrow cap we could ask the ETHW people to remove this address from the freeze list but I’m not sure if it is worthwhile to even engage with them at all.\\nThanks to everyone who discussed this proposal on today’s developer community call 1!\nSome highlights from the discussion:\n\n\n@jmo suggested that the proposal could be submitted in multiple parts (eg split up the borrow cap from interest rate model change), which could potentially allow for faster action by the DAO\n\n@rleshner discussed timing of when proposal would be submitted, and timing of when to submit follow up proposal to set rates back into a more normal range\n\nI’m not great a paraphrasing so suggest listening to the recording if you’re interested \\nThere’s one more factor to consider, following the launch of Compound III 3, which might increase the need to adjust the v2 ETH interest rate model. More on this in a bit…\nThe logic for adjusting the ETH rate model is the expectation that:\n\nUsers will want to hold ETH in their wallet in the moments before and during a fork in order to receive POWETH tokens.\nCompound v2 will likely not exist (or will instantaneously be liquidated) on a fork chain in which stablecoins are valueless. A miner, in the first block, will find a way to extract the POWETH.\nETH supplied to Compound v2, logically, will be withdrawable on Mainnet, but will likely be frozen on a POWETH fork 2.\nKnowing this, during the Merge, users might want to withdraw ETH from Compound v2, in order to receive POWETH.\n\nNow, back to Compound III. One of the unique advantages of the upgrade is that collateral is isolated; it remains your property, and can never be withdrawn by other users (except during liquidation).\n\nETH supplied to Compound III likely will be withdrawable on a POWETH fork; USDC collapsing won’t liquidate borrowers (in fact, the opposite).\nKnowing this, during the Merge, users might want to supply ETH to Compound III, in order to simultaneously borrow USDC and receive POWETH.\nThis could further exacerbate an ETH liquidity crisis on Compound v2.\n\nGiven this new possibility, I feel more confident acting earlier, and with more conviction, in updating the Compound v2 ETH interest rate models & borrowing caps. There’s a little over two weeks until the Merge (at the shortest end)–excited for this to come to a formal proposal soon.\\nHi all,\nWith the merge fast approaching, and the new cETH interest rate model deployed, I’m planning to submit the on chain proposal for cETH merge risk mitigations later today. Below is a preview of proposal actions and text.\n\nProposal Actions\nSet ETH borrow cap of 100,000 ETH\nComptroller > _setMarketBorrowCaps > cETH 1 > 100,000E18\nUpdate ETH interest rate model with the following parameters\n\nBorrow rate at 0% utilization: 2%\nOptimal utilization (kink): 80%\nBorrow rate at optimal utilization: 20%\nBorrow rate at 100% utilization: 1000% APR\n\n\n1600×757 52.8 KB\n\nNote that 1000% borrow APR yields a maximum compounded APY of ~2.2 million %, or roughly 2.8% borrow cost per day. The block time assumed by the interest rate model has also been changed to 12 seconds (from 15 seconds previously).\ncETH > _setInterestRateModel > 0xF9583618169920c544Ec89795a346F487cB5a227 3\nTransfer 100 COMP to Maker governance timelock at 0xBE8E3e3618f7474F8cB1d074A26afFef007E98FB\nCOMP > Transfer > 0xBE8E3e3618f7474F8cB1d074A26afFef007E98FB > 100E18\nTransfer 45 COMP to @arr00 at 0x2b384212edc04ae8bb41738d05ba20e33277bf33 1\nCOMP > Transfer > 0x2b384212edc04ae8bb41738d05ba20e33277bf33 > 45E18\n\nSummary\nThis proposal will make several changes to the cETH market on Compound v2 in preparation for the upcoming Ethereum merge and switch to POS consensus. This includes setting a borrow cap of 100,000 ETH, as well as updating the interest rate model to a jump rate model with much higher rates after exceeding 80% borrow utilization, up to a maximum of 1000% APR.\n\nReasoning\nThe upcoming merge has the potential to cause disruption to ETH lending markets due to the possibility of receiving airdrops of ETH fork tokens. This may incentivize excessive borrowing from ETH lending pools, which leads to negative user experience for depositors who cannot withdraw funds when utilization reaches 100%, as well as safety concerns due to potential to interfere with liquidations of ETH collateralized positions. The proposed changes should help reduce risk of the cETH market reaching 100% utilization. See the forum discussion for further details.\nThe proposal includes payments to proposal contributors, including 100 COMP to MakerDAO governance timelock for Block Analitica (Maker risk) team’s analysis, as well as 45 COMP to Arr00 for development work."
  },
  {
    "number_of_comments": 11,
    "postid": "de22c77b-ffb7-4028-81ba-fff6d821ddfe",
    "posturl": "https://www.comp.xyz/t/grant-to-pooltogether-for-launch-of-tether-prize-saving-pool/1581",
    "combinedcontent": "PoolTogether is a protocol for no loss prize savings built on Compound. The protocol is one of the very first, largest, and longest running protocols to be built on top of Compound. PoolTogether was launched in June of 2019 and has been providing supply side capital and users to Compound continuously since then (at one point there were more unique depositors using Compound via PoolTogether than Compound directly). Currently PoolTogether supplies ~$220 million of supply side assets to Compound.\nIn addition, PoolTogether has been an exceptionally value positive actor in the Compound ecosystem. Since the COMP governance token launched PoolTogether has never sold any COMP. All COMP accrued has either been stored with PoolTogether governance or given away to depositors. PoolTogether has also not participated in any recursive yield farming activities with COMP. Meaning relative to other depositors with the same size of deposits, COMP accrued as been much less.\nAll this leads me to my ask \nPoolTogether has deployed a new prize savings pool for the Tether asset type. Because of the nature of the protocol, PoolTogether is uniquely reflexive in growth. Larger prizes lead to more deposits, more deposits create larger prizes. This creates a bootstrapping problem for new prize pools to attract large amounts of capital.\nI’m asking for a 1,000 COMP grant to help bootstrap the Tether prize pool and ultimately bring more capital into the Compound protocol. At current market prices 1,000 COMP is $475,000. This COMP would not be sold but rather given out over the first 10 weeks of the new Tether prize pool as prizes. This means the Tether prize pool would be bootstrapped to an initial ~$47,500 weekly savings prize for the first 10 weeks. This would make the prize closer to the current Dai and USDC prize pools.\nUltimately this proposal is good for Compound as it will bring even more depositors and users to the protocol via a third party interface. It also shows Compound’s appreciation and valuing of its longest term integration partner. It’s good for PoolTogether has it bootstraps network effects needed to help more people save money.\nThat’s it! I’m open to any questions.\nP.S.\nAs I am a reviewer on the grants committee I will not be participating in this decision due to conflict of interests.\\nI think this is a fantastic idea that would clearly benefit Compound!\nThat said, the size of the requested grant ($475k) strikes me as too large to be provided by the Compound Grants Program. Our grants budget is $2mm in total, and allocating ~24% of that sum towards one grant doesn’t feel like it would be in the spirit of the program.\nFor a grant of this size, I think it makes sense to go directly through governance. To that end, I’d recommend starting a CAP 6 to solicit votes from the community.\nVery curious to hear what others think of this approach as well!\\nWhile I think this is generally a good idea, I feel that the cost is too high for the benefit it brings to Compound. PoolTogether can switch the yield source through governance so there’s not necessarily a long term value capture to Compound from a big USDT prize pool.\nI think it could make more sense for Compound to use funds from the USDT reserves to sponsor the new pool for a certain period of time. This would create larger pool prizes, help incentivize deposits and the virtuous cycle of growing pool reserves, but at a much lower cost - Compound can retrieve the USDT in the future once the prize pool has grown sufficiently large.\\nI actually meant to include this!\nPoolTogether actually can’t change the yield source on any of the Compound prize pools (or any prize pools for that matter). By design, we prioritized security so the prize pool contracts are not upgradeable. That means for this prize pool + all the others compound is guaranteed to always be the yield source.\\nAh, thank you for clarification! Can the pool reserves be removed by governance?\\nPool reserves can be. If you look here: PoolTogether Vote 4\nChoose “reserve contract” then function “withdraw reserve”.\nThe prize strategy is upgradeable by governance so new prize strategies can be implemented. But the prize pool that stores deposited funds and manages interaction to the yield source can’t be changed.\\nThe Compound protocol is already distributing COMP to a future USDT PoolTogether pool, through the standard COMP Distribution.\nIncreasing the distribution to one particular application does not strike me as scalable, or fair in this case, and I am against this proposal.\\nwith all due respect to the PoolTogether project and an interesting and innovative approach to the DeFi sector, I think that the proposal should not be considered at all.\nI understand that everyone has a different perspective when it comes to DeFi but would love to see more initiatives for progress in the Layer2 area, oracle, protection from liquidations, move to decentralized governance etc.\nI think lottery isnt a priority\\nThis isn’t zero sum. Certainly would be great to see people working on the things you mentioned but giving a grant to an existing and proven integration to fund expansion doesn’t preclude what you mentioned.\nIt’s a strange precedent to only give grants to new ideas but not help support the proven ones that have been working the longest and bringing the largest amount of value to the protocol.\\nI think for the reasons noted in my post there is a very solid rationale for the protocol to offer additional support and incentives beyond the uniform COMP distribution.\nThat doesn’t have to take the form of a COMP grant but thus far that has been the primary mechanism for the protocol to support projects.\nI’m open to any ideas and certainly not set on this taking the form of a COMP grant. I’d just love to get some support in scaling this as it will be mutually beneficial.\\n\n\n\n lay2000lbs:\n\nThis isn’t zero sum\n\n\ngiven the COMP maximum supply it is a zero sum game.\nI may be wrong, but I think certain problems that users have with the protocol need to be addressed first.\nDon’t get me wrong, I think PoolTogether deserves a grant, but there are issues that are ignored for months and are existentially significant to certain users.\nIt’s just a priority question\\nI am going to try to attack this from another angle. I think an investment or compound liquidity program can fit this bill."
  },
  {
    "number_of_comments": 16,
    "postid": "0eb5f009-930e-4aa2-be1b-55cd39e33495",
    "posturl": "https://www.comp.xyz/t/gauntlet-weekly-market-updates-ethereum-usdc/4538",
    "combinedcontent": "[Gauntlet] Weekly Market Update: Ethereum USDC (7/21/23 - 7/28/23)\nGauntlet would like to provide the community with an update on metrics from the Ethereum USDC comet over the past week and will include any relevant recommendations.\nSimple Summary\n\nGauntlet has an outstanding proposal 5 to increase supply caps for WBTC, LINK, and UNI. Otherwise, Gauntlet recommends no additional changes at this time.\nBased on community preferences, we will more aggressively move to deprecate the v2 market in favor of v3.\nUSDC borrows are down 0.51%.\nUSDC supply is down 1.88%. However, on the date of this post, a user deposited $45.4M USDC, increasing the USDC supply by 9.8%. This will be included in the market update next week.\nThe comet accumulated $43.6k USDC reserves over the past week, with an average reserve growth of 14.1%.\nThe comet distributed $230.3k COMP rewards over the past week, for a Net Protocol Profit of -$186.7k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week.\nMarket Growth\nScreen Shot 2023-07-28 at 6.54.24 AM1806×488 66.4 KB\nTotal Collateral (USD) is down 0.77%, from $748.65M to $742.87M.\nScreen Shot 2023-07-28 at 6.54.37 AM1806×488 65.8 KB\nUSDC Borrows are down 0.51%, from $363.31M to $361.45M.\nScreen Shot 2023-07-28 at 6.54.56 AM1806×486 67.5 KB\nUSDC Supply is down 1.88%, from $387.69M to $380.41M. However, on the date of this post, a user deposited $45.4M USDC, increasing the USDC supply by 9.8% to $425.63M. This will be included in the market update next week.\nSupply Caps\nScreen Shot 2023-07-28 at 7.29.48 AM3552×622 111 KB\nAs seen above, WBTC (75.2%), COMP (90.5%), LINK (100%), and UNI (100%), all have supply cap utilizations > 75%. We have an outstanding proposal 5 to increase supply caps for WBTC, LINK, and UNI, and we recommend not increasing the COMP supply cap due to lower liquidity.\nScreen Shot 2023-07-28 at 7.07.09 AM3544×838 152 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-07-28 at 8.05.01 AM1660×436 41.2 KB\nThe minimum USDC utilization was 93.7%, and the maximum was 97.9%.\nThe minimum USDC reserve growth was 4.4%, and the maximum was 18.3%. The average USDC reserve growth was 14.1%.\nScreen Shot 2023-07-28 at 8.06.02 AM1680×454 35.5 KB\nThe comet steadily accumulated $43.6k USDC reserves, while distributing $230.3k COMP rewards, for a weekly Net Protocol Profit of -$186.7k.\\n[Gauntlet] Ethereum USDC Market Update (8/4/23 - 8/10/23)\nGauntlet would like to provide the community with an update on metrics from the Ethereum USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are up 1.97%, from $388.63M to $396.27M.\nUSDC Supply is up 2.12%, from $389.99M to $398.26M.\nUSDC utilization slightly decreased from 99.7% to 99.5%.\nThe comet accumulated $12.97k USDC reserves, with an average reserve growth was 3.0%.\nThe comet distributed $189.44k COMP rewards for a weekly Net Protocol Profit of -$176.47k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week.\nMarket Growth\nScreen Shot 2023-08-11 at 4.45.45 PM1808×490 70.5 KB\nTotal Collateral (USD) is down 4.89%, from $769.90M to $732.25M.\nScreen Shot 2023-08-11 at 4.47.35 PM1806×488 69 KB\nUSDC Borrows are up 1.97%, from $388.63M to $396.27M.\nScreen Shot 2023-08-11 at 5.10.15 PM1808×498 70.4 KB\nUSDC Supply is up 2.12%, from $389.99M to $398.26M.\nScreen Shot 2023-08-11 at 5.03.09 PM3544×618 103 KB\nAs seen above, only COMP (93.3%) has a supply cap utilization> 75%. We recommend not increasing the COMP supply cap due to lower liquidity.\nScreen Shot 2023-08-11 at 5.03.45 PM3544×844 160 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-08-11 at 5.18.41 PM1648×414 43.6 KB\nUSDC utilization slightly decreased from 99.7% to 99.5%.\nThe minimum USDC utilization was 96.6%, and the maximum was 99.7%.\nThe minimum USDC reserve growth was 0.2%, and the maximum was 9.4%. The average USDC reserve growth was 3.0%.\nScreen Shot 2023-08-11 at 5.19.02 PM1660×448 34.2 KB\nThe comet accumulated $12.97k USDC reserves while distributing $189.44k COMP rewards for a weekly Net Protocol Profit of -$176.47k.\\n[Gauntlet] Ethereum v3 USDC Update (09/08/2023 - 09/14/2023)\nGauntlet would like to provide the community with an update on metrics from the Ethereum v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are up 2.30%, from $282.41M to $288.9M.\nUSDC Supply is down 2.25%, from $319.29M to $312.1M.\nUSDC utilization increased 4.64%, from 88.5% to 92.6%.\nThe minimum USDC reserve growth was 13.8%, and the maximum was 17.4%. The average USDC reserve growth was 16.7%.\nThe comet accumulated $37.55k USDC reserves while distributing $127.98k COMP rewards for a weekly Net Protocol Profit of $-90.43k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nTotal Collateral (USD)1128×297 37.7 KB\nTotal Collateral (USD) is up 3.15%, from $537.2M to $554.11M.\nUSDC Supply1143×336 46.8 KB\nUSDC Supply is down 2.25%, from $319.29M to $312.1M.\nUSDC Borrows1143×336 44.3 KB\nUSDC Borrows are up 2.30%, from $282.41M to $288.9M.\nUSDC utilization1143×336 8.32 KB\nUSDC utilization increased 4.64%, from 88.5% to 92.6%.\nSupply Caps\nSupply Cap2950×746 98.5 KB\nAbove are the current supply cap utilizations for each collateral asset.\nSupply Cap Series1690×520 35.1 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nUtilization1143×336 7.16 KB\nThe minimum USDC utilization was 88.4%, and the maximum was 93.5%.\nThe minimum USDC reserve growth was 13.8%, and the maximum was 17.4%. The average USDC reserve growth was 16.7%.\nCumulative1143×336 8.33 KB\nThe comet accumulated $37.55k USDC reserves while distributing $127.98k COMP rewards for a weekly Net Protocol Profit of $-90.43k.\\n[Gauntlet] Ethereum v3 USDC Update (09/15/2023 - 09/21/2023)\nGauntlet would like to provide the community with an update on metrics from the Ethereum v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are up 2.37%, from $288.9M to $295.75M.\nUSDC Supply is up 8.30%, from $312.1M to $337.99M.\nUSDC utilization decreased 5.47%, from 92.6% to 87.6%.\nThe minimum USDC reserve growth was 13.2%, and the maximum was 17.6%. The average USDC reserve growth was 15.6%.\nThe comet accumulated $36.73k USDC reserves while distributing $157.59k COMP rewards for a weekly Net Protocol Profit of $-120.86k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-09-22 at 11.43.57 AM1804×474 67.1 KB\nTotal Collateral (USD) is up 1.91%, from $568.11M to $578.98M.\nScreen Shot 2023-09-22 at 11.44.18 AM1806×470 67 KB\nUSDC Supply is up 8.30%, from $312.1M to $337.99M.\nScreen Shot 2023-09-22 at 11.44.37 AM1802×478 66 KB\nUSDC Borrows are up 2.37%, from $288.9M to $295.75M.\nScreen Shot 2023-09-22 at 11.44.52 AM1662×452 36.5 KB\nUSDC utilization decreased 5.47%, from 92.6% to 87.6%.\nSupply Caps\nScreen Shot 2023-09-22 at 11.46.23 AM3802×618 105 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-09-22 at 11.46.49 AM3796×846 204 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-09-22 at 11.45.20 AM1648×420 36.9 KB\nThe minimum USDC utilization was 87.6%, and the maximum was 93.9%.\nThe minimum USDC reserve growth was 13.2%, and the maximum was 17.6%. The average USDC reserve growth was 15.6%.\nScreen Shot 2023-09-22 at 11.45.36 AM1666×462 32.9 KB\nThe comet accumulated $36.73k USDC reserves while distributing $157.59k COMP rewards for a weekly Net Protocol Profit of $-120.86k.\\n[Gauntlet] Ethereum v3 USDC Update (09/29/2023 - 10/05/2023)\nGauntlet would like to provide the community with an update on metrics from the Ethereum v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are up 4.89%, from $290.94M to $305.18M.\nUSDC Supply is up 2.92%, from $334.66M to $344.42M.\nUSDC utilization increased 1.92%, from 86.9% to 88.6%.\nThe minimum USDC reserve growth was 11.8%, and the maximum was 15.0%. The average USDC reserve growth was 13.3%.\nThe comet accumulated $29.64k USDC reserves while distributing $203.75k COMP rewards for a weekly Net Protocol Profit of $-174.11k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-10-06 at 6.18.10 PM1804×474 70.3 KB\nTotal Collateral (USD) is up 3.35%, from $591.64M to $611.45M.\nScreen Shot 2023-10-06 at 6.18.37 PM1802×468 66.1 KB\nUSDC Supply is up 2.92%, from $334.66M to $344.42M.\nScreen Shot 2023-10-06 at 6.18.56 PM1800×458 66.2 KB\nUSDC Borrows are up 4.89%, from $290.94M to $305.18M.\nScreen Shot 2023-10-06 at 6.19.11 PM1654×460 46.2 KB\nUSDC utilization increased 1.92%, from 86.9% to 88.6%.\nSupply Caps\nScreen Shot 2023-10-06 at 6.21.46 PM3538×614 103 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-10-06 at 6.22.03 PM3536×844 197 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-10-06 at 6.19.30 PM1650×416 38.5 KB\nThe minimum USDC utilization was 85.8%, and the maximum was 90.1%.\nThe minimum USDC reserve growth was 11.8%, and the maximum was 15.0%. The average USDC reserve growth was 13.3%.\nScreen Shot 2023-10-06 at 6.19.45 PM1664×456 35 KB\nThe comet accumulated $29.64k USDC reserves while distributing $203.75k COMP rewards for a weekly Net Protocol Profit of $-174.11k.\\n[Gauntlet] Ethereum v3 USDC Update (10/13/2023 - 10/19/2023)\nGauntlet would like to provide the community with an update on metrics from the Ethereum v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are up 4.51%, from $280.95M to $293.61M.\nUSDC Supply is up 1.52%, from $310.2M to $314.92M.\nUSDC utilization increased 2.94%, from 90.6% to 93.2%.\nThe minimum USDC reserve growth was 11.8%, and the maximum was 16.9%. The average USDC reserve growth was 16.0%.\nThe comet accumulated $36.09k USDC reserves while distributing $180.41k COMP rewards for a weekly Net Protocol Profit of $-144.32k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-10-20 at 2.20.49 PM1804×508 67 KB\nTotal Collateral (USD) is up 2.92%, from $573.28M to $590.0M.\nScreen Shot 2023-10-20 at 2.21.06 PM1802×490 68.4 KB\nUSDC Supply is up 1.52%, from $310.2M to $314.92M.\nScreen Shot 2023-10-20 at 2.21.32 PM1800×466 66.2 KB\nUSDC Borrows are up 4.51%, from $280.95M to $293.61M.\nScreen Shot 2023-10-20 at 2.21.49 PM1680×466 44.9 KB\nUSDC utilization increased 2.94%, from 90.6% to 93.2%.\nSupply Caps\nScreen Shot 2023-10-20 at 2.23.05 PM3786×608 105 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-10-20 at 2.23.26 PM3786×796 194 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-10-20 at 2.22.04 PM1668×432 41.4 KB\nThe minimum USDC utilization was 90.4%, and the maximum was 93.9%.\nThe minimum USDC reserve growth was 11.8%, and the maximum was 16.9%. The average USDC reserve growth was 16.0%.\nScreen Shot 2023-10-20 at 2.22.22 PM1682×460 32.8 KB\nThe comet accumulated $36.09k USDC reserves while distributing $180.41k COMP rewards for a weekly Net Protocol Profit of $-144.32k.\\n[Gauntlet] Ethereum v3 USDC Update: (11/03/2023 - 11/09/2023)\nGauntlet would like to provide the community with an update on the USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows decreased 0.4%, from $345.48M to $344.1M.\nUSDC Supply increased 0.04%, from $365.79M to $365.95M.\nUSDC utilization decreased 0.44%, from 94.45% to 94.03%.\nThe minimum USDC reserve growth was 6.67%, and the maximum was 17.0%. The average USDC reserve growth was 11.28%.\nThe comet accumulated $42.4k USDC reserves while distributing $225.41k COMP rewards for a weekly Net Protocol Profit of $-183.01k.\n\nCollateral Asset Supply\nThis graph shows the time series of total supply of all collateral assets.\nSupply1920×1080 90.7 KB\nTo see updated statistics, please see the live version of this graph here 1.\nBase Asset Borrows\nThis graph shows the time series of total borrows of the base asset.\nBorrows1920×1080 60.9 KB\nTo see updated statistics, please see the live version of this graph here.\nUtilization\nThis graph shows the utilization (borrow / supply) of the base asset over the past week.\nUtilization1920×1200 210 KB\nSupply Cap Usage\nThis graph shows the supply cap usage (supply / supply cap) of all collateral assets over the past week.\nSupply Cap Usage1920×1200 145 KB\\n[Gauntlet] Ethereum v3 USDC Update: (11/10/23 - 11/16/23)\nGauntlet would like to provide the community with an update on the Ethereum v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows increased 4.28%, from $350.15M to $365.12M.\nUSDC Supply increased 4.85%, from $368.84M to $386.74M.\nUSDC utilization decreased 0.55%, from 94.93% to 94.41%.\nThe minimum USDC reserve growth was 7.03%, and the maximum was 17.0%. The average USDC reserve growth was 10.06%.\nThe comet accumulated $37.09K USDC reserves while distributing $208.06K COMP rewards for a weekly Net Protocol Profit of $-170.98K.\n\nCollateral Asset Supply\nThis graph shows the time series of total supply of all collateral assets.\nSupply1920×1080 88.1 KB\nTo see updated statistics, please see the live version of this graph here.\nUSDC Borrows\nThis graph shows the time series of USDC borrows.\nBorrows1920×1080 57.9 KB\nTo see updated statistics, please see the live version of this graph here.\nUtilization\nThis graph shows the utilization (borrow / supply) of USDC over the past week.\nUtilization1920×1200 202 KB\nSupply Cap Usage\nThis graph shows the supply cap usage (supply / supply cap) of all collateral assets over the past week.\nSupply Cap Usage1920×1200 141 KB\\n[Gauntlet] Ethereum v3 USDC Update (09/22/2023 - 09/28/2023)\nGauntlet would like to provide the community with an update on metrics from the Ethereum v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are down 1.79%, from $295.75M to $290.45M.\nUSDC Supply is down 1.08%, from $337.99M to $334.35M.\nUSDC utilization increased 0.84%, from 87.6% to 88.3%.\nThe minimum USDC reserve growth was 12.8%, and the maximum was 14.1%. The average USDC reserve growth was 13.5%.\nThe comet accumulated $30.36k USDC reserves while distributing $183.45k COMP rewards for a weekly Net Protocol Profit of $-153.09k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-09-29 at 12.49.09 PM1802×484 67.9 KB\nTotal Collateral (USD) is down 0.49%, from $567.45M to $564.7M.\nScreen Shot 2023-09-29 at 12.49.31 PM1802×472 65.7 KB\nUSDC Supply is down 1.08%, from $337.99M to $334.35M.\nScreen Shot 2023-09-29 at 12.49.52 PM1804×494 67.1 KB\nUSDC Borrows are down 1.79%, from $295.75M to $290.45M.\nScreen Shot 2023-09-29 at 12.50.10 PM1650×450 42.5 KB\nUSDC utilization increased 0.84%, from 87.6% to 88.3%.\nSupply Caps\nScreen Shot 2023-09-29 at 12.51.49 PM3546×620 103 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-09-29 at 12.52.12 PM3548×844 185 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-09-29 at 12.50.45 PM1656×430 37.4 KB\nThe minimum USDC utilization was 87.1%, and the maximum was 88.9%.\nThe minimum USDC reserve growth was 12.8%, and the maximum was 14.1%. The average USDC reserve growth was 13.5%.\nScreen Shot 2023-09-29 at 12.51.09 PM1660×444 33.5 KB\nThe comet accumulated $30.36k USDC reserves while distributing $183.45k COMP rewards for a weekly Net Protocol Profit of $-153.09k.\\n[Gauntlet] Ethereum v3 USDC Update (10/06/2023 - 10/12/2023)\nGauntlet would like to provide the community with an update on metrics from the Ethereum v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are down 7.94%, from $305.18M to $280.95M.\nUSDC Supply is down 9.93%, from $344.42M to $310.2M.\nUSDC utilization increased 2.21%, from 88.6% to 90.6%.\nThe minimum USDC reserve growth was 13.6%, and the maximum was 16.1%. The average USDC reserve growth was 14.5%.\nThe comet accumulated $33.93k USDC reserves while distributing $188.29k COMP rewards for a weekly Net Protocol Profit of $-154.37k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-10-13 at 3.28.05 PM1808×488 66.3 KB\nTotal Collateral (USD) is down 5.23%, from $587.08M to $556.35M.\nScreen Shot 2023-10-13 at 3.28.24 PM1802×478 63.7 KB\nUSDC Supply is down 9.93%, from $344.42M to $310.2M.\nScreen Shot 2023-10-13 at 3.28.34 PM1808×474 63.9 KB\nUSDC Borrows are down 7.94%, from $305.18M to $280.95M.\nScreen Shot 2023-10-13 at 3.29.36 PM1658×450 45 KB\nUSDC utilization increased 2.21%, from 88.6% to 90.6%.\nSupply Caps\nScreen Shot 2023-10-13 at 3.30.58 PM3792×616 106 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-10-13 at 3.31.20 PM3790×790 178 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-10-13 at 3.29.58 PM1658×428 36.6 KB\nThe minimum USDC utilization was 88.2%, and the maximum was 91.7%.\nThe minimum USDC reserve growth was 13.6%, and the maximum was 16.1%. The average USDC reserve growth was 14.5%.\nScreen Shot 2023-10-13 at 3.30.14 PM1680×454 32.9 KB\nThe comet accumulated $33.93k USDC reserves while distributing $188.29k COMP rewards for a weekly Net Protocol Profit of $-154.37k.\\n[Gauntlet] Ethereum v3 USDC Update (10/20/2023 - 10/26/2023)\nGauntlet would like to provide the community with an update on metrics from the Ethereum v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are up 10.22%, from $293.61M to $323.6M.\nUSDC Supply is up 10.15%, from $314.92M to $346.9M.\nUSDC utilization increased 0.10%, from 93.2% to 93.3%.\nThe minimum USDC reserve growth was 7.6%, and the maximum was 17.0%. The average USDC reserve growth was 13.7%.\nThe comet accumulated $37.46k USDC reserves while distributing $196.85k COMP rewards for a weekly Net Protocol Profit of $-159.39k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-10-27 at 6.16.17 PM1800×480 69.1 KB\nTotal Collateral (USD) is up 6.65%, from $705.54M to $752.45M.\nScreen Shot 2023-10-27 at 6.16.35 PM1802×458 67 KB\nUSDC Supply is up 10.15%, from $314.92M to $346.9M.\nScreen Shot 2023-10-27 at 6.16.53 PM1800×494 67.7 KB\nUSDC Borrows are up 10.22%, from $293.61M to $323.6M.\nScreen Shot 2023-10-27 at 6.17.14 PM1674×468 48.3 KB\nUSDC utilization increased 0.10%, from 93.2% to 93.3%.\nSupply Caps\nScreen Shot 2023-10-27 at 6.19.31 PM3784×614 107 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-10-27 at 6.19.54 PM3796×834 208 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-10-27 at 6.17.32 PM1664×430 49 KB\nThe minimum USDC utilization was 89.4%, and the maximum was 95.6%.\nThe minimum USDC reserve growth was 7.6%, and the maximum was 17.0%. The average USDC reserve growth was 13.7%.\nScreen Shot 2023-10-27 at 6.18.47 PM1678×456 33.5 KB\nThe comet accumulated $37.46k USDC reserves while distributing $196.85k COMP rewards for a weekly Net Protocol Profit of $-159.39k.\\n[Gauntlet] Ethereum v3 USDC Update (10/27/2023 - 11/02/2023)\nGauntlet would like to provide the community with an update on metrics from the Ethereum v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are up 6.75%, from $323.6M to $345.45M.\nUSDC Supply is up 5.43%, from $346.9M to $365.74M.\nUSDC utilization increased 1.25%, from 93.3% to 94.4%.\nThe minimum USDC reserve growth was 6.5%, and the maximum was 17.0%. The average USDC reserve growth was 12.4%.\nThe comet accumulated $41.55k USDC reserves while distributing $206.9k COMP rewards for a weekly Net Protocol Profit of $-165.36k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-11-03 at 11.13.35 AM1798×512 67.7 KB\nTotal Collateral (USD) is up 2.02%, from $776.59M to $792.28M.\nScreen Shot 2023-11-03 at 11.13.51 AM1796×504 65.4 KB\nUSDC Supply is up 5.43%, from $346.9M to $365.74M.\nScreen Shot 2023-11-03 at 11.14.09 AM1802×504 65.4 KB\nUSDC Borrows are up 6.75%, from $323.6M to $345.45M.\nScreen Shot 2023-11-03 at 11.14.24 AM1672×462 55.3 KB\nUSDC utilization increased 1.25%, from 93.3% to 94.4%.\nSupply Caps\nScreen Shot 2023-11-03 at 11.16.34 AM3536×622 105 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-11-03 at 11.16.59 AM3540×848 198 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-11-03 at 11.14.37 AM1664×430 52.5 KB\nThe minimum USDC utilization was 90.9%, and the maximum was 96.9%.\nThe minimum USDC reserve growth was 6.5%, and the maximum was 17.0%. The average USDC reserve growth was 12.4%.\nScreen Shot 2023-11-03 at 11.14.52 AM1666×452 36.2 KB\nThe comet accumulated $41.55k USDC reserves while distributing $206.9k COMP rewards for a weekly Net Protocol Profit of $-165.36k.\\nEthereum USDC Market Update (7/28/23 - 8/3/23)\nGauntlet would like to provide the community with an update on metrics from the Ethereum USDC comet over the past week.\nSimple Summary\n\nGauntlet’s proposal 1 to increase supply caps for WBTC, LINK, and UNI was executed on 7/31/23.\nUSDC borrows are up 7.52%.\nUSDC supply is up 2.52%.\nThe comet accumulated $27.8k USDC reserves over the past week, with an average reserve growth of 9.2%.\nThe comet distributed $225.8k COMP rewards over the past week, for a Net Protocol Profit of -$198.0k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week.\nMarket Growth\nScreen Shot 2023-08-04 at 7.15.10 AM1802×494 67.5 KB\nTotal Collateral (USD) is up 5.54%, from $726.93M to $767.18M.\nScreen Shot 2023-08-04 at 7.15.26 AM1804×496 67.2 KB\nUSDC Borrows are up 7.52%, from $361.45M to $388.63M.\nScreen Shot 2023-08-04 at 7.15.49 AM (1)1804×496 65.7 KB\nUSDC Supply is up 2.52%, from $380.41M to $389.99M.\nSupply Caps\nGauntlet’s proposal 1 to increase supply caps for WBTC, LINK, and UNI was executed on 7/31/23.\nScreen Shot 2023-08-04 at 7.39.29 AM3536×620 102 KB\nAs seen above, only COMP (93.6%) has a supply cap utilization> 75%. We recommend not increasing the COMP supply cap due to lower liquidity.\nScreen Shot 2023-08-04 at 7.39.01 AM3552×832 181 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-08-04 at 7.54.15 AM1662×420 41.4 KB\nThe minimum USDC utilization was 84.9%, and the maximum was 100.3%.\nThe minimum USDC reserve growth was -1.0%, and the maximum was 18.3%. The average USDC reserve growth was 9.2%.\nScreen Shot 2023-08-04 at 7.53.03 AM1670×452 35.8 KB\nThe comet accumulated $27.8k USDC reserves while distributing $225.8k COMP rewards for a weekly Net Protocol Profit of -$198.0k.\\n[Gauntlet] Ethereum v3 USDC Update (08/11/2023 - 08/17/2023)\nGauntlet would like to provide the community with an update on metrics from the Ethereum v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are down 12.18%, from $396.27M to $348.01M.\nUSDC Supply is up 2.14%, from $398.26M to $406.77M.\nUSDC utilization decreased 14.23%, from 99.5% to 85.3%.\nThe minimum USDC reserve growth was -0.9%, and the maximum was 14.3%. The average USDC reserve growth was 7.4%.\nThe comet accumulated $28.21k USDC reserves while distributing $171.5k COMP rewards for a weekly Net Protocol Profit of -$143.29k.\n\nNote that, as of today, USDC supply has decreased to $351M and USDC borrows have decreased to $298M. These updates will be included in the weekly update next week.\nA large reason USDC borrows have decreased is due to liquidations in the recent market downturn, particularly the user with address 0x930af7923b8b5f8d3461ad1999ceeb8a62884b19. This user supplied $35.97M collateral (98.5% WETH, 1.5% COMP) and borrowed $28.37M USDC.\nAs a result of the recent liquidations, Ethereum v3 USDC reserves have increased from $1.39M to $2.93M (+$1.54M), effectively increasing total Compound USDC reserves across v2 and v3 comets by ~10%.\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-08-18 at 1.51.13 PM1800×482 67.6 KB\nTotal Collateral (USD) is down 7.57%, from $703.19M to $649.94M.\nScreen Shot 2023-08-18 at 1.51.47 PM1802×470 67.1 KB\nUSDC Supply is up 2.14%, from $398.26M to $406.77M.\nScreen Shot 2023-08-18 at 1.52.14 PM1802×474 67.9 KB\nUSDC Borrows are down 12.18%, from $396.27M to $348.01M.\nScreen Shot 2023-08-18 at 1.52.47 PM1660×454 34.6 KB\nUSDC utilization decreased 14.23%, from 99.5% to 85.3%.\nScreen Shot 2023-08-18 at 2.07.01 PM3546×618 101 KB\nAs seen above, only COMP (91.1%) has a supply cap utilization> 75%. We recommend not increasing the COMP supply cap due to lower liquidity.\nScreen Shot 2023-08-18 at 2.08.38 PM3540×826 167 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-08-18 at 1.53.14 PM1652×410 36.5 KB\nThe minimum USDC utilization was 85.3%, and the maximum was 100.3%.\nThe minimum USDC reserve growth was -0.9%, and the maximum was 14.3%. The average USDC reserve growth was 7.4%.\nScreen Shot 2023-08-18 at 1.53.46 PM1666×446 32 KB\nThe comet accumulated $28.21k USDC reserves while distributing $171.5k COMP rewards for a weekly Net Protocol Profit of -$143.29k.\\n[Gauntlet] Ethereum v3 USDC Update (08/18/2023 - 08/24/2023)\nGauntlet would like to provide the community with an update on metrics from the Ethereum v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are down 21.76%, from $348.01M to $272.28M.\nUSDC Supply is down 19.38%, from $406.77M to $327.93M.\nUSDC utilization decreased 2.95%, from 85.6% to 83.0%.\nThe minimum USDC reserve growth was 9.3%, and the maximum was 14.2%. The average USDC reserve growth was 11.8%.\nThe comet accumulated $23.73k USDC reserves while distributing $142.46k COMP rewards for a weekly Net Protocol Profit of -$118.73k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-08-25 at 3.33.09 PM1806×492 69.2 KB\nTotal Collateral (USD) is down 14.19%, from $600.32M to $515.12M.\nScreen Shot 2023-08-25 at 3.33.37 PM1804×486 69 KB\nUSDC Supply is down 19.38%, from $406.77M to $327.93M.\nScreen Shot 2023-08-25 at 3.34.17 PM1806×478 67.5 KB\nUSDC Borrows are down 21.76%, from $348.01M to $272.28M.\nScreen Shot 2023-08-25 at 3.34.48 PM1656×448 44.9 KB\nUSDC utilization decreased 2.95%, from 85.6% to 83.0%.\nSupply Caps\nScreen Shot 2023-08-25 at 3.38.16 PM3536×622 102 KB\nAs seen above, only COMP (91.9%) has a supply cap utilization > 75%. We do not recommend increasing the COMP supply cap due to lower liquidity.\nScreen Shot 2023-08-25 at 3.38.48 PM3542×848 166 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-08-25 at 3.35.18 PM1646×424 38.1 KB\nThe minimum USDC utilization was 82.6%, and the maximum was 89.0%.\nThe minimum USDC reserve growth was 9.3%, and the maximum was 14.2%. The average USDC reserve growth was 11.8%.\nScreen Shot 2023-08-25 at 3.35.43 PM1666×446 33.1 KB\nThe comet accumulated $23.73k USDC reserves while distributing $142.46k COMP rewards for a weekly Net Protocol Profit of -$118.73k.\\n[Gauntlet] Ethereum v3 USDC Update (08/25/2023 - 08/31/2023)\nGauntlet would like to provide the community with an update on metrics from the Ethereum v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are up 4.19%, from $272.28M to $283.68M.\nUSDC Supply is down 2.57%, from $327.93M to $319.52M.\nUSDC utilization increased 6.56%, from 83.0% to 88.5%.\nThe minimum USDC reserve growth was 9.6%, and the maximum was 14.3%. The average USDC reserve growth was 11.4%.\nThe comet accumulated $23.06k USDC reserves while distributing $141.9k COMP rewards for a weekly Net Protocol Profit of $-118.84k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-09-01 at 7.38.04 AM1802×482 67.7 KB\nTotal Collateral (USD) is up 4.56%, from $524.75M to $548.66M.\nScreen Shot 2023-09-01 at 7.38.25 AM1800×486 66.4 KB\nUSDC Supply is down 2.57%, from $327.93M to $319.52M.\nScreen Shot 2023-09-01 at 7.38.45 AM1806×476 66 KB\nUSDC Borrows are up 4.19%, from $272.28M to $283.68M.\nScreen Shot 2023-09-01 at 7.39.03 AM1654×454 33.1 KB\nUSDC utilization increased 6.56%, from 83.0% to 88.5%.\nSupply Caps\nScreen Shot 2023-09-01 at 7.42.00 AM3540×612 102 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-09-01 at 7.42.19 AM3542×840 158 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-09-01 at 7.39.21 AM1640×432 27.7 KB\nThe minimum USDC utilization was 83.0%, and the maximum was 89.1%.\nThe minimum USDC reserve growth was 9.6%, and the maximum was 14.3%. The average USDC reserve growth was 11.4%.\nScreen Shot 2023-09-01 at 7.40.08 AM1662×458 34.2 KB\nThe comet accumulated $23.06k USDC reserves while distributing $141.9k COMP rewards for a weekly Net Protocol Profit of $-118.84k.\\n[Gauntlet] Ethereum v3 USDC Update (09/01/2023 - 09/07/2023)\nGauntlet would like to provide the community with an update on metrics from the Ethereum v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are down 0.25%, from $283.68M to $282.97M.\nUSDC Supply is up 0.04%, from $319.52M to $319.65M.\nUSDC utilization decreased 0.29%, from 88.8% to 88.5%.\nThe minimum USDC reserve growth was 11.8%, and the maximum was 15.4%. The average USDC reserve growth was 14.3%.\nThe comet accumulated $30.61k USDC reserves while distributing $134.49k COMP rewards for a weekly Net Protocol Profit of $-103.88k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-09-08 at 9.02.48 AM1806×500 68.3 KB\nTotal Collateral (USD) is up 2.89%, from $521.13M to $536.22M.\nScreen Shot 2023-09-08 at 9.03.08 AM1804×482 66.2 KB\nUSDC Supply is up 0.04%, from $319.52M to $319.65M.\nScreen Shot 2023-09-08 at 9.03.27 AM1804×484 66.5 KB\nUSDC Borrows are down 0.25%, from $283.68M to $282.97M.\nScreen Shot 2023-09-08 at 9.03.46 AM1662×460 38.9 KB\nUSDC utilization decreased 0.29%, from 88.8% to 88.5%.\nSupply Caps\nScreen Shot 2023-09-08 at 9.06.35 AM3542×616 103 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-09-08 at 9.07.21 AM3540×846 159 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-09-08 at 9.04.36 AM1658×420 32 KB\nThe minimum USDC utilization was 85.7%, and the maximum was 90.7%.\nThe minimum USDC reserve growth was 11.8%, and the maximum was 15.4%. The average USDC reserve growth was 14.3%.\nScreen Shot 2023-09-08 at 9.04.59 AM1664×452 32.6 KB\nThe comet accumulated $30.61k USDC reserves while distributing $134.49k COMP rewards for a weekly Net Protocol Profit of $-103.88k."
  },
  {
    "number_of_comments": 18,
    "postid": "14248481-539a-466b-8894-3e927b61af6a",
    "posturl": "https://www.comp.xyz/t/stmatic-compound-listing/4293",
    "combinedcontent": "## Topic: Add stMATIC to Compound\nHey Everyone, I’m Kyros, Lido on Polygon BD contributor. Here is a proposal to add stMATIC as a collateral asset on Compound. stMATIC has proven its security and performance as a collateral asset on several other lending platforms like Aave v3, 0vix, and QiDao on Polygon with healthy utilization rates and a lot of value add for the ecosystem. With its deep liquidity and extensive composability on Polygon PoS, adding stMATIC as collateral makes sense for both Compound and Lido based on the asset maturity level.\n## Background\nLido on Polygon is a DAO-governed liquid staking protocol for the Polygon PoS chain. It allows users to stake their MATIC tokens on the Ethereum blockchain and mint stMATIC, which represents their share of stake without the need to maintain the staking infrastructure. Users get staking rewards and still control and can use their stMATIC tokens in secondary markets on Ethereum and Polygon.\nMore technical details are available here: https://docs.polygon.lido.fi\n 9\nThe purpose of Lido on Polygon is to help the Polygon blockchain be more secure and decentralized and to integrate stMATIC with a variety of protocols and DeFi applications on the Ethereum mainnet and Polygon PoS chain.\nLido on Polygon is enabling users to:\n\nStake their MATIC tokens in a decentralized and secure way\nUse their stMATIC on the secondary market\n\nDo all of the above simply and easily with a click of a button on the UI\n## [stMATIC token](Lido: stMATIC Token | Address 0x3a58a54c066fdc0f2d55fc9c89f0415c92ebf3c4 | PolygonScan 4)\nstMATIC is an ERC20 token that represents the share of MATIC tokens routed through the Lido on Polygon protocol. The value of stMATIC is increasing over time due to the rewards accumulated from the Polygon validator reward emissions. There is no slashing mechanism on Polygon POS and the risk of reductions to accumulated staking balances from slashing is not part of the technology.\nLido on Polygon protocol contracts are audited: Audits | Lido On Polygon Docs.\n\nstMATIC has also been listed on Aave since May 2022.\nstMATIC also has a Chainlink oracle calculated price feed 2 which makes it extremely easy to integrate into any lending market\nstMATIC growth even in the black swan events of crypto such as Terra , 3AC , Celsius and FTX has been remarkable compared to its competitors\n\n\nScreenshot 2023-05-12 at 10.42.33 pm1384×664 34.8 KB\n\n\nScreenshot 2023-05-12 at 3.33.21 am2840×848 167 KB\n\nThe Lido DAO would also be open to using Pyth price feeds for stMATIC integration on Compound.\n## Parameters\nLiquidity - stMATIC currently has a TVL of 90M$ and total liquidity of 40M$ on ecosystem dexes with Balancer being the lead along with KyberSwap and QuickSwap having the maximum volume.\nI propose a Max LTV of 50%, Liquidation threshold of 65%, and Liquidation penalty of 10% which is a reference taken from Aave parameters to be consistent with the same numbers\n\nScreenshot 2023-05-12 at 10.42.33 pm1384×664 34.8 KB\n\n\nScreenshot 2023-05-12 at 8.59.07 pm1596×1144 56.4 KB\n\nThe biggest stMATIC LP Pool on Balancer Polygon\nI appreciate the support from the Compound community, and thank you for considering our proposal! Hope to hear from you soon, and I am available to provide any additional support or resources that the community may need.\nBest regards,\nKyros\\nWelcome Kyros and thanks for starting this discussion!\nI personally think stMATIC makes a lot of sense to add as a collateral asset in the existing USDC Comet market. Since Comet does not support rebasing tokens, is there a wrapped version of stMATIC that could be used instead?\nAnother idea (which can be done in tandem) is to launch a separate Comet market with MATIC as the base asset. Users would be able to supply stMATIC as collateral and borrow MATIC to unlock the use case of leveraging up on MATIC staking yields (similar to the cWETHv3 market 1 that’s on Ethereum mainnet).\\nstMATIC is a non-rebasable token, and therefore functions in the same way as wstETH does without the requirement to be wrapped up as stETH does.\nHaving a Comet market with MATIC as the base asset would be a great addition to the protocol in my opinion, however I would suggest the use of additional collateral assets with similar security and performance to stMATIC so that the protocol is not solely reliant on a single collateral asset.  While purely speculation, a section of Coinbase’s wallet suggests that they may soon have a MATIC staking solution that may function similarly to cbETH.  I’m aware that there are other tokens that represent staked MATIC that may also warrant further research for consideration of adding as collateral assets in a MATIC Comet market.\\n\n\n\n gazmagik:\n\nstMATIC is a non-rebasable token, and therefore functions in the same way as wstETH does without the requirement to be wrapped up as stETH does.\n\n\nThat’s good to know!\n\n\n\n gazmagik:\n\nhowever I would suggest the use of additional collateral assets with similar security and performance to stMATIC so that the protocol is not solely reliant on a single collateral asset.\n\n\nAgreed, would prefer there to be other staked MATIC assets with sufficient liquidity to add as collateral before we start such a Comet market.\\nHi hi!\nI’m Lido DAO contributor for the BizDev. Happy to see the proposal having a positive initial reaction.\nWould just like to add color here on why we should not wait for other staked Matic solutions to mature for bundle listing. stMATIC is the biggest staked asset in the Polygon market, and listing on Compound would help with derisking of lending market centralization.\nCurrently, stMATIC is listed on Aave and Mai Finance however, Aave is dominating the whole ecosystem. I firmly believe that to grow and further push utilization of stMATIC and lending markets in general, markets itself need to be more diverse.\nI’m not at liberty to discuss Coinbase situation, but it does not have sense to limit Compound growth based on speculation and time to maturity of other protocols in the bear market.\nOpen for feedback and discussion here.\nThanks everyone!\\nHey!\nIgnacio from Avantgarde! just wanted to say that we’re totally on board with this proposal. If you’re looking to mix things up a bit in the collateral assets, we’ve got a suggestion: how about adding Stader Labs’ Staked Matic, MaticX? It’s a robust and widely used asset that’s gaining a lot of popularity in Balancer.\\nAppreciate the support on the proposal. However cbMATIC/MaticX’s liquidity is a issue . The LP size on balancer is almost 4 times smaller than stMATIC and would create problems in case of liquidations . Adding MaticX/cbMATIC atm would not be the best choice of collateral compared to stMATIC which has LP depth and max composability on polygon\\nI have personally been working with the Lido team and Kyros for over a year now to help grow stMATIC on Polygon PoS. That being said, I am 100% in favour of this proposal and feel that this would be a great addition to Compound v3 on Polygon.\nBorrowing USDC against $stMATIC makes more sense that borrowing against $MATIC since you are essentially borrowing USDC at a discount, since the staking APR of $stMATIC is 4.28% which is greater than the borrow APR of $USDC which is currently 3.92% with extra $COMP rewards of 3.35%. As a result you are getting paid to borrow $USDC against stMATIC on compound \n\nScreenshot 2023-05-15 at 6.20.19 PM1072×468 64.4 KB\n\n\nScreenshot 2023-05-15 at 6.20.05 PM860×592 51.2 KB\n\nThere is already ~24m $stMATIC tokens on Aave and this proposal would definitely see a huge inflow to compound v3. Dont think there is much to think about here \nLets      \\nHi!\nPersonally, I am a big supporter of this proposal. As we’ve seen across Mainnet and PoS chains broadly, LSD adoption remains a core driver for DeFi. Adding stMATIC as collateral is a natural fit, allowing Compound to differentiate its Polygon markets relative to other chains, as well as increases ecosystem adoption of the Compound platform, while simultaneously bolstering network security as stMATIC adoption grows.\\n\n\n\n Apegen:\n\nWould just like to add color here on why we should not wait for other staked Matic solutions to mature for bundle listing.\n\n\nJust wanted to clarify that the bundle listing discussion was in regards to a new Comet market on Polygon where MATIC is the base asset. This new market will serve the use-case of levering up on staked MATIC yield (e.g. deposit stMATIC, borrow MATIC, buy more stMATIC).\nIn terms of adding stMATIC as a collateral asset to the existing Comet market where USDC is the base asset, I agree there is no need to wait for other versions of staked MATIC to take form.\nAlso just curious if the Polygon or Lido team currently know what the most popular use-cases for using stMATIC as collateral is. Is it generally used to borrow a stablecoin or MATIC?\\nThis is certainly a natural add on. With LSD on the rise having multiple LSD tokens listed as collateral would be also helpful in growing their ecosystem on the chain.\nstMatket also has the biggest market outside of ETH and has seen tremendous success on AAVE as well. This would serve as the natural way to grow it even more.\\nMost of the stMATIC on Aave is used to borrow wMATIC via e-mode. However, there is also a lot of demand to borrow stablecoins as well. stMATIC is also used as collateral to borrow $MAII]($1.00 | miMATIC (miMATIC) Token Tracker | PolygonScan) which is an over collateralised stablecoin from QiDao\nstMATIC has been around for more than a year and continued to grow inspite of the events like terra, FTX etc… and I strongly feel that it is mature enough and battle tested to be listed on compound v3 (Polygon)\\nThank you @Kyros for writing and publishing this proposal, and I appreciate the discussion and comments in this thread. Seems like there is a lot if interest for stMATIC.\nListing stMATIC as a collateral on Compound will be beneficial for the entire Polygon ecosystem as it brings us one step closer to making stMATIC the ultimate Polygon DeFi asset.\nBeing on Compound in the early days sets the base and enables building the supply for protocols built on top of Compound V3.\nOne important factor is to distribute the risk from the current lending platforms and provide more flexibility for protocols and users which is seriously improved by expanding to Compound as one of the tier 1 lending markets in the space.\nJakov, Lido on Polygon head\\nWe are supportive of listing stMATIC on Compound III Polygon as well as a separate deployment for supplying stMATIC and MATIC as the borrowable asset. The suggested parameters seem reasonable, however, we might need a separate discussion if a stMATIC/MATIC comet market is deployed.\nBoth deployments capture the current market demand for stMATIC nicely and will be a great revenue driver for Compound!\\nIf Compound was interested in a Token Report similar to those made in our recent grant 4 please reach out.  Our token report quickly mitigates the smart contract lists from the OpenZep checklist 2.\\nOnce this proposal has reached more community consensus, Gauntlet is happy to conduct analysis on stMATIC from a market risk perspective.\\nThere is a compound community call on May 31st where this would be discussed. I think that should serve as enough consensus \\nThere seems to be enough support for the proposal on the forum here and even discussed this during the compound community call as well . Would be great to have the risk analysis here so we can move forward with the deployment\\nPlease find Gauntlet’s analysis and parameter recommendations for stMATIC here 12."
  },
  {
    "number_of_comments": 9,
    "postid": "604b8a86-df45-4e9f-b3c2-3c4dcffcc079",
    "posturl": "https://www.comp.xyz/t/migrate-cdai-implementation-from-cdaidelegate-to-cerc20delegate/807",
    "combinedcontent": "When the DSR (Dai Savings Rate) was introduced by MakerDAO, the Compound team developed a CDaiDelegate 15 to take advantage of DSR (pre governance). With DSR remaining at 0% for such an extended period of time, it would be advantageous for the sake of gas fees to change back to a system which doesn’t enter DSR.\nI propose setting the cDai implementation to a CErc20Delegate 11 which is already the implementation for cUSDT which can be found here 14.\nThe following are rough gas estimates for a CDaiDelegate vs CErc20Delegate transactions. They are a very naive at just a few of the most recent transactions–a goes on behind the scene, but there is a definite and significant downwards trend.\nEstimated Gas Savings:\nMint 262,972 -> 187,459\nRedeem 316,559 -> 186,721\nBorrow  565,067 -> 320,452\nRepay Borrow 335,807 -> 196,837\\nThis is a great idea–\nThe cDAI contract was designed to gracefully migrate back and forth between DSR and non-DSR implementations. It might be as simple as switching to the original contract.\\nIt seems like this is a relatively straightforward, uncontested proposal. I will deploy a CAP in the coming days after running some simulations to ensure expected behavior.\\nI ran a forking simulation 11 for this proposal. It switches the cDai implementation to the current cUSDT implementation (a standard cERC20Delegate). The forking simulation indicates that all Dai will be retrieved from DSR successfully and cDai will continue to work as expected.\\nI have deployed a CAP for this proposal here 12.\nPlease delegate to 0xE73423c907842744585a93455Efcf3C86dC250b4 to support this CAP.\\nis it some advantage to return back to non-DSR version of cDAI contract instead of using cErc20Delegate for cDAI?\\nThe advantage is the significant gas savings for users.\\nI don’t handle large funds so I always calculate gas fee costs. With this move, you significantly reduce gas fee cost. Great move man. \\nCAP status update: 71k delegates received, another 29k needed.\\nProposal 34 10 is live. I wrote a forking simulation 22 post propose and everything checks out."
  },
  {
    "number_of_comments": 15,
    "postid": "dd57d9a3-850a-4eeb-9cb9-29cd6ba1d2a6",
    "posturl": "https://www.comp.xyz/t/proposal-openzeppelin-security-partnership-2022-q2-adjustment/3100",
    "combinedcontent": "\nSimple Summary\nAs outlined in the original OpenZeppelin partnership proposal, this governance proposal will update the ContributorCompSpeed at the beginning of the next quarter. This also includes a migration to using Sablier streams 7.\n\nBackground\nStarting on Dec 21st, 2021, OpenZeppelin was selected 23 to offer the Compound DAO security services including continuous audit, security advisory, and monitoring.\nOpenZeppelin has finished auditing the Compound Deployed Contracts and finalized the CToken Refactor audit (details here 3). On the Security Advisory services, OpenZeppelin has been focusing on improving the security of the Proposal Process 1 and Asset Listings. Finally, on Security Monitoring, OpenZeppelin is aiming to release an initial solution by the end of March.\nYou can see more details on our progress in the January 3 and February 4 Security Updates. We’ll also have a more detailed Update  of our Partnership available by the end of next week for the month of March.\n\nReplacing Existing Contributor Grant with Sablier Stream\nAs outlined in the original proposal, at the start of every quarter OpenZeppelin will create a proposal to update the service fee payment in accordance with the formula outlined in the proposal 23.\nAs it is the beginning of a new quarter, OpenZeppelin is updating its streaming grant and replacing its existing COMP stream with a Sablier stream. This governance proposal sets the OpenZeppelin ContributorCompSpeed to zero and sets up a Sablier stream to OpenZeppelin instead.\n\nSpecification\nAs of March 23, 2022, the average 30 day $USD/COMP VWAP 4 is $112.00, so based on the Quarterly Retainer Fee 10 formula the quarterly fee for the next quarter is 8,928.571429 COMP tokens.\nThe start date for this new fee will be 04/01/2022 0:00:00 and the end date will be 07/01/2022 0:00:00 GMT. At the end of this period we would submit a new proposal for the following quarter with an updated retainer fee calculation.\\nPlease vote YES on our governance proposal here 24 once it passes the review stage\\nPSA: We’ll be cancelling proposal 94 1 due to a timing issue with the Sablier stream. The startDate for the stream is set for AFTER the proposal can be executed so the function will revert. We’ll be cancelling the current proposal and resubmitting a new one on Monday.\nBig thanks to @arr00 for pointing out the issue.\\nThe new proposal is here: Proposal 95 22\\nHi everyone, \n\nUnfortunately, our second compensation proposal has also run into a timing issue with the Sablier stream. The proposal was not queued in the Timelock with enough time to execute by the startDate. Given that this is our third attempt, I want to take the time to explain the issue we’re facing in more detail and show how we’re planning to address it for a final time. Hopefully, this will also be useful for others that are using Sablier in future Compound proposals.\n\nSablier’s createSteam() Timing Issue Explained\nAs explained in a previous post 2, Sablier is a protocol for creating real-time payment streams within a given time frame. Sablier has been proposed as a more robust replacement for the setContributorCompSpeed function that is currently used for streaming payments to Compound contributors and vendors due to Sablier’s ability to set specific durations and amounts, as opposed to a continuous stream that must be directly turned off by governance.\n\nTo migrate a stream to Sablier through a Compound proposal, the process appears simple. You can see an example of a successful migration with Gauntlet’s proposal 82 2 with each step summarized below:\n\nEnd the setContributorCompSpeed() stream by setting the value to 0 (if applicable)\nGive the Sablier contract approval to access the necessary amount of COMP\nCall createStream() on the Sablier contract and pass it the appropriate parameters\n\nThe issue that has come up multiple times now is with the third step when calling createStream() which accepts 5 parameters as seen below. In addition to the payment details of the recipient, deposit amount and tokenAddress, the stream must know when the startTime and stopTime are for the stream.\nfunction createStream(address recipient, uint256 deposit, address tokenAddress, uint256 startTime, uint256 stopTime) public returns (uint256)\n\n{ …\n   require(startTime >= block.timestamp, \"start time before block.timestamp\");\n...\n\nMost importantly, the startTime must be set AFTER the function is called and will otherwise revert. While this seems like an obvious requirement, it becomes more complicated to fulfill when going through a 7-day governance process 1 before execution can occur. Even if the original startTime is set to be longer than 7 days, any delay in the proposal’s timeline runs the risk of missing this deadline.  \n\nIn OpenZeppelin’s case, our first proposal was submitted a day later than originally planned when we created the startTime and the parameter was not updated accordingly. This issue was raised by @Arr00 during the Review stage and so we canceled this proposal before voting began. With the second proposal 2, we double-checked that we had an extra day to execute the proposal before the startTime deadline. However, we failed to realize that any delay in queuing the proposal after it passed would also cause the same issue. The proposal passed its voting stage over the weekend while the team was not actively watching it. As a result, it was not queued in the Timelock with enough time to execute before the startTime.\nWhile this is ultimately a user error on our part, we are not alone as other Compound proposals, namely Proposal 79 1, ran into the same timing problem as well. It’s important for other Compound community members to be aware of how Sablier works to ensure it does not result in other failed proposals. We would also encourage the Sablier team to consider design changes in future versions of their protocol that are more accommodating for time-variable governance proposals.\n\nOur Plan for a Third Proposal\nUltimately, OpenZeppelin bears responsibility for failing to account for these timing concerns in both proposals submitted. Both myself and our operations team went through an internal process to check that the accounts and amount details were correct, but we failed to fully account for meeting the Sablier stream’s startTime requirement in both cases. Given that this has occurred twice, we owe the community a detailed plan for a successful third proposal. \n\nFirst of all, we intend to refund the gas costs of all 21 voters on Proposal 95 2 to ensure that our mistakes here don’t discourage community participation. It’s important to note that this is not meant as an encouragement or expectation that these same voters must vote YES on future OpenZeppelin proposals. While we feel fortunate not to have had a single NO vote on this proposal, we would be refunding those accounts as well if they existed.\nIn our third proposal’s createStream parameters, we will be providing ample buffer time for the startTime to fall comfortably after the proposal can be executed. This will include testing the startTime parameter prior to deployment of the proposal to ensure that a repeat of the first proposal is not possible. We’ll also be adjusting the compensation amount to account for the time that the old stream will continue running in Q2.\nTo address the issue of timely queuing and execution, we’ll be using a tool we frankly should have been using from the start, Defender Autotasks 2. Autotasks are a robust feature that allows web3 developers to run code snippets in response to a schedule, webhook or on-chain transaction. In this case, we’ll be configuring an Autotask that will regularly check the status of our proposal. Once it reaches the appropriate stages, our Autotask will immediately call the Queue function and then later the Execute function for the proposal. By eliminating the need for human intervention, we’ll ensure no time is lost between stages.\nGiven that this sort of Autotask would be useful for automating the lifecycle of other proposals, we plan to eventually publish this Autotask for the Compound community to use themselves. With a free-tier account on Defender 3, a Compound user will be able to setup Autotasks to automate their own proposals and other on-chain operations to easily prevent issues with Sablier stream proposals and any other time-sensitive tasks.\n\nSummary\nIn short, we are addressing the issues faced by the first two proposals by doing the following:\n\nExplaining the issue we faced using Salbier so others can avoid it\nRefunding gas costs to voters to encourage future engagement\nEnsuring the startTime parameter is sufficient with tighter internal checks and tests\nAutomating the manual steps to prevent any loss of time\n\nDespite the annoyance this situation may have caused, we’ve learned about how to be better and we hope the community will find value in the solutions that will be shared as a result.\nWe expect our third proposal will be submitted by this Friday so stay tuned.\\nThis is a known limitation of the Sablier v1 protocol, a release which is really a minimal streaming contract and in which we focused on providing a safe product above anything else (including user experience).\nIn future releases of Sablier, we will add the functionality to start a stream at block.timestamp, such that the start time gets set to whenever the tx gets mined in the future. This will make it easier for DAOs to create streams.\\nWe’ve submitted our third (and what should be our final) adjustment proposal. Voting should start at the start of next week: Compound 3\nAs we shared earlier, we have done the following:\n\nRefunded gas costs to voters on the 2nd proposal. These voters should see a transaction from this address: Address 0xec405bcd169633c0d8edc8ef869e164e42b9ec1e | Etherscan 1\n\nProvided ample time for execution before the startTime. Our proposal should be ready for execution as soon as April 15th but our startTime is set for April 20th to account for any potential delays.\nAutomated the queue and execute stages using Defender Autotasks. You can see the example we’ve developed on GitHub 5 although there are improvements we are planning to implement before officially recommending it to other community members.\n\nWe appreciate the community’s patience here as well as Sablier’s willingness to address the timing issue in their future releases.\\nWondering if anyone can explain to me the discrepancy I am noting:\nThe current proposal says that the original proposal (Dec 21) indicated:\n“…at the start of every quarter OpenZeppelin will create a proposal to update the service fee payment in accordance with the formula outlined in the proposal.”\nI am noting a discrepancy with what was originally proposed and approved in the initial proposal back on 22 Dec 2021- which actually says:\n“At the start of every 6 month period for one year OpenZeppelin will create a formal Compound Governance Proposal to update the service fee payment in accordance with the simple formula below:” (see page 5 of the proposal from link in the original proposal: Compound DAO Continuous Audit Proposal - Final - Google Docs 2)\nJust wondering why we are not following the 6 month period as approved in the Dec 21 proposal which had significant back/forth as well as significant discussion surrounding the payment and method.\\nHi @kleong\nThanks for pointing this out. The 6-month statement you’re referencing in the Proposal document is outdated and should be corrected. The text of the on-chain proposal 3 passed on 22 Dec 2021 references quarterly payments and that’s the arrangement we’ve communicated to the community both before and after it passed. The section in the Final Proposal 1 where the statement is located also has a formula called Quarterly Payments, so this is mainly an issue of consistency in the text.\nI’ll correct that in the Final Proposal to ensure it all lines up with what was approved on-chain.\\nJust to be clear then - So what this proposal appears to be doing is actually amending the previous proposal provisions that were passed/approved in the previous vote in December?  I note that you have gone into the Final Proposal (which was linked as the Final in the actual approved/voted on Proposal), and the doc has been modified to reflect that there are changes to the Fee Structure ((i) modification from 6-months to Quarterly, and lower, under bulletpoint 2 from the retainer fee from 2 payments, to 4 payments.)\nAlthough I understand the need to sometimes update inaccuracies in proposals/contracts - however, not sure that this proposal as drafted here is actually disclosing and appropriately modifying the previous language of the proposal/fee structure.  (As currently worded, this proposal seems a bit innocuous, in that, we are simply being asked to approve what was previously agreed to - which I would suggest is not exactly the case here).  Perhaps that might have been what was agreed upon in theory, in the forum discussions, but the actual language of the Final Proposal, as approved and stipulated to be the Final,  reflects a different reality.  If this was a $10,000 contract/proposal, I might not be as concerned, but we are talking about $1M/quarter. = $4M this year.  Not chum change to have unclear contract/proposal provisions about a fee structure.\n\nFee Structure\nOpenZeppelin charges a service fee for its best-in-class security solutions that seeks to be commensurate with the value that our solutions add to protocols. OpenZeppelin also wants to provide a strong signal of our alignment with the protocol. At the start of every 6 3 month period for one year OpenZeppelin will create a formal Compound Governance Proposal to update the service fee payment in accordance with the simple formula below (note the edits (cannot include the strikeout letters, but you can note the changes.):\nQuarterly Fee [COMP] =Retainer Fee\n\nRetainer Fee [COMP]\n\n$1,000,000/quarter, payable in COMP at the Volume Weighted Average Price of COMP of the previous month (source: Messari)\n** Two Four quarterly payments* paid in advance at the beginning of each 6 3 month period or 1 year streaming\n\nThis inclusive of all services provided in this proposal\n\n\n\\n\nSo what this proposal appears to be doing is actually amending the previous proposal provisions that were passed/approved in the previous vote in December?\n\nNo, that is incorrect. The text in the on-chain proposal, which was read and passed by COMP voters in December, specifically says “quarterly payments” which you can see in the governance dashboard 2 and in the tx hash on-chain 1.\n\nOpenZeppelin has revised its original proposal to focus on community feedback and excludes performance fees. OpenZeppelin’s fee will be the equivalent of $1 million USD in COMP every quarter for one year, to provide these services. This fee covers all services defined in the proposal.\n\nThe Google Doc Proposal text was not updated fully but the text contained in the final forum post and the immutable, on-chain proposal are all referencing quarterly payments as the model to be used.  Even the text you posted makes multiple references to quarterly payments and having those prices adjusted on a quarterly basis.\nI apologize for the confusion that the text in the Final Proposal has caused but the payment terms that were approved by the DAO has always been for quarterly payments. This current proposal is following those same terms.\\nThe Google Doc was incorporated by reference into the on-chain proposal (and thereby binding as well) and spells out the specifics of the services provided, as well as the detailed Fee Structure that contains the inconsistencies. All I am saying is that I think we should be careful about allowing unilateral interpretations of conflicting provisions like these, and making assumptions on what the community may/may not have understood they were agreeing to when voting.\nTo close my comments out here, I do want to clarify that I don’t actually have an issue with quarterly payments in this case, nor the merits of the proposal itself.  And I also completely appreciate OZs discloser and really thoughtful discussion surrounding some of the challenges you have faced in this process.\\nI agree with your concern about discrepancies and it will definitely be noted for the future. My personal view is that the on-chain text takes precedence over an off-chain document that’s linked since that wouldn’t be immutable. However, the discrepancy is still a potential issue in case people voted primarily based on an understanding of the Google Doc’s terms. Luckily, all the community members I’ve spoken to always assumed it was quarterly so it doesn’t appear anyone voted with the wrong assumptions. I do appreciate you bringing this to our attention so we can be more careful and consistent in future proposals.\\nSeems like the proposal execution failed as the deposit sum must be divisible by stream duration\n  \n      \n\n      dashboard.tenderly.co\n  \n\n  \n    \n\nTenderly Dashboard 6\n\n  Tenderly is an Ethereum monitoring, debugging and analytics platform. It empowers blockchain teams and individuals by providing a powerful Debugger, intuitive Gas Profiler, Transaction Simulations, Monitoring and Alerting capabilities, Advanced...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\\nYes, we found out yesterday and realized it after investigating the failed execution.\nAt this point, we’re disappointed with the number of failed proposals and will go with the simplest way to resolve this for good.\\nWe’ve submitted a new compensation proposal that’s available here 7 and canceled the old one that failed to execute.\nTo avoid any further issues, we are no longer using Sablier and will instead continue using the built-in streaming function. We’ve also simulated the proposal execution in Tenderly to be certain of success.\nAs we did previously, we will be refunding gas costs to voters on the last failed proposal. We’re disappointed with the recent issues that have been faced in our past proposals and we really appreciate the community’s patience as we work through them.\nWe’d also like to note that while we think Sablier is not currently well-suited for use in Compound governance proposals today, we look forward to future versions of the protocol that plan to address these issues. The Sablier team has been doing great work and we wouldn’t want our struggles here to undermine that fact. We’ve learned a lot ourselves and hope to share some best practices in the future that will help others avoid similar proposal issues."
  },
  {
    "number_of_comments": 12,
    "postid": "f6338d2a-2ea1-4ee8-9f40-2af11329bf4f",
    "posturl": "https://www.comp.xyz/t/rfp-16-dynamic-comp-reward-distribution/2016",
    "combinedcontent": "Upon browsing Compound Grants I noticed a proposal request related to an idea I had earlier: Weighted Algorithmic COMP Distribution 36, so I’ve taken this request under my wing.\n\nWhat\nThis proposal splits COMP rewards distributions between borrowers and suppliers. Upon passing, governance will be enabled to set reward rates specifically for borrowers vs. suppliers in any market.\nExample: ZRX currently has a distribution rate of 0.0014625 per block - 0.0014625 for suppliers and 0.0014625 for borrowers. This can be changed to 0.002925 for suppliers and 0 for borrowers, the inverse, or anything in-between (or higher/lower).\n\nWhy\nIf governance is able to change the ratio, we can more effectively incentivize, develop, and maintain markets. For example, distributing all rewards for a market to its suppliers is a good way to incentivize deposits. Or we could distribute more to borrowers to incentivize borrowing.\n\nHow\nCall Comptroller#_setCompSpeeds(cToken …, supplySpeed …, borrowSpeed …).\n\nStatus\n\n[x] Write core code\n[x] Ensure the update didn’t break any tests\n[x] Write new test cases\n[ ] Write simulation script for the proposal - after the changes are reviewed\n[ ] Create the proposal\n\n\nLinks\n\nGithub: https://github.com/compound-finance/compound-protocol/pull/144 71\n\n\\nI love to see it! This is a much-needed improvement that I think governance would be happy to implement.  Thank you for taking the lead on this.\\nGreat work getting started here. A few notes though. Firstly, I actually started working on this recently as I started in the last community call. Please publicize what you are working so we don’t waste time having two projects working to solve the same issue. If you’d like, we can coordinate work and do this together.\nSecond, I do not think that this is the correct method for going about solving this issue. While the UX of it is that we would say 50/50 or 70/30, we definitely should not be storing ratios and then calculating the amount every time. This actually would make it significantly more confusing for governance (more parameters to set) and add on unnecessary gas overhead.\nWhat should be done is just setting the borrowRate and supplyRate for each asset. I started with this, but then got sidetracked on the whitelisting account project. You can see what I did so far here 8.\\nGood points about simplifications for governance and also for gas costs.\nI was so excited to start working on grants that I forgot to ask if anyone was already working on this one. \nI think I’ll set up some project management system that we can all use to organize the community’s human resources.\nBut anyway, I’m happy to work on this with you. I’ll send you a message on Discord. \\nI’ve updated this proposal after working with @arr00 on this. Please re-read the first post.\nIt’s ready for a review! \\nAwesome work! Looking forward to seeing this get added.\\nTesting of “split COMP rewards distribution” is live on the Ropsten testnet. The new rates are as follows.\nSupply ( per block):\n\nETH: 0.02 (from 0.01)\nUSDC: 0 (from 0.01)\nUSDT: 0 (from 0.01)\nCOMP: 0.02 (from 0.01)\nDAI: 0 (from 0.01)\nUNI: 0.0002 (from 0.0001)\nZRX: 0.0002 (from 0.0001)\nBAT: 0.0002 (from 0.0001)\n\nBorrow ( per block):\n\nETH: 0 (from 0.01)\nUSDC: 0.02 (from 0.01)\nUSDT: 0.02 (from 0.01)\nCOMP: 0 (from 0.01)\nDAI: 0.02 (from 0.01)\nUNI: 0 (from 0.0001)\nZRX: 0 (from 0.0001)\nBAT: 0 (from 0.0001)\n\n\nEdit: I wrote the wrong speeds. They are correct as per the rates in the contract now.\\n\nCommunity Member Testing\nAs I mentioned in the Compound Developer Community Call, I’d love to see community members help test this improvement. Here’s how you can help.\n\nTesting distribution rates\n\nSetup\n\nSwitch over to the Ropsten testnet\n\n\nGet some ETH from here 6 or here 4\n\nUse Compound’s faucets to get all other tokens (other than COMP)… go to the app dashboard 3\n\nSelect a token you want to get from the faucet, go to the withdraw tab, click on the “FAUCET” link, then send the transaction. Once confirmed, you’ll receive some tokens.\n\nimage451×773 26.2 KB\n\n\n(Optional) Swap ETH for other tokens using Uniswap 1 or Sushiswap 1 if you want more than what the Compound faucet provides.\n\n\nTesting\n\nDistribution rates\nWe want to ensure that COMP is distributed correctly as per the rates in the post above.\nTo do this, deposit and/or borrow some tokens. The amount of COMP you should receive is as follows.\naccrued(COMP) = sum of: for each market, for both supply-side and borrow-side, delta blocks * user portion of market * rate for market\nWhere delta blocks is the change in block number from the time you borrowed/supplied to the time you withdrew/repayed (or present), and the rate for market as defined in the post above.\nExample: If I supply 10% of the TVL for the BAT (supply-side) market for 100 blocks, I should receive 100 * 0.10 * supplyRate(UNI) = 100 * 0.10 * 0.0002 COMP = 0.002 COMP\nYou can get the block number for each transaction by viewing the transaction on Etherscan.\nIt’s simple to calculate the rates when you’re only active in one market, but it gets more complex with the more markets you’re active in. It’s important to test that the distribution rates are correct when active in multiple markets on both the supply-side and borrow-side.\n\nClaiming\nWe want to ensure that users are able to successfully claim their accrued COMP. Please try claiming such and let us know if you’re unable to.\n\nUsing the protocol as you normally would\nTry using the protocol as you normally would and ensure that everything works as expected.\n\nConclusion\nThis may be tricky for users not used to using testnets or debugging applications, so if you do wish to help us test this, just do what you can. Any help is appreciated!\\nThanks for your work.\nI’d like to check some points before sharing it in community testers.\n\nWhat kinds of test results you need? Does it need to create some reporting format? (ex. block height and tx for each activies, comp rewards tx)\nFor self verification, I’m not sure how to get user portion of market factor.\n\\nClaiming COMP works fine for me on Ropsten.\nThe CompoundLens contract needs an update for the new supply and borrow comp speeds CompoundLens on Ropsten 0xc1a7ab77932cfd41c4164a253faf5a06afc3906e 5.\n\nfunction cTokenMetadata(CToken cToken) public returns (CTokenMetadata memory)\nfunction cTokenMetadataAll(CToken[] calldata cTokens) external returns (CTokenMetadata[] memory)\n\nThe struct returned only has the old compSpeed.\n    struct CTokenMetadata {\n        address cToken;\n        uint exchangeRateCurrent;\n        uint supplyRatePerBlock;\n        uint borrowRatePerBlock;\n        uint reserveFactorMantissa;\n        uint totalBorrows;\n        uint totalReserves;\n        uint totalSupply;\n        uint totalCash;\n        bool isListed;\n        uint collateralFactorMantissa;\n        address underlyingAssetAddress;\n        uint cTokenDecimals;\n        uint underlyingDecimals;\n        uint compSpeed;\n        uint borrowCap;\n    }\n\nI think we might want to remove compSpeed and add the two new mappings?\n\nmapping(address => uint) public compBorrowSpeeds;\nmapping(address => uint) public compSupplySpeeds;\n\\n@dakeshi for each deposit, borrow, repay, withdraw, or COMP claim transaction, please report a link to the transaction on Etherscan. We can figure out all details from those.\nTo get the portion of the market,\n\nGo to the network config file 5\n\nFind the address of the cToken you’re interacting with (ex: cUSDC)\nGo to the Etherscan page for the contract (search by address), go to the “Contract” tab, then click on “Read Contract” (ex: https://ropsten.etherscan.io/address/0x2973e69b20563bcc66dC63Bde153072c33eF37fe#readContract 2)\nIf borrowing in this cToken market, read totalBorrows. Otherwise, read totalSupply.\nAdd the amount you are borrowing/supplying to totalBorrows/totalSupply (respectively)\nDivide the amount of the underlying you are borrowing/supplying (mantissa) by totalBorrows/totalSupply (respectively) to get the portion of the market.\n\nExample for borrowing 1000 USDC:\n\nUSDC has 6 decimal places, so the mantissa USDC amount is 1000e6 = 1000000000\nAt this time, totalBorrows = 1115004256069\nAfter my borrow transaction, totalBorrows = 1115004256069 + 1000000000 = 1116004256069\nPortion of market = 1000000000/1116004256069 = 0.000896 = 0.0896%\n\\nCongrats everyone, we made it!!! \nProposal 62 51\\nGauntlet has reviewed the patch and will will be voting FOR. It is good to see additional scenario tests added on the supply/borrow for set/reset which can test permutations of borrow and supply.\nThe potential long-term drawback of this approach is if we did end up in a situation where COMP had a lot of markets and most of them don’t offer COMP rewards. That would be inefficient since functions like claimComp would always check each market for rewards. In any case, this is hard to avoid because some users have accrued rewards from before a market was unCOMPed. In the future, there could be a fastClaimComp function that slightly saves gas by only attempting to claim for markets with non-zero borrow/supply speeds. In that case, a user would call claimComp first to “clear” their record for any removed markets and then fastClaim from thereon."
  },
  {
    "number_of_comments": 13,
    "postid": "142f61fc-2556-43f6-bc1c-4c4ca2f90dc8",
    "posturl": "https://www.comp.xyz/t/governance-proposal-22-borrow-limits-comptroller-patch-renamed-to-borrow-caps/233",
    "combinedcontent": "I plan to deploy the current version of the new Comptroller with borrow limits implemented over the weekend. View the PR here 29. Assuming the deployment goes as planned and matches successfully, the proposal will be made sometime next week.\nUpdate: Deployed here 15.\nThis was a somewhat long process with multiple postings about it:\n  \n    \n    \n    Borrow Limits for each Market Comptroller patch Protocol Development\n  \n  \n    With the addition of WBTC collateral, many people have been talking about putting up more safeguards to avoid total loss in an infimint situation. While without migrating WBTC to a new cToken, I don’t think a supply cap is possible, but blck and I thought it’d be beneficial if we implement borrow limits for each cToken to reduce potential losses. \nFor example, ETH is rarely borrowed, so why should it be possible to borrow the whole supply when the only possible situation for that happening will …\n  \n\n\n  \n    \n    \n    Borrow Limits Rules Governance Process\n  \n  \n    With the Borrow Limits patch code completed, it is now time to decide on a set of rules that the community board will follow when setting the borrow limits. If approved by the governance process, this board will control a multisig that will act as the Borrow Limits Guardian. The community board will consist of 6 addresses with 4 required consensus: two controlled by members of the compound team and 4 by the community. \nThis set of rules dictates how the community board sets market borrow limits.…\n  \n\n\n  \n    \n    \n    Community Multisig (4-of-6) Deployment Governance Process\n  \n  \n    This post references this Discord message posted in the #governance channel of the Compound Protocol Discord, and refers to Discord usernames from that channel. \n\nIn a community-driven effort with stewardship from @arr00, a Community Multisig has been deployed that can, through the governance process, be voted into usage by the protocol for a variety of purposes: for example, to act as Pause Guardian ; or in the future, to act as Borrow Limit Guardian . \nSix community members have been proposed …\n  \n\n\nFollowing the requests of the community, initially, the new code will be introduced without actually being utilized. The newly made community multisig will be set as the “Borrow Limit Guardian” but will not impose any limits. In future proposals, the limits will be set and the role of the community multisig will be further established.\nI have written a test suite and a fork simulating the new proposal to test the new code, and it all checks out. Additionally, the new patch has been the comptroller on Koran for the past few weeks. Please proof read the code yourself before voting on the proposal. The code does not touch any core features of the protocol, so it not been audited professionally.\nAnother new idea I would like to introduce in this proposal is rewards to users who build on the protocol to encourage further development. In the future, this could be automated by the Governance Alpha contract, but for now, this must be added as additional actions to the proposal. Following recommendations, I propose withdrawing $5000 from the SAI reserves (equivalent to 2360 SAI) to my address as a reward for bringing this feature to reality. I don’t have any precedent to base this number on, so if you think it should be different, say so!\nAll in all, the planned actions of the proposal are as follows:\n\nUnitroller _setPendingImplementation(NewComptroller)\nNewComptroller _become(Unitroller)\nUnitroller _setBorrowLimitGuardian(community multisig)\ncSAI reduceReserves(2360 * 1e18)\nSAI transfer(Arr00, 2360*1e18)\n\\nAll good, matched the PR bytecode to the deployed Comptroller contract\n\nUsing network mainnet https://mainnet-eth.compound.finance 1\nMatching contract at 0x707b501cbce95c5fdb25005a51f33c5b1aa30607 to Comptroller with args []\n Successfully matched Comptroller to 0x707b501cbce95c5fdb25005a51f33c5b1aa30607 with args []\n\\nPrior to creating the proposal, can you verify the steps taken to peer-review or audit the final contract, or get the PR 4 into a “final” state?\nThis proposal would be include the first community-developed contracts with a self-issued reward (bounty) for the engineering effort – kudos on the creative approach to using protocol reserves.\nIf the community supports this approach, it could serve as a template for other community-developed protocol upgrades (without requiring a “development fund” or centralized approval).\\nI’m going to attempt to simulate using the deployed contract, I’ll report back here with results.\\nWe won’t be proposing this for the time being. Still working on further testing/auditing. Will be back soon with more updates.\\nI support this patch, but want to call out that we should not name the upgrade “Borrow Limit” as that term is already well known/used in the interface as an individual’s borrowing limit based on their supply (example - 1 3 example - 2 3).\n“Borrow Cap” works great .\\nI was already questioning my word choice as I feel “cap” works better in the macro environment but was reluctant to change this far along. Combined with the chance of confusion, I agree with you. “Borrow Cap” it is.\\nI may have jumped the gun deploying before. I took some more input into account in recent commits from the community, Compound team, and Open Zeppelin audit 8. Again, please review the most recent commit. I’ll redeploy (hopefully for the last time) this weekend.\\nThe PR 4 is now in a final state with the comments in the audit taken into account. The contract has been redeployed here 3.\\nI simulated 4 what would happen on mainnet upgrading to  0x7b5e3521a049c8ff88e6349f33044c6cc33c113c, and found that the caps are working as expected, as far as I was able to check. There is a minor increase in gas costs for borrowing (looks about 4K or 1.66%). I checked setting up liquidations / borrows with and without the caps, before and after the upgrade. I also checked that borrows get rejected when they hit against the cap.\nTo run the simulation above, you will need to run your own ganache, or wait until the ganache on demand service is back up and running (temporarily down).\\nchecked the newly deployed contract it does matches with the github PR, all good again.\n\nUsing network mainnet https://mainnet-eth.compound.finance 1\nMatching contract at 0x7b5e3521a049c8ff88e6349f33044c6cc33c113c to Comptroller with args []\n Successfully matched Comptroller to 0x7b5e3521a049c8ff88e6349f33044c6cc33c113c with args []\n\\nThe proposal is live here 10. I pushed a post proposal fork simulation to GitHub 1 which shows that the execution will be successful and does some post execution sanity checks.\\nWondering if there has been any further discussions since the original post around what the borrow limits should be. Seems like we should develop a framework around that.\\nYes! Now that the framework to use them is in place we should discuss using it. Let’s take this to another thread."
  },
  {
    "number_of_comments": 21,
    "postid": "464786c0-090d-4797-8c2d-121838f16a4e",
    "posturl": "https://www.comp.xyz/t/ccomp-fix-with-solutions/1238",
    "combinedcontent": "Hello, so I have found some issues with cCOMP and its COMP distribution, and I have an easy solution, so no worries.\ncCOMP Dashboard 17\nCOMP Speeds 7\nLatest COMP speeds recalibration 10\nRegarding the COMP speeds and distribution:\nLets look at the data:\n\nInterest Paid/Day=$10,597.82\n10,597.82/474 = 22.35 Comp/day\n(Speeds) COMP Per Day=65.70\nTo Borrowers (daily) =32.85\nSupply APY 1.91%\nDistribution APY=3.94%\nNet Rate= 5.85%\nBorrow APY=8.97%\nDistribution APY=14.11%\nNet Rate= -5.14%\nNumber of Borrowers=91\n\nBorrowers are getting paid more comp than they pay in interest, meaning that they have no reason to ever pay it back, and are currently “locked” into farming and making a lot of leveraged comp. They are not participating in governance, they sold it/lent it.\nIf you do the math brought up in the recursive borrowing piece 5, and use the:\n\ncCOMP Collateral Factor=60%\nIteration Sum:(1/(1-cfactor))\n1/(1-.6)\n\nThen:\n\nLending size is leveraged 2.5 times and 1.5 borrow with a net borrow rate of 5.85%, and -5.14% respectively\nThe original borrower can make 27.475% from borrowing and lending comp recursively\n\n(Check the bottom of this post for accounts, take a look)\nAnother Issue arises when you look at the borrow caps.\nThis is the reason the borrowers are “locked”\nborrow Caps 3\nWe have a borrow CAP on COMP,as is it stated in\nCompound Chapter 2: Verse 27: Detail 5\n(plz laugh, or chuckle; it was cute)\n\n“A borrowing cap of 50k COMP is set as well.”\nEDIT UPDATE: The borrow cap is under control of the community multisig, but with a limit at 100k (its at around 91k ATM)\n\nA proposal is needed to up the comp the borrow cap, or lower the comp speeds.\nMarket liquidity:\nIn business, economics or investment, market liquidity is a market’s feature whereby an individual or firm can quickly purchase or sell an asset without causing a drastic change in the asset’s price\nUtilization for cCOMP: Percentage of supplied funds that are borrowed, calculated as supply/borrow\nBut utilization is “the action of making practical and effective use of something.”\nIf you cant borrow any more because all of the allowed funds are being borrowed, then there’s effectively no liquidity and 100% Utilization.\nSo what?\nWell, the liquidation system is THEORETICALLY tainted.\nLets say these accounts get margin called, someone’s gonna buy COMP on a DEX, and liquidate the portfolio.\nAnother way, in a self sustained enjoinment would be for another account to borrow comp on their own account, with no slippage, pay back the debt and close the account.\nBut… There’s no liquidity in this situation, even though the UI says there’s liquidity, on a closed end system (testnet, compound cash, extreme market conditions,Black Thursday 3), there could be a slight issue.\nAgain, this angle shouldn’t be too much of a concern, its mostly theoretical and to prove a point.\nSo, what’s the Solution?\nFor starters, we need to lower comp speeds on cCOMP to fix this imbalance.\nBecause we cant just yet change it from 50:50- Borrowers : Suppliers , cutting cCOMP speeds by 80% will make cCOMPS balance sheet make economic sense. From there we work on allowing different speeds other than 50:50.\nBecause hopefully soon we will have a grants committee up and sunning soon, I want to take half the COMP grant and attempt to fund this development, in a transparent, socially conscious and equitable way.\nIf you have any ideas about community development and equal opportunity in tech, PM me to help out.\nSo the changes:\nFrom this:\n\nComptroller._setCompSpeed(“cCOMP”, 5000000000000000)\nTo this:\n\nComptroller._setCompSpeed(“cCOMP”, 1000000000000000)\n\nComptroller._grantComp(“0x8fC1151dD92aB093EF0EB7cb144D573592510cAA”, 31000000000000000000)\n\nComptroller._grantComp(“0xdF2D2F76E8E8827f92814E49e7D95Fc1e33E4148”, 31000000000000000000)\n\nAPENDEX\nLast two addresses are large\n\nhttps://etherscan.io/tx/0x20d61a9f251e3ff6f5e4251fa34e778c09d412a241f3d51723420b5489e70e7c\nhttps://etherscan.io/tx/0x1f251bc7168ff22f78f7c8dd6149cf7a364434d5552931383729868410701bad\nhttps://etherscan.io/tx/0x769d3a98b1b7064234fb30516517b3ab95743e3d16dbc670d7ed772a061fdab4\nhttps://etherscan.io/tx/0x4cd129dc2edd4ccde766003812cc3a5b798491bec97da3bd579341eb7333cdb2\nhttps://etherscan.io/tx/0x19d7147d6a6dd43b4be050c8d4ad099d1af3b8c436e28d851e115e83af828db3\nhttps://etherscan.io/tx/0xe7354239a37e71ecf66f0653d084448e900346a41db475604d5f854077f27e48\nhttps://etherscan.io/tx/0xc4fc3098072b4f9727ac704df0a651eb569a3465a99f76a886bb7a79293f922e\nhttps://etherscan.io/tx/0x5e2a86b573ac0a15dc8a1af894808c3086daa55d9e75fb5d5143738c2423a07d\nhttps://etherscan.io/tx/0xb4a798954bf3862bbe8551def8aa6b98dd21bbb29b2b50c4ce3cc9ac98689896\n\n\n\n  \n    \n    \n    COMP Distribution Speeds Governance Process\n  \n  \n    -----BEGIN PGP SIGNED MESSAGE----- \nHash: SHA256 \nIn order to add some data to compare real borrowing to the suggested changes from @rleshner and @Sirokko, we tried to construct lower bounds on the amount of recursive leverage used. Recursive leverage is somewhat tricky to define, so we instead used a lower bound as a proxy. Before looking at the data, we will first define what recursive borrowing is (and how it is used to maximize COMP rewards) and then describe we computed a proxy. \nRecursive …\n  \n\n\n  \n      \n\n      compound.finance\n  \n\n  \n    \n\nCompound | Market Detail - COMP 17\n\n  Real-time market data across all markets in the Compound protocol.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n  \n      \n\n      compound.finance\n  \n\n  \n    \n\nCompound | Proposal Detail #35 10\n\n  COMP Speed Recalibration\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n  \n      \n\n      compound.finance\n  \n\n  \n    \n\nCompound 7\n\n  Compound is an algorithmic, autonomous interest rate protocol built for developers, to unlock a universe of open financial applications.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n  \n      \n\n      Medium – 10 Apr 20\n  \n\n  \n    \n\nCrypto ‘Black Thursday’ Under the Microscope 3\n\n  A closer look at what happened to the market microstructure on Black Thursday\n\n  \n    Reading time: 11 min read\n  \n\n  \n\n  \n    \n    \n  \n\n  \n\n\n  \n    \n    \n    COMP Distribution Speeds Governance Process\n  \n  \n    I think it’s important that these rates be set dynamically rather than every other day by governance post. It’s important to have a dedicated team attack this issue from an economic and financial perspective, like how will this affect borrowing on other markets? \n\nAccording to this study, compound is the first mover in rates and people will move their money if dissatisfied. \nAcademic research needs to be cited, evaluated and it can’t be done just by analyzing recuring borrows from comp folding. …\n  \n\n\n  \n    \n    \n    Improving Liquidation Ideas\n  \n  \n    I respect your statement, it’s okay to have a different opinion. However as far as the adoption of DeFi technology is concerned I think the ultimate goal is to fix the shortcomings of the traditional financial system, not to create a gambling place. \nWe are not wallet addreses, we own many wallet addreses and for differentiate that we try to build protocol which serves humans rather than bots. \nI have nothing against speculators and liquidation bots I just think there is a better way to solve ac…\n  \n\n\n  \n      \n\n      compound.finance\n  \n\n  \n    \n\nCompound | Docs - Security 1\n\n  Compound is an algorithmic, autonomous interest rate protocol built for developers, to unlock a universe of open financial applications.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\\ngreat suggestion\n\n\n\n massnomis:\n\nFor starters, we need to lower comp speeds on cCOMP to fix this imbalance.\nBecause we cant just yet change it from 50:50- Borrowers : Suppliers , cutting cCOMP speeds by 80% will make cCOMPS balance sheet make economic sense. From there we work on allowing different speeds other than 50:50.\n\n\ncutting cCOMP speeds and ratio reduction on borrowing side (<50%) will improve economics.\nIn general, I think the 50:50 COMP distribution does more harm than good when it comes to all supported assets. I don’t understand the effect of incentivising borrowing to that extent.\\nIt’s quite the opposite actually. Nothing great in that suggestion, while it’s still a possible solution.\nThere’s absolutely no reason to touch comp distribution for COMP market, it isn’t anything great in number, and there was a reason for setting it where it is now, as it was done to provide additional rewards for COMP holders.\nCurrent imbalance is a result of existing Borrowing CAP, which was expected to be raised, but was not. And as soon as it will be adjusted to where it should, market will likely come back to balance where it should be.\nWhy community multisig took no action, i have no idea, nobody shared reasoning at forum. But if the CAP is going to stay where is, then indeed another way to fix imbalance is to lower COMP distribution for COMP market.\\n\n\n\n Sirokko:\n\nCurrent imbalance is a result of existing Borrowing CAP, which was expected to be raised, but was not. And as soon as it will be adjusted to where it should, market will likely come back to balance where it should be.\n\n\nI just want to clear up some confusion here. Back when the cCOMP market was added, there was concern about governance manipulation though borrowing upwards of 100k COMP, as such, it was decided that setting the borrow cap above 100k is a significant action which should only be done by governance. The community multisig is following expectations, and raised the borrow cap multiple times until reaching this point.\n\n\n\n massnomis:\n\nFrom there we work on allowing different speeds other than 50:50\n\n\nI would like to see governance have the ability to fine-tune COMP distribution beyond the 50:50 allocation.\nThis issue goes much beyond the 50:50 COMP allocation though. While COMP holders definitely should receive special rewards from the protocol, I don’t believe that cCOMP is the means of distributing it. A long term staking process for COMP is needed, and personally, I don’t think this involves cCOMP. I don’t understand the drive to give the cCOMP market excess rewards, as this goes against the COMP agenda. Users who deposit to cCOMP are not participating in governance and forfeiting their voting power for monetary collateral and interest. The cCOMP market is needed, but I don’t it think should be manipulated with excess COMP rewards—an alternative for COMP staking with a different structure is needed. There is an argument to allow cCOMP holders to vote, but this comes with its own list of issues.\nTo fix this current situation, we need a new type of interest rate model which would be cognizant of the borrow cap. Treating the cap as 100% utilization would quickly “fix” the market. It is quite clear that there is significant excess demand at the borrow cap, and the interest rate should be higher. This in conjunction with lowering the cCOMP COMP distribution to be inline with other small-cap collateral assets (1950000000000000) makes sense.\\nSo this proposal is in two parts\n\nBand aid (quick lower of comp speeds)\n\n\n\n\n\nFund research and development of said permanent fix\n\nIncluding but not limited to\ndynamic interest rates cognizant of borrow caps, ccomp staking and voter participation, non 50:50 comp speeds.\nI Think Arr00 is a perfect example of who should lead the charge with development, hence the reward I suggested to him/her. It doesn’t have to be them specifically, but i think it is important to set this future/forward thinking developmentally focused precedent.\nYour ideas are exemplary and I think governance should enchorage this type of forward thinking compound community citizenship.\nDo you feel like the the 30 comp would help with this? Could it be utilized well in a transparent manner for the implementation of this idea?\\n\n\n\n Sirokko:\n\nThere’s absolutely no reason to touch comp distribution for COMP market, it isn’t anything great in number, and there was a reason for setting it where it is now, as it was done to provide additional rewards for COMP holders\n\n\nIf the user borrows a COMP token is it a holder or shorter? He is obviously a holder by the very fact that he owns COMP. However I am interested in what is the positive implication of equal distribution (50:50) of COMP tokens to someone who is a long-term believer and to someone who play short on COMP?\nWhy is borrow cap a better solution than reducing COMP speed / ratio distribution?\nIf you have already commented on this somewhere and you do not want to repeat it, feel free to put a link. Tnx\\n\n\n\n Sirokko:\n\nWhy community multisig took no action, i have no idea, nobody shared reasoning at forum. But if the CAP is going to stay where is, then indeed another way to fix imbalance is to lower COMP distribution for COMP market.\n\n\nThank you for these questions.\n“Traditional” Financially speaking, a user who borrows COMP is short the asset, because they owe it.\nHowever, COMP being a governance token, there seems to be the idea that borrowed funds will be used for that, therefore they must have a strong belief in the underlying. However, if you look at the numbers, the borrower would benefit if COMP goes down. BUT… There’s comp speeds, and just like other markets, users, both on the long and the short side get compensated in COMP for their position.\nThis works perfectly for DAI, ETH, USDC, etc, but ccomp is spoecial because the “boost” is in kind.\nThat’s the problem with the ccomp market AS OF NOW, there is no good use case as of now for borrowing, besides for gaming the system. We do have some ideas in the works, like @arr00 said, like ccomp voting, VESTING and others.\nAs of now, the mechanics are as follows,\nThere is a multisig that allows to raise the borrow cap\n-It was put in to make sure that there was no large borrows to avoid vote manipulation\n-100k Limit, for whatever reason, (I wasn’t the decision maker, I don’t hate it or disagree, it just is) before something new comes up to a proposal.\nThere are 91 Borrowers GETTING PAID 5.85% to BORROW Comp.\nThese accounts, see above, are simply recursively borrowing/farming it.\nthe last time on discord 1 The cap was raised publicly, it was to 85k, where the cap is now at like 91k.\nBecause there is no precedence for announcing new borrow limits in any other markets, only select accounts with this “inside information” (its viewable on etherscan its not a secret) have the ability to take advantage of this glitch.\nThe second the cap is updated, one tx can be made to borrow, supply, borrow supply, etc., to make 28% APY with zero risk on 4 times the total position (borrow+supply).\nIts a pointless game and it makes it seem like supplying comp is a good idea, but no one is really borrowing because the borrowers being funded entirely by comp distribution and are taking on no risk.\nSomeone who is borrowing comp is clearly gaming the system and therefore does not belive in COMP’s long term success, because they would be open and honest about the problem.\nSimilar to the “free money glitch” robinhood had, it was patched quickly and individuals were barred from taking advantage.\nBorrow cap raising will work, but doesn’t solve the issue, people will still game the system the second the cap is raised and the comp distrobution will foot the bill.\nRight now the unrealized loss is over 30 COMP/day for the system as a whole (two separate contracts, ccomp and comptroller/unitroller?)\nBut if you look at both contracts from a zoomed out veiw, you can see the loss.\nSo  Raising the cap will NOT solve the problem, because with comp speeds like this it is still a game (also we have almost reached the 100k limit)\n-But raising the cap will SEEM like it will help, more borrow demand will allow the “natural rate” to increase, but again, the borrow interest is paid by comp speeds.\nLowering comp speeds on ccomp will stop this little leverage/free money game, stopping select individuals with technical skills with intention to profit from the system.\n-lowering comp speeds doesn’t make it perfect either, we still dont have a great use case for CCOMP, but we have some in the works.\nTo speed up this process, i suggested using some of the money we save (30/Comp a day, or $5.5 million a YEAR) to help fund and speed up community development.\nFor more details on that check here 1.\nHope I answered some stuff, LMK.\\n\n\n\n massnomis:\n\nthere is no good use case as of now for borrowing, besides for gaming the system\n\n\nFor holders COMP token utility is trade-off between capitalization and governance. If somebody borrow (short) COMP what is reason for that action? If with leverage take capital gains then borrower doesnt have trade-off and use both utility features.\nMaybe I am wrong but protocol token inside its protocol needs more utility, especially if it is offered as other assets\\n\n\n\n massnomis:\n\nDo you feel like the the 30 comp would help with this? Could it be utilized well in a transparent manner for the implementation of this idea?\n\n\nThe precedent within the Compound ecosystem is for payment to come after work. We don’t bundle proposals—for a grants committee to exist, that must be its own proposal. As of now, if I decide to implement a different method to distribute COMP, I will get paid after completion.\\n\nIn the past, when a contributor wanted to claim a bounty, they would have to put the grant within the governance proposition itself. Some examples are,\n“The last two calls withdraw from the cSAI reserves about $5000 worth of SAI and sends it to Arr00 as a bounty for organizing and implementing this proposal.”, source \n“The last two actions take 622 Sai (currently valued at $1200) from reserves and transfer them to Arr00 as a bounty for his work on this proposal.” source\n“1. Award ourselves a COMP grant (1000 COMP) to offset development and audit costs.” _grantcomp in Action \n\nBy precedence, do you mean what you have done? You say “we” don’t bundle proposal, yet you were the first to transfer SAI to yourself.\nCompound Finance is a community and while you have certainly been a trailblazer in many respects.\nHowever, on a personal level, it shocks me to rely on precedent, or status quo, as a way of governance.\nImagine if someone told you before your first time spending dozens of hours, unpaid, to improve a community project, for a friend to come up and say, “we don’t do this here”. Just because it was, does not mean it is, and certainly does not mean it will be.\nRegarding the payment after completion, sometimes its not fully clear,\n“So far it has been me and a few anonymous donors, but I’m hoping that it will be funded by the protocol eventually” 1\nThere is no precedence here, and even if there were, I don’t see it a a reason to continue down this way.\nThis is a clear example of survivorship bias, Imagine how many proposals could have gotten passed with payment upfront or along the way, because as of now you are the only pure example of successful community development within governance.\nThis post and hopefully CAP is not about the grants committee, rather I am exploring the affects that having a committee will have on community governance.\nI would love to get some feedback on the following questions:\n\nDo you feel like the the 30 comp would help with this?\nCould it be utilized well in a transparent manner for the implementation of this idea?\n\nWe are looking for transparency in this process, with open documentation and financing along the way. How would you react/work with these new hypothetical implementations?\\n\n\n\n dabar90:\n\nWhy is borrow cap a better solution than reducing COMP speed / ratio distribution?\n\n\n50/50 distribution is just how protocol works currently, we can’t do uneven split at the moment. However to answer your question why i think raising a CAP is better solution, is because CAP itself is creating market imbalance.\nYou see, people are complaining that someone is getting paid to borrow COMP and then they can use it to supply it back, leveraging their position. It’s true, but that’s because no new borrowers can enter. By allowing, like another 50k borrowers to borrow, borrow rate will quickly become negative, and on COMP market rates are quite high, so ALL borrowers are going to pay for borrowing, instead of now none of the borrowers are paying for borrowing. COMP market CAP is just creating artificial non-market condition which makes that an issue.\nI’m not really that much opposing decreasing COMP speed, the thing is it was hard enough to introduce even a little bit more inclusion of all markets in COMP distribution instead of dropping 80% of distribution to stable coins exclusevly. And now we discussing about taking it back and what? Put it back to stable coins? Or just decrease overall daily distribution if putting it nowhere? It’s not out of the table, but how it’s better than just letting COMP market to reach balance naturally. If it wouldn’t be able to do it with raising CAP, than sure, cutting distribution is logical next step.\n\n\n\n arr00:\n\nI don’t understand the drive to give the cCOMP market excess rewards, as this goes against the COMP agenda. Users who deposit to cCOMP are not participating in governance and forfeiting their voting power for monetary collateral and interest.\n\n\nI used to think that way also, but looking at developments in DeFi, i adjusted my opinion. I now don’t think holding COMP or, locking it in staking contract, or vesting should be really that much incentivised. Is extremelly capital-inefficient and just create dead capital.\ncComp on the other hand is a sort of a wrapped COMP. It doesn’t itself take away capital from defi financial system, as it’s continie to work as collateral. I believe cCOMP probably should be upgraded to be used as primary location for COMP for long term holders and voters. It might be not the best solution, but it’s surely better than plain COMP and vested COMP. Staked COMP might be different, depending on what is actually done with that staked COMP, but since it’s not exist, cCOMP is the only thing what we have.\nI understand concerns that some people have about borrowing COMP to be used in governance, even if personally don’t see it as an issue, so far nobody showed interest in borrowing COMP to propose something and i doubt it will happen in future, as even with 200k CAP it will easily be outvoted.\nBut that being said, while i was very much FOR COMP market, my main idea was to let COMP holders use it as collateral, to allow them use their capital, or at least portion of it, borrowing side excluded.\n@arr00 Since you are active participant in Compound development side, here is another concept to think about. It might be relatively long-shot solution, but relatively simple to implement as it based on already existing code.\nIntroduction of cCOMP2 market. Why and for what:\nPrimary difference of cCOMP2 market from cCOMP market would be zero borrowing side, done with 0 borrow CAP. So COMP placed there would be able to used as collateral to borrow other tokens but not allowed to be lend.  That would allow long term holders to place their COMP there and not to be exposed to any risks from the borrow side.\nSecond, since cCOMP2 pool couldn’t be lend and just sit there in the same amount, then vote delegation could be introduced, so the Supplier to cCOMP2 could delegate his votes either to himsef, or to other address he choose. That would achieve second goal of letting Supplier to preserve it’s voting power.\nAs a future options, there could be done a sort of timelock, for example on ability to withdraw tokens from pool. As well as some additional rewards could be distributed to Suppliers (“Stakers”). “Stakers” could potentially also have a bigger voting weight in future or something like that. That would be logical step in evolution of governance for Compound from what it is currently.\\n\n\n\n Sirokko:\n\nBut that being said, while i was very much FOR COMP market, my main idea was to let COMP holders use it as collateral, to allow them use their capital, or at least portion of it, borrowing side excluded.\n\n\nYes. Perfect Situation\\nI think borrowing should be barred from ccomp (or at least reexamined) and comp speeds taken to near zero. As @rleshner says, we need to find the levers that can incentivice user and capital behavior and the ccomp market is just the place to allow for a “federal reserve” or a hub of economic policy.\\n\n\n\n Sirokko:\n\nI’m not really that much opposing decreasing COMP speed, the thing is it was hard enough to introduce even a little bit more inclusion of all markets in COMP distribution instead of dropping 80% of distribution to stable coins exclusevly. And now we discussing about taking it back and what? Put it back to stable coins? Or just decrease overall daily distribution if putting it nowhere? It’s not out of the table, but how it’s better than just letting COMP market to reach balance naturally. If it wouldn’t be able to do it with raising CAP, than sure, cutting distribution is logical next step.\n\n\nFor this idea, just ccomp comp speeds will be adjusted, nothing else.\\n\n\n\n Sirokko:\n\nIntroduction of cCOMP2 market. Why and for what:\nPrimary difference of cCOMP2 market from cCOMP market would be zero borrowing side, done with 0 borrow CAP. So COMP placed there would be able to used as collateral to borrow other tokens but not allowed to be lend. That would allow long term holders to place their COMP there and not to be exposed to any risks from the borrow side.\n\n\nI think we should try and keep one unified market, since the contracts are changeable, whereas cETH and cUSDC (I think) are immutable.\\n\n\n\n Sirokko:\n\n\n\n\n arr00:\n\nI don’t understand the drive to give the cCOMP market excess rewards, as this goes against the COMP agenda. Users who deposit to cCOMP are not participating in governance and forfeiting their voting power for monetary collateral and interest.\n\n\nI used to think that way also, but looking at developments in DeFi, i adjusted my opinion. I now don’t think holding COMP or, locking it in staking contract, or vesting should be really that much incentivised. Is extremelly capital-inefficient and just create dead capital.\n\n\nI understand your arguments and I think they are reasonable, however my opinion is that the COMP token itself does not have enough incentives on their native protocol (Compound). Given the “utility trade-off,” what is the capitalization advantage of COMP tokens over ETH or WBTC? Due to the smaller collateral factor (75% vs 60%) and the very low interest rate on the capitalization of the COMP token, there is a lack of economic incentives.\nMaybe my point of view is wrong because I look at things from a user perspective?\\nIn my opinion, increase the borrow CAP on COMP will help a lot.\\nAfter reading the replies to this proposal, I’m beginning to wonder who are the owners of the addresses holding cCOMP?  They are just making money on money!  If any of them happen to be part of the Compound team, we have a lot more to do than just creating a proposal.\\nIt will, but at this moment it is not the most realistic because the multisig wont go above 100k\\nIts not those who hold ccomp, but those who are borrowing COMP\\n\n\n\n Undertow:\n\nIn my opinion, increase the borrow CAP on COMP will help a lot.\n\n\nWhy is there even a borrow cap on COMP? Its the only asset on Compound with a borrow cap.\nPossible solution::\n\n\nSet 100% market utilization at, or near, the borrow cap for borrowers only. When deriving the supplier’s interest rate:\n\n\nIf supply > borrow cap Then,\n\n\n\n\nutilization % = (total borrow / supply) / 2\n\n\n\nElse If supply < borrow cap Then,\n\n\n\n\nutilization % = (supply / total borrow) / 2\n\n\n\nElse If supply = borrow cap Then,\n\n\n\n\nutilization % = 50% or 0.5\n\n\n\nEnd If\n\n\nIf an asset has a borrow cap (like cCOMP) then the Compound community should NOT use the standard “Interest Rate Model” (IRM).  The economics of Utilization vs. APY break down once the supply increases beyond the borrow cap since the standard IRM uses the percentage borrowed from total supply as the utilization percentage.  An IRM (that accounts for a borrow cap) would have to make 100% utilization be within 10% of the borrow cap which would make since, because the maximum amount of COMP that can possibly be borrowed is borrowed.\nI think a borrow cap could be implemented on markets, but only as a percentage of the current supply, and only when/if a market’s interest rate gets too out of whack - usually right where the fold is when looking at the IRM chart.\nI noticed while looking through the forum, there was a proposal that caught my eye. It enabled borrow caps to be enabled at a later date. Here is the proposal:\n\n  \n      \n      compound.finance\n  \n  \n    \n\nCompound | Proposal Detail #22 1\n\nNew Feature: Borrow Caps\n\n\n  \n  \n    \n    \n  \n  \n\n\nHonestly, to me, this whole cCOMP ordeal looks completely doctored up and pre-planned.  Those that have a big bag of COMP can supply it to get even more, but those that don’t have a bag, must buy COMP and/or slowly build up a COMP balance\\nSo this IRM is something that is on my research agenda. I am hopefully getting a COMP grant to research exactly this. I am glad you see the issue as well."
  },
  {
    "number_of_comments": 30,
    "postid": "fde37208-5a91-463b-999d-e47891be88cc",
    "posturl": "https://www.comp.xyz/t/compound-proposal-63-temporary-patch-for-comp-distribution-bug-9-29-21/2327",
    "combinedcontent": "Starting a thread to discuss this proposal, will link to it from the proposal body and begin discussion below  in a moment.\nThe current plan is to temporarily disable COMP claims until a full patch can be tested. More info coming soon\\nAs long as distribution is halted I’m fine with it. If we can still accrue comp that would be ideal.\\nThe proposal was posted last night and will be in a review phase until 10/1 when voting will begin at 10pm https://compound.finance/governance/proposals/63 211\\nSuggest voting against this proposal because it would brick the integrations which expect being able to call claimComp which will be always reverting with the proposed change.\n    function grantCompInternal(address user, uint amount) internal returns (uint) {\n        require(false, \"COMP rewards paused\");  // <--\n\n        Comp comp = Comp(getCompAddress());\n        uint compRemaining = comp.balanceOf(address(this));\n        if (amount > 0 && amount <= compRemaining) {\n            comp.transfer(user, amount);\n            return 0;\n        }\n        return amount;\n    }\n\nExample integration that would break: YearnV2-Generic-Lev-Comp-Farm/Strategy.sol at ceeceb624196e3d7eda5d706984d4001c1c73307 · jmonteer/YearnV2-Generic-Lev-Comp-Farm · GitHub 36\\nStrong AGAINST this proposal 63 vote from me as well. (Note this forum post is also being used for proposal 64, which fixes the issue that 63 had. I’m not against 64.)\nI’ve just finished simulating this governance proposal on forked mainnet, and it breaks parts of OUSD (which currently uses Compound as an investment strategy), including large deposits.\n\nimage1154×976 135 KB\n\nWe do have ways to work around this, however, it feels really wrong to just start reverting on everyone like this. Not everyone has upgradable contracts or reconfigurations available by governance.\\nShould vote against, i think a simple return instead of revert to make the function essentially a ‘noop’ without the side effects of the claim, may be less intrusive for upstream integrations, would patch the issue while a long term fix can be put in place.\\nFolks are definitely receptive to these valid concerns with respect to integrations. Another approach 33 that uses a simple filter to skip claims potentially amplified by the bug – in a way that favors potentially skipping some valid claims over permitting any invalid claims – is under review. Eyes and comments from integrators and others welcome!\\nHi, I’m from idle.finance 11, the proposal as it’s implemented right now would break our integration too eg here idle-contracts/IdleTokenGovernance.sol at c0368b4fb4f0aec1ec7a93043e4da35dc658a5a1 · Idle-Labs/idle-contracts · GitHub 18 so we suggest to vote against this proposal.\nThis approach instead\n\n\n\n allthecolors:\n\nAnother approach  that uses a simple filter to skip claims\n\n\nseems to be good enough for us\\nI like the revised approach you posted, @allthecolors. Just affecting some accounts, and not all accounts, and simply not sending COMP at this time, vs reverting, seems like a great temporary way of handling this. Much better.\\n\nProposal 64\n\nCode 107\nDetails 110\n\\nHey man after this temporary disabled claim on comp ability is over should expect to see a comp balance because since Wednesday I did not know why it was a line on my claim comp ability to view my earnings.\\nSome one needs to speak  up because I know that defi and or centralized platforms work to leave earnings in wallets of users and that is rule your jeopardizing my hard inventory and my earnings I legally and fairly supplied.speak\\nI feel that if your gonna rewrite the code you should tell me what to expect once your done rewriting the code because logically speaking I have not even hit the claim button on my comp that you disabled so I demand to know my balance before I agree to your restoration and me having to wait for you all to rewrite the code.thank you tell me my comp balance I’m gonna see it anyway 3 days and counting.\\nHey @Theo, we apologize for having you and others in the dark about everyone’s COMP rewards. Please bear with us as we’re all hard at work fixing this and doing damage control.\nEveryone is still earning COMP at the regular rate. We temporarily disabled the ability to view accrued COMP and also to claim it from the UI while we conduct our investigation. The ability for users to view their accrued COMP will be restored before proposal 64 goes to vote.\nProposal 64 does two things:\n\nFixes the accrual bug\nTemporarily disables claiming COMP for the users potentially affected by the bug\n\nAfter proposal 64 executes, there’ll be a follow-up proposal to fix the accrued COMP amounts for the users affected by this bug (set to the amount they fairly and rightfully earned). At this point, we’ll restore the ability for everyone to claim their COMP.\nWith that being said, everyone will continue accruing COMP at their normal and expected rates. This is true in the moments leading up to proposal 64 as well as after. The COMP fairly earned will without a doubt be given to users.\nUsers who have not interacted with the affected markets (those without COMP rewards) since proposal 62 was executed will be able to claim their COMP just fine. For those who have, I estimate it’ll take about a week for their ability to claim COMP to be restored.\nOnce again, I want to make it clear that COMP rewards are still accruing at normal and expected rates and that everyone will be able to claim their rightfully earned COMP - the majority right away and the rest shortly after.\nWe thank everyone for their patience and their trust in Compound protocol!\\nIf I understand correctly, can you pick up rewards from users who have not yet taken them?  Is it okay at all?  Doesn’t sound like decentralization\\ni am a casual user and this is confusing, can someone answer the below questions please.\n\nI have almost a year worth of COMP token sitting there unclaimed, now i cannot see the balance nor claim it from GUI.\nToday:  is there a way to see how many COMP token i have unclaimed + claim those tokens via API or other methods\nAfter this proposal:  I will be able to see and claim the COMP tokens again from GUI. But sounds like you guys going to do something to it to reduce the balance already received for some random people due to the bug.  Is that it?  But why disable the VIEWING of everyone’s COMP balance, the whole point of defi is it’s open.  Makes no sense.\n\nThanks\\nWouldn’t it make much more sense to just set comp accrual speed for all markets to non-zero to avoid the bug?\\nyou can see your comp balance through the API https://api.compound.finance/api/v2/governance/comp/account?address= 41\nJust add you address at the end of that.\nin the response, comp_allocated is what you are looking for.\\nHey everyone, I have to go now so I can’t answer the questions above, but I’m here to post the link to the upgrade scenario to ensure that proposal 64 does what it’s expected to do.\nhttps://github.com/TylerEther/compound-protocol/blob/dynamic-comp-rewards-distribution-2/spec/sim/0064-fix-comp-accruals/hypothetical_mainnet_upgrade.scen 49\\n\nScreenshot_20211003-231220_Chrome1440×3040 228 KB\n\nThink there’s some issue when doing the math here on the bugged txs\\nYou can use as an option: DeFi Saver 8\n\nTrack Address\\n@hayesgm just posted an analysis of prop 64 34 from Compound Labs\\n@dvf Any more info on the severity of the issue if 63 and 64 BOTH pass would be VERY helpful right now. This means that claimComp() would revert for a little more than 2 days before the functionality was restored. Obviously no one wants to go this route if it’s possible to avoid it, but if token holders can better understand the severity on your side, they can make the decision that best balances the risks to the Compound community with the impact on your protocol\\nIn our case, Idle Finance, deposits for all idleTokens with Compound integrated would revert for basically 2 days, while redeems will be possible but only by giving up COMP tokens. The issue on Idle will also spread to other integrators that use our protocol under the hood, with unknown impact, like Yearn, Harvest Finance, Enzyme Finance, Gro and some others too (and most of them usually don’t have the ability to skip the redeem of specific gov tokens)\\nOUSD itself will be fine, we can disabled rewards collection and our users will be unaffected.\\nWhen billions are blocked on the platform and it can be trillions,  code is the main problem now.  The end justifies the means.\\nAppreciate this clarification.\nSpeaking individually, I supported Prop 63 so that the community would have an additional tool at its disposal – the ability to suspend COMP claims earlier than 64, should the need arise and only if the benefit to the protocol is greater than the risk to integrations / partners like Idle.\nThere are 2 days to get comfortable with Prop 64, pass it, and cancel Prop 63.\\nOUSD 5 has completed its governance action and will be unaffected by the possible upcoming reverts from prop 63.\\nWhat is the process for cancelling proposals when a substitute proposal comes into action within the 2-day window (ex. Proposal 64 resulting in the cancellation of Proposal 63 earlier today)? Do members of the community need to remove votes or is that a decision made by the multisig?\\nGenerally, to cancel a proposal, either the proposer has to cancel it, or the proposer needs to fall below the proposal threshold. However, in this case, the proposer (me) was a whitelisted proposer. For whitelisted proposers, either the proposer can cancel, or the multisig can cancel.\\nHeads up that Proposal 63 - revert when collecting COMP - was canceled by the community multisig several hours ago, and so the reverts will not be happening.\nProposal 64 (no revert) has now passed, and is waiting in timelock."
  },
  {
    "number_of_comments": 54,
    "postid": "ad4a0b4b-01bb-4330-87e4-415c88ebf203",
    "posturl": "https://www.comp.xyz/t/implement-ctoken-sweeptoken-and-return-accidentally-sent-funds/1147",
    "combinedcontent": "Two months ago, a significant amount of cUNI was accidentally sent to the cUNI contract in this transaction 51. While it is quite common for accidents to occur within DeFi, I believe that as a community, we should try to return accidentally sent funds when possible and establish a policy regarding it. This proposal acts to do so.\nI propose a process as follows:\n\nCreate a proposal at most once every 6 months to return accidentally sent funds.\nImpose a 10% penalty on the sent funds.\nOnly return funds to the address they were sent from.\nRequire  the sender to reply below this post requesting their funds back along with a signed signature or on-chain transaction with call data verifying their identity.\nA minimum value of $1,000 at the time of the post.\nPost by the sender must be made within 6 months of the accidental send. After this timeframe, the funds will be considered community assets.\n\nTechnical\nI have added a function sweepToken(address token) to the CErc20 smart contract. This will send the entire balance of any token other than the underlying to the admin (Timelock). It is callable by anyone allowing for future proposals returning user funds to be a simple as possible (the funds will already be in the Timelock).\nI have also commented out unused verify hooks for gas savings.\nThe changes implemented here are very straightforward and limited in scope so I don’t believe a formal audit is required. The new feature is extensively tested through scenario testing and the proposal has been simulated through fork simulations. The code has already been reviewed by many community members, and hopefully more will review it in the coming days. View the changes in the PR 9.\nProposal\nThe proposal will contain the following calls:\n\nUpdate the cUNI implementation to the implementation here 19\n\nsweep cUNI out of the cUNI contract into the Timelock\nTransfer 90% of the cUNI to the sender: 0xf22c2e3475e4a066f4e9f44567c950dd36112d05\n\nGrant 30 comp to Arr00\n\nI have been in touch with the sender of cUNI to the cUNI contract and they have sent me a signature confirming their identity and requested for their cUNI to be returned. The signature is posted below. The message is signed by the wallet that executed the transaction and is one of the owners of the multisig which sent the cUNI. The cUNI will be sent back to the multisig.\n{\n  \"address\": \"0x3f9a39602853f6fa8a6f134dd1d63e8a48a7a0ba\",\n  \"msg\": \"“We sent cUNI to cUNI contract. We would like it returned to the original wallet.”\",\n  \"sig\": \"0x2c0ab1890fe2df7d1793d36e213b1d70620c232f1e566de4f5ebcdb92f136d90320b9a1a825d92662ddbea9f95de2fdd010fd9aa39707649335ac541572557711b\",\n  \"version\": \"3\",\n  \"signer\": \"MEW\"\n}\n\nDiscussion throughout development: CToken sweepToken function 24\\nI love the idea that you have set up a clear process, and I mostly agree with the terms. I think the minimum value thing is something to talk more in depth on.\nThe work you have done has been noted and I think you deserve the 30 COMP for this, but the reward, IMO should be separate from the proposal.\nA grants/contributor committee would be a better place to talk about it, rather than including it here.\nI think this can set unintended consequences…What other funds need to be swept back?\nMaybe if every few months we put together a large handful of accidental transfers and reverse them, instead of one by one.\nOverall, I support this and I personally appreciate the effort you have put into this and I hope it gets passed.\nI hope this opens up more discussion about the future of how we as a community deal with problems like this.\\n\n\n\n massnomis:\n\nI think this can set unintended consequences…What other funds need to be swept back?\nMaybe if every few months we put together a large handful of accidental transfers and reverse them, instead of one by one.\n\n\nCurrently, there are not any other funds eligible for return under this policy. The proposal which occurs once every six months will return all funds eligible to be returned at once.\\n@arr00 thank you for taking charge of this development; having a sweep() function on cTokens will be useful for recovering trapped assets. We just have to make sure the community (and governance) have a clear process for when/how governance retrieves trapped assets, since it can be relatively onerous.\nCouple quick questions:\n\nIs the PR 2 final? Can you confirm it matches the deployed implementation?\nAny reason why we can’t upgrade COMP, or USDT/DAI to the latest implementation in the proposal as well?\nHave you tested the implementation on testnet? Does the simulation remove cUNI from cUNI successfully?\n\nAwesome work!\\n\n\n\n rleshner:\n\nIs the PR final? Can you confirm it matches the deployed implementation?\n\n\nThe PR is final and hasn’t had changes to the contracts in 9 days. You can use saddle match to verify the new implementation was deployed correctly.\n\n\n\n rleshner:\n\nAny reason why we can’t upgrade COMP, or USDT/DAI to the latest implementation in the proposal as well?\n\n\nWe could, however, since there are no funds which are returnable under the tentative policy trapped in any other upgradable cToken, I would prefer to wait to update their implementations.\n\n\n\n rleshner:\n\nHave you tested the implementation on testnet? Does the simulation remove cUNI from cUNI successfully?\n\n\nYes, I have run forking simulations 4 which show it will work as expected. Additionally, I updated cUSDT on Kovan 1 to use the new code. You can see the sweepToken usage on Koran here 3.\\nI think this is valuable work that you are doing here, I question whether the 30 comp should come from the community fund. (I’m not against it) I think we should consider a penalty on the sender (offender) in this situation as had they not erred, the work would be unnecessary.\\nGreat point. The new policy actually places a 10% on all cases of returning accidentally sent funds. In this first instance, that penalty will be ~$70k in cUNI which is significantly more than the value of the COMP being granted. While we could give cUNI instead of COMP, I think that contributors to the protocol should be granted COMP when possible.\\nI second that opinion – granting COMP to developers is (1) the intended use of COMP, (2) aligns incentives, (3) sets a sustainable precedent since it’s repeatable, where cUNI is not. I’m a fan!\\nGreat work, @arr00!\nIs it worth considering not allowing cUNI transfers to the cUNI contract and adding similar functionality to the other ctoken contracts?\\nThe deployed contract bytecode was successfully verified with the source code.\n\nMatching contract at 0xa1849880593E96d2f7dF77D0D38a7f2372aE10E0 to CCompLikeDelegate with args []\n Successfully matched CCompLikeDelegate to 0xa1849880593E96d2f7dF77D0D38a7f2372aE10E0 with args []\n\\nProposal 37 is now live. I have pushed a post propose fork simulation, which confirms that the proposal was formatted correctly and will execute as expected, here 6.\\nGreat proposal, thanks for setting this up.\nQuick question on the 10% penalty, where will those funds go to?\\nThey go to the Timelock contract, which is controlled by COMP Governance. You can think of it as going to the protocol.\\nAssuming this passes, will this apply retroactively to accidental deposits that happened more than 6 months ago, as long as the sender can verify its identity?  What will be the process for requesting a return of funds?\\nThere are some cases where it is not possible recovering.\nThe old version of cToken contract like cETH cUSDC cWBTC cZRX cBAT cSAI cREP are immutable contracts and cannot implement any new function to them,\nthe only possible contracts to recover accidentally sent tokens are the comptroller itself or cDAI cUSDT cUNI cCOMP\nanother thing that makes the recovers impossible if lets say DAI was sent directly to cDAI contract, in that case the DAI become part of the supply and all the “current” supplier receive it as extra interest, in other words only those token can be recovered if it is different than the underlying token.\\nSorry, I’m not very technically-savvy on these things.  Would this transaction be recoverable?\n\n  \n      \n      Ethereum (ETH) Blockchain Explorer\n  \n  \n    \n\nEthereum Transaction Hash (Txhash) Details | Etherscan 15\n\nEthereum (ETH) detailed transaction info for txhash 0x9d9a1fc7b18267a66acc782755fc36f3f1e8b38f95d0171cdf2fa28690761eb6. The transaction status, block confirmation, gas fee, Ether (ETH), and token transfer are shown.\n\n\n  \n  \n    \n    \n  \n  \n\n\\nunfortunatelly not, this is one of the case when the accidentally sent token is the same what the ctoken contract holds:(\\nThat’s what I was afraid of, but thanks for the explanation.  I still think it’s a good proposal and am definitely going to vote for it.\\nto those who want to vote and not spend gas can use  https://comp.vote/ 14\nit’s in beta though, i think\\nThe interface doesn’t allow me to vote (Proposal 037). it’s shows me “View Only”. The timing of the vote (Manual) is over?\\nThe proposal ends at block 11837442, which is in 1284 blocks from now. You should be able to vote as normal still.\\nI need to post the transaction details here, right?\\nYes, please post the details here.\\naddress 0xd7D19E87685eF99FF05f30fB6EEDdcD5850E9280 phished me on reddit, and stole my ETH (0x301b835e9793363daa46a53bf23043445641560e205a899ae8d6fc2a48d157a2)  and my COMP (0x5122afd6f8e57c92f1c46c39b5712c5d45eca6d047bacf12815e4bee4137765d)\\nI am sorry to hear, but this is not recoverable using the cToken sweepToken function.\\nwhy not?  is there at least a way to shut this guy down?\\nWe are unable to stop this person from phishing. This is one of the downsides of a decentralized blockchain.\\nAnd why wouldn’t the function work?\\nAre there any other ways to recover these funds?\\nHe then sent it to 0x23dd18fc61248395ad63cb8c25f8136652908295, does that help?\\nI’m sorry, but your funds are lost. The main point of a decentralized blockchain is that transactions are trustless and irreversible. This function can only recover funds accidentally sent to compound managed addresses.\\nI understand, thank you for replying.\nWould EtherScan be able to help?  I’ve reported the addresses to them.\\nHello, I’ve accidentally just send 5000 usdt to this compound address: Compound: cDAI Token | 0x5d3a536E4D6DbD6114cc1Ead35777bAB948E3643 13 by copypasting wrong address (not sure how it happened)\nIs there any chance it could be transferred back?\\nYes, 100%. Please post a signature proving that you own the wallet (using a tool like My Ether Wallet) and you will be queued for the next proposal (~August).\\n{\n  \"address\": \"0x2abfc5e72c2144750a332e06da376d067cc71605\",\n  \"msg\": \"Hello, I’ve accidentally just send 5000 usdt to this compound address: Compound: cDAI Token | 0x5d3a536E4D6DbD6114cc1Ead35777bAB948E3643 by copypasting wrong address (not sure how it happened)\\nIs there any chance it could be transferred back?\",\n  \"sig\": \"0x1f3182544ca45ffd44a3a3ddd06463f60bddf15cda659c9cfdb6ccb3f0ba0c072983fe3165ecd90a959e1bd07fb4be32b687547bd4e5ff08bf6440caa464b80401\",\n  \"version\": \"3\",\n  \"signer\": \"MEW\"\n}\\nHi. Newbie here. I sent CETH from MetaMask wallet to Coinbase. Didn’t know I had to convert back to ETH first. My own fault. I know.\nTransaction hash: 0x69b42920f0f3b84faa4a8cbfca5183f76ae14dce5798639ee009b5b7744831b1\nI have contacted Coinbase with no response. Is there any way I can get access to these funds?\nApologies if this is the wrong place to post this. Any assistance is greatly appreciated.\n\nimage762×1755 198 KB\n\\nHey,\nUnfortunately, there is nothing we can do to help here. Coinbase can help, but it is often quite hard to get them to respond and actually follow through in these types of situations. I wouldn’t consider it a lost cause, but only Coinbase can help at this point.\\nThank you. Your response is consistent with what I have found on Twitter and Reddit. Appreciate you taking the time to respond. All the best.\\nCoinbase have responded and advised that there is nothing they can do. However, if they end up supporting CETH it will solve the problem. They won’t confirm or deny whether or not CETH is on their roadmap. They just keep directing me back to this blog post:\n\n  \n      \n\n      help.coinbase.com\n  \n\n  \n    \n\nUnsupported Crypto Recovery  | Coinbase Help 6\n\n  Learn about what happens to unsupported crypto sent to Coinbase and crypto sent to Coinbase on the wrong network.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nWould anyone here know if there are any initiatives underway at Compound to get CETH supported by Coinbase?\nAppreciate any feedback or insights.\\nHi Community\nI have a big problem and may be some one here could help me, I was trying to send wbtc in metamask from my account 1 to account 2 (ledger) and by mistake, I sent to compound controller (my mistake) 0x3d9819210a31b4961b30ef54be2aed79b9c9cd3b. how can i see the balance on compound (if it´s posible) or how can i take wbtc back ? those are all my savings I´m desesperate. thanks in advance\nTransaction Hash:\n0x5a4e6a6f29894914be5250d793fa2bb6d98a01d13528ea0d1f39afa731934629\\nAh you found it, good. You are unable to get back your WBTC on your own; however, governance may be able to help you. Please follow the directions established here (post sig) and we will do our best to return your funds to you.\\nThanks a lot, look I sign  msg using My Ether Wallet and the masage comes codify not whith my words, I don´t know if its right or I have to do somthing different. I really apreciate your help\\n{\n“address”: “0x6918fe32bb0ccfba961c554699c1b506ed0bd458”,\n“msg”: “0x486920436f6d6d756e6974790a4920686176652061206269672070726f626c656d20616e64206d617920626520736f6d65206f6e65206865726520636f756c642068656c70206d652c20492077617320747279696e6720746f2073656e64207762746320696e206d6574616d61736b2066726f6d206d79206163636f756e74203120746f206163636f756e74203220286c65646765722920616e64206279206d697374616b652c20492073656e7420746f20636f6d706f756e6420636f6e74726f6c6c657220286d79206d697374616b6529203078336439383139323130613331623439363162333065663534626532616564373962396339636433622e20686f772063616e206920736565207468652062616c616e6365206f6e20636f6d706f756e6420286966206974c2b47320706f7369626c6529206f7220686f772063616e20692074616b652077627463206261636b203f2074686f73652061726520616c6c206d7920736176696e67732049c2b46d2064657365737065726174652e207468616e6b7320696e20616476616e63650a0a5472616e73616374696f6e20486173683a0a307835613465366136663239383934393134626535323530643739336661326262366439386130316431333532386561306431663339616661373331393334363239”,\n“sig”: “1cfc668470a95d902b25de8d7a318f8deb93b8a24e73adc9bb2904d54384a0b415965bf4848c9e441847cd8589eab6e7ed6bd761b7db9472f8ff337448bcf0471b”,\n“version”: “3”,\n“signer”: “MEW”\n}\\nYou did it correctly, but it seems like MEW broke the signing functionality in their latest release. Try using the older version here https://v5.myetherwallet.com 5.\\n{\n“address”: “0x6918fe32bb0ccfba961c554699c1b506ed0bd458”,\n“msg”: “Hi Community\\nI have a big problem and may be some one here could help me, I was trying to send wbtc in metamask from my account 1 to account 2 (ledger) and by mistake, I sent to compound controller (my mistake) 0x3d9819210a31b4961b30ef54be2aed79b9c9cd3b. how can i see the balance on compound (if it´s posible) or how can i take wbtc back ? those are all my savings I´m desesperate. thanks in advance\\n\\nTransaction Hash:\\n0x5a4e6a6f29894914be5250d793fa2bb6d98a01d13528ea0d1f39afa731934629”,\n“sig”: “0x1cfc668470a95d902b25de8d7a318f8deb93b8a24e73adc9bb2904d54384a0b415965bf4848c9e441847cd8589eab6e7ed6bd761b7db9472f8ff337448bcf0471b”,\n“version”: “3”,\n“signer”: “MEW”\n}\\nAny recommendations on if and how to recover the funds from this transaction?\n\n  \n      \n\n      Ethereum (ETH) Blockchain Explorer\n  \n\n  \n    \n\nEthereum Transaction Hash (Txhash) Details | Etherscan 3\n\n  Ethereum (ETH) detailed transaction info for txhash 0xcc43573e5ddbbaf648f055a1027bd3565b42ba7d525a185908c1331d8ab74c74. The transaction status, block confirmation, gas fee, Ether (ETH), and token transfer are shown.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\\nIt seems like you swapped $6k cCOMP for dust COMP using ZeroEx. This occurred because you used a Uniswap pool with almost zero liquidity, and you caused massive slippage. Someone else came and equalized the pool shortly after: Ethereum Transaction Hash (Txhash) Details | Etherscan 3.\nYour funds are gone, maybe try complaining to whatever frontend you used for this trade as it gave you a very bad route.\\nThank you for the reply.  I appreciate the response. A couple of follow up questions as Im trying to make sure I understand.\n-When you say ‘front end’ in this case I used Uniswap.  Is that what you’re referring to?\n-I only chose to use Uniswap when The compound.finance interface wouldn’t convert my CCOMP to COMP when I tried to disable it as collateral.\n-when you say someone else came in and equalized the pool, could you elaborate on that more.?\nWould you recommend an alternative to Uniswap for transactions like this in the future?  I obviously won’t be using that app again.\nDoes Compound Labs offer any support for this sort of thing or nah?\\nYeah I’m referring to the website you used. The fact that it let you execute that trade is sad on their behalf. The Compound.finance 1 interface allows you to convert cCOMP to COMP and should be used for this type of transaction.\nUnfortunately, there is nothing Compound Labs or the Compound protocol can do for you at this point.\\nI am planning to submit a proposal which returns these funds to @fedevido. This proposal will use code as written in PR #193 1. The proposal has been reviewed and signed off by OZ as seen here 6. We will recover the WBTC sent to the comptroller and send 90% back to 0x6918fe32bb0ccfba961c554699c1b506ed0bd458 which sent the WBTC in tx 0x5a4e6a6f29894914be5250d793fa2bb6d98a01d13528ea0d1f39afa731934629 6.\\nYou can view the exact amounts of tokens sent to the comptroller here 8. The bounty is part of tokens that were accidentally sent to cUNI last year. The transaction sweeping the cUNI is here 4.\\nYes so i see that my claimed 400 uni was sent to the compound deployer i should see the balance reflect on my wallet address.also and plus that time lock also froze my initial deposits of uni,thus the smart contract i added and supplied liquidity to? y/n\nMy Wallet address is:\\nTheo, it has been explained multiple times that the Uniswap Airdrop received by the Compound protocol (Comptroller address) does not belong to you, nor do Uniswap Airdrops received by other addresses.\nThe claim process that Uniswap created allowed any public address to “push” an eligible Uniswap Airdrop to the recipient address, and it seems that you submitted the transactions on the Compound protocol (Comptroller address) behalf, e.g. this transaction 5. Please consider this topic closed, and please stop privately harassing people about it.\\ncheck this bro i dont pay a 12-13 dollars of my money for you to tell me i dont read my own receipts.and to its not even in the comptrollers address.my claimed uni of 400 its in the Deployer 1 address.\n\nScreenshot 2022-07-21 143850 vad1680×1003 142 KB\n\\nThe same logic applies to the Compound Deployer address 2: any Ethereum address was able to submit the claim function on behalf of any Uniswap Airdrop recipient; if an address received tokens it is that addresses property."
  },
  {
    "number_of_comments": 35,
    "postid": "59195dbe-8cf4-48f0-b557-6ec3bdf6a129",
    "posturl": "https://www.comp.xyz/t/initialize-compound-iii-weth-on-ethereum/3737",
    "combinedcontent": "The USDC market has been live for over 2 months now, and undergone 4 governance proposals (116 7, 124 3, 128 6, 132 4). The market (as of 10/27/2022) has $91.33M of collateral assets, $45.33M of USDC borrowed, and $88.86M of USDC supplied. It has generated $22.84K of USDC reserves since its inception.\nWith the Compound III codebase 8 having proven itself thus far, we think now would be a good time to launch a second market on Ethereum Mainnet, utilizing WETH 6 as the base asset: cWETHv3. The deployment pull request can be found here 4.\nWe propose initializing the market with 2 collateral assets: wstETH 7 and cbETH 12.\nFor the base asset, we use the ChainLink  ETH/USD 3 price feed, as is used for WETH in the cUSDCv3 market. For wstETH, we use a custom wrapper around the ChainLink stETH/USD 4 feed to multiply by the wstETH/stETH exchange rate. This contract is currently in OpenZeppelin’s queue for audit. For cbETH, Coinbase has just launched a new cbETH/USD price feed 11 together with ChainLink.\nAs before, we would first deploy the contracts to mainnet (with supply caps set to 0), and update this thread with the verified contract addresses. We would then initiate a proposal to raise supply caps and launch the market with an initial set of reserves. The contracts can be deployed once we have (1) received input on parameters from Gauntlet and the rest of the community here, and (2) received audit feedback from OpenZeppelin on the pull request.\nWe suggest following up with another proposal after this one (as was done for cUSDCv3), to migrate remaining rewards from v2 to cWETHv3 and cUSDCv3.\\nThanks, @jared and all for the proposal. Gauntlet is conducting analysis and will return to the forums.\\nHello @jared\nThank you for your proposal.\nIt would help to know more about the Chainlink/Coinbase price feed you mentioned - I couldn’t find anything with a quick Google search. In my work at Avantgarde, I actually handle asset listings for Enzyme, so am quite familiar with the requirements for Chainlink’s verified price feeds. It seems to me that cbETH doesn’t meet the bar for one of those in terms of liquidity, so I assume there’s some sort of alternate methodology and that it’s a custom  feed. From my perspective, this will be important in evaluating cbETH as a potential collateral asset.\\nThanks for checking it out @ignaciorsg.eth, the feed can be found here: cbETH / USD | Chainlink 9. We welcome the input - what bar do you believe is not satisfied?\\nPutting the chainlink listing process aside. The distribution of liquidity is what concerns us. In the event of a liquidation, the protocol would absorb cbETH and use its WETH reserves to do so. If those reserves were depleted, it would sell cbETH at a discount to replenish. Coinbase’s off-chain orderbook is the most liquid market for cbETH right now, making the arb difficult for any potential buyers.\nWith that said, we are hugely supportive of alternative liquid staking derivatives and appreciate the effort Coinbase is going to across the ecosystem to increase the adoption of theirs. A gated launch with a tiny borrow cap that gets ramped up as on-chain liquidity increases could be a sensible way to do it.\\none thing that @ignaciorsg.eth and I were chatting about is that even during a guarded launch, it’d be nice if Coinbase could provide some sort of backstop - e.g. a standing onchain bid for cbETH at the oracle price minus whatever the discount might be when the protocol has to sell collateral to replenish the base asset. Not sure what their appetite is to do that (or what the technical implementation might look like), but figured I’d throw it out there. Also totally open to hear other ideas to that end.\\nWith Compound III (unlike prior versions) illiquidity is not a significant obstacle to supporting an asset, due to the presence of supply caps. For any non-major asset, the question should be, “how much cbETH can be safely supported, and at what risk parameters?”\nI’m excited for Gauntlet to provide this risk analysis for the community.\\nHello @jared, thank you for the proposal. I am curious to better understand the route Compound III is taking here.\nFirst, I thought Compound II and Compound III had very little overlap: Compound II would enable an interest rate market for a large diversity of assets, while Compound III would be a much less risky way to earn interests or obtain financing in cash (~usdc). I remember @rleshner sharing similar thoughts in a Twitter Space with Instadapp (please correct me if I misunderstood).\nIn your opinion, should the Compound Governance decide to deploy more and more versions of Compound III with different assets (hence overlapping with compound II)? Or should it be just for USDC and ETH (~web3 cash)?\\nAny thoughts on when this analysis will be available?\\nHi @0x7751 - we are targeting in ~2 weeks, but it is dependent on several strategic decisions the community should make which would impact our recommendations. We will pose those questions to the community shortly to kickstart the conversation.\\nGauntlet would like to kickstart the conversation around an important strategic consideration for the wETH market: how would wETH supply be encouraged?\nOn Compound V2 markets, there is natural demand to supply wETH, given that the wETH can be used as collateral. However, this natural demand as collateral does not exist on the Comet market, as the wETH supplied can not be used as collateral. Of course, wETH supply is a limiting factor to the growth of this market.\nIn the case of the wETH Comet market, the yield of supplying wETH will be significantly more important to attract wETH supply than it is on the V2 market.\nAlthough Gauntlet does not make strategic recommendations to the DAO, we wanted to kickstart the conversation, as the decisions the DAO makes here may impact our recommendations on the interest rate configuration for wETH.\nSeveral options for the community are:\nOption A: Configuring the wETH interest rate curve differently on Comet than on V2.\n\nBenefits: the interest rate curve can be configured to provide a higher yield to wETH suppliers on V3 to encourage supply while aiming to prevent excessive utilization.\nTradeoffs: there is currently a scarcity of data on the elasticity of users with regard to interest rate curves.\n\nOption B: Pause supplying of ETH on V2 to encourage supply on V3.\n\nBenefits: encourages migration to both the Comet USDC and wETH markets.\nTradeoffs: there are nuanced risk tradeoffs here. For example, pausing supply of ETH on V2 prohibits borrowers from adding more ETH collateral to their positions in times of market stress.\n\nOption C: Distribute COMP incentives on Comet (for the supply side of wETH)\n\nBenefits: encourages supply to wETH market.\nTradeoffs: cost to the protocol. In addition, there is currently a scarcity of data on the elasticity of users with regard to interest rate curves. Any future COMP incentive reduction should be made gradually to not shock the market.\n\nWe look forward to hearing the community’s input. Of course, the community may choose one or a combination of the above options.\\nThanks for starting this conversation, @pauljlei!\nAll three approaches are suitable (and possibly necessary), but as a subjective preference, I would order them as A (at launch), C (at launch, or immediately thereaftrer), B (after V3 wETH is stable). My reasoning is as follows:\nThere is no downside, cost, or risk for cWETH to use a custom interest rate model that is different than the legacy interest rate model. In fact, given the starting assets and parameters, the use-case for borrowing Ether from this market (likely to efficiently apply leverage to staked Ether) will create very different rate expectations than the existing Compound protocol. There is likely significant borrowing demand for Ether (say, 2-4% APR) below the staked Ether yields (currently about 6-8% APR), and these users will likely be borrowing for extended periods of time. On the supply side, users (individuals, institutions, or other applications) may tolerate a lower yield given the liquidity of Compound markets (say, 1-3%). This market should likely have an interest rate model that targets high levels of utilization, with a ceiling on borrowing costs in line with staked Ether yields.\nIn regards to including WETH in the distribution of COMP, you’re right that it makes more sense to include the suppliers of Ether (and not borrowers) in this market, as borrowing will be the more “obvious” use-case, whereas USDC is the opposite (massive supply of USDC seeking yield).\nOnly after this market is running, at scale, would it make sense for the community to debate whether to disable functionality in the V2 protocol (though my view is, it might make sense to keep it as-is in perpetuity, just incentivize migration in economic ways).\nExcited to hear others opinions and ideas.\\n\nweth_utilization1177×356 34.5 KB\n\n\nstETH_deposit_eth_borrow2742×1614 392 KB\n\nAs you can see in charts(weth utilization rate/sthETH deposit/ETH borrow on Aave v2 1), utilization rate has dramatically changed after March 2022 1(at that period, stETH collateral supporting has been started).\nSo, I agree that if we add liquid staking ETH as collateral in comet, it is likely to get increased demand of borrowing ETH(mainly from dapps using liquid staking ETH strategy such as instadapp lite, rocketpool, and coinbase)\n\neth_staking_table845×395 45 KB\n\nsource: Ethereum ETH Staking Deposits \uD83E\uDD69\n\n\nCOMP distribution\nFor user groups(wstETH, cbETH, and ETH), there is a strong incentive(for borrowing ETH) to deposit their wstETH and cbETH on comet but it has not much merit for native ETH holders. So, I think that it is good idea to assign some COMP distribution for ETH deposit side.\n\n\nImproved IRM\n\nWith high utilization, it is important to keep low ETH borrowing rate level for liquid staking eth strategy players.(making maximum profit)\nCurrently, I’m not sure it should be using the same IRM for both wstETH and cbETH. Because cbETH can be considered as centralized asset in the community.\n\n\ncomments about this topic from instadapp: https://twitter.com/smykjain/status/1590835867798892544?s=20&t=AuARy3QcguITPE4cexE0fA 2\n\\nIs the analysis already available?\\nLido is really glad to see the discussion of Compound III moving forward. I wanted to hop in to mention that we have been in touch with Gauntlet to make any data needed available. We would be glad to offer our analytic team’s time and effort to the Compound community as well if there is a need/opportunity in support of this discussion.\nWe also have a great deal of experience in helping protocols that integrate stAssets manage supply and borrow demand. We will be happy to consult!\\nGauntlet has conducted an initial market risk analysis on the parameterization of the wETH Comet market.\n\nstETH\nOur systems observe ample liquidity in the stETH/wETH market. The slippage/liquidation curve suggests that a $130mm market sell of stETH through 1inch only incurs 1.5% slippage, implying a liquid market. We also ran simulations hypothesizing the amount of insolvencies and liquidations given various CFs and stETH/ETH deviations for accounts that have historically supplied stETH and borrowed wETH - and found that only at high deviations is there a likelihood for insolvencies.\nBecause of this, it can prudent to initialize the market at $100mm supply cap, 90% CF (collateral factor), 93% LCF (liquidation collateral factor), and 5% LF (liquidation factor). Combining capital-efficient borrowing parameters with a stringent supply cap will be valuable to understanding and studying the pool behavior.\n\n1357×450 23.5 KB\n\n\ncbETH\nWe recommend a lower supply cap for cbETH but with the consistent CF/LCF/LF parameters as wstETH.\ncbETH is less liquid on DEXes compared to wstETH (1.5% slippage for $1.3mm sell of cbETH, versus $130mm sell of wstETH). cbETH is also not very liquid on CEX - where a $1mm sell incurs roughly 2% slippage. In addition, cbETH is only redeemable to locked ETH on Coinbase, whereas wstETH is redeemable to stETH, which is more easily transferable and more liquid. There exists a 2-3% discount on cbETH, perhaps due to this intransferability.\nHowever unlikely, we may imagine a situation in which insolvency news on the CEX leads to the cbETH market rate plummeting, creating risk for Compound. Although this existential risk is neither a market risk nor a quantifiable risk, a conservative supply cap can mitigate the potential loss posed to Compound.\n\nSpecification\n\nwstETH: $100mm supply cap, 90% CF, 93% LCF, and 5% LF\ncbETH: $10mm supply cap, 90% CF, 93% LCF, and 5% LF\n\nBy approving this proposal, you agree that any services provided by Gauntlet shall be governed by the terms of service available at gauntlet.network/tos.\\nThanks, good question (and sorry, I’ve been away). Personally I think a USDC and ETH market both make sense on Ethereum mainnet. Beyond that, I would think the focus should be on USDC and/or native token base markets on other chains, starting with popular L2s/natively bridged chains.\\nThanks Paul and Gauntlet team for these analyses. We’ve applied the parameters suggested to the deployment 1 and initial proposal script 2. There was a last-minute change to the cbETH price oracle, which is going to be posted against ETH instead of USD from here on out, so we have some price feed wrapper contracts 5 to handle that, which ought still be audited by OpenZeppelin before deploying and launching this market.\\nHey @jared , quick drop by to check how are we doing w the deployment?\nAre we close, and have a launch date estimation?\\nHey Frank, we are still awaiting audit of the price feed changes, and overall feedback on the deployment from OpenZeppelin. At the same time, we discovered an issue with the deployment tooling that will end up deploying a different Configurator/Rewards contract than cUSDCv3, which we want to reuse, so we are making a change to support that. I’m not sure which of these things will finish first, and we are also in prime holiday season, but we should be very close! I would guess a few more weeks.\\n\nSummary\nBelow are Gauntlet’s initial recommendations on interest rate curves for the WETH Comet for Compound III.\nwstETH/cbETH supply → 0% (standard)\nWETH borrow\nUtilization 0% → 1% APR\nUtilization 90% → 5.2% APR (match stETH stake rate)\nUtilization 100% → 10.4% APR (double stETH stake rate)\nLinear in between\nWETH supply\nUtilization 0% → 0% APR + COMP incentive\nUtilization 90% → 1.7% APR + COMP incentive\nUtilization 100% → 7.8% APR + COMP incentive\nLinear in between\n\n2144×1372 112 KB\n\n\nRecommendation Specification\nborrowKink = 900000000000000000\nborrowPerSecondInterestRateBase = 315360530\nborrowPerSecondInterestRateSlopeHigh = 16398720000\nborrowPerSecondInterestRateSlopeLow = 1639871893\nsupplyKink = 900000000000000000\nsupplyPerSecondInterestRateBase = 0\nsupplyPerSecondInterestRateSlopeHigh = 19236960000\nsupplyPerSecondInterestRateSlopeLow = 536111569\nNote that these numbers will differ slightly from the APR numbers because of compounding interest in blocks.\n\nMethodology and Rationale\nSome core assumptions to this recommendation are that 1.) stETH has an annualized yield of approximately 5%, 2.) the current utilization statistics of the Compound II ETH market are based on the fact that there is no strong use case for borrowing ETH and not that the interest rates are too aggressive, 3.) the Compound III USDC Comet provides reasonable data points, but the WETH Comet will have a significantly different use case, 4.) users will be elastic with regards to interest rate changes around the boundary of stETH annualized yield.\nCurrent USDC Comet IR curves \n2110×1372 109 KB\n\nSince the WETH borrowed cannot be used as collateral, there is less market risk concerning WETH liquidations, and thus there is less need to make the max interest rate especially high.\nBy approving this proposal, you agree that any services provided by Gauntlet shall be governed by the terms of service available at gauntlet.network/tos.\\nThis might be a silly question, but per the USDC IR Curves, how is the USDC supply interest rate higher than the USDC borrow interest rate at 100% utilization?\\nHello @pauljlei !\nIf my calculations 6 are correct, with the curves you are proposing the implicit reserve factor at 90% utilization would be 70% (and 53% at half utilization).\nThis instance of Compound III is very promising and could have a real organic traction without any rewards, with small reserve factors (see the metrics of stEth/Eth on Aave, Morpho-Aave, Euler…). It don’t understand why it should depend so much on COMP rewards.\nIs the goal of this to profit from COMP distribution to increase the reserve of the Compound DAO? In this case, it should at least be explicit, in order for the community to discuss about it, especially if the reserve factor is so high.\\nThe logic surrounding borrowing costs makes sense, @pauljlei.\nI agree with @MathisGD that the implicit reserve factor could be significantly lower (e.g. interest rates across utilization rates could be higher); one of the original concerns with the market would be that native ETH suppliers would have below-average incentives to participate (vs, staking their Ether natively via proof of stake).\nAre there other models that maintain positive implicit reserve factors, simply by adjusting supplyPerSecondInterestRateSlopeLow?\\nThanks all for the feedback - relayed to our team and will come back here.\\nHi @AndrewA - great question, some color can be found below:\n\n  \n    \n    \n    USDC V3 Reserves/Interest Rate Issue Protocol Development\n  \n  \n    This is an intended feature of the current interest rate model; it generates reserves when above 50% utilization, and slightly churns reserves when below 50% utilization (though in a way that should, in theory, grow the market size). \nYou can see the model in this thread from the launch of the market. \nIt should be noted that the interest rate model is only one of two systems that can increase and decrease reserves; the liquidation process also increases and decreases reserves. \nIn aggregate, th…\n  \n\n\\nWe agree that a highly important consideration is properly incentivizing supply. Our original supply interest rate (even without COMP incentives) is in line with historic data for ETH supply rates. We acknowledge that since ETH cannot be used as a collateral in this Comet, the protocol requires a higher supply interest rate to motivate users to supply.  However, given the behavior of these linear curves, having interest rate values that are closer together (both at the base value and at the kink) leads to additional utilization points where there will be a negative reserve rate.\nOur recommendation above (which we shall refer to as Option 1) constructs the interest rate curves such that there are fewer points in which there are negative reserve rates. There are some tradeoffs here:\n\nWith fewer areas in which there are negative reserve rates, there is less risk of “running out” of reserves.\nHowever, the tradeoff is that if you want to minimize areas where there is a negative reserve rate, this means that the supply rate has to be lower. When the supply rate is lower, this means that there is less incentive for users to supply wETH. As such, growth in the market may be bootstrapped slower.\n\nTo @rleshner’s point about maintaining a positive reserve rate, one of our most important considerations was minimizing the behavior of negative reserve rate. However, if we assume that the utilization of the pool will mainly be around the CF and LCF (90%, 93%), we can build the curves below where suppliers have more incentive, but there will be more regimes with negative rates (thus increasing the chances that reserves “run out” in extended regimes of negative rates).\nOpen questions: what is the total amount of ETH reserves that the Comet plans to launch with? That will help give us bounds for how negative the reserve rate can be. If we seed the Comet with more ETH reserves, then we can be more aggressive with allowing negative reserve rates. With this additional data point, we can further fine-tune these recommendations.\nLargest negative reserve rate for Option 2: -0.69423076% at utilization: 36.54%. This means that a reserve of $100k ETH (in USD) will run out in approximately 140 days.\nOriginal recs (Option 1):\nborrowKink = 900000000000000000\nborrowPerSecondInterestRateBase = 315360530\nborrowPerSecondInterestRateSlopeHigh = 16398720000\nborrowPerSecondInterestRateSlopeLow = 1639871893\nsupplyKink = 900000000000000000\nsupplyPerSecondInterestRateBase = 0\nsupplyPerSecondInterestRateSlopeHigh = 19236960000\nsupplyPerSecondInterestRateSlopeLow = 536111569\nUpdated recs (Option 2):\nborrowKink = 900000000000000000\nborrowPerSecondInterestRateBase = 157680000\nborrowPerSecondInterestRateSlopeHigh = 19552320000\nborrowPerSecondInterestRateSlopeLow = 1639871893\nsupplyKink = 900000000000000000\nsupplyPerSecondInterestRateBase = 0\nsupplyPerSecondInterestRateSlopeHigh = 9460800000\nsupplyPerSecondInterestRateSlopeLow = 1356048000\n\nimage (20)2000×1302 123 KB\n\\nFollowing the conversations on this thread, and discussions on recent community calls and in Discord, the parameters to deploy cWETHv3 have been finalized on the pull request 1:\nThe initial risk parameters follow Gauntlet’s recommendations:\n\nwstETH: $100mm supply cap (64,500 tokens), 90% CF, 93% LCF, and 5% liquidation fee\ncbETH: $10mm supply cap (7,100 tokens), 90% CF, 93% LCF, and 5% liquidation fee\n\nThe interest rate models follow Gauntlet’s first recommendation, with an adjustment to increase the supply rate slope. After the launch of the market, Gauntlet will monitor and consider recommendations to the interest rate model based on preliminary utilization & growth.\nThe price feeds of the deployment use the following inputs:\n\nThe base asset, WETH, uses a constant price feed 2 of unity in terms of ETH with 8 decimals.\nFor wstETH, we deployed a custom wrapper 1 around the ChainLink stETH/ETH feed to convert using the wstETH/stETH exchange rate.\nFor cbETH, Coinbase has launched a new cbETH/ETH price feed 1 together with ChainLink, which we wrap with a scaling price feed 1 to convert the decimals to the constant 8 decimals that Comet expects.\n\nOpenZeppelin has completed an additional audit 1 of modifications to the price input contracts, since the original post in this thread.\nThe cWETHv3 market has been deployed to 0xA17581A9E3356d9A858b789D68B4d866e593aE94 6.\nA new proxy contract, to support advanced user transactions, has been deployed to 0xa397a8C2086C554B531c02E29f3291c9704B00c7 3. Please note that the UI will ask you to approve a new proxy contract to bundle multiple actions into a single transaction.\n\nInitialization Proposal\nTo initialize the market, the deployment process deviates from the script used to initialize the USDC market, since the WETH market shares contracts in common with the USDC market, including the Configurator and Rewards contract.\nThe initialization proposal will take the following actions:\n\nMigrate the COMP distribution, 38.70/day, from Compound v2 ETH to the new WETH market\nSet the CometFactory for the new Comet instance in the existing Configurator\nConfigure the Comet instance in the Configurator\nDeploy an instance of the newly configured factory and upgrades the Comet instance to use that implementation\nConfigure the existing Rewards contract for the newly deployed Comet instance\nSeed the market with 500 WETH of reserves, by wrapping Ether held by the Timelock, then\nTransfer the Timelock’s WETH to the deployment\nTransfer 25,000 COMP from the Comptroller to the Rewards contract to refresh it’s supply\n\nThe deployment and proposal migrations have been built using the Comet scenario framework and deployed using the Comet deployment manager 1. The deployment was run through a GitHub action using seacrest and the scenario checks can be seen from the proposal branch CI checks.\\n\n\n\n jared:\n\nThe initial risk parameters follow Gauntlet’s recommendations:\n\nwstETH: $100mm supply cap (64,500 tokens), 90% CF, 93% LCF, and 5% liquidation fee\ncbETH: $10mm supply cap (7,100 tokens), 90% CF, 93% LCF, and 5% liquidation fee\n\n\n\nOne thing I’m curious on is Gauntlet’s denomination in $USD with their initial recommendation. On November 22nd of last year, ETH was $1100 USD. Today, ETH is closer to $1500 USD. With liquidity for wstETH and cbETH both being mainly in ETH, this means there’s deeper $USD liquidity for these assets as well.\nIf the whole market is denominated in ETH, could Gaunlet denominate their recommended supply caps in ETH as well? cc @pauljlei\n\nFrom November 22nd to now, liquidity for newer assets like cbETH has improved meaningfully as well (greater than ETH’s increase in price). At the time of their initial recommendation, there was 1.5% slippage for $1.3mm sell of cbETH. Today, there is roughly 1.5% of slippage on a $7.3mm swap.\n1inch, 1.5% slippage cbETH swap:\n\nimage908×858 53.8 KB\n\\nYour intuition is right, the governance cycle is too long, the market changes cause the data to change too much, and the previous data is not enough to support the conclusion of the parameters.\nThe result is that the coefficients introduced cannot match the market, and then they will modify the parameters many times to do some repetitive work.\\nThe proposal is now live: Proposal 144 22.\\nAgree. Is there any possibility we can automate this process?\\nHi @AndrewA, the proposal 7 initializes the market with a supply cap of 64,500 wstETH and 7,100 cbETH.\\nYep! The issue I was referring to was how this supply cap was suggested originally in $USD terms back in November, which leads to a meaningful difference in ETH terms.\nThe recommendation from November for $100mm of wstETH and $10mm of cbETH would have been roughly 82k wstETH and 9.2k cbETH at the time. This is a pretty meaningful difference from the 64.5k wstETH and 7.1k cbETH that was formally proposed.\nAnd since this market and most of the liquidity for the two are denominated in ETH, I’d guess that suggesting parameters for this market in ETH terms would be more accurate as prices move.\\n\nimage1080×248 22.2 KB\n\\nTill now (27Jan23), I summarized the borrow and supply statistics for cbETH:\n+----+-----------+----------+-------------+-------------+---------+\n|    | tx_hash   |   amount | direction   | from_addr   | token   |\n|----+-----------+----------+-------------+-------------+---------|\n|  0 |           |      112 | Supply      | 0x74a0      | cbETH   |\n|  1 |           |      188 | Borrow      | 0x74a0      | WETH    |\n|  2 |           |      170 | Borrow      | 0x74a0      | WETH    |\n|  3 |           |      170 | Supply      | 0x74a0      | cbETH   |\n|  4 |           |      150 | Borrow      | 0x74a0      | WETH    |\n|  5 |           |      150 | Supply      | 0x74a0      | cbETH   |\n|  6 |           |      135 | Borrow      | 0x74a0      | WETH    |\n|  7 |           |      135 | Supply      | 0x74a0      | cbETH   |\n|  8 |           |      125 | Borrow      | 0x74a0      | WETH    |\n|  9 |           |      125 | Supply      | 0x74a0      | cbETH   |\n+----+-----------+----------+-------------+-------------+---------+\n[address]: 0x74a0 | [total borrow]: 768 | [total supply]: 692      \n+----+-----------+----------+-------------+-------------+---------+\n|    | tx_hash   |   amount | direction   | from_addr   | token   |\n|----+-----------+----------+-------------+-------------+---------|\n|  0 |           |       31 | Supply      | 0xEC9E      | cbETH   |\n|  1 |           |       28 | Borrow      | 0xEC9E      | WETH    |\n|  2 |           |       28 | Supply      | 0xEC9E      | cbETH   |\n|  3 |           |       26 | Borrow      | 0xEC9E      | WETH    |\n|  4 |           |       26 | Supply      | 0xEC9E      | cbETH   |\n|  5 |           |       23 | Borrow      | 0xEC9E      | WETH    |\n|  6 |           |       23 | Supply      | 0xEC9E      | cbETH   |\n|  7 |           |       21 | Borrow      | 0xEC9E      | WETH    |\n|  8 |           |       21 | Supply      | 0xEC9E      | cbETH   |\n|  9 |           |       19 | Borrow      | 0xEC9E      | WETH    |\n| 10 |           |       19 | Supply      | 0xEC9E      | cbETH   |\n| 11 |           |       17 | Borrow      | 0xEC9E      | WETH    |\n| 12 |           |       17 | Supply      | 0xEC9E      | cbETH   |\n| 13 |           |       15 | Borrow      | 0xEC9E      | WETH    |\n| 14 |           |       15 | Supply      | 0xEC9E      | cbETH   |\n| 15 |           |       14 | Borrow      | 0xEC9E      | WETH    |\n| 16 |           |       14 | Supply      | 0xEC9E      | cbETH   |\n| 17 |           |       12 | Borrow      | 0xEC9E      | WETH    |\n| 18 |           |       12 | Supply      | 0xEC9E      | cbETH   |\n| 19 |           |       11 | Borrow      | 0xEC9E      | WETH    |\n| 20 |           |       11 | Supply      | 0xEC9E      | cbETH   |\n| 21 |           |       10 | Borrow      | 0xEC9E      | WETH    |\n| 22 |           |       10 | Supply      | 0xEC9E      | cbETH   |\n+----+-----------+----------+-------------+-------------+---------+\n[address]: 0xEC9E | [total borrow]: 196 | [total supply]: 227\n+----+-----------+----------+-------------+-------------+---------+\n|    | tx_hash   |   amount | direction   | from_addr   | token   |\n|----+-----------+----------+-------------+-------------+---------|\n|  0 |           |     5039 | Supply      | 0xccFa      | cbETH   |\n|  1 |           |     4250 | Borrow      | 0xccFa      | WETH    |\n|  2 |           |      500 | Supply      | 0xccFa      | cbETH   |\n|  3 |           |      200 | Borrow      | 0xccFa      | WETH    |\n|  4 |           |      400 | Borrow      | 0xccFa      | WETH    |\n+----+-----------+----------+-------------+-------------+---------+\n[address]: 0xccFa | [total borrow]: 4850 | [total supply]: 5539\ntotal WETH borrowed： 5814, total cbETH supplied:6458\n"
  },
  {
    "number_of_comments": 13,
    "postid": "6fc1186d-ca2f-439b-9d2a-41baafc08695",
    "posturl": "https://www.comp.xyz/t/certora-formal-verification-proposal/3116",
    "combinedcontent": "\nFormal Verification and Path Coverage Tools for Continuous Security of the Compound Protocol and its dApps\n\nSummary\nA proposal for significantly and continuously improving the security of the Compound platform and the dApps built on top of it, by offering our formal verification and path coverage tooling service to the Compound Platform contributors and the Compound Protocol dApp developers. This is a follow-up to our recent work on the Comet protocol with the Compound labs team. The idea is to provide access to the community and educate the community, write formal specifications, and review code changes. This proposal is orthogonal to the Open Zeppelin proposal, which has already been suggested using the Certora prover. This proposal also suggests writing formal correctness rules for the Compound Protocol which will be reviewed by the community and OpenZeppelin. We have already written some formal requirements for Comet and prevented huge security breaches.\nThis is an update of an earlier unsubmitted proposal 7 discussed in November 2021.\n\nBackground\n\nCompound Protocol Security\nThe Compound Protocol is a fully decentralized system that provides sophisticated tools for accessing liquidity. The protocol’s ongoing management is controlled by a governance protocol that allows external contributions and voting by stakeholders. Such a system opens up the possibility of creating powerful new DeFi protocols built on top of it, without giving up on decentralization.\nOne of the major risks in managing a complex system of smart contracts is that it is harder to ensure that changes introduced via governance are safe and that they do not break the behavior of the protocol. While those problems are common to every piece of evolving software, decentralization introduces additional risks.\nThese risks were highlighted by a recent incident in which a bug was introduced in a Compound governance proposal 2, leading to huge nonrecoverable financial losses. Of course, such bugs are not exclusive to Compound — they have been affecting many DeFi protocols.\nDetecting vulnerabilities before they are deployed is difficult because of the lack of good security tools for smart contract development. Smart contract developers often rely on manual auditing to prevent bugs, but audits are not well suited for the setting of ongoing, time-controlled governance proposals. Moreover, even the best auditor can miss critical bugs due to the complexity of the code.\n\nAbout Certora\nOur team 5 consists of 60 experts in smart contract security, formal verification 5 methods, and compilation techniques. 12 of our team members have PhDs in formal verification from top schools in US, Europe, and Israel. We have built a world-class and unique developer tool that seamlessly integrates into CI/CD, allowing continuous verification of contract correctness.\nWe enable companies to “move fast and break nothing”, taking months off of the time to market, by decreasing code audit time. Our product, the Certora Prover, allows both pre- and post-deployment code verification: it statically analyzes the code before it is deployed but also runs on the EVM bytecode of the deployed contracts. The product verifies code by checking that it obeys high-level semantic rules. For example, a rule could assert that the sum of token balances remains unchanged by all operations that do not mint new tokens. A free demo of the Certora Prover can be found here 6. The real tool is quite advanced and handles multiple contracts.\nOur product is used by the industry’s leading companies (Aave, Balancer, Benqi, Compound Finance, MakerDAO, OpenZeppelin, Sushi, TradeJoe, Bancor, and more) and has prevented more than 100 safety-critical bugs, including 20 “solvency” bugs (a bug in which a user can steal money from the contract). Each bug can lead to tens or even hundreds of millions of dollars in losses. The Certora technology has also uncovered critical security bugs in the Solidity compiler itself and in deployed contracts of major DeFi organizations. We are also the only team that was able to formally verify interesting security properties including preservation of funds.\nCertora developed a new language CVL https://docs.certora.com/ 2 for writing code specifications. As part of this project, the Compound Community will get access to the language and the technology (see Offering Section below).\n\nCertora and Compound Relationship\nWe have been working with the Compound team almost since its inception. Originally, we formally verified the first Price Oracle implementation 4. Using the Certora Prover tool, we were able to find a subtle bug after the contract completed auditing by Trail of Bits. The part that we did not verify, the MoneyMarket contract, was later found to contain a serious vulnerability in the liquidation function, potentially breaking the protocol’s solvency, allowing an attacker to steal money from the contract. Compound Labs then asked us to help in formally verifying a sophisticated bugfix, whose goal was to block potential exploits. We have written rules for that purpose, successfully showing that no code paths can trigger the exploit.\nIn addition, we formally verified the V2 version of the protocol 2, including the MoneyMarket contract. There, we were able to find more bugs in pre-deployment and post-audit. For example, a rule for ETH borrows written by Jared Flatow (VP Engineering) from the Compound team found a mismatch in the computation of accrued interest, that prevented users from fully repaying loans. We have found many more costly bugs in the platform, preventing immense losses and protecting its users, including in the OpenOracle protocol 1.\nIn the verification of the Open Oracle framework with the Uniswap based anchor, we have identified a high-risk miscalculation of the prices of cTokens. In addition, we have verified a set of properties for this contract. The full report is available here https://www.certora.com/wp-content/uploads/2022/02/CompoundUniswapAnchoredOpenOracleAug2020.pdf 2\nCertora is currently verifying Comet and already found 10 critical and high severity issues, including increasing assets by self transferring, missing updates of collateral used, missing updates to the supply and borrow rates and more. We have verified approximately 50 rules. The rules show that the system obeys important security properties of the protocol spanning the core logic of the protocol, high-level mathematical properties of operations such as accrue and, important properties of all the public methods including absorbing and buying collateral. This project will be concluded by the end of March, after which a full verification report will be published.\n\nSpecific Rules\nAs described previously, we have been working closely with the Compound core team. However, we are not working with the Compound Platform contributors (those not directly employed by Compound) nor the Compound dApps developers (those developing dApps on top of the Compound Protocol). Recent exploits (such as the claimComp bug discovered in Proposal 62 2) show that this approach is insufficient; these exploits have caused hundreds of millions of dollars of losses.\n\nCase Study 1: Applying Certora Prover to check Proposal 62\nProposal 62 pertains to parts of the code that were not verified by Certora, but applying simple rules could have identified the bug. For example, the bug would be detected by the following rule:\n\nRule: maxCompReward\n 3\nDescription: The max COMP reward of a user has an upper limit, based on the current supplyState, borrowState and compAccured.\n\nClaimed COMP should also satisfy other simple properties, and checking these properties could detect and prevent additional bugs. For example:\n\nRule: noFrontRunningGainOrLoss\nDescription: The amount of COMP received at a specific state is the same whether or not another user (possible the admin) has performed an operation just before.\n\n\nRule: zeroGainOnZeroBlocks\nDescription: A user with zero borrows can not gain COMP in a single block (for example by using a flash loan).\n\nAn executable formalization of the rules detects the bug introduced in Proposal 62, in particular, the rule called maxCompReward triggers a violation 3.\n\nCase Study 2: A formal rule for Comet integrity of token balance change\n\nRule: totalCollateralPerAsset 3\nDescription: The sum of collateral per asset over all users is equal to total collateral of asset. This rule is also applied to check all public and external methods of the Comet contract.\n\nThis important solvency property which can be easily formulated in Certora’s specification language was violated in a version before deployment and shows a critical vulnerability of self transferring.\n\nRule: balance_change_vs_accrue\n 1\nDescription: Whenever the system’s borrow token balance changes, it changed after accrue was called and the borrow and supply rates are up to date. This rule is also applied to check all public and external methods of the Comet contract.\n\nAfter 4 minutes of running time, the Certora Prover located inputs violating the rule in two methods in the methods before deployment:\n\n\nBuyCollateral - thus enabling to buy collateral (at a discount) when there might be enough reserves and therefore should not be allowed\n\n\nwithdrawReseves - thus enabling the governance to withdraw possible more reserves than there should be able to\n\n\n\nProposal: Dev Tools for Smart Contract Correctness\nWe propose two approaches to significantly increase the confidence in the platform itself and in the smart contracts built on top of it. The first is the Certora Prover, and the second is a new symbolic execution tool (path coverage). Both of these methods leverage the high-level rules to guarantee that all paths of the code satisfy the rule.\nThe two approaches are complementary: The Certora Prover will prove that specific critical parts of the code are sound and will find hard to detect bugs, while the symbolic execution tool will be running 24/7 to continuously increase the coverage for specifications that are too complex to prove using the Certora Prover.\nIn the following sections, we will specify the deliverables of our proposal.\n\nFormally Specifying High-Level Rules of dApps in Open-Source CVL\nWe will carefully review every proposal of a code change by the community and write high-level correctness rules in English and in CVL, Certora’s domain-specific specification language. This includes development for DeFi smart contracts built on top of Compound, as well as for the Compound platform itself via its contributors. These rules will be used later for the Certora Prover and the new path coverage tool. In the unlikely event that a bug is reported, we will generalize the bug and write high-level rules preventing similar bugs to occur in the future.\nThese engineers are readily available and familiar with the logic of the Compound Protocol. Our current backlog for customers is several months.\nFitting the DeFi community spirit, we will make the CVL rules open-source, allowing decentralized rule writing and rule auditing. This will allow knowledge-sharing in rule writing between contributors and dApp developers, allowing developers to reuse application rules.\nWe will compensate non-Certora rule writers with COMP tokens according to their contributions.\nWe will create a forum for community rule writing. Contributors will have access to the Certora platform (including education material and rules from other projects) and the current set of rules.\nEvery rule suggestion will be reviewed jointly by the Compound labs and the Certora team. The reward will be decided jointly based on the significant contribution.\n\nFormal Verification of Smart Contracts during CI/CD\nThe above rules will be used by the Certora Prover, which will be available to all Compound platform developers. The CVL will be integrated as part of a VSCode extension, with tooling to integrate CVL specs to the Solidity code project. The extension will also be able to invoke the Certora Prover, and present an ongoing status of the rules: whether they are proven or violated. When a rule is violated, the extension will return a concrete call trace showing the input for which the rule is violated, in a view similar to a debugger.\n\nCode Coverage for Smart Contracts\nThis project is inspired by the Sage Direct Fuzzing Project which prevented million-dollar bugs for Microsoft.\nWe will develop a new symbolic execution product for increasing code coverage of the Compound smart contracts. For each smart contract, the product will automatically generate inputs and check that the contract obeys the rule for those inputs. The product will harness SMT solvers to continuously enumerate additional control flow execution paths. The tool can continue to run post deployments and results will be available on a dashboard. This approach complements formal verification when the code is too complex for it or when using it is too computationally intensive. The Kotlin source code developed in this project will be publicly available from June 1st to allow contributions from the community. This includes CVL and its interpreter. The proposed license is https://polyformproject.org/licenses/noncommercial/1.0.0/ 1\nThis Code Coverage Tool is similar to Dynamic Monitoring provided by Forta, however, our tool is different in two aspects: it uses high-level security rules and leverages SMT solvers for triggering rarely executed scenarios.\nIn addition to creating the product, we will create a dashboard that will report the inputs and the paths covered by the different inputs, as well as rule violations that may occur. Also, for each input, we check and report that every CVL rule holds.\n\nMeasures of Success\nIn general, we note that there is no silver bullet in smart contract security and in formal verification specifically. Still, we plan to measure success according to the following measures: (1) the number of paths covered by the path coverage tool, (2) the number of rules formally proved, (3) the number of bugs identified by the Certora Prover before deployment, (4) the number of bugs identified by the path coverage tool, and (5) the number of missed bugs in code analyzed by Certora found by auditors and other means. The numbers will be available on a daily basis.\nThe Certora Prover takes a Solidity contract and a CVL specification as input and formally proves that the contract satisfies the specification in all scenarios. Importantly, the guarantees of Certora Prover are scoped to the provided specification, and cases that are not covered by the specification are currently not checked by Certora Prover.\n\nCertora Services Summary\nWe offer the following services by the company:\n\nWe will create a two-week online course on writing rules designated to the Compound Protocol. The course will be recorded for availability for all developers.\nWe will hold as many as needed Zoom calls for users of the Certora Prover and the new path coverage tool whose source is available using the Polyform License.\nThe Certora team will create a shadow git repository for the Kotlin code. Anybody in the Compound Community can get limited and revocable access to the repository against a signed NDA. We will allocate up to 20 keys to DApp developers accessing our SaaS platform with unlimited compute usage. Computation time will be billed to Certora. Additional keys are available for $4000/month as approved by the Compound ecosystem.\nWe will allocate a team of two R&D personnel to write rules for up to 8 weeks per year. Additional weeks will be available subject to charge and availability. We will offer a 30% reduction on the normal professional service prices.\nWe will create a dashboard displaying the results of the path coverage product, including rule verified and covered paths, visible to all Compound users. Rule violations will be reported automatically to the Compound security team, but not visible to all users to avoid revealing potential exploits.\nWe will create an open-source database of CVL rules and the code they refer to, to decentralize rule writing and promote decentralized contributions to rules.\n\n\nPrice\n\nThe annual price is $2,000,000. $1,000,000 is paid in USDC. $1,000,000 is paid in COMP tokens.\nAn additional sum of $400,000 in COMP paying decentralized community rule writers. This sum will not be used for any other purpose and returned if not used or moved to the following year if the contract is renewed. These tokens will be transferred to a special-purpose multisig wallet controlled by Certora and elected members of the Compound ecosystem\n\\nThanks Shelly for renewing the discussion around continuous formal verification. I personally would like to see something like this adopted by the protocol as soon as possible, as formal verification with Certora has indeed been an important part of the quality assurance process we use for smart contract development at Compound Labs. I would very much like to see the community adopt a similar practice around all potential modifications to the protocol. I think the exercise of trying to articulate invariants about code being added or changed in itself helps increase the security of the protocol significantly, let alone the actual corner and counter cases discovered by running specific rules.\nCertora is offering to help write rules, to teach the community how to write rules, and to run the rules on their servers. They are also offering to make their source code available to the community, which is something they have never done before. This means that Compound protocol developers would be able to run formal verification locally, understand what it’s doing, and potentially diagnose or patch any issues they may encounter. I think this is an important consideration for adopting such a new technology, and am really glad to see Certora moving in this direction. This is hopefully a step in the direction of them completely open-sourcing the technology, which is something I think would give the community a lot of confidence that investing into CVLs will have a minimum long-term value for the protocol. I also think it aligns with the spirit of development for the Compound protocol and decentralized finance overall.\nIn my opinion, continuous formal verification is a step towards more efficient governance. It would decrease risk for the protocol and give further insight to governance, which can inform decision making. It should be part of our complete strategy for maintaining the integrity of the protocol, and this proposal gets us a long way towards those goals.\\nGauntlet is supportive of Certora’s formal verification proposal. As the Compound community continues to innovate via governance, it is paramount that protocol changes do not introduce security risks to the Compound ecosystem. Continuous formal verification can drive quality assurance and help prevent damaging security breaches, so we view formal verification and path coverage tools as highly valuable for decentralized governance.\nOne question @Shelly : can you provide more detail on how Certora will help ensure that Asset Listings to the protocol do not introduce security risks? The community has been focused on adding new assets to the protocol to further diversify and strengthen the Compound ecosystem.\\nHi Paul and thanks for your feedback.\nCertora’s rules match an interface. Therefore, once written for a contract like an ERC20 token or a cTokenm, they can be executed on any implementation of the interface.\nCertora has written a spec for some basic properties of a cToken. We plan to extend it to cover more properties of the listed assets, and to write ERC20 properties that address the specific assumptions of Compound’s system.\\nSuch services are helpful to Protocol, just for the user to choose to use Compound, it adds a little more trust.But the expense is disproportionate to the corresponding benefit.\nActually, This kind of service is useless, the protocol will not become more secure because it has passed formal verification, and formal verification will not be responsible for security incidents.\nOpenZeppelin is already doing well, Certora worked with Compound early, but failed to prevent the bugs produced by Proposal 62,Is it because Compound didn’t pay for the partnership service before?\nSupporters are willing to pay up to 2M(6% of Protocol’s annual revenue) for these services, but Compound’s highest bounty in immunefi is only 50k. The successful case of immunefi is more cost-effective than certora. At least the fees paid at immunefi are guaranteed to solve some problems.\\nThis was posted by Michael George yesterday.\nThank you for your reply. I think your questions indicate a few misconceptions about the proposal that would be helpful to clear up.\nCertora’s previous work with Compound only covered the core protocol; we did not verify governance proposals. Above we described a rule that would have detected the bug in Proposal 62, but we wrote this rule after-the-fact to show that Certora’s technology could have caught the bug. If Compound had been performing continuous verification as outlined in this proposal, it is possible that they would have discovered the bug before Proposal 62 was accepted. The Certora Prover integrates into CI/CD so after every change in the code, similar to regression testing, the rules are re-verified that the new code is still correct\nThe Certora Prover’s primary purpose is finding bugs during development, rather than proving their absence (although a successful verification also shows that a protocol takes safety and security seriously). It is only possible to produce proof if the protocol being verified actually satisfies its specification - if there are vulnerabilities in the contracts being verified, the Prover will demonstrate those vulnerabilities, allowing the developers to find them and fix them.\nThe power of the Certora Prover as a bug-finding tool was recently demonstrated by TrustToken’s “red team” review of their security practices 1. They injected security vulnerabilities to see what security tools were best at catching them. Of the 23 injected bugs, Certora’s Prover found 17 of them, while manual auditing uncovered only 8 of them and bug bounties found only 3. Only 4 vulnerabilities were found by others but missed by the Prover, while 12 vulnerabilities were found by the Prover but missed by others:\n\nimage (4)1760×932 45.3 KB\n\nAudits and bug bounties are useful and work well with formal verification in the DeFi security toolkit. See Auditing and Formal Verification - Better together. Auditors can review the CVL rules. Whitehackers can insert bugs into the code before it is deployed and see if the current rules suffice. The OpenZeppelin proposal also suggests that the Certora tool will be used.\nOpen Zeppelin is using Certora to formally verify their governance contract, see the latest verification report 1.\\nHello everyone,\nAs Compound’s Security Advisor, I’d like to clarify a few things around how we, at OpenZeppelin, expect Certora’s Formal Verification Proposal to work within our existing partnership and Compund’s overall security.\n\nPast Discussions of Formal Verification\nAs we mentioned during the Audit Proposal discussions in November, we see Formal Verification (FV) as a component of an overall holistic security process that must include coding best practices, manual auditing, education, threat monitoring, other testing suites and bug bounties. We have an existing relationship with Certora and are fans of the progress their technology is making but we leave the decision of pricing and vendor selection up to the Compound community. We had previously mentioned the possibility of offering FV security services but that never materialized in our existing proposal since there was no active FV vendor at the time.\n\nAudit Scope for FV Rules\nFormal verification is only as good as the initial description of the desired properties of critical elements of the protocol, which is why it is important that FV rules are written correctly. However, given the current scope of our security partnership, OpenZeppelin auditors would not be involved in either writing or reviewing FV rules. We’ve learned how important it is to prioritize auditor time to avoid backlogs on important proposals and reviewing FV rules line-by-line in addition to the code would mean potentially doubling the timeline of any audit engagement on protocol changes.\nDuring our audits, we do often look at tests to understand developer intentions in the code but we do not offer security reviews of the tests themselves and FV rules would be no different in this regard. We would assume that any necessary FV-specific supporting services will be performed separately by the Certora team as part of their proposal.\nThis all being said, we do think that having robust FV rules would enhance the security of governance proposals and would be a good way to prepare for an audit by catching bugs during development. We already recommend that our audit clients build a robust test suite, which can include FV, as part of their  Audit Readiness 2.\n\nOther Clarifications\nThere are a few other things we wanted to address from the forum discussion:\n\n\nAsset Listings - Asset Listing Proposals would certainly be a good use for FV and could cover some of the items in our recent Asset Listing Checklist. However, it’s important to note that not all checks may be possible with FV rules written for Compound on external assets so additional security tooling and manual checks may still be necessary.\n\nComparison to Forta - @shelly earlier mentioned that Certora’s Code Coverage tool was similar to the Forta Network being used in our monitoring solution for Compound. It’s important to distinguish that Certora’s tooling here is used for evaluating code prior to deployment while Forta is built to monitor live smart contract events after deployment in either the protocol itself or external contracts such as oracles and ERC20 contracts for CToken markets. We see these two tools as entirely separate to be used for different security needs.\n\nSecurity Decision-maker - Certora makes mention of both Compound Labs and a “Compound security team“ that would be involved in deciding new FV rules and handling payments to community rule-writers. When OpenZeppelin has needed the DAO to make security-related decisions, we’ve defaulted to asking the Guardian Multi-sig but we’re happy to go with Labs or a different entity if that’s preferred by the community. Therefore, we’d like to understand if Certora expects Compound Labs to be the decision-maker in all these instances or if they are expected to fall elsewhere. In any case, it’s important that security decision responsibilities are clearly defined to avoid confusion and potential problems in the future.\n\n\nSummary\nOverall, more security tooling is always a good thing and we’re happy to see discussions on additional ways to protect Compound beyond just security audits, advisory and monitoring. OpenZeppelin’s feedback above is intended to be neutral on Certora’s proposal since we believe this needs to be guided by Compound’s vendor selection process. Our goal here is to clarify some assumptions on how OpenZeppelin’s existing partnership would be impacted and where FV could be used in Compound’s proposal lifecycle. We leave it to Certora and the community to decide how they’d like to proceed.\\nHi and thank you @cylon for your feedback.\nWe want to clarify the role of formal verification in the security process. A widespread misconception about formal verification is that it is only useful during development, before an audit. In fact, our tool sometimes finds bugs after audits, and even after the code is deployed.\nFor example, here are some bugs that we found with our technology after completion of a traditional audit:\n\nAave V1 was audited by OpenZeppelin and Trail of Bits. The Certora Prover identified a security bug that could cause denial service, previously unknown.\nIn Aave V3, the Certora team started after the external audits (including OpenZeppelin) and found a critical security issue Repaying with aTokens lack validation of health factor · Issue #265 · aave/aave-v3-core · GitHub 2 fixed before the code was deployed.\nBalancer V2 was also audited by OpenZeppelin and Trail of Bits. The Certora Prover then identified a solvency bug in flash loan functionality Formal Verification helps finding insolvency bugs — Balancer V2 Bug Report | by Uri Kirstein | Certora | Medium 2. This bug was fixed before the code was deployed.\nCompound’s V1 Price Oracle had rare assert failures https://www.certora.com/wp-content/uploads/2022/05/CompoundV1PriceOracleVerificationReport.pdf. This bug was identified after a complete audit, just before the protocol launched.\n\nTo be clear, traditional audits are also an important part of any security process; formal verification does not replace auditing. Rather, FV and audits are complementary techniques that can discover different classes of bugs - both are important.\\nObviously , spending 2M$/year for FV service is not cost-effective .\nAs with the proposal to donate to the Nomic Foundation , supporting the development of the Ethereum infrastructure is good , but the expenditure is too large and the community (mostly VCs ) rejects it .\nNo matter how you describe FV’s effectiveness in Defi contract auditing , OpenZeppelin doesn’t want to increase the workload , and the community has chosen OpenZeppelin .\nLet’s make an assumption that the community is only willing to pay Grant about 150k/year , is Certora still willing to provide services ?\nIf Certora starts to serve Compound for free now , until Certora finds serious security problems and helps the protocol to be patched , the community will propose a corresponding bonus to Certora ?\nCan Certora accepts this form of cooperation ?\\nDDoS is a big threat to websites and users,Should I pay a large service fee for CloudFlare regardless of the website is DDoSed or not?\nCloudFlare can describe a lot of their cases and strategies to prevent DDOS, you only need to pay for it, and sometimes it is unnecessary.\n“Cloudflare’s 121 Tbps network blocks an average of 86 billion threats per day, including some of the largest DDoS attacks in history.”\nIt is not friendly to pay high service fees in advance,free service and pay for security bugs are more reasonable, or lower service fees.\nOf course ,you should be lobbying the head VCs, they control the votes.\\nOur pricing is based on allocating 20 users to the platform. If the community demands more we can increase this. For example, in the Aave grant, we allocated 100 keys.\nWe are a team of 65 people developing sophisticated software, solving the hardest problem in computer science–statically verifying that the code behaves as expected. Many of us spent 10 years in grad school. It is unfair to compare our price to DDOS protection services.\nIn addition, we allocate 2 people for 8 weeks to write new security rules. We include unlimited support in this package. This adds an extra layer of security on top of great auditors. We believe that for a protocol holding $6bn in TVL, the cost of a potential hack or failure could be far worse than the costs of extra security.\\nCompound protocol revenue is about 30M/year\n\nimage1700×632 53.6 KB\n\n$1M a quarter for Gauntlet. $4M/Year\nThere is no evidence that the optimization of Gauntlet’s parameter tuning has improved capital utilization efficiency or borrowing volume growth for Compound users .\nAssuming each parameter adjustment brings in more Borrowing volume, Compound’s borrowing volume should grow, in fact it dropped from 7.8B to 2.4B a year ago.\n\nimage1700×685 57.7 KB\n\n$1M a quarter for OpenZeppelin. $4M/Year\nAt present, it has done a good job and optimized the assets listing process.\n$0.9M for CompoundGrants.\nhttps://compoundgrants.org/list-of-funded-grants 2\nNomic Foundation has tried to apply for 16% of the annual revenue for free.\nThere may be some costs that I don’t know. There’s not much left.\nI will not doubt your professional ability and the professional level of your personnel, you must be the top among the enterprises that can provide FV services.\nAs a security advisor, OpenZeppelin’s reputation will be affected by negligence and hacking incidents, and they will not relax.\nIf OpenZeppelin missed some major security issue, but then Certora’s FV service found it.\nOpenZeppelin should take some responsibility for this, but I think it’s very unlikely to happen, OpenZeppelin has done a good job, and Certora has not found major issues after the OpenZeppelin audit, which is the norm.\nCertora provides services first, and Certora finds problems after OpenZeppelin, and it is more appropriate for the community to pay Certora fees each time. This is a better way to cooperate.\nTrail of Bits and ChainSecurity may also be able to provide more multiple security services, and they may also find other security issues after the OpenZeppelin and Certora audits.\nTo maximize security, Compound should buy all security services and audits regardless of expense.\nwe already have OpenZeppelin and pay $4M/year for it, I don’t think it’s necessary to spend another $2M.\\nWhile I agree that costs all around are too high, I personally think all changes to the protocol should go through some sort of formal verification process first and developers should think through invariants to changes they are making. I think that’s something that the protocol has unquestionably benefitted from and will continue to benefit from. We should re-evaluate all the services and whether the fees should be adjusted, but we shouldn’t sacrifice formal verification just because we are paying too much for other services.\\nCloudflare: Proof of Stake and our next experiments in web3\n  \n\n      twitter.com\n  \n\n  \n    \nCloudflare 5\n@Cloudflare\n\n\n  Today, we are excited to announce the start of our experiments for the next generation of web3 networks that are embracing Proof of Stake; beginning with Ethereum. https://t.co/vTWAkkm8Ds #PlatformWeek\n\n\n\n  6:01 AM - 16 May 2022\n\n    \n      \n        \n      \n      1.4K\n    \n\n    \n      \n        \n      \n      336\n    \n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"
  },
  {
    "number_of_comments": 19,
    "postid": "8f35cb17-c9d5-4f6c-b2da-aae2eb132ae9",
    "posturl": "https://www.comp.xyz/t/security-and-agility-of-compound-smart-contracts-via-continuous-formal-verification/4007",
    "combinedcontent": "\nAbout the Compound System\nCompound’s contracts implement an algorithmic, autonomous money market protocol that has been innovating in the lending space since its inception. The smart contract Solidity codebase is written in a clean and sophisticated way. From our experience, the Compound team invests a massive effort in proper system design before the code is even written, using a security-first approach. The R&D team includes some of the best engineers in the space\n\nAbout Compound Security\nThe Compound team deploys several complementary techniques to improve code security:\n\nVarious techniques for scenario-based testing and fuzzing have been implemented.\nThe protocol engages with top auditing firms to review each code change.\nStarting in 2018, the Compound team has been working with Certora on formally verifying the code before it is deployed. Formal verification techniques have been used to secure hardware and safety-critical systems across various industries. The Certora Prover plugs into your normal development pipeline, and the formal verification process itself is useful for bug finding before and after manual auditing. In the process, Compound and the Certora teams wrote formal specifications of the code and used the Certora Prover to prove the rules mathematically. When the property does not hold, the Certora Prover generates a test case indicating a scenario in which the property is violated. This test case indicates a bug in the code or the property.\n\nThe theme of this proposal is making the tool available to the whole Compound community, including limited access to the source code. We want to allow everybody to have the same access to Prover as the Compound internal team and the Aave community.\nBy their very nature, complex systems are susceptible to bugs, and this remains true even of carefully planned and executed protocols like those authored by Compound. Bugs in Compound code and governance proposals have been a source of vulnerability for the protocol. For example, a bug in proposal 62 3 resulted in the distribution of extra COMP awards, and a bug in proposal 117 3 introduced a buggy oracle, causing the ETH markets to be paused.\nCertora has developed a unique domain-specific language, the Certora Verification Language (CVL), for expressing high-level DeFi properties. Certora has also developed a unique product, the Certora Prover, which statically proves that smart contracts obey properties written in CVL. The Prover is integrated into CI/CD pipelines and allows a developer to ensure that changes to the code do not violate the specified properties. Notably, the Prover can perform these checks with every change to the code, both before and after manual audits. A whitepaper of the technology is available here 1.\nWe have been working with the Compound team almost since its inception. Originally, we formally verified the first Price Oracle implementation 2. Using the Certora Prover tool, we found a subtle bug after a third party audited the contract. The part that we did not verify, the MoneyMarket contract, was later found to contain a serious vulnerability in the liquidation function, potentially breaking the protocol’s solvency, allowing an attacker to steal money from the contract. Compound then asked us to help formally verify a sophisticated bugfix, which aimed to block potential exploits. We have written rules for that purpose, showing that no code paths can trigger the exploit.\nTen engineers wrote a total of 80 CVL rules for Compound Code. The rules are available in the following repos: CompoundV2 4, CompoundV2 Open Oracle 3 and CompoundV3 5.\nSince 2018, the tool has revealed critical issues in Compound Finance, allowing them to be addressed and preventing potential discovery and exploitation in the wild. These bugs are really hard to detect by humans. The Full reports of our past verification projects and findings are available for CompoundV3 3 and CompoundV2 1.\nWe propose to improve the security and agility of the Compound protocol in three ways:\n(1) By granting access to the Certora Prover to anybody checking the Compound code. This includes accessing the Certora Prover’s source code under NDA.\n(2) By allocating a team of 3 Certora security engineers for 12 weeks per annum to review code changes and propose new correctness rules.\n(3) By creating a community of security researchers to review code changes and propose new correctness rules.\nEvery code change and governance proposal will be reviewed by both internal and external teams using the Certora prover to prevent bugs when the code is upgraded.\nA similar service is currently active with Aave: twenty external researchers have contributed 380 rules defining security properties for Aave smart contracts. We prevented six critical bugs and wrote over 500 correctness rules.\nRead more about the Aave verification community in the Secureum articles - Aave-Certora-Secureum: A DeFi Security Collaboration part 1 2 and part 2 2.\n\nAbout Certora\nOur technical team of over 90 employees comprises formal verification and static analysis experts, security engineers, and security researchers. Many have PhDs in their fields, decades of security experience, and a strong history of significantly increasing the security of the most significant protocols in DeFi, such as Aave 1, Balancer 1, Bancor, Compound 3, Euler, MakerDAO 1, OpenZeppelin 1, TrueFi 2, and others.\n\nServices Provided by Certora\nIf this proposal is accepted, Certora will provide the following services:\n\nWe will create a two-week online course on writing rules for the Compound Protocol. The course will be recorded and available to all developers.\nWe will hold weekly office hours via Zoom for users working on Compound verification.\nWe will release a tool for converting CVL rules to Foundry tests. This will allow using the CVL rules for unit testing and fuzzing with Foundry. The first version will support CVL invariants.\nWe will engage with ChainSecurity 1, Code4rena, Sherlock, Macro 1, OpenZeppelin, Secureum, SigmaPrime and Spearbit to create a community of security researchers who will review changes to the Compound code and propose new rules. Certora will pay for the engagement with the community a total of $200,000 from the Certora budget.\nThe Certora team will create a mirror git repository for the source code for the Certora Prover. Anybody in the Compound Community can get limited and revocable access to the repository, provided they sign the appropriate NDA. This addresses Compound’s concern about the availability of the Certora Prover code.\nWe will allocate as many keys as needed to security researchers working on Compound. Certora will cover cloud computation time.\nWe will allocate a team of three Certora security engineers to write rules for up to 12 weeks. Compound’s community will determine weeks. This team will review the code and community rules and suggest new rules.\nOne of the challenges in formal verification is coming up with a good formal specification; therefore, we released an open-source tool called Gambit 5 for mutation testing in Solidity code. This tool will be used to review specifications written by the community. We will apply this tool to check the coverage of the rules developed by the community and the Certora team.\n\n\nPricing\nThe proposed price for the project is $1.5M per annum. A termination of the agreement will be available through governance voting given a 30 days notice.\nPayment method:\n$1.0M USD/USDC vested linearly over one year, and $0.5M worth of COMP vested linearly over one year, with a 1-year cliff to show our long-term alignment with the community.\nPrice Breakdown:\nRegular price for SaaS customers of the Certora Prover is $2M per year; The weekly price for professional services is $80K per week; therefore, the total price for such engagement would normally be $2.960M. We are giving a discount of $1.46M on this proposal. In addition, Certora will allocate $200K to incentivise independent security researchers to review contracts and write specifications. The incentive model for this program can be found here 7.\\nIt seems that you don’t need to re-initiate discussions for this proposal. \nhttps://www.comp.xyz/t/continuous-formal-verification\nhttps://www.comp.xyz/t/certora-formal-verification-proposal/\\nMr. Clairvoyant’s comment in those previous post makes sense.\nMost DeFi protocols are running under loss, annual revenue for AAVE is 16MM while expenditure is 26MM (or 17MM deducting the development cost of $GHO). Currently, Compound’s revenue is about 2MM, while there’s a fixed cost of at least 4MM from Gauntlet and OZ.\nI suppose it’s a wise choice to think longer about the sustainability of the protocol.\\nHi @ClairvoyantLabs  and @RapidsCapital thanks for your comments; we understand and appreciate your concerns!\nFormal verification is complementary to audits, risk management, and bug bounties. It adds an extra layer of security on top of these other layers and can be used before and after auditing. In the Aave project, we prevented six critical bugs while working in parallel with several top auditing firms. We’re also enabling agility by checking the code, and preventing critical bugs while enabling rapid development. The Certora Prover is integrated into the normal development pipeline and analyses the code after every change through CI/CD.\nWe believe that for a protocol holding approximately $2bn in TVL, the cost of a potential hack or failure could be far worse than the costs of extra security.\nWe are also committing ourselves to the well-being of the project by accepting 1/3 of the price in COMP, locking it for one year.\nAdditionally, we are developing complementary open-source technology that integrates with other tools and gives value independent of Certora’s proprietary tools, namely Gambit and CVL to foundry integration; so even if the agreement is not renewed the CVL rules can still be used together with fuzzers and other external tools.\\nThis is very expensive, and I’m curious as to whether it’s sustainable to be paying this much.  Is there any way that the cost can be brought down?\\nThanks for your question @VonNeumann2022!\nIn addition to unlimited access to the market-leading verification platform, Compound will have a team of Certora senior security engineers and researchers actively reviewing code changes and providing formal security rules before manual audits. These rules are also used for unit testing, e.g., with tools like Foundry and Echidna. This is currently working well for Aave. Our team works before audits to prevent critical bugs while enabling rapid code development. To help offset the total cost, Certora is giving back to the Compound community $200,000 to increase the security posture through open-source rule writing competitions.\\nGenerally speaking, we are a huge fan of this proposal and believe the service is both appropriate and needed. When looking at the budget concerns brought up, we have a slightly different opinion and take. We’ve worked together with Certora in the past, and believe that in today’s environment, with the passing of other service providers of similar skill and depth across many of the defi blue chip players, this specific scope of work is priced pretty competitively. The ideal bull market scenario of $3m for this type of service would be quite hefty in this market, but taking this down to $1.8m to the core team makes this seem very appropriately priced.\nOne thing we would like to learn more about is the $200k chunk that is for the Compound community. How is the team looking to proportion this amount and how does the community at large get involved here?\\nCiting Mr. Clairvoyant’s comments posted here 7:\n\nCertora provides services first, and Certora finds problems after OpenZeppelin, and it is more appropriate for the community to pay Certora fees each time. This is a better way to cooperate.\nTrail of Bits and ChainSecurity may also be able to provide more multiple security services, and they may also find other security issues after the OpenZeppelin and Certora audits.\nTo maximize security, Compound should buy all security services and audits regardless of expense.\n\nIf certora really wants to proceed the proposal, it’s wise to consider other alternative payment programs.\\nDear @pennblockchain,\nThanks for your supportive comments, given your deep involvement in governance proposals and understanding of security.\nThe $200K in community grants incentivize security researchers to review suggested changes and submit CVL rules. Certora will provide free Prover access to any Compound community members that want to learn CVL and write rules for the Compound protocol. Throughout the year, Certora will host rule writing competitions through Secureum, Code4rena, and via the Certora discord channel. The payouts for those competitions will be funded from the $200K budget in this proposal.\\nDear @RapidsCapital\nThanks for your feedback on this proposal.\nIn addition to the Services components of the proposal, at its core, Certora Prover is a development tool designed to be integrated into Compound’s CI/CD pipeline before manual audits. This proposal enables Compound developers to continue the hard work they are doing to take a shift-left approach to write secure code by checking every change, similar to the way compilers are used. Every time the code is changed the Certora Prover is invoked via a command line interface in order to find which rules hold.\nAs stated in a previous discussion [here] 1\n\n“This all being said, we do think that having robust FV rules would enhance the security of governance proposals and would be a good way to prepare for an audit by catching bugs during development. We already recommend that our audit clients build a robust test suite, which can include FV, as part of their Audit Readiness.”\n\nIncluding FV enhances the auditing process and enables a better outcome for Compound’s security. Certora is fully committed to protecting Compounds community of users.\\nWe’ve used Certora on OUSD, and it’s been a good thing. It’s a layer of security that takes a different angle than audits, and it can find things that audits typically don’t. Formal verification also has a particular strength in working with math heavy contracts such as Compound/AAVE.\nI do think that continuing to work with Certora would be advantageous for Compound.\nThis proposal would be a considerable increase in the scope of work that Certora would be doing with Compound. So the question would be if the extra gains beyond periodic formal verification audits as have been done in the past would be worth the price difference.\\n\nAdjustments to the original proposal\nAfter collecting feedback from the community and in order to address the concerns expressed, we understand the best way to proceed is to submit the same proposal, but for half a year. This period will allow Certora to prove the value that formal verification can add to the protocol’s security before committing to a longer period.\nWe did something similar with AAVE, and the proposal passed just with sufficient votes. After proving our value, we raised an annual renewal proposal 2 and got 651.7k votes, 331.7k more than the 320k necessary votes.\nTo summarize, we are proposing the following changes:\n\nOffering\nThe agreement period will be 6 months. Certora will allocate a team of three security engineers to write rules for up to 6 weeks.\n\nPricing\nThe proposed price for the project is $750k for 6 months. Termination of the agreement will be available through governance voting given a 30 days notice.\n$750k worth of COMP vested linearly over six months.\nCertora will allocate $100K to incentivize independent security researchers to review contracts and write specifications.\nWe have been thrilled with community researchers’ results using our Prover platform - our recent contest with Code4rena for Blockswap uncovered 6 bugs of medium severity, and all injected high-severity bugs were found.\\nWe don’t doubt your professionalism, but Aave’s support for your decision is not related to Compound. Based on practical considerations, I still can’t agree to this increase in expenditure.\\n\"The bug survived 9 rounds of auditing by top security companies and even formal verification. It was not found until being exploited. \"\n  \n\n      cs.purdue.edu\n  \n\n  \n    \n\nICSE23.pdf 21\n\n  409.58 KB\n\n  \n\n  \n    \n    \n  \n\n  \n\n\\nHi @ClairvoyantLabs. Thanks for your involvement and for sharing that.\nNobody can guarantee 100% lack of bugs. The interesting question is whether formal verification finds additional high-severity bugs that are hard to be caught by manual auditing (and therefore weren’t reported). Formal verification doesn’t claim to find all of the bugs in a system, but it allows one to approach security from a different angle that often finds a different class of issues.\\nFormal Verification Report 6 for Euler\nBut Euler Finance has been hacked for 177M USD. \\nThat Certora report was from October 31, 2021. The bug was added after that, on July 5th, 2022.\n\n  \n\n      github.com/euler-xyz/euler-contracts\n  \n\n  \n    \n  \n    \n  \n\n  \n    \n      start reserves at non-zero value after market activation 2\n    \n\n    \n      \n        committed \n        \n          \n        \n        Jul 5, 2022\n      \n      \n\n      \n        \n          \n          hoytech\n        \n      \n\n      \n        \n          +761\n          -220\n        \n      \n    \n  \n\n\n  \n    - Prevents governance from drawing down reserves below the initial level\n- donat…eToReserves() function so users can donate dust to the reserves (which can be cheaper than withdrawing it)\n\nThis commit was developed by Doug, Tari, and Darek\n  \n\n  \n\n  \n    \n    \n  \n\n  \n\n\\nThank you, @dvf. The Euler team acknowledged that the newly added functionality was not reviewed by the Certora team or checked with the Certora Prover.\nDear @ClairvoyantLabs, the idea in this proposal is to run formal verification every time the code is changed to avoid situations like Euler.\\nHi @sofiviss,\nWould love to introduce you to Omniscia. We do audits, pen tests, tokenomics analysis and due diligence. We could probably help Compound in some ways for your upcoming audits.\nWe have audited close to 250 projects like L’Oreal, Republic, Morpho, Tokemak, AvaLabs, Matic, LimitBreak, OlympusDAO since 2021.\n\nOur reports are web-based and include aggressive gas-saving recommendations\nWe have a clean track record (not on the rekt leaderboard)\nStatic analysis represent < 10% of the work we will conduct on your contracts. The bulk of the audit consists of having extremely senior security engineers manually review your contracts\n\nHappy to connect by email at clement@omniscia.io or TG at @ClementBarbier\nWhat are the most important criteria when picking an audit firm to you?\nBest\\nThere are a few reasons why I plan to vote against this proposal:\n\nThe Compound III (comet 2) codebase is already formally verified, and there shouldn’t be material changes to the codebase going forward; a key differentiator between comet and v2, is that comet uses a factory model to deploy new instances (generally a single smart contract), reducing the need to add new code or properties. In many ways, it’s “complete” already.\nThis proposal doesn’t demonstrate a clear path for how community developers will use Certora–making the tools available is great, but it’s likely to have little “up-take” without significant investment from developers.\nAny changes to the security surface of the Compound ecosystem, at this point, should be planned holistically based on a needs-based strategy.\nIn general, I am opposed to funding requests outside of the grants program. There is even a security section of the grants program, which could accomodate specific security-based endeavors.\n\nMaking Certora available to a distributed ecosystem of developers would make a lot of sense if/when a gen-4 protocol is being written from the ground up."
  },
  {
    "number_of_comments": 25,
    "postid": "66c3a0ff-1e8e-49e6-9723-aa69bd931173",
    "posturl": "https://www.comp.xyz/t/proposal-to-upgrade-to-uav-v3/3270",
    "combinedcontent": "Hi everyone, over the last several months we have been working with the folks at Chainlink to develop the UniswapAnchorView (UAV) V3. OZ has successfully audited the contract and it is ready for implementation.\n\nOverview\nBelow are the high-level changes and improvements:\n\nUAV V3 uses Uniswap V3 Oracles instead of V2\nIncreased number of markets available\n\nThe shift from using UNI-V2 to UNI-V3 is significant because V2 liquidity is drying up as liquidity providers move to V3 to capitalize on concentrated liquidity provisioning strategies. Having Compound’s UAV Oracle mimic this shift provides higher reliability and more security by reducing the risk of price manipulation or any other issues related to low liquidity.\n\nUAV V3\nThe code can be found and reviewed on the UAV V3 Github page 42.\nBelow is the summary of changes:\n\nThe new UniswapAnchoredView (UAV) no longer records and keeps observations but instead queries the respective Uniswap V3 pool’s observe function to get a TWAP with the set anchor period when it’s needed (i.e., when a reporter calls validate on the UAV).\nThe price of an asset in Uniswap V3 is a function of the “tick” of the pool. The formula is price = 1.0001^tick. The math required for conversion between posted prices and the Uniswap V3 TWAP and the UAV representation has been modified to support this. Relevant libraries (TickMath and FullMath) have been included from the Uniswap V3 codebase.\nThe tests from the original UAV have been adapted to the new Uniswap V3 architecture - i.e., observations/TWAP tests were removed. Hardhat (+typechain) has been implemented so real Uniswap V3 pools can be used via forked mainnet in tests and eliminate the need for mocking Uniswap V3 pools.\n\nThis code has been audited, which you can see the results here: compound-uav3-audit - Google Drive 28\nBelow is a list of coins the protocol supports in the existing oracle contract and the TVL of the anchor market. Overall the change from v2 to v3 is very positive, however, there are a few markets where it is less.\n\nLiquidity v2\nETH: $165m - 0xB4e16d0168e52d35CaCD2c6185b44281Ec28C9Dc\nDAI: $18m - 0xA478c2975Ab1Ea89e8196811F51A7B7Ade33eB11\nUSDC: N/A\nUSDT: N/A\nWBTC: $21m - 0xBb2b8038a1640196FbE3e38816F3e67Cba72D940\nBAT: $370k - 0xB6909B960DbbE7392D405429eB2b3649752b4838\nZRX: $268k - 0xc6F348dd3B91a56D117ec0071C1e9b83C0996De4\nREPv2: $347k - 0x8979A3Ef9D540480342AC0F56e9D4c88807b1CBa\nSAI: N/A\nUNI: $22m - 0xd3d2E2692501A5c9Ca623199D38826e513033a17\nCOMP: $246k - 0xCFfDdeD873554F362Ac02f8Fb1f02E5ada10516f\nLINK: $3m - 0xa2107FA5B38d9bbd2C461D6EDf11B11A50F6b974\nTUSD: N/A\nAAVE: $496k - 0xDFC14d2Af169B0D36C4EFF567Ada9b2E0CAE044f\nSUSHI: $490k - 0xCE84867c3c02B05dc570d0135103d3fB9CC19433\nMKR: $4m - 0x95b4eF2869eBD94BEb4eEE400a99824BF5DC325b\nYFI: $661k - 0x2fDbAdf3C4D5A8666Bc06645B8358ab803996E28\nUSDP: N/A\nFEI: $1m -  0x7713DD9Ca933848F6819F38B8352D9A15EA73F67\nNot yet added to the protocol, but on the oracle contract:\nMATIC: $1m - 0x819f3450dA6f110BA6Ea52195B3beaFa246062dE\nRAI: $4m - 0x8aE720a71622e824F576b4A8C03031066548A3B1\nLUSD $28k - 0xF20EF17b889b437C151eB5bA15A47bFc62bfF469\nFRAX: $137k - 0xFD0A40Bc83C5faE4203DEc7e5929B446b07d1C76\n\nLiquidity v3\nETH: $308m - 0x88e6a0c2ddd26feeb64f039a2c41296fcb3f5640\nDAI: $54m - 0xc2e9f25be6257c210d7adf0d4cd6e3e881ba25f8\nUSDC: N/A\nUSDT: N/A\nWBTC: $267m - 0xcbcdf9626bc03e24f779434178a73a0b4bad62ed\nBAT: $1m - 0xae614a7a56cb79c04df2aeba6f5dab80a39ca78e\nZRX: $880k - 0x14424eeecbff345b38187d0b8b749e56faa68539\nREPv2: $19k - 0xb055103b7633b61518cd806d95beeb2d4cd217e7\nSAI: N/A\nUNI: $12m - 0x1d42064fc4beb5f8aaf85f4617ae8b3b5b8bd801\nCOMP: $6m - 0xea4ba4ce14fdd287f380b55419b1c5b6c3f22ab6\nLINK: $13m - 0xa6cc3c2531fdaa6ae1a3ca84c2855806728693e8\nAAVE: $4m - 0x5ab53ee1d50eef2c1dd3d5402789cd27bb52c1bb\nSUSHI: $280k - 0x73a6a761fe483ba19debb8f56ac5bbf14c0cdad1\nMKR: $13m - 0xe8c6c9227491c0a8156a0106a0204d881bb7e531\nYFI: $2m - 0x04916039b1f59d9745bf6e0a21f191d1e0a84287\nNot yet added to the protocol, but on the oracle contract:\nMATIC: $11m - 0x290a6a7460b308ee3f19023d2d00de604bcf5b42\nRAI: $106k - 0x14de8287adc90f0f95bf567c0707670de52e3813\nLUSD $114k- 0x9663f2ca0454accad3e094448ea6f77443880454\nFRAX: couldn’t find anything worth referencing.\\nGreat job @getty on getting this done! Having liquidity to back up our back-stop prices is important to have reliable, accurate, and manipulation minimized prices. Liquidity can move around making it important to have our code follow.\nI’d like to share some words of caution when using Uniswap v3 as an anchor. While concentrated liquidity is better for traders, it’s worse for providing accurate and reliable oracle data.\nA lack of liquidity higher or lower can mean it’s really easy to manipulate the price. Let’s take the COMP-ETH pool with the highest TVL (the 0.3% fee pool):\n\nimage899×495 28.3 KB\n\nWe see a lot of sell positions at the 0.0413 ETH price, a lot less between 0.0413 and the current price of 0.0354, and barely any buy positions below that price.\nThere’s very little liquidity supporting the price, so if someone were to dump COMP on this specific pool, the price would take a massive hit and could stay there for a while. COMP prices would then fail to update, or bad prices could slip through, or in the case the failover was active, we could suffer gruesome attacks.\nWe also noticed this with the UST-USDC pairs with the recent de-pegging event where UST’s Uniswap v3 price was at about $0.91 for a number of days while it was actually trading at a small fraction of that. This problem could have possibly affected other pairs as well.\nFrom an oracle developer standpoint, I recommend having the requirement of sufficient full-range liquidity with some sort of guaranteed lock-up to prevent the liquidity from being removed.\\nVery cool! It seems to me like the scenario @TylerEther offers could be at least partially mitigated through a wrapper contract that simply takes a TVL-weighted average of the UAV V2 and UAV V3. Perhaps the benefits aren’t sufficient to justify the extra gas, but curious about your thoughts on whether UAV V2 could play a role in mitigating the concentrated liquidity risks of V3. Do you or others have a write-up relevant to these ideas that we could check out? Thanks!\\nIt’s best to not use stale prices, but doing so would damper the impact of such a failure scenario.\nAggregating Uniswap V2 and V3 directly on the other hand would result in a really strong price. I have roughly 4 months of data to support this - let me know if you want access.\\nFewer changes, fewer mistakes, less protocol revenue, less attention.\\nI’m curious what Compound’s past with using the failover has been. Do chainlink oracles go down for extended periods of time? Have any TWAP attacks been tried before?\\nThis would mean aggregating and averaging V2 and V3 prices weighted by their respective liquidity, correct? Many newer tokens have a majority of liquidity concentrated on V3 - would hate to see low liquidity on V2 be an attack vector or a barrier to asset listing.\nThanks for working on this @GFXlabs  - low liquidity on V2 has been a barrier to asset listing for many great projects.\\n\nI’m curious what Compound’s past with using the failover has been. Do chainlink oracles go down for extended periods of time? Have any TWAP attacks been tried before?\n\nThe failover has never been activated. So far we haven’t experienced any issues although the price feeds aren’t being monitored as far as I’m aware.\n\nThis would mean aggregating and averaging V2 and V3 prices weighted by their respective liquidity, correct? Many newer tokens have a majority of liquidity concentrated on V3 - would hate to see low liquidity on V2 be an attack vector or a barrier to asset listing.\n\nAggregating prices across multiple DEXs would be best. There’s not always going to be sufficient liquidity on Uniswap V2 or even V3. Sometimes the majority of liquidity can even be on Sushiswap or Curve in rare cases such as stETH-ETH. Fragmented liquidity has been a barrier to asset listing in the past and will likely continue to be a barrier, unfortunately.\nI’d like to note that the minimum liquidity required for a Uniswap V2 pool is a lot lower than most people think. If we want the price to be 99% accurate, it must be profitable to perform arbitrage on the pool resulting in about a 1% change in price. This is a function of both liquidity and gas price. Because we use TWAPs (assuming the pool has minimum required liquidity for an accurate price), an attacker would have to manipulate the price every block over the TWAP period. It becomes even more expensive with miners performing arbitrage as well.\nThe real question becomes how do we guarantee that the fallback pool always has enough liquidity for an accurate price.\\nI also have a comment on Uniswap v3’s observation cardinalities.\nAll of the UAV price updates call the Uniswap v3 pool oracle’s observe function. This function will revert if the pool oracle doesn’t have an observation for the past 15 minutes (i.e. the TWAP period).\nUp to one observation is written per block (if there’s at least one trade in the block). So we must handle the worst case scenario where there’s a trade in the pool every block. Therefore, the underlying pool oracle’s observation cardinality must be at least 15 minutes / blockTime = 900 seconds / 10 seconds = 90. We use 10 seconds for block time as that’s what’s expected after the merge. Since blocks aren’t produced exactly every 10 seconds (or 11 seconds on average currently), we must use a bit higher than a cardinality of 90. I’d suggest requiring an observation cardinality of 120 to be safe.\\nHi Compound Community,\nThe Chainlink Labs team is pleased to announce that we have successfully deployed the UniswapAnchoredView (UAV) V3 using the latest UAV configurations!\nUniswapAnchoredView Contract 23\nTo recap, the update from Uniswap V2 to Uniswap V3 pools is crucial because V2’s liquidity continues to drop as liquidity providers migrate to V3. Compound’s UAV Oracle must follow this shift to provide a highly reliable and highly-secure price feed.\nBelow is the summary of the changes:\n\n\n\n GFXlabs:\n\n\nThe new UniswapAnchoredView (UAV) no longer records and keeps observations but instead queries the respective Uniswap V3 pool’s observe function to get a TWAP with the set anchor period when it’s needed (i.e., when a reporter calls validate on the UAV).\nThe price of an asset in Uniswap V3 is a function of the “tick” of the pool. The formula is price = 1.0001^tick. The math required for conversion between posted prices and the Uniswap V3 TWAP and the UAV representation has been modified to support this. Relevant libraries (TickMath and FullMath) have been included from the Uniswap V3 codebase.\nThe tests from the original UAV have been adapted to the new Uniswap V3 architecture - i.e., observations/TWAP tests were removed. Hardhat (+typechain) has been implemented so real Uniswap V3 pools can be used via forked mainnet in tests and eliminate the need for mocking Uniswap V3 pools.\n\n\n\nIn addition, we’ve also increased the Uniswap observation cardinality to 150 on pools that were below 150. To elaborate, the “cardinality” in Uniswap is a term for the TWAP lookback history. It represents the state changes per block, so ten trades in a block get condensed and recorded as one update in the lookback. To save on gas costs, the cardinality starts at 2, and anyone can pay the gas cost for a deeper lookback. Therefore, increasing the cardinality to 150 ensures a safe minimum for the lookback history.\n\nUniswap V3 Liquidity Pools - As of Aug 5\nListed below are the Uniswap V3 pools and liquidity levels for each price feed in the UAV3. Note that it is required to reference the ETH-paired pools specifically:\n\n\nETH: $185M - 0x88e6a0c2ddd26feeb64f039a2c41296fcb3f5640\n\nDAI: $19M - 0xc2e9f25be6257c210d7adf0d4cd6e3e881ba25f8\n\nUSDC: N/A\n\nUSDT: N/A\n\nTUSD: N/A\n\nWBTC: $244M - 0xcbcdf9626bc03e24f779434178a73a0b4bad62ed\n\nBAT: $827K - 0xae614a7a56cb79c04df2aeba6f5dab80a39ca78e\n\nZRX: $928K - 0x14424eeecbff345b38187d0b8b749e56faa68539\n\nREPv2: $30.5K - 0xb055103b7633b61518cd806d95beeb2d4cd217e7\n\nSAI: N/A\n\nUNI: $16.2M - 0x1d42064fc4beb5f8aaf85f4617ae8b3b5b8bd801\n\nCOMP: $4.3M - 0xea4ba4ce14fdd287f380b55419b1c5b6c3f22ab6\n\nLINK: $8.1M - 0xa6cc3c2531fdaa6ae1a3ca84c2855806728693e8\n\nAAVE: $3.9M - 0x5ab53ee1d50eef2c1dd3d5402789cd27bb52c1bb\n\nSUSHI: $56.5K - 0x73a6a761fe483ba19debb8f56ac5bbf14c0cdad1\n\nMKR: $11.8M - 0xe8c6c9227491c0a8156a0106a0204d881bb7e531\n\nYFI: $178.5K - 0x04916039b1f59d9745bf6e0a21f191d1e0a84287\n\nFEI: $233 - 0x2028D7Ef0223C45caDBF05E13F1823c1228012BF\n\nWe note that low liquidity in Uniswap v3 pools makes the TWAP price in those pools easier to manipulate. That may cause the Uniswap anchor to be less accurate than expected. However, this will not affect the accuracy of the primary price feed from Chainlink.\n\nNext Steps\nIf the Compound community supports this upgrade, the following are the recommended next steps to proceed with this proposal:\n\nThe Compound community will signal support using the above Uniswap V3 pools for the current UAV3 deployment.\nIf supported, Chainlink Labs will configure all existing validator proxies to push prices to the new UAV candidate while continually pushing to the existing one. This is so that Compound voters can see the new UAV working as it should while the current UAV maintains full operation.\nLastly, if the vote is passed and executed on-chain, The Comptroller will point to UAV V3 and use it for prices. At this stage, Chainlink will decommission UAV V2 by setting the ValidatorProxies to only point to the new one.\n\\nReally excited for Proposal 117 20 - fantastic collaboration all around to get to this \\nintegration with uniswap v3 pools sounds great.\nand I’d like to check something before my voting activity.\nI have checked compound v3 document but can not find out an anchor mechanism in there. It looks like comet is only depend on Chainlink price feed. isn’t it?\\nHi @dakeshi, that’s right - Compound III is designed to use Chainlink Price Feeds directly. This is in line with how other major protocols use Chainlink. It also allows the Compound community to onboard new markets and deploy on more chains without a dependency on Uniswap.\\nAn hour ago, Proposal 117 was executed, which updated the price feed that Compound v2 uses. This price feed contained an error that is causing transactions for ETH suppliers and borrowers to revert. Effectively, the cETH market is temporarily frozen.\nOpenZeppelin, Chainlink, Compound Labs, and many members of the community are working to diagnose and fix this issue ASAP; GFX has created Proposal 119 10 which will revert the price feed to it’s original state.\nYou can still supply Ether collateral, and no users should be at risk of liquidation, or at risk of losing funds. If you have any questions, please join the community in Discord 5.\\nHere is an update on the incident and next steps for the Compound community:\nOn Aug 30th at 6:20 PM UTC, Compound Proposal 117 18 was executed 7 to upgrade the Oracle Contract 8 to a new version 6 that uses Uniswap V3 instead of V2 for price feeds. It had passed a governance vote after being proposed by GFX Labs on behalf of ChainLink. The changes 19 were reviewed by OpenZeppelin along with Dedaub and ABDK.\nAt 6:38 PM UTC, the proposal executor noticed that all calls coming from the Comptroller to getUnderlyingPrice for the cETH market were reverting and notified Compound developers of the issue. All other cToken markets appeared to be unaffected. Compound Labs, ChainLink, GFX Labs and OpenZeppelin teams were immediately notified and jumped into a war room.\nAt around 6:47 PM UTC, the proposal executor reported that because cETH does not have an underlying() method assumed to be present in every cToken contract by the new oracle implementation, the getUnderlyingPrice function 23 returns empty bytes that cannot be decoded and the call reverts. This source of the issue was verified by Compound Labs and OpenZeppelin shortly after. This leaves withdrawals and liquidations in the cETH market effectively frozen while the issue remains in place.\nAt 7:14 PM UTC, a new proposal 18 was submitted by GFX Labs to revert the upgrade and return to using the V2 Oracle contract. It is set to be passed and executed after a 7 day governance process. In the meantime, Compound Labs issued Twitter 16 and Discord posts notifying users of the price feed issue while the war room participants continued to investigate the issue further to determine the impact on existing users.\nThe primary issue right now is a temporary denial of service for the cETH market which will be resolved by the new governance proposal. No funds are at risk at this time. The rest of the cToken markets on Compound V2 and all of V3 remain functional.\nHowever, any users that deposited ETH and obtained cETH for opening borrow positions must be aware that they might get instantly liquidated whenever the fix proposal executes IF by that time the price of ETH has dropped significantly. These users can add collateral and repay borrowed assets normally to cover for eventual price drops by monitoring their borrow positions accordingly.\nWe’ll continue to work with the Compound community to ensure a speedy resolution and keep everyone informed on updates as they come. Once the immediate situation is resolved, a post-mortem will be forthcoming.\\nWhat is the eventual longterm plan? To alter UAV3 to accommodate cETH not having an underlying() function and redeploy?\\nYes, there appears to be a simple fix to UAV3 that will accommodate cETH. We’ll focus on that after resolving the current disruption.\\nHere is an update on the price feed issue for the cETH market. The fix proposal is still underway and expected to be executed in ~6 days.\nAs mentioned previously, users should be able to avoid any sudden liquidations caused by price changes once the fix proposal goes through by adding collateral or repaying borrowed assets. However, we’ve identified that some downstream protocols may not be capable of managing their positions at this time. For that reason, there may be a need to pause liquidations immediately before the proposal is executed to give these protocols time to adjust their positions.\nOpenZeppelin has worked with Arr00 to prepare a proactive proposal 2 that can be used to unpause liquidation IF the Pause Guardian decides that pausing is necessary. If market conditions do not change drastically and/or all positions are safe, then this will be unnecessary and the proposal will be canceled. We have also tested this proposal thoroughly to ensure it will work as intended since this will be the first time the unpause functionality may be used.\nIf the Pause Guardian does need to pause liquidations following the fix to protect some positions from unfair liquidations, this will only last for ~23 hours as the unpause proposal will reverse it shortly afterwards. Since this pause will impact all cToken markets, not just cETH, there are risks assumed by the protocol in case any other markets suffer a drop in price but it should be minor over the course of ~23 hours. If you are part of a protocol that is unable to manage positions or may otherwise be at risk of liquidations when price feeds turn on, please let us know in the community Discord.\nWe’ll continue to keep the community updated as we work with the Pause Guardian and other Compound participants to ensure the resolution is as smooth as possible. As we monitor the situation over the next week, we will also begin to work on a post-mortem that can be shared shortly after the fix is finalized.\\n\n\n\n cylon:\n\nHowever, we’ve identified that some downstream protocols may not be capable of managing their positions at this time.\n\n\nThanks @cylon. For transparency, could you share a preliminary list of the aforementioned protocols?\\nCurrently, only Index Coop as mentioned in the Compound discord 8. I’ve confirmed that a few others are not at risk and will take the next few days to see if anyone else comes forward with concerns.\\n@cylon Got it, thank you for clarifying. Have you/the team been in contact with the folks at Index Coop?\n\n\n\n cylon:\n\nCurrently, only Index Coop as mentioned in the Compound discord . I’ve confirmed that a few others are not at risk and will take the next few days to see if anyone else comes forward with concerns.\n\n\\nWe have indeed been in touch with the Compound team, in discord and in DMs.\nThank you for flagging this on both forums 4!\\nMinor update: Proposal 118 13 was cancelled by Gauntlet to reduce changes to the v2 protocol until after the fix passes. Gauntlet intends to re-submit the proposal at a future date\\nHey @CL_Michael, is it possible to add RAI (Pool: 0x14de8287adc90f0f95bf567c0707670de52e3813) to the UAV when it is redeployed? \\nThe fix proposal 7 has been executed and the cETH v2 markets are fully functional again. Pausing was not necessary to prevent unfair liquidations so Proposal 121 5 has been cancelled.\nMore updates will be shared in today’s community call followed by a post-mortem later this week.\\nA post-mortem of the price feed incident has been posted here: cETH Price Feed Incident: Post-Mortem 29"
  },
  {
    "number_of_comments": 19,
    "postid": "eb892f38-bd1c-4acc-a1c1-00f63a687e0e",
    "posturl": "https://www.comp.xyz/t/remove-ccomp-borrow-cap/1903",
    "combinedcontent": "Proposal: remove the borrow cap on the cCOMP market.\nOn last Wednesday’s Community Call (I tried and failed to record the call…next time) we discussed removing the cCOMP borrow cap of 100k COMP. The borrow cap was originally put into place to prohibit someone from using the market to borrow COMP, create a governance proposal, vote, and return the capital in seconds. After the upgrade to Governor Bravo and the installation of a review period, the governance process is significantly more cost-prohibitive for a borrower to obtain the necessary votes to meet the proposal threshold. As well, the review period gives users 48-hours to position their COMP holding for a vote, unlike Governor Alpha which used an instantaneous voting system. If a malicious user were to borrow the required COMP to meet the proposal threshold the community would have sufficient time to defeat their proposal.\nRemoving the borrow cap will help the COMP market develop. Trading firms (onchain and offchain) utilize money markets like Compound and Aave in their systems to borrow assets and particularly ones that are not abundant in supply in traditional borrow/lend services like BlockFi, Genesis, Nexo, and Celsius. It is helpful for everyone if spot and futures markets are efficient.\nNext Steps: Changing the borrow cap does not require a traditional governance proposal since caps are controlled by the community multisig. The multisig holders follow the direction of the community. Below is a 7-day poll for removing the borrow cap on the cCOMP market. If you vote “YES” you think the borrow cap should be removed. If you vote “NO” you do not think the borrow cap should be removed. If the poll revolves to “YES” after the 7-day period the multisig holders will remove the borrow cap.\nDiscontinue cCOMP borrow capYESNO19voters\n                    \n                    Closed Jul '21\n                   \n                \n                Results will be shown on vote.\n               \\nI think removing the borrow cap is a great idea! As it stands now, a few users are earning excessive COMP rewards through farming the cCOMP market, and the borrow cap prevents additional borrowers from helping the market reach equilibrium.\nCouple comments on the governance process to support this change:\n\nDiscourse forum polls are naturally vulnerable to sybil attacks (anyone can create an account and sway the vote). Selecting the “show who voted” option and changing “show results” option to “on vote” can help with this, as it will be easy to identify new accounts with no forum activity and disregard their vote. You can also use the “limit voting to these groups” option and select trust levels 1-4, which excludes trust level 0 (newly created accounts with less than 10 minutes of read time). Adding an abstain option is also nice, it lets people see the results even if they don’t want to sway the outcome of the vote.\n\n\nimage608×684 26.2 KB\n\n\nThe vote can also be held in the Compound Snapshot space 4. Power over borrow caps is already delegated to the community multisig so a snapshot vote isn’t strictly necessary, but it offers a more objective view of tokenholder support versus forum polls.\n\\nI think I configured the vote to be “show who voted” and “show results” option to “on vote.” As for the “limit voting to these groups,” I’ll try that out next time; good idea.\nIf the poll looks manipulated, we can try doing a snapshot instead.\\nThank you so much @getty for hosting this! Yes, lets raise it to 2.1 Million and get our farmers into more productive assets. I am all for a snapshot one or however @monet-supply says is great. I think there is consensus here, especially after hearing @rleshner speak about this last call.\\nI made a snapshot post here incase someone prefers to the express themselves using their votes\\nI don’t think it is wise for us as a community to change the offchain terms of a proposal long after it is executed. The multisig never had terms to follow the result of discourse or snapshot votes. If the borrow cap for COMP were to be removed, I would much prefer a real governance proposal. Also, the community should remember the main reason for borrow caps—safety not restricting assets.\nI would also like for the COMP rewards given to cCOMP market to be decreased to that of what other markets get. Doubling the COMP rewards isn’t an effective way to reward COMP holders.\\nBorrowing Cap history:\n\n\nProposal 26 9 set the Borrow Cap Guardian to be the Community multisig\nHow the Community multisig manages the Borrow Caps was established in Discord / forums 4\n\n\nProposal 27 4, which added COMP support, set the initial borrowing cap to 50k COMP; the discussion around the 100k borrowing cap ceiling was a Community multisig discussion in Discord if I recall\n\nThe Community multisig is empowered (at a protocol level) to set these limits; if there is a strong community consensus, I don’t think that it needs to take place through an on-chain formal proposal.\\nRemoving the borrow cap for COMP is a very significant change to the tokenomics of COMP and how governance works (Justin Sun may borrow all 400k COMP to bolster his TUSD efforts).\nI don’t believe that we can get an honest representation of community consensus off chain for a few reasons:\n\nWe have seen new voices come out at voting many times. Many delegates do not follow along in the forums until we come to an on-chain proposal\nMany votes are managed by Coinbase custody which can’t be signaled on Snapshot or other off chain voting methods\nI don’t recall the community ever making significant decisions off chain before\n\nThis change deserves the scrutiny of a main governance proposal and any decision made off-chain claiming strong community consensus will likely be more of a finger in the air temperature check.\\nExcellent and convincing counter-point!\\nipfs://bafybeifq5ac4wg4w4c3eu3x3tmn7cy2ez4aenrqx5a7kyvvdhlrvtiltau/#/comp-vote.eth/proposal/QmZUtFVmqBPkurntoCqLYrKZebwiCwbrfE8YLvWSECJjRM\nif the link broke?\\nthe multisig in itself is off chain - if the power is there to use, so it shall be used\\nI don’t think its a major change to tokenomics. For community consensus, snapshot is better than off chain for that exact reason, gnosis sig and coinbase custody don’t support it, we want the consensus to be community led.\nBut the decision to raise the borrow cap individual times has failed to even notify the community of it happening. One multisig isn’t “on chain governance”. The “limit” of 100k has been hit, and the community wants it raised infinitely, so how come we must ask to raise it, but when it is raised without being asked or even told,  it is ok? The community multisig has stated that they have too much power, so the time is to remove it. If governance wants to set another borrow cap, that can go on chain, but the point was for the community to have that power in a multisig.\\nit’s not off chain. all of it’s actions can be monitored at any time. however the discussion is.\\nMy bad. Yes, the multisig wallet is a smart contract deployed on chain. It’s signers live in the real world.\\nI wanted to bump this thread because I think it is an ongoing thorn in Compound’s side. Currently cCOMP borrow has a yield -7.36% and due to the borrow limit, this market is well outside the bounds of its natural equilibrium. The unique part about cCOMP market is that the supplemental distribution is paid in its native asset. While this is obvious, the implication is that this market should be the most efficient at reaching market equilibrium because its yield has zero-price risk (assuming adequate collateral), unlike other markets where there is COMP price risk that must be accounted for.\nThe major risk at play in this discussion is a malicious actor borrowing large amounts of COMP and then making and/or voting on proposals that may not be in the best long-term interest of the protocol. The borrow CAP is our main deterrent from this sort of actor and in a functioning market, an increasing interest rate would also act as such.\nThe following are the options I have considered to fix this market: Increase Borrow Cap, Raise Interest Rates, Remove/Lower COMP Distribution\nIncrease Borrow CAP:\nIncreasing the COMP borrow cap could help to mitigate the issue, however there is still a risk that we hit the new borrow cap immediately and are in the same situation. See below for the analysis… For the increased borrow cap to allow for positive borrow yield in excess of the supply yield, the CAP would have to be increased to a value far greater than the community might be comfortable with.\nIf you look at our current situation (~100k COMP Borrow CAP), in order to achieve a net zero interest rate, we will need the supply to decrease by roughly 5x to achieve a utilization ratio of ~80%.\nAn increase in Borrow CAP to 150k COMP would still require the amount of COMP to supplied to decrease in order to have a net zero interest rate.\nAn increase to 200k COMP would be able to achieve market equilibrium (as per my definition) assuming no change in COMP supplied. Additionally, the supply net interest rate at the breakeven point (~1.1 million COMP) would be ~1.36% which is far less than the current 2.98%. Essentially, it is unlikely that the supply will grow to levels that cause the borrow rate to go negative because the interest rate will be far too low to support that value.\n\nRaise Interest Rates:\nOne relatively simple fix that I have not seen mentioned is simply increasing interest rates on the cCOMP token. The only parameter I would suggest changing is the 0% utilization rate, due to the already high interest rate model of cCOMP. However, this “fix” will not effect any real change in alleviating the issue at hand unless it is done in tandem with one of the other solutions.\nRemove/Lower COMP Distribution:\nThis is by far the easiest fix, as setting the distribution to zero would fix all issues immediately. However, this is probably not ideal since the entire point of distributing COMP is to reward the users of the protocol and incentivize activity.\nConclusion:\nI propose that the cCOMP borrow CAP be increased to 200k COMP. The additional 100k COMP does pose a risk to the protocol in that a malicious actor could create a proposal with that borrowing but is enough to have meaningful influence on its passage. This activity is mitigated by allowing the cCOMP market to achieve a net positive borrow rate (by making the malicious attack costly). 200k COMP Borrow at the current supply of ~550,000 COMP would result in a net borrow rate of ~6.9% (12.91% Borrow Rate - 6.00% COMP Distr.). While this rate is not exorbitantly expensive, it is better than a negative rate and has a real economic cost if amount borrowed reaches the CAP. Having properly functioning markets, especially for the protocol’s governance token, is very important for Compound’s long-term success.\nWhat does everyone think? Do any additional scenarios need to be explored? If we can achieve consensus on the forum, we could try to enact this change in the AAVE,SUSHI,etc market addition proposal that is upcoming from @getty and team.\\n\n  \n    \n    \n    cCOMP Market Report ++ Ideas\n  \n  \n    Greetings. \nIts been a few months since I have last spoken about cCOMP and the oddities that the market contains. \nWhile originally I came into this topic hoping to make some quick and sharp changes regarding either the interest rate parameters, borrow cap, or even the cfactor. But as it usually is, hastily made decisions don’t always hold the most predictable consequences. \nThis journey led me to learn about what it really means to be your own bank, the difference between stealing and being edu…\n  \n\n\nI already did a report on this, got a grant for it, written a handful of forum posts, voted on the forum, voted on snapshot, talked about this at basically every call, I am beat.\nIf someone wants to sponsor a prop, so be it, but take a look at who historically has borrowed comp, this is an uphill battle and a painful situation for me personally, governance as a whole and individual contributors.\nHopefully a multisig to control reserve factors and borrow caps will be in place, but the current multisig wont touch the borrow caps after all this, so be it.\nCOMP…POUND! Still love you guys\\nI think we should follow up on this topic and use the new feature which allows for splitting COMP speeds. I propose stopping COMP rewards for borrowers in the cCOMP market. This will allow for the market to fall back into equilibrium.\\nI agree; Proposal 62 (besides introducing a bug that has since been patched) allows the COMP distribution to a market to more accurately reflect usage of that market; cCOMP is a market crowded out by artificial borrowing demand. By making the user distribution exclusive to suppliers (removing COMP speed from the borrow side), the market will no longer rest at it’s borrowing cap.\nSimilarly, the community might want to consider utilizing this feature to adjust the allocation across all other collateral markets, which see little natural borrowing demand.\\nStrongly support setting compBorrowSpeed for the cCOMP market to zero (or at least near-zero). I agree that there is a case for a system-wide reallocation of COMP across lending and borrowing markets, but the cCOMP borrow market is by far the most egregiously misaligned with the interests of the protocol. Let’s take care of that market first, and then adjust the rest.\\nProposal 68 has been executed. This ended the COMP rewards for COMP borrowers and caused the majority of COMP loans to be repaid. There is now 40,962 COMP borrowed, down from 90,750 a week ago. COMP is now borrowable from the COMP market, and the net borrow rate is 4.6%."
  },
  {
    "number_of_comments": 10,
    "postid": "f100b77e-7524-4af9-9d57-0cc6f19d0f7b",
    "posturl": "https://www.comp.xyz/t/more-rigorous-process-on-reviewing-large-code-changes-re-comp-bug-9-29-21/2326",
    "combinedcontent": "In response to the comp distribution bug that has been recently exploited, I think it’s important we respond to this with a better process in reviewing large code changes before implementation. We are very lucky that this bug was only a bug that distributed more comp to other people and didn’t threaten the protocol.\nThe bug was exploited after proposal 62 30 was passed. Looking at the discussion thread 22 for this proposal, there was not much testing outside of what TylerEther tested on his own as well as a deployment on the Ropsten testnet to check that it worked correctly, which only a few people tested.\nOne idea we should absolutely be doing is implementing large code changes on a copy of the current state of the Ethereum blockchain and observing the changes made to peoples addresses. We also need to encourage more people to help test these changes before it gets deployed. We shouldn’t be voting yes on proposals that make big changes unless its been through rigorous testing.\nAnother idea to avoid something like this from happening again is that we could give the multi sig the power to revert a proposal. Giving the multi sig this ability will be for emergencies that require quick responses, like the current comp distribution bug.\nI want to invite others to continue this thread with ideas to improve our methods of testing in order to prevent a bug like this to occur again.\\nCompletely agree with threading ideas here! I believe that we should be creating some form of a Compound Audit DAO (a group of 5-7 smart contract auditors) that is tasked and compensated for executing all audits. In this way, we can add more structure to the proposal review process and within the voting delay period the audit team can take a look at all protocol changes alongside @phazejeff’s aforementioned simulation to significantly reduce the likelihood that a similar scenario doesn’t occur in the future.\\n+1 to the idea of emergency revert, regardless of the complexity of the changes.\nQuick thoughts on testing, more thorough ones to follow. The question that should be asked is what could have been reasonably done to catch the issue.\nPerhaps a testnet + QA process for a period of time before deploying sufficiently complex code changes would make a difference in this case? Some kind of simulation of a deployment on real data may have also made this issue apparent.\\nOver at idle.finance 5 we have developed a tool for simulating proposals built on a compound governance like system, and we have used it to simulate test and debug complicated proposals before going on-chain. It was is built as a plugin for hardhat, but currently it only works for GovernorAlpha proposals, but it could easily be extended to also support GovernorBravo proposals as well.\n\n  \n      \n\n      GitHub\n  \n\n  \n    \n\nGitHub - Idle-Finance/hardhat-proposals-plugin: hardhat plugin for governance... 7\n\n  hardhat plugin for governance proposals. Contribute to Idle-Finance/hardhat-proposals-plugin development by creating an account on GitHub.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\\nNice!\nI also noticed that prop 62 was many different changes rolled into one. It seems like this may have increased the complexity unnecessarily so it may be worthwhile to question the necessity of bundling change up like that, especially if a quick revert is implemented, you’ll appreciate the granularity.\\nWhat would help catch bugs like this in the future is using test driven development methodology. Doesn’t have to be purest; but, what you want is 100% code coverage ideally. According to DeFi Safety we only have 52%.  How TDD works is before writing your code you write tests for what you want the code to do, and then you test it during development. You start off with extremely simple code that solves the first test, and then add additional tests for each behavior the function should have. It would catch bugs like this almost immediately.\nAlso has the added benefit of producing simpler code, and speeds up later development. The initial development just takes longer. But, you end up with a much more robust code base.\n\n\n\n 4D_compound:\n\nNice!\nI also noticed that prop 62 was many different changes rolled into one. It seems like this may have increased the complexity unnecessarily so it may be worthwhile to question the necessity of bundling change up like that, especially if a quick revert is implemented, you’ll appreciate the granularity.\n\n\nTest driven development would do this too. If not using TDD, at least require code to have 100% branch coverage. I think 70% to 90% is the normal benchmark sought. But, since this involves potentially billions of dollars 100% might be a good idea.\n\n  \n\n      en.wikipedia.org\n  \n\n  \n    \n\nTest-driven development 1\n\nTest-driven development (TDD) is a software development process relying on software requirements being converted to test cases before software is fully developed, and tracking all software development by repeatedly testing the software against all test cases. This is as opposed to software being developed first and test cases created later. \n Software engineer Kent Beck, who is credited with having developed or \"rediscovered\" the technique, stated in 2003 that TDD encourages simple designs and in...\n\n  \n\n  \n    \n    \n  \n\n  \n\n\\nIndeed–some “sandboxing” techniques make sense, and for all future innovations an API playground will help. Stay strong Compound community!\\nI want to echo ElPro’s sentiments here. He (like myself) is a voting delegate over at Maker.\nWe have a strong interest in a healthy DeFi ecosystem, and if there is a way that we can be helpful as you build out and ruggedize your processes, please do not hesitate to reach out.\nBest wishes and kind regards.\\nI believe Code Arena is rolling out DAO upgrade bounties, would be great to enroll Compound in this\\nCompound was built on being a stable, safe, error-free protocol. Over the last dozen proposals, bugs (of increasing consequence) have made their way into the protocol, culminating in Proposal 62, which distributed COMP incorrectly.\nFirst off, I completely understand the mindset from the community that we need a rigorous process for evaluating proposals. Historically, all changes to the protocol were audited by Open Zeppelin and / or Trail of Bits, included complete code coverage and invariant testing (formal proofs), and were manually QA’ed on test-nets before any patches were pushed to production / governance. As more community members contribute patches, the process has become more ad hoc. This will be the case in any truly decentralized community, but we should ask ourselves what process we think makes sense to keep the protocol safe, and use Proposal 62 to immediately re-calibrate. We can use this framework to collectively evaluate if patches and changes have met that bar, a priori of discussing the deeper implications of those changes.\n\nFirst, we should become a “default no” community instead of “default yes.” Patches are not considered safe until we (the community) collectively decide so. If a patch is put up too early, it’s fine to vote against it and ask for more checks and assurances from the developers or other community members.\nLess is more. Patches should cover the smallest ground possible. Don’t refactor module X and change its behavior at the same time. This often leads to subtleties that are hard to evaluate and discover. Refactor a module with no changed behavior and then make your patch as a follow-up. Smaller changes are easier to evaluate and will help get a patch through significantly faster than grouping several changes together.\nTesting should be crucial to any patch. This should include standard tests, “scenario” tests, and, if possible, invariant testing. Patches should be evaluated on test-net before being proposed to main-net. Every piece of evidence that the patch is safe should be evaluated and understood, with a special emphasis on looking for blind spots where there is no coverage (not just in line counts, but in actual real-life scenarios).\nCommunicate! Talk on the forums about your patch; ask other members in Discord; post it in draft form to a GitHub pull request. Get the juices flowing early so people can give directional signals and help make sure the best decisions are being made.\nAll patches should be audited. I understand that it can be difficult to coordinate an audit in a decentralized environment. The protocol could specifically reserve the time of an auditor in advance, or proposers could look to repay auditors as part of the proposal. Ideally, the protocol could retain enough time with an auditor which could be used flexibly, as proposals come up.\nThe protocol should offer a standing bounty to anyone who finds a vulnerability with a pending proposal. Implementors should have some amount of their reward escrowed for a period to evaluate that the patch works as intended before being released to the contributor.\nThe community should consider establishing a technical committee to establish development standards; review, and approve changes before they are proposed to governance.\n\nProposal 62 was a catalyst for us to improve, and build a world-class decentralized development process. The community will have to find the long-term balance of security and risk tolerance, but I know we can use this to get on the right track.\nExcited to discuss any of the above.\n~~ Geoff\\n\n  \n    \n    \n    More Rigorous Process On Reviewing Large Code Changes (RE: Comp Bug 9/29/21) Governance Process\n  \n  \n    Compound was built on being a stable, safe, error-free protocol. Over the last dozen proposals, bugs (of increasing consequence) have made their way into the protocol, culminating in Proposal 62, which distributed COMP incorrectly. \nFirst off, I completely understand the mindset from the community that we need a rigorous process for evaluating proposals. Historically, all changes to the protocol were audited by Open Zeppelin and / or Trail of Bits, included complete code coverage and invariant t…\n  \n\n"
  },
  {
    "number_of_comments": 17,
    "postid": "15c9b4bc-177d-4012-89b6-473defb827cb",
    "posturl": "https://www.comp.xyz/t/deploy-compound-v3-on-arbitrum/4100",
    "combinedcontent": "\nDeploy Compound v3 on Arbitrum\nPreamble\nType: Meta Process\nTitle: Deploy Compound v3 on Arbitrum\nAuthor: FranklinDAO Governance (prev. Penn Blockchain)\nProposal Introduction\nPoint of contact: @pennblockchain & @haymond (Offchain Labs)\nDescription: We propose the deployment of Compound III on Arbitrum One for the community.\nGrant Application: Applied\nAbstract\nArbitrum is the leading Layer 2 on Ethereum from a TVL perspective 2 and also by number of live dApps 2. Arbitrum provides a 90-99% decrease in gas costs for users 1 while also expanding Ethereum’s computational capacity up to 7X mainnet Ethereum with the latest Nitro upgrade 1.\nWith over 500+ live applications, 129M+ transactions and over $3B+ in assets bridged in, Arbitrum is a logical ecosystem for Compound to deploy to.\nAbout Arbitrum\nThis proposal is to authorize Compound Labs to deploy Compound III on the Arbitrum One optimistic rollup chain on behalf of the community. We will briefly outline the most important reasons:\n\nArbitrum is a leading DeFi ecosystem on Ethereum Layer 2.\n\nArbitrum has experienced tremendous growth after launching a little over a year ago in August 2021. During this period Arbitrum has seen a rise in various defi projects including:\n\nArbitrum native defi teams - GMX, Radiant, Mycelium, Vesta Finance, Cap, JonesDAO, Shell Protocol etc.\nBlue-chip Ethereum teams - Uniswap, Sushiswap, Balancer, Curve, 0x/Matcha, Aave V3\nMany smaller up-and-coming DeFi projects\n\nThe Arbitrum team did all of this without any incentives and we believe they’re uniquely positioned to help Compound succeed.\n\nDeploying on Arbitrum can have a lot of benefits\n\nDeFi projects that have deployed on Arbitrum have experienced a number of benefits, including:\n\nGas Cost savings\n\nAccording to L2fees 1, the average cost to send ETH is $0.04 and the average cost to swap tokens is $0.12.\nCompare that to Ethereum’s $0.79 fee to send ETH and $3.93 fee to swap tokens.\nFurther work is being done (EIP-4844) to lower gas costs for users, so this will continue to get better for Arbitrum users!\n\n\nUser base growth\n\nAccording to Nansen Pro, Arbitrum One has 400k unique monthly active users for the past 30 days, which is up substantially from launch\n\n\nReturn to the original vision, on a truly aligned Ethereum rollup\n\nEven though the bear market has seen declining gas costs for users, we know this won’t stay this way forever. Eventually the bull market will return and gas costs will spike again. If Compound proactively deploys on Arbitrum, as every other major DeFi blue chip has, the Compound community will have a future proof system to scale as Ethereum does.\n\n\nMature infrastructure\n\nArbitrum has support from numerous Ethereum infrastructure providers including Etherscan (Arbiscan), The Graph, Chainlink, Alchemy, Truffle, Dune Analytics and Nansen. Tangentially, Offchain Labs also has also gained momentum with support for Google Cloud on Arbitrum Nova and the sentiment towards infrastructure providers working with Arbitrum technology is on the path to becoming even stronger.\nArbitrum also has the strongest exchange support of any Layer 2 with support from Binance, Kraken, Coinbase, Huobi & Crypto.com as well as native USDC support coming soon 2 from Circle, which is relevant to Compound’s usage on Layer 2.\n\n\nPartnership support\n\nOffchain Labs, the team behind Arbitrum, is agnostic in being helpful and supportive of all teams in the Arbitrum ecosystem. They can be helpful with intros to teams, co-marketing and technical support to ensure that Compound III has success.\n\n\n\n\nArbitrum is aligned with Vitalik’s vision of a rollup centric Ethereum roadmap\n\nIn Vitalik’s original post 2 on a rollup centric Ethereum roadmap, Arbitrum is exactly that long term solution that handles execution and computation on the Arbitrum blockchain & posts the data back to Ethereum for its security. In tandem, acquiring Prysmatic Labs, the team behind the leading Eth proof-of-stake client, helps establish this alignment even more as Offchain Labs provides the resources to realize this vision. As Compound was born natively on Ethereum, its community should be aligned with the most advanced Ethereum rollup to date.\nProposal Details\nNon-Technical Evaluation\n\nTVL on chain: ~$3.3 billion: Arbitrum TVL - DefiLlama 3\n\nNumber of protocols on the chain: >500: Arbitrum TVL - DefiLlama 3\n\nUnique addresses: 2.8 million: Arbitrum Unique Addresses Chart | Arbiscan 1\n\nNumber of unique active users: Nansen - Analytics for Crypto, DeFi and NFT\n\n\n\n1 day: 72k\n7 days: 310k\n30 days: 672k\n\nDeploying on Arbitrum\nWe believe the Arbitrum One optimistic rollup is a great next step as a deployment option for Compound III for several reason:\n\nArbitrum One is the leading DeFi ecosystem on Layer 2 despite not offering any protocol incentives\nDeploying to Arbitrum One can bring a lot of benefits (gas cost savings, fastest growing L2, etc.)\nArbitrum is aligned with Vitalik’s vision of a rollup centric Ethereum roadmap 2\n\n\nWe submit this proposal for your consideration and look forward to hearing your feedback.\nCopyright Waiver\nCopyright and related rights waived via CC0 1.\nLicense Exemption\nWe are requesting an exemption that will allow the Arbitrum One network to obtain a Compound Business Source License (BSL) to use the Licensed Work, update compound-community-licenses.eth, and deploy it on the Arbitrum One Network, provided that the deployment is subject to Ethereum Layer 1 Compound Protocol governance and control.\\n(Note: We understand the core team’s priorities and bandwidth regarding deployments of Comet and don’t want to rush them. This is simply to start discussion around this topic and timeline dependent on other priorities)\\nWe are kicking off work on the governance receiver contract for Arbitrum shortly and actually have already tentatively scheduled an audit with OZ for 3/27!\\nTo follow up on this discussion, do y’all have any asset suggestions for the initial market? Right now we are looking at WETH, WBTC, and possibly GMX, although we want to investigate the project more closely.\\nIn discussions with the team, they have mentioned having:\n\nWBTC\nWETH\nLINK\nAlso possibly GMX as well.\n\nThis is based on data from Arbiscan 1 as well as Compound’s existing markets on Ethereum mainnet.\\nAdd the $GLP\n\n  \n      \n\n      gmxio.gitbook.io\n  \n\n  \n    \n\nGLP 3\n\n  GLP is the platform's liquidity provider token.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nStaked GLP token address (Arbitrum): 0x1aDDD80E6039594eE970E5872D247bf0414C8903 2\n\nAdd $Magic 1\n\n  \n      \n\n      docs.treasure.lol\n  \n\n  \n    \n\nWhat is $MAGIC?\n\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\nimage1669×524 86.8 KB\n\\nFor the initial assets, I would also add wstETH, USDC and DAI along with what Penn Blockchain recommended. GMX would be a nice-to-have.\\nWhere may I find Arbitrum Goerli deployments?\\nAdd $ARB\nThe launch of $ARB\n  \n\n      twitter.com\n  \n\n  \n    \nArbitrum (\uD83D\uDC99,\uD83E\uDDE1)\n@arbitrum\n\n\n  Today The Arbitrum Foundation is extremely excited to announce the launch of DAO governance for the Arbitrum One and Arbitrum Nova networks, alongside the launch of $ARB. \n\narbitrumfoundation.medium.com/arbitrum-the-n…\n\n\n\n  1:41 PM - 16 Mar 2023\n\n    \n      \n        \n      \n      2.6K\n    \n\n    \n      \n        \n      \n      810\n    \n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\\nHere you go Public Chains | Arbitrum Documentation Center 1\\nI believe it make sense to support protocol own governance token, COMP as well. Even while from collateral point of view it’s relatively minor asset. But i strongly believe that having support of own tokens is very important in long run.\\nI’m looking for Comet contracts deployed on Arbitrum Goerli to run testnet on them. Do you know if Comet is deployed on Arbitrum Goerli yet?\\nFollowing Gauntlet’s recommendations for deploying Compound v3 on Optimism 11 and Polygon 2, we are currently conducting analysis for Arbitrum as well.\\n\nGauntlet Initial Parameter Recommendations - Compound v3 Arbitrum USDC Comet\n\nSummary\nWe provide two options to the community below. Option 1 is very conservative for the purpose of testing out Compound V3 mechanics. As such, the conservatism is less so derived from market risk (which is Gauntlet’s focus) but more so on the smart contract and other technical risks. Option 2 is less conservative and assumes that the community does not need to test Compound V3 mechanics on a new chain.\n\nOption 1: Very Conservative (Test out Mechanics)\n\n\n\n\n\nWETH\nWBTC\nARB\nGMX\n\n\n\n\nSupply Cap\n2k ($3.79M)\n100 ($3.04M)\n1M ($1.22M)\n10k ($780k)\n\n\nLiquidation Factor\n50%\n45%\n40%\n30%\n\n\nCollateral Factor\n45%\n40%\n35%\n25%\n\n\nLiquidation Bonus\n5%\n5%\n7%\n7%\n\n\n\nStorefront price factor: 80%\nIR Curve: Same as Ethereum and Polygon USDC comets\n\nOption 2: Conservative (Assume mechanics are working, then gradually increase aggressiveness of parameters)\n\n\n\n\n\nWETH\nWBTC\nARB\nGMX\n\n\n\n\nSupply Cap\n5k ($9.47M)\n300 ($9.04M)\n4M ($4.88M)\n50k ($3.90M)\n\n\nLiquidation Factor\n85%\n77%\n60%\n45%\n\n\nCollateral Factor\n78%\n70%\n55%\n40%\n\n\nLiquidation Bonus\n5%\n5%\n7%\n7%\n\n\n\nStorefront price factor: 80%\nIR Curve: Same as Ethereum and Polygon USDC comets\nThe supply caps are set as a function of on-chain liquidity and can be increased after the initial launch. The proposed LFs for the initial listing are set conservatively while still being capital efficient enough for usability.\n*Note that the ARB token has only been on the market for 3 weeks. It may be prudent for the community to wait longer to list this asset.\nBy approving this proposal, you agree that any services provided by Gauntlet shall be governed by the terms of service available at gauntlet.network/tos.\\nThank @nlord for sharing your thoughts on the asset risk parameter recommendation.\nIt’s great to see Compound taking steps ahead of AAVE on the supported assets.  We would to share our thoughts on $ARB and $GMX.\n\n\nThe comparison between $ARB on Compound and $OP on AAVE seems appropriate, as both tokens share similar tokenomics. If the $ARB adoption rate on Compound follows a similar pattern to that of $OP on AAVE, we can expect a low percentage of $ARB’s total supply to be deposited.\n\n\nWe agree with @ClairvoyantLabs  that GLP would be more suitable for a lending protocol over $GMX. Since $GMX itself can generate interest, there might not be enough incentive for people to use it as collateral at a low LTV. On the other hand, GLP could offer a more stable and less risky alternative due to its lower volatility compared to $GMX.\n\n\\nFollowing the conversations on this thread, recent community calls, Discord, and GitHub, a cUSDCv3 market has been deployed to Arbitrum from this pull request 1.\nThe parameters to enable the market are being finalized on this pull request. It includes Gauntlet’s recommended risk parameters for the following assets::\n\nARB: $5.2M supply cap (4M tokens), 55% CF, 60% LCF, and 7% liquidation fee\nGMX: $3.4M supply cap (50K tokens), 40% CF, 45% LCF, and 7% liquidation fee\nWETH: $9.3M supply cap (5K tokens), 78% CF, 85% LCF, and 5% liquidation fee\nWBTC: $8.6M supply cap (300 tokens), 70% CF, 77% LCF, and 5% liquidation fee\n\nThe interest rate model is currently configured the same as cUSDCv3 on mainnet. After the launch of the market, Gauntlet will monitor and consider recommendations to the interest rate model based on preliminary utilization & growth.\nThe assets and price feeds of the deployment use the following inputs:\n\nThe base asset, USDC, uses 0x50834F3163758fcC1Df9973b6e91f0F0F0434aD3 3.\nFor ARB, 0xb2A824043730FE05F3DA2efaFa1CBbe83fa548D6 2.\nFor GMX, 0xDB98056FecFff59D032aB628337A4887110df3dB 1.\nFor WETH, 0x639Fe6ab55C921f74e7fac1ee960C0B6293ba612 1.\nFor WBTC, 0xd0C7101eACbB49F3deCcCc166d238410D6D46d57 2.\n\nOpenZeppelin has completed an additional audit 5 of the bridged governance contracts and flows.\nFinally, see the Initialization Proposal section below for more information about the next steps to enable the deployed market.\n\nDeployed Contracts\n\ncUSDCv3: 0xA5EDBDD9646f8dFF606d7448e414884C7d905dCA 5\n\nThis is the main proxy contract for interacting with the new market. The address should remain fixed and independent from future upgrades to the market. It is an OpenZeppelin TransparentUpgradeableProxy contract.\n\ncUSDCv3 Implementation: 0x9aB958D306Beb81711e5f5CA0731C1E4772dF9cb\n\nThis is the implementation of the market logic contract, as deployed by the Comet Factory via the Configurator.\n\ncUSDCv3 Ext: 0x1B2E88cC7365d90e7E81392432482925BD8437E9 1\n\nThis is an extension of the market logic contract which supports some auxiliary/independent interfaces for the protocol. This is used to add additional functionality without requiring contract space in the main protocol contract.\n\nConfigurator: 0xb21b06D71c75973babdE35b49fFDAc3F82Ad3775 2\n\nThis is a proxy contract for the ‘configurator’, which is used to set and update parameters of a Comet proxy contract. The configurator deploys implementations of the Comet logic contract according to its configuration. This pattern allows significant gas savings for users of the protocol by ‘constantizing’ the parameters of the protocol.\n\nConfigurator Implementation: 0x8495AF03fb797E2965bCB42Cb0693e1c15614798 1\n\nThis is the implementation of the Configurator contract, which can also be upgraded to support unforeseen changes to the protocol.\n\nProxy Admin: 0xD10b40fF1D92e2267D099Da3509253D9Da4D715e\n\nThis is the admin of the Comet and Configurator proxy contracts. It is a ProxyAdmin as recommended/implemented by OpenZeppelin according to their upgradeability pattern.\n\nComet Factory: 0xe2AA5194E45B043AfdD6E98F467c0B1c13484ae9\n\nThis is the factory contract capable of producing instances of the Comet implementation/logic contract, and invoked by the Configurator.\n\nRewards: 0x88730d254A2f7e6AC8388c3198aFd694bA9f7fae 1\n\nThis is a rewards contract which can hold rewards tokens (e.g. COMP, ARB) and allows claiming rewards by users, according to the core protocol tracking indices.\n\nBridge Receiver: 0x42480C37B249e33aABaf4c22B20235656bd38068\n\nReceives bridged governance messages from the Arbitrum gateway contracts and forwards them to the bridge timelock.\n\nBridge Timelock: 0x3fB4d38ea7EC20D91917c09591490Eeda38Cf88A 1\n\nThe governor of the Comet deployment, exclusively receiving input from Ethereum mainnet governance through the bridge receiver.\n\nInitialization Proposal\nTo initialize the market, the deployment process is similar to the initialization of cUSDCv3 on Ethereum mainnet, the primary difference being the bridging of governance actions to Arbitrum, instead of taking place directly on the governance chain (Ethereum mainnet).\nThe initialization proposal will take the following actions:\n\n\nSet Comet configuration, deploy new Comet on Arbitrum, and set rewards configuration. This sends the encoded setConfiguration, deployAndUpgradeTo and setRewardConfig calls across the bridge to the governance receiver on Arbitrum.\n\n\nApprove Arbitrum’s L1 Arb-Custom Gateway to take Timelock’s USDC, in order to seed the market reserves through the bridge.\n\n\nBridge USDC from mainnet to Arbitrum Comet, via the Arbitrum L1GatewayRouter\n\n\nApprove Arbitrum’s L1 ERC20 Gateway to take Timelock’s COMP, in order to seed the rewards contract through the bridge.\n\n\nBridge COMP from mainnet to the Arbitrum rewards contract, via the Arbitrum L1GatewayRouter\n\n\nWrite the ENS TXT record v3-official-markets on v3-additional-grants.compound-community-licenses.eth containing the official markets JSON.\n\n\nTurn off COMP distribution on Compound v2 USDT borrows, as the distribution is being shifted over to bootstrap the Arbitrum market.\n\n\nThe deployment and proposal migrations have been built using the Comet scenario framework and deployed using the Comet deployment manager. The deployment was run through a GitHub action using seacrest and the scenario checks can be seen from the proposal branch CI checks 1.\\nThe proposal 16 has been created and will commence voting in 2 days.\\nThe proposal was executed on Arbitrum earlier today and the market is now live 5 for everyone to use!"
  },
  {
    "number_of_comments": 53,
    "postid": "0ff4ce48-ffc7-457a-a48a-e542f7e1ac72",
    "posturl": "https://www.comp.xyz/t/cgp-2-0-updates-and-renewal/4518",
    "combinedcontent": "CGP 2.0 Updates and Renewal\nFirstly, thank you @adam @Doo_StableLab @allthecolors @Michigan_Blockchain @cylon for reviewing the proposal and sharing your valuable feedback, and to everyone who participated in the discussions to renew CGP 2.0 11.\nSummary\nAfter successfully running CGP 2.0 10 for two quarters and taking into consideration its impact, we propose to renew Compound Grants Program 2.0 10 with a budget of $970,000 spread across 3 domains. We have received great feedback from the community, builders and domain allocators to renew CGP 2.0 11 and are grateful for their valuable inputs.\nBackground  and Progress\nCGP 2.0 went live in Jan, 2023 with a grants budget of $800k 1 spread across four domains. Over the past two quarters, CGP 2.0 domain allocators approved proposals requesting $670,000 in grants  3and disbursed a total of $252,000 to accepted proposals from a pool of 100+ proposals. These domain allocators were elected from the community and by the community. 10 The specific information regarding the accepted and funded proposals can be found here 3. Please find below CGP 2.0’s funding breakdown, relevant metrics, insights, and proposed improvements going forward.\n\n\n Funding Breakdown\nCGP 2.0 Domainwise Funding Breakdown 6\n\n\n\nMetrics\n\nCGP 2.0 team has received exceptional feedback from the builder community. After conducting a survey to gather feedback from the proposers and grantees, CGP 2.0 received an impressive NPS score of 8.6/10 and experience score of 8.2/10. Below are some comments provided for reference.\n\nFeedback from Proposers2512×672 195 KB\n\nThe CGP 2.0 grants committee has accepted proposals from a diverse pool of over 100 submissions with an acceptance rate of 40%. This acceptance rate indicates high quality of proposal submissions and proposal pipeline. Throughout the grants program, CGP 2.0 grants team has consistently maintained a remarkable average communication turnaround time (TAT) of less than 48 hours and an average funding TAT of less than 2 weeks after the milestone has been completed. Furthermore, the milestone completion rate for all funded proposal and accepted proposals stands at ~ 57% and ~27% respectively.  1\n\nFeedback from proposer3424×244 221 KB\nFeedback from proposer3426×374 90 KB\nFeedback from proposer3242×552 101 KB\nCGP 2.0 Metrics1920×956 44.6 KB\n\nTotal number of proposal funded - 23\nNumber of funded proposals with new interfaces for supplying/borrowing/governance interaction - 9\nNumber of funded proposals supplying/borrowing on Compound - 3\n\n*Note : These figures exclude proposals that have not yet completed their first/initial milestones. The majority of the accepted proposals are actively progressing towards accomplishing their milestones, and we will keep the community informed about the progress of proposals in each bucket such as proposals supplying/borrowing on Compound, TVL  through community calls and reports.\n\n\n\nInsights and Feedback\n1.Specific domains made it easier for builders to understand the scope and structure their grants proposal\nIn case of CGP 2.0, we took a fundamentally different approach by ensuring that each focus area (domain) is communicated clearly. We streamlined the program to four specific domains, providing builders with a more focused and structured framework.\nFeedback from proposer3434×110 74.1 KB\nCGP 2.0 domains on Questbook1920×810 46.9 KB\n2.Knowing the allocator’s name for their domain helped builders with a significantly faster query resolution and TAT\nThe public availability and accessibility of the domain allocator played a crucial role in maintaining a very low turnaround time (TAT) for CGP 2.0. Builders who applied to a specific domain could directly connect with the respective domain allocator, enabling them to ask additional questions/seek clarifications.\nFeedback from proposer3428×232 221 KB\nFeedback from proposer - 23426×374 90 KB\nFeedback from proposer3394×150 80.1 KB\n3.A transparent and objective review process significantly benefitted builders, enabling them to incorporate actionable feedback into their resubmissions\nIn addition to maintaining a communication TAT of less than 48 hours, domain allocators assessed proposals using a domain-specific rubric in a transparent manner. This objective feedback provided builders with a clear understanding of the areas that needed improvement, allowing them to revise and resubmit their proposals based on actionable feedback.\nEvaluation Rubric1894×2100 214 KB\nFeedback from proposer3426×172 155 KB\n\n\n\nProposer’s Experience and Comments\n\nGovernance Voting Participation and Decentralization Tracker 2 by Signal Corps\nAlcancía - Be your own bank for Latin Americans by Juan Diego Oliva Heinsen\n1Delta DAO - The decentralized margin broker by Achim and Kevin\nA Flexible Voting Money Market on Compound V3 by Ben DiFrancesco 2\nWido - Collateral Swap Extension by Roman\n\nTAT\nFeedback from proposer3428×232 221 KB\nFeedback from proposer1920×219 23.8 KB\nNPS1920×1038 119 KB\n\n\n\nCGP 2.0 Demo Day\nCGP 2.0 Demo Day 3\n\n\n\nChallenges and Expected Improvements\nWhile CGP 2.0 team has received great feedback from the builder community and Compound, there is still room for further improvement.\n1.Ensuring a consistent TAT for all proposals - Although the average communication TAT for all proposals was below 48 hours and the average funding TAT was less than 2 weeks, there were instances where proposers experienced delays in response or were unaware of the reasons behind the delay. Similarly, some grantees encountered delays in receiving funding. While these delays can be attributed to factors such as product bugs, going forward the CGP team will ensure a consistent communication and funding TAT to all the teams and ensure that these delays are communicated proactively.\n2. Addressing funding amount discrepancies - CGP 2.0 will be able to honor all funding commitments to all the accepted proposals. Despite that, there were cases where proposers did not receive the exact amount initially committed due to fluctuations in the value of COMP and funding delays. To mitigate this issue, the CGP team will aim to payout the grantees as soon as the proposer hits the milestone. We further propose paying out accepted proposals in stables rather than COMP to avoid potential discrepancies caused by COMP price volatility.\n3. Improving operational efficiency - Questbook is working with Synapse and Docusign to integrate their service and enable KYC and agreement signing directly from Questbook. This will significantly improve the operational efficiency and funding TAT going forward.\n\nProposal\nBased on the impact and insights derived from CGP 2.0, we propose renewing CGP 2.0 with a budget of $970k for two quarters. The domain allocators will utilize this budget to fund proposals that align with Compound’s roadmap. After researching, gathering feedback from domain allocators, active community members, and builders,  we propose supporting the following domains:\n\n\n\n\nDomain\nDomain Allocator\nProposed Budget\n\n\n\n\nDapps and New Protocol Ideas\nallthecolours\n$450,000\n\n\nMulti - Chain/Cross chain Strategy, Dev Tooling\nDoo from Stable Labs\n$200,000\n\n\nSecurity Tooling\nMichael from Openzepplin\n$150,000\n\n\n\n\n\nGiven that Dapps and New Protocol Ideas 5 domain received more than twice the number of proposals compared to all the other domains combined, we propose merging Multi-chain and Cross-chain Strategy domain with the Dev Tooling domain.\n\n\nAdditionally, we propose increasing the allocated grants budget for Dapps and New Protocol Ideas domain to $450,000 and reducing the grants budget of the Security Tooling domain to $150,000 taking into consideration the number of proposals and allocated grant amounts for both domains during CGP 2.0 3.\n\n\nSpecifications and Implementation\nSimilar to the model implemented in CGP 2.0, the renewed grants program will be run using Delegated Domain Capital Allocation Model 1. Each domain allocator will run their respective domain on-chain for full transparency using Questbook. The data and performance across key metrics will be visible to the community.\nThe disbursement of the grant will take place on-chain from a multi-sig wallet controlled by the program manager & the domain allocator. The domain allocator will approve or reject the application based on evaluation rubric. A Grants SAFE, with 3/4 multi-sig, between the program manager and 3 domain allocators will be setup. We will then have 3 SAFEs for each of the domains with a 2/2 between the program manager and the specific domain allocator. The funds for the grants program will flow from the treasury into the Grants SAFE. This SAFE will hold the funds related to operational costs, committee compensation, and the grants budget. Funds that will be disbursed to the proposers will reside in the domain-level SAFEs.\nDelegated Domain Allocation3366×1462 125 KB\nAfter the end of two quarters, the grants committee and the Compound community shall evaluate the performance of each domain using publicly available data and decide to change the domain, change the domain allocator or the program manager. CGP 2.0 closed reviewing proposals on June 30th. CGP 2.0 grants team will proceed with initiating payouts for the remaining milestones to the accepted proposals from the allocated budget upon approval of this proposal.\nCompensation\nSourcing, reviewing, funding, marketing, tracking and nurturing proposals requires significant expertise and time commitment from the grants committee members and they should be fairly and competitively compensated for their efforts. Based on the learnings from CGP 2.0 specified above, we believe that a Program Manager is expected to dedicate approximately 25 hours per week, while the domain allocator is required to allocate an estimated 15 hours per week. However, these time commitments may vary depending on the number of proposals received for a domain and the domain allocators may exceed or work for less than 15 hours per week based on the proposal volumes. We propose keeping the hourly price the same for the Domain Allocators and the Program Manager as in the case of CGP 2.0\n\n\n\n\nRole\nPer Hour Cost\nHours Dedicated Per Week\nTotal\n\n\n\n\nProgram Manager\n100\n25\n$60,000\n\n\nDomain Allocator\n80\n15\n$90,000\n\n\nOperations Cost, Misc.\n\n\n$20,000\n\n\n\n\nTotal Amount Required\n$170,000\n\n\n\n\nQuestbook will provide the grants committee its grants orchestration tool free of cost.  We suggest that the grants committee continue with Synapse for KYC services and Docusign for all contractual agreements, as we have been using these services throughout CGP 2.0.\nHowever, for any specific asks from the grants team in order to run the process more smoothly, Questbook will charge for any additional feature requests based on the development overhead through a retrospective grant proposal from Compound at the end of two quarters.\n\n\n\nCGP 2.0 Funding Overview\n(As on 22nd July, 2023)\n\n\n\n\nDomain (A)\nAmount Available (B)\nGrant Amount Paid Out (C)\nGrant Amount Allocated (D)\nSurplus (E)\n\n\n\n\nDapps and protocol ideas\n~$476,000 (6,834.5595 COMP)\n$160,000\n$384,000\n~$251,000\n\n\nMulti - chain and Cross Chain\n~$207,000 (2,921.8384 COMP)\n$50,001\n$89,003\n~$168,000\n\n\nDeveloper Tooling\n~$96,000 (1,402.8018 COMP)\n$16,000\n$90,877\n~$22,000\n\n\nCompound III Security Tooling\n~$246,000 (3,465.0216 COMP)\n$26,000\n$105,800\n~166,000\n\n\nEstimated Allocated Operational Cost\n~($100,000)\nNA\nNA\nNA\n\n\nTotal\n$1.025M\n$252,000\n$678,031\n~$500,000 (Surplus - Operational allocated cost)\n\n\n\n\nConsidering the funding breakdown and COMP’s price on July 22, 2023, ~$520,000 will be reserved from the CGP 2.0 budget’s available funds. This allocation will be utilised to fulfill the milestone payout obligations for accepted proposals and committee compensation for the month of May, June, and July.\nThe unallocated funds from every domain will be returned to the treasury from the renewal budget at the end of 2 quarters\n\n\nKPIs and Expectations\nProgram Success\n\nIncrease in the number of contributors, proposals, and funded projects\nIncrease in milestone and proposal completion rates\nIncrease in NPS score from all proposers and grantees\nLower response turn around time to delegates’ and community’s queries\nDiversity in projects being funded across technologies, geographies, and demographics, to name a few. We encourage the community members to review the proposals across different domains during  community calls regularly\nTimely publishing of comprehensive monthly grants report, outlining the status, progress, and impact of the program, ensuring transparency and accountability\n\nEnhanced Community Involvement\n\nIncrease in community engagement across :\n\nDiscourse\nDiscord, Telegram\nSocial media (Twitter, Reddit)\n\n\nIncrease in the community members’ participation to keep domain allocators and program manager accountable (measured by the number of people looking at the dashboard and participating in the program)\n\nBrand Awareness\n\nStrengthened contributors’ sentiment and word of mouth towards Compound measured through frequent sentiment surveys/ polls to gauge satisfaction\nEnhanced Compound’s brand recognition and awareness within contributor circles through surveys or social media analytics, tracking mentions, reach etc.\n\nContribution of Funded Projects to Compound\n\nNumber of users onboarded by the funded proposals onto their app/protocol\nTVL (if applicable) of the selected proposals\nNumber of projects supplying/borrowing on Compound\nNumber of new interfaces for supplying / borrowing / governance interactions\nNumber of projects that have raised follow on capital after getting a grant from CGP 2.0\n\nDomain Allocator Roles & Responsibilities\n\nAll Domain Allocators and the Program Manager will continue to uphold their designated responsibilities as outlined in the CGP 2.0 proposal.\nDomain allocators may request an audit for the considered/accepted proposals, particularly those that involve Solidity code being deployed into production and directly impacting Compound. In order to streamline the code auditing process and avoid potential time-consuming challenges, the domain allocators will provide assistance to the considered/accepted proposals by offering feedback on code quality and design.\nSimilar to CGP2.0, the Program Manager will collaborate with the Compound Labs team and the elected domain allocators to create and list out necessary RFPs in order to ensure alignment with Compound’s priorities and roadmap. For reference, a similar list of priorities and proposal writing guides for CGP 2.0 can be seen here 3 and here 1\n\nAbout Questbook\n\nQuestbook (YC-W21) is a decentralized grant orchestration tool, currently being/previously used by Polygon, AAVE, Celo, Solana, TON, Aleph Zero etc.\nConsidering the achievements of CGP 2.0, as well as the time commitment and operational expertise necessary for running an effective grants program, Questbook will continue in the role of the Program Manager.  Ruchil 3 from Questbook will assume the responsibilities of the Program Manager in place of @harsha due to bandwidth constraints. Ruchil was the program manager for Polygon facilitating the disbursal of ~ $1M in grants. He works with Solana foundation and ecosystems within Solana on a daily basis to help them design their grants program. He has received a grant of $250K from the Solana foundation for the same. He also worked closely with CGP 2.0 Program Manager and Grants team.\n\nNext Steps\nWe welcome the community members to participate in the temperature check and share their feedback and comments below.\nTemperature check on the proposal. Please vote! I am in favour of this proposal. Any unallocated funds from CGP 2.0 should be returned to the treasury before renewal. I am not in favour of this proposal and I will post a reply that shares my thoughts18voters\n                  \n                  Votes are public.\n                 Results\\nHi Harsha, and everyone involved,\nI appreciate the work put into this proposal. As a CGP 2.0 grantee, I fully support the renewal. Just want to add, I noticed that community engagement can be a bottleneck for grantees when soliciting feedback for project refinements. I believe CGP can help elevate this involvement:\n\nFeedback Incentives: We could stimulate quality feedback from the community by acknowledging their contributions in meaningful ways.\nInteractive Sessions: We could create more venues for discussions by facilitating structured question periods or topic-specific discussions. This could foster active participation and a more dynamic exchange of ideas.\n\nUnderstanding the community’s needs is crucial in delivering something truly valuable. We need feedback and I believe these improvements could encourage it. Looking forward to seeing the growth of CGP 2.0!\\nThank you @bendi , @juandi for coming forward and sharing your experience with CGP 2.0. Really appreciate your comments and support!\\nHi all, just wanted to provide some feedback based on our experience as grantees of GCP 2.0 (April 2023).\nOur team is very grateful of the work that has been put into the past grant program. Our initial proposal 1 was thoroughly reviewed within 2 days. Domain allocator @allthecolors provided actionable feedback that helped us scope the proposal on elements that are the most important for the ecosystem. Within a week, we were able to revise our proposal 1 and received funding quickly after.\nWe are in favor of renewing the grant program, as specified by the proposal.\\nThank you so much @Warden_Finance for your support and for sharing your experience with CGP 2.0\\nCGP 2.0 was very well organized. @harsha from questbook and the domain allocators(@allthecolors and @cylon) were very approachable and gave very good feedback.\nIt was also great to see all the demos at the end of the cycle.\\nThank you so much @robinnagpal\\nHey everyone!\n0xBroze from Anthias.xyz here.\nWe were fortunate to receive a grant in CGP 2.0, and it was one of the smoothest grant processes our team has seen. The feedback from @allthecolors on our first submission was straightforward and fast, and once we made the adjustments and resubmitted, we received approval shortly after. The Questbook milestone process was great and allowed us to receive funds as we completed set deliverables in the grant as opposed to waiting an arbitrary amount of time. Overall, very happy with the grants process put together by @harsha and his team and looking forward to continuing to support the Compound ecosystem.\\nHey Compound Forum!\nAs a grantee of CGP 2.0, I can tell that this is easily one of the most linear and straightforward grant programs in all of web3. I do support the decision of using the remnants of the program to keep funding development of Compound-related dApps and integrations.\nAlcancía’s and my personal $COMP bags will vote yes \\nThank you so much @Doo for your kind words and support!\\nThank you @blake for your kind words! Really appreciate you sharing thoughtful comments and suggestions. Yes, we will work on providing additional opportunities for community feedback and organize interactive sessions such as the demo day more frequently. Such initiatives will foster active community participation and facilitate the exchange of ideas between proposers and community members.\\nThank you so much @cylon for sharing your thoughtful comments and elaborating the benefits to Compound because of CGP 2.0.\\nThank you @signal_corps for your support and sharing your experience as a CGP 2.0 grantee.\\nThank you @kallolborah for sharing your thoughtful comments and support.\n\n\nBased on the number of proposals received and the allocated budget for each domain, we have proposed adjustments to the budget, specifically increasing the grants budget for the Dapps and New Protocol Ideas domain. As you rightly pointed out, the proposed time commitments may vary depending on the number of proposals received for a domain and the domain allocators may exceed or work for less than 15 hours per week based on the proposal volumes. However, drawing from  the learnings from CGP 2.0 and feedback from the domain allocators, we expect a domain allocator to allocate an estimated average time of 15 hours per week. While CGP 2.0 has consistently maintained an average communication turnaround time (TAT) of less than 48 hours and an average funding TAT of less than 2 weeks after the milestone has been completed, we are committed to maintaining a consistent TAT for all proposals and maintaining a high NPS score.\n\n\nThank you for providing your valuable suggestions. We agree that in order to ensure the success of CGP 2.0, it is vital to incorporate feedback from proposers and grantees. At the end of CGP 2.0, we gathered feedback from CGP 2.0 grantees through a survey 1 and connected with them through calls, Discord to better understand their experiences and incorporate their feedback. Going forward, we will share feedback surveys with all proposers and grantees more frequently and provide additional opportunities, organise interactive sessions such as the demo day for proposer’s, grantees’ to share their feedback.\n\n\\nThank you @harsha ! And I am happy to see that you already have taken into account the TAT, NPS and the feedback loop. I am happy to support and contribute to CGP 3.0.\\nI just want to start off by saying that it’s awesome to see all the developer activity the grants program has generated for Compound. This surge of engagement undoubtedly benefits the protocol by encouraging further enhancements and innovations. I would like to thank Questbook for starting up CGP 2.0.\nThat being said, while I understand the eagerness to start CGP 3.0 4, I think it’ll be beneficial to give the community some more time to digest the results of CGP 2.0. Given the recent conclusion of CGP 2.0, the community hasn’t had the opportunity to fully assess the impact of the funded projects and determine how best to enhance the program for CGP 3.0. There are also ongoing discussions by other parties, such as Alastor & w3s 3 and Alphagrowth 1, that can shape the direction of CGP 3.0 and we should allow those discussions to develop before rushing to fund a specific program.\nI would also like to provide my thoughts of the grants program from an objective bystander’s point of view by pointing out some areas of improvement for future grants programs.\nQuestionable value from current inbound approach\nThe lessons learned from CGP 1.0, as articulated by Larry Sukernik 1, emphasized the superiority of Request for Proposals (RFPs) in yielding valuable contributions for the protocol, in contrast to an inbound approach. To quote Sukernik:\n\n\n\nCompound Grants Program - Lessons and Next Steps\n\nWhen we launched Compound Grants, we thought people would know what to work on. In retrospect, we should have known that it’s easy to boil the ocean when thinking about what to work on. What happened is applicants would apply for grants for all sorts of projects, putting us in the position of assigning priorities to applications after they were submitted. Instead, the right approach is for grants programs to understand the level of priorities for the protocol and put out RFP’s that community members can begin working on. We didn’t do that at first, but we learned quickly. Now, we have a list of RFP’s  that are a priority for the protocol to complete and encourage community members to work on them.\n\n\nI want to preface the following section and say that I trust all involved parties (grantees, domain allocators) act in good faith for the protocol’s betterment. Yet, I reference specific examples not to call out specific projects, but to highlight how the current inbound approach falls short in optimizing protocol value.\nReviewing the projects funded by CGP 2.0, I question the value that some projects provide to Compound. Funded projects are occasionally redundant, given existing solutions. An example is the $30K grant to launch Comet on Optimism 2, which was approved despite Compound Labs already communicating the work being done there months prior. This grant was revoked after Labs cleared up the miscommunication, but it was concerning to see the cross-chain domain allocator’s lack of familiarity with the cross-chain work being done for the protocol. Another example is the Compound API kit 5 grant to build a Typescript SDK for interacting with Compound, which already exists 9.\nFor novel projects not replicating existing work, there are certain projects that get paid a good amount of money and get to leverage Compound’s brand to launch a product that mainly benefits themselves rather than the protocol. I’d like to avoid providing specific examples here, but there are many projects that collect fees from users, none of which are given to the DAO. There are other projects that seem more like a solution in search of a problem, as is the case with cross-chain infrastructure projects that benefit greatly from being adopted by a blue-chip protocol like Compound.\nConsidering the significant portion of the DAO’s funds allocated to these grants, it’s wise for the DAO to reevaluate its fund allocation strategy, aiming to efficiently direct resources toward projects that generate maximum value for the protocol.\nLack of a dedicated quality oversight stakeholder\nThere are three major stakeholders as part of this CGP arrangement:\n\nQuestbook/Program Manager\nDomain Allocators\nGrantees\n\nAlthough each of these stakeholders likely operates with good intentions, none are strictly incentivized to look out for the best interest of the DAO. Questbook/the program manager is paid to run the program. Domain allocators are paid to allocate their funds. Grantees get paid by hitting milestones. However, none of them are solely responsible for maintaining project quality, which opens the door to potential harm to the protocol’s reputation if issues arise.\nThis came to our attention when we realized a grant project received the green light from OZ to launch on mainnet, despite lingering security concerns tied to their contract. This situation raises significant concerns, as the protocol’s brand could suffer irreparable harm if any issues were to arise. While there seemed to be an implicit assumption that OZ, as the official security partner of the DAO, would uphold the quality standard, it appears this expectation was somehow overlooked. Given that any project launched via the grants program is backed by the DAO’s funding and implicitly carries the DAO’s stamp of approval, maintaining a high project quality benchmark is essential. Clearly defining the responsible entity for upholding this standard going forward is crucial to safeguarding the protocol, especially as more grant projects are shipped using the Compound banner.\nSuggestions for improvement\nBased on the issues presented above, here’s a list of suggestions to improve the grants program moving forward:\n\n\nFocus on RFP model instead of inbound approach\n\nHave the active members of the community curate a list of projects to be funded, prioritizing projects that hold the highest potential value for the DAO. This guarantees that projects with the most positive impact on the protocol receive priority and attention.\n\n\n\nDesignate OZ as the quality oversight stakeholder\n\nLeverage the DAO’s existing relationship with OZ by designating them as an impartial security advisor that champions the protocol’s best interests. As a current security partner, OZ is well-positioned to uphold a quality benchmark for projects launched under the Compound name. Approval from OZ could even serve as a prerequisite for milestone payouts, promoting the delivery of high-quality code over expedited development.\n\n\n\nTie milestone structure to value creation\n\nAlign milestone payouts with tangible protocol value creation rather than task completion devoid of substantial impact. Shift focus towards funding projects that genuinely enhance the protocol. Implementing performance-based milestones, such as payouts triggered by a Dapp driving TVL to Compound, can help achieve this goal. Another approach is to consider retroactive grants, rewarding projects based on their actual contribution to protocol value.\n\n\n\\nThank you so much @kevin for sharing your valuable feedback and inputs. Really appreciate the encouraging words and your initial support to kickstart CGP 2.0. We recognise that there is a scope of further improvement in CGP 2.0, and believe that the active involvement of Compound Labs, community members, and other stakeholders will be instrumental in making CGP 2.0 renewal a success. Such actionable and constructive feedback regarding an individual initiative can be viewed as great progress in itself. We have actively taken feedback from active community members over Discord and Compound community calls on a frequent basis and have proactively worked towards resolving them. We invite additional comments on this initiative. Going forward, we will also ensure to amplify our efforts to seek feedback and provide additional opportunities for comments throughout the duration of the grants program rather than near its conclusion or renewal initiatives.\nFurthermore, we are thankful for the opportunity to engage in discussions aimed at further improving the grants program and its design. This transparent approach 3, distinct from other opaque programs that lack adequate data points to facilitate open discussions about crucial improvements, has allowed us to seek actionable feedback from Compound Labs team and the community members.\nRFPs vs CGP 2.0’s approach\nPrior to the launch of CGP 2.0, each domain allocator wrote comprehensive guides outlining the RFPs as well as guidelines for crafting well-structured proposals. The detailed documentation can be found on Questbook’s dashboard just below the title of the domain.\nProposal Dashboard Screen1920×205 25.4 KB\nThe guides for each domain are referenced below.\n\nDapps and protocol ideas: CGP 2.0 Domain: Protocol Ideas and dApps - Google Docs 1\nDev Tooling: Notion – The all-in-one workspace for your notes, tasks, wikis, and databases.\nSecurity Tooling: Compound Grants 2.0 - Security Domain - Google Docs\nCross Chain & Multichain Strategy: Compound Grants 2.0 - Multi Chain - Google Docs\n\nOutbound vs Inbound\nWhile each domain had well-defined RFP and proposal guides, we acknowledge that we can further scale our outreach and sourcing efforts. To accomplish this, we have significantly scaled our team and efforts to grow the number of builders submitting high-quality proposals. We already have an organic traffic of 20k+ builders and have now appointed dedicated resources to source proposals aligned with the domain-specific RFPs to increase our sourcing and outreach efforts. In addition to this, we worked and will continue to work with the Compound Labs team to participate and conduct hackathons at various events to increase awareness about the renewed grants program and source projects aligned with the domain-specific RFPs. This will include various initiatives, including guided sessions, workshops, and sourcing activities, which were previously implemented to attract talent and attention to CGP 2.0. Furthermore, the RFPs for the renewed program will be finalized only after actively taking feedback from the members of the Compound Labs and community members. We are open to receiving further feedback on scaling our outreach and sourcing efforts.\n\nReviewing the projects funded by CGP 2.0, I question the value that some projects provide to Compound. Funded projects are occasionally redundant, given existing solutions. An example is the $30K grant to launch Comet on Optimism, which was approved despite Compound Labs already communicating the work being done there months prior. This grant was revoked after Labs cleared up the miscommunication, but it was concerning to see the cross-chain domain allocator’s lack of familiarity with the cross-chain work being done for the protocol. Another example is the Compound API kit  1 grant to build a Typescript SDK for interacting with Compound, which already exists.\n\nThe evaluation criteria of “Relevance to our ecosystem\" was designed as an obligatory field for domain allocators to complete proposals under consideration. Its significance lies in carefully evaluating proposals, ensuring their alignment with Compound. This requirement emphasizes the selection of proposals closely tied to the Compound’s objectives.\nEvaluation Rubric Parameter1920×424 46.6 KB\nReally appreciate you pointing this out. We’ve thoroughly addressed the situation regarding the Comet launch project, clarifying the matter with the proposer and resolving any miscommunication. As part of our efforts to prevent such occurrences, we’ve enhanced the role of the Program Manager. The Program Manager will now ensure that the accepted proposals are known to the members of Compound Labs every week, and over community calls along with syncing with domain allocators twice a week.\n\nI’d like to avoid providing specific examples here, but there are many projects that collect fees from users, none of which are given to the DAO. There are other projects that seem more like a solution in search of a problem, as is the case with cross-chain infrastructure projects that benefit greatly from being adopted by a blue-chip protocol like Compound. Considering the significant portion of the DAO’s funds allocated to these grants, it’s wise for the DAO to reevaluate its fund allocation strategy, aiming to efficiently direct resources toward projects that generate maximum value for the protocol.\n\nThank you for highlighting this. Really appreciate it! With the Program Manager’s expanded role and the valuable input from Compound Labs, we are committed to prioritizing projects that align with the finalised RFPs and demonstrate a commitment to contributing their fees to the DAO. Additionally, we will actively seek inputs before finalizing evaluation rubrics to ensure that projects aiming to both generate and contribute fees to the DAO and provide value receive funding. We expect that the alignment of funded projects will improve significantly as we actively gather input from both Compound Labs and community members on domain-specific RFPs and evaluation rubrics.\nLack of Community Oversight\nWhile we have and will continue to encourage community members to actively oversee the evaluation process and the status of each submitted proposal on Questbook 3, going forward, we will collaborate closely with approved proposals to facilitate the sharing of their proposal details and milestones with the Compound community. Some of the initiatives taken towards this direction can be seen here 2. Further initiatives will involve more frequent demo days, similar to the one conducted near the end of CGP 2.0, where the community can gain deeper insights into the ongoing projects and their impact to Compound. It is in the best interest of all stakeholders to deliver what is best for Compound to avoid any significant reputation damage.\n\nThis came to our attention when we realized a grant project received the green light from OZ to launch on mainnet, despite lingering security concerns tied to their contract. This situation raises significant concerns, as the protocol’s brand could suffer irreparable harm if any issues were to arise. While there seemed to be an implicit assumption that OZ, as the official security partner of the DAO, would uphold the quality standard, it appears this expectation was somehow overlooked. Given that any project launched via the grants program is backed by the DAO’s funding and implicitly carries the DAO’s stamp of approval, maintaining a high project quality benchmark is essential. Clearly defining the responsible entity for upholding this standard going forward is crucial to safeguarding the protocol, especially as more grant projects are shipped using the Compound banner.\n\nWe appreciate you raising this concern. This issue has been thoroughly addressed through in-depth discussions with the domain allocators, with special attention from the OZ members. As a result, the responsibilities of domain allocators have been extended to requesting an audit for the considered/accepted proposals, particularly those that involve Solidity code being deployed into production and directly impacting Compound.  In order to streamline the code auditing process and avoid potential time-consuming challenges, the domain allocators will also provide assistance to the considered/accepted proposals by offering feedback on code quality and design.\nWe are dedicated to implementing quality assurance measures to safeguard the protocol and uphold and enhance Compound’s brand reputation.\nFurther Improvements\nWe acknowledge the potential for further enhancements, as highlighted above, and greatly appreciate the valuable feedback you’ve provided:\n\n\nRFPs vs Inbound approach - Although all proposers were encouraged to review domain-specific RFPs before submitting their proposals, we will finalize the RFPs for the updated program after actively seeking input from both Compound Labs members and the broader community to ensure alignment with Compound’s focus areas and roadmap.\n\n\nDesignate OZ as the quality oversight stakeholder - We are actively collaborating with OZ on this initiative and have incorporated their feedback in this proposal to expand the role of domain allocators for improved quality oversight as initial steps.\n\n\nTie milestone structure to value creation - During CGP 2.0, all proposals were funded based only after the milestone was accomplished. We have taken feedback from a few Compound Labs members on this matter and have linked metrics such as TVL to the grants program’s success.\n\n\nWe extend our sincere gratitude for your valuable suggestions towards further improving CGP 2.0 We request you and the community to proceed and participate in the CGP 2.0 renewal, taking the following into consideration:\n\n\nConsidering CGP 2.0 stopped reviewing proposals on June, 30th, a large number of small teams and proposals who could potentially contribute significantly to Compound are awaiting review on their proposals for over a month\n\n\nWhile we acknowledge and appreciate the initiatives undertaken by other entities to contribute to Compound’s growth, we believe that their focus is primarily directed towards business development, expansion, and funding proposals within this domain, rather than prioritising and funding core protocol-related and technical proposals. We also believe that they can complement CGP 2.0’s efforts should the community decide to proceed with their proposal. We are open to collaborating with them to accomplish what is best for Compound\n\n\\nPreviously, I was the multi-chain domain allocator before moving to Nethermind.  It was a pleasure being part of CGP2.0, and they have shown how grant programs should be conducted, focusing on transparency, accountability, and a fast TAT.\nDuring my time as the DDA, it was a great and seamless experience utilizing Questbook and working with the team. If I had any issues with using Questbook, they were resolved swiftly.\nCompound would benefit from renewing this program.\\nThank you @Anthias.xyz, @gjaldon for sharing your experience with CGP 2.0. Really appreciate it!\\nThank you so much @Bobbay_StableNode for sharing your experience and your support.\\nAs a CGP 2.0 grantee, I had very similar positive experience as shared by others above.\nIn terms of the grants application process, the Questbook tool was straightforward and seamless to use.\nI also very much appreciated the transparency provided in the grant allocation process. Prospective grantees, and the Compound community overall, can see funding decisions and feedback provided by Domain Allocators on Questbook. This was very beneficial to me as a prospective grantee to ensure that my grant request was within scope of the grants program and was appropriately sized.\nAll that said, I found the team very capable and competent stewards of the grants program so support the renewal as specified in the proposal.\\nThank you @kevin for laying out observations and suggestions for improving the next iteration of CGP. They overlap closely with many of the considerations that domain allocators like myself have been navigating within each domain and with each other in facilitating the program as a whole. I thought I could add some color here for the community about how the program has operated in practice this cycle, as I think it is relevant to how we might go about implementing some of the suggestions.\n\n\nOn inbound vs RFP approach: In practice, CGP 2.0 operated on a hybrid model where sample RFPs were provided as a part a general call for proposals in each domain. Larry’s observation of relative chaos in the absence of RFPs with CGP 1.0 is valid, especially given the context of the heady days post-DeFi summer during which that program was in full swing. At the same time, during somewhat quieter times like now, there is value in permitting applications beyond a strict set of RFPs: it widens the scope of potential builders by allowing folks to bring their own ideas (which is often more intrinsically motivating than working on “someone else’s idea”); reduces the impression of scarcity (“someone else/faster/better will claim that bounty first, so I won’t bother”); and taps the broader community who may have better ideas within each domain than what we articulate as priorities for the protocol. In domains like mine, the vast majority of received proposals fell outside of the provided RFPs, and some could be argued to be duplicative – analytics stands out as an example – though I would counter that having multiple accurate analytics providers with different focuses and goals benefits the protocol from the standpoint of accessibility for sophisticated users. Token for token, I would have liked to see more of my domain’s support go to initiatives meaningfully and creatively introducing uninitiated communities to Compound, but the quality of proposals serving more sophisticated users was (perhaps not surprisingly) far higher on average. Community refinement of a key set of RFPs in each domain would be valuable, and I welcome ideas for stimulating that discussion. We struggled with limited community engagment on these in the development of CGP 2.0 despite robust discussion about what the structure of the program should look like.\n\n\nOn tying milestone structures to value creation: The quality of milestone articulation in CGP 2.0 varied strongly from project to project, but overall I agree that there is room for improvement. Milestones in CGP 2.0 were proposed directly by the applicant and either approved or rejected along with the entire application. I often encountered situations where a project was supportable in principle, but the proposer’s milestones were strictly completion-oriented and therefore poorly aligned with the interests of the protocol, including many that did not include a mechanism for feedback from the community (which was articulated as a requirement in my domain’s program description). In many of these cases, I worked with builders to revise milestones as part of a complete proposal resubmission, which was clunky (and a fair amount of effort on both sides) but at least resulted in better-aligned milestones in most cases. It will not be hard to pull up examples where I let some poorly articulated milestones slide, often due to the aforementioned challenges of needing to revise and resubmit the entire application (requiring also a new eth payout address in Questbook) for any 3rd, 4th, etc round of milestone tweaks.\nTying of milestones to value creation in the form of quantifiable protocol metrics is definitely preferable in principle to completion-based milestones. It also presents additional challenges on the ground that limited our use of them in practice:\n\n\n\n\nfor many projects, it is difficult to attach a timeline to KPIs within the authorization timeline of CGP. If a project articulates a plan to drive 1M USD equivalent stablecoin borrowing activity to the protocol, but it takes 9 months to do so, in the current model, CGP would not have a way to fund this milestone (and the builder would be stomaching an inordinate amount of risk to agree to it). There are some web3-native solutions to this class of problem (e.g. UMA’s KPI options), but these would still require holding CGP-allocated COMP in a smart contract potentially beyond the authorization period of the program. Furthermore, favoring projects with short-term KPI delivery could bias the program away from projects that build longer-term value that can’t be readily measured within the program timeline.\n\n\nfor other projects, metrics for value creation can be difficult to articulate: metrics for security, dev tooling, analytics, governance research, and education/outreach initiatives are all either more challenging to articulate or more challenging to verify. I think we’ve done an okay job with this for a majority of accepted proposals in my domain, despite its breadth, but I’m certainly open to suggestions for how some of them could be improved. The best place to do this is in the discussion thread on each proposal in Questbook, and ideally before the proposal is formally reviewed.\n\n\nmonitoring of the TVL or related KPIs may in some cases require additional development beyond the scope of the proposal; this can be added, but needs to be taken into account and represents a kind of embodied cost (albeit likely worthwhile) to sharpening milestones this way. Given that many projects in my domain were asked to trim features to stay below our per-grant limit, it becomes more difficult in some cases to justify adding on these costs.\n\n\n\nOn lack of community oversight: I’m supportive of the idea of elevating OZ’s role in oversight, although it will require some close discussion with OZ about the parameters of that commitment within OZ’s ongoing B2DAO arrangement with the protocol. I disagree (civilly) with the suggestion that the CGP director and domain allocators are not directly incentivized to look out for the best interest of the DAO on account of their being paid by the DAO for their efforts. On the contrary, by analogy to a democratically elected politician, the allocators and program manager are directly incentivized to operate in the best interest of the DAO or else they will be voted out of office: if our performance had been truly misaligned with protocol interests, the conversation would more likely be about transferring responsibility entirely over to Alastor or Alphagrowth rather than exploring how they might work with the CGP 2.0 team to improve future rounds of Compound Grants.\n\nFinally, I just wanted to note that while I respect @Harsha’s decision to afford the community additional time to review @kevin’s concerns and reflect any changes in a new on-chain proposal, the postponement is not entirely net-neutral because it exacerbates uncertainty of timeline and commitment among builders in the community, several of whom came to Compound specifically in response to the opportunities created by CGP 2.0. Some have already submitted proposal concepts in anticipation of a potential renewal. Others developed proposals in CGP 2.0 whose scope/cost exceeded what could be funded with the resources available in that program and chose to break their proposal into two or more components, some of which are planned (or even in progress) for a next iteration of CGP.\nAnd one last note re: retroactive funding: while I agree that this could be a component of a next iteration of CGP, I have objections to the concept of CGP being a retroactive-first program. Retroactive funding has its place, but based on my experience interacting with our builders in CGP 2.0, most of the builders in our community are developing as a/the primary source of income. Retroactive programs that don’t commit support until after the fact are poorly aligned with their needs and would likely reduce both the quantity and quality of applications. Proactive funding is more inclusive and will help bring forward the best ideas for the protocol.\\nAs a grantee of CGP 2.0 earlier this year, I just want to say thanks for making the experience a smooth one. Not all web3 grants programs run smoothly, but this one did. Thanks for supporting our work to enhance DAO Governance.\\nIt’s an honor to be selected as a potential Domain Allocator. As Compound is a well-known and reputable protocol, there have been various applications for grants. However, as I have witnessed in different protocols, without a proper accountability, the protocol will be worse off as they have to share the limited resources as well as potential PR / branding damage.\nTherefore, as Harsha has laid out in the proposal above, it’s critical to continue to have such process to ensure more accountability and transparency to ensure the protocol can benefit more from the program.\\n@blake thanks for clarifying. Again, I want to re-emphasize that I only used your project as an example of redundancy in funded projects, without assessing the project’s quality. I appreciate the nuances and the added features it offers on top of the existing Compound.js. A better example of redundancy would have been the funding of 3 analytics dashboards that @allthecolors had mentioned.\\n@harsha, while neither I nor Labs requested for the proposal to be cancelled, I appreciate the gesture of cancelling to allow for more constructive discussions to take place before a renewal is made.\n\n\n\n harsha:\n\nBefore submitting the renewal proposal for an on-chain vote, we also conducted a temp check vote to gauge the community’s sentiment towards the renewal proposal where we witnessed a clear majority towards the renewal.\n\n\nTo be fair, it does look like the majority of the participants in the temp check vote are either directly involved with CGP 2.0 or are grantees who got funding from the program.\n\n\n\n harsha:\n\nWe respect these considerations and request you to provide specific timelines for concluding these discussions along with actionables.\n\n\nRespectfully, neither I nor Labs hold the authority to establish timelines here. My intention is solely to express the concerns shared by myself and some of my colleagues. Ideally, the service provider can consider these suggestions and address them before proceeding to a renewal.\nIf the community is happy with the grant program in its current form, there is nothing Labs can do to stop the renewal.\n\n\n\n harsha:\n\nWe assumed that we have taken detailed feedback from the community through demo day, forms, community dev calls, polls and replies on the forum post before submitting an on-chain vote.\n\n\nI admit we could have surfaced these concerns sooner. I wanted to wait for the conclusion of the grants program to thoroughly analyze it and provide feedback, as well as give the program a chance to run through completion.\nAs an outside observer, keeping tabs on the grants program was rather difficult since everything lived in the Questbook dashboard. Tracking the progress of the grants program required clicking into each project individually, a time-consuming task that only domain allocators are likely to do on a frequent basis.\nOne recommendation is to give weekly/monthly updates directly on the forums to convey which projects were funded and paid out. The Aave Grants program does a great job 1 of communicating updates 1 and CGP should strive to do the same.\n\n\n\n harsha:\n\nWe will initiate a community vote and gather feedback during a community developer call before presenting the refined proposal for consideration.\n\n\nThank you. We look forward to helping to refine the program.\\n@allthecolors, thank you for your response and I appreciate all the work you have been doing for the Compound protocol.\n\n\n\n allthecolors:\n\nAt the same time, during somewhat quieter times like now, there is value in permitting applications beyond a strict set of RFPs\n\n\nCorrect me if I’m wrong, but the grants program (CGP 1.0 included) has historically allowed applications that fall outside the scope of RFPs. I was not suggesting we only stick to RFPs, since the grants program should remain flexible to fund a diverse set of ideas. However, I think we may disagree on the extent of approving these non-RFP applications.\nMore often than not, applications that deviate from RFPs aren’t well-aligned with the program’s goals. While exceptions exist, these may be rare if the RFPs are well-defined. Allocators should lean towards rejecting such projects rather than approving them. Justifying funding of out-of-scope projects due to the lack of in-scope applications leads to a loss of program focus.\nIn essence, rejections of applications should be the norm. It is fine, and often expected, for a grants program to be selective. On a side note, it would be valuable to know the acceptance/rejection rate for CGP 2.0.\n\n\n\n allthecolors:\n\nIn many of these cases, I worked with builders to revise milestones as part of a complete proposal resubmission, which was clunky (and a fair amount of effort on both sides) but at least resulted in better-aligned milestones in most cases.\n\n\nThat is good to hear, and I have seen examples of you reducing scope or altering milestones via the Questbook program. Unfortunately, as you stated, this process is cumbersome and requires resubmitting a new proposal, cluttering the dashboard even further. Hopefully, that experience can be improved going forward.\n\n\n\n allthecolors:\n\nTying of milestones to value creation in the form of quantifiable protocol metrics is definitely preferable in principle to completion-based milestones. It also presents additional challenges on the ground that limited our use of them in practice:\n\n\nYou’ve outlined three difficulties with defining performance-based milestones and I agree with all of what you said. Forward-looking milestones (e.g. TVL) are certainly difficult to do. But perhaps there is a middle-ground, and this is heading towards my suggestion of an objective quality assurance stakeholder.\nThe problem with a lot of the milestones is they are completion-based. I’ve seen $4k be paid out for a system diagram + wireframe of an extension that is not reviewed by anyone and will likely never be used by the protocol. There are plenty of milestones that simply require a grantee to finish writing code, without any quality assurance on the code being written. In these cases, the grants program is paying for work to be done, without the expectation of ever using that work because there is no QA involved.\n\n\n\n allthecolors:\n\nI disagree (civilly) with the suggestion that the CGP director and domain allocators are not directly incentivized to look out for the best interest of the DAO on account of their being paid by the DAO for their efforts.\n\n\nApologies if I seemed accusatory; that was not my intention. This conclusion came from my previous point of milestones being weighted towards completion rather than quality. Consequently, I was seeing projects being paid out and launched with limited testing/auditing, which I wouldn’t expect a protocol like Compound to ever endorse. Given that these projects, especially public goods initiatives, carry the Compound DAO’s implicit approval, they should meet a stringent quality standard.\nMy concern around incentive misalignment for domain allocators is that allocators may feel the need to deploy most of their capital. You might disagree with me on this point, but let me ask you this: If all 100 applications did not meet your bar, but were not outright scams either, would you be comfortable rejecting all of them and telling the program manager you allocated $0 out of the $450k? I assume you’d feel pressured to allocate some funds at that point. However, a person solely focused on the protocol’s best interest may not allocate any funds.\n\n\n\n allthecolors:\n\nthe postponement is not entirely net-neutral because it exacerbates uncertainty of timeline and commitment among builders in the community, several of whom came to Compound specifically in response to the opportunities created by CGP 2.0.\n\n\nSince the recently cancelled proposal was for a ~$1M renewal of the program, I think it’s fair to evaluate how this new funding should be best spent. Builders who took part in the initial phase of CGP 2.0 should not be blocked by these discussions as the original agreed upon funds have already been allocated to the grants program.\nAgain, thanks your for detailed response and hope we can find a way to improve the next iteration of CGP.\\n@kevin\nI am one of the grantees of CGP 2.0 but I am writing here in general about some of your comments. I bring along more than two decades of experience of writing software, running new businesses, and rolling out software products for them.\n1. RFPs vs Open calls\nAre you trying to outsource work or inviting proposals that can bring in fresh perspectives ? RFPs may be good to attract ‘service providers’ but Open calls will perhaps attract entrepreneurs and new ideas, some of which may add a lot of value to Compound. Open calls with RFPs as guidelines may serve as a middle path, but Open calls may provide better signaling to attract bright new ideas, some of which you may not have even thought about. For example, returns by offchain borrowers of tokenized RWAs are higher than what onchain returns on staked collateral is on a lot of Defi platforms, and you may see a gradual withdrawal of liquidity from protocols such as Compound go to RWAs - I guess it will be hard to frame a RFP that will adequately capture this problem ?\n2. TVL / Metrics\nDefi is in its infancy. $42 billion of TVL is not great considering that there are hundreds of protocols and Dapps and that $42 billion is not going up. Who knows, TVL may not be the correct metric after all as capital efficiency goes up on Defi platforms. Locking in capital in a protocol may not be a good sign unless that capital is creating returns for liquidity providers. Perhaps, transaction volumes are a better indicator ? Again, this is something that submissions on open calls may shed some light on if they are given the flexibility of specifying their own set of metrics.\nFinally, cancelling CGP 2.0 may have been done in a hurry and somewhat unilateral way, unless we are not aware of the need to take that decision in a way. There are some 20-30 grantees in the CGP and many of them (including us) have discussed doing work across CGP 3.0. Although, our interests are not greater than the interest of the Compound protocol, giving some consideration to grantees show the maturity of thinking of the team behind the CGP/Compound. As a core contributor, we expect you to lead the way in these matters in a thoughtful and mature way.\\nWe could possibly have a retro session either during one of the dev calls or as a separate call since many things were done well, but there are also areas for improvement.\nConducting a fully transparent process requires a lot of effort, and I believe Questbook and the domain allocators did an excellent job.\nI personally feel we should establish two tracks:\n\nRFP, which addresses the immediate needs related to rollouts, integrations, content, and growth, etc.\nAnother open track (or domains) for all kinds of ideas.\n\nIn the review or grant allocation process, we could introduce an additional step where domain allocators assist in shortlisting and refining the applications. A final review could then be undertaken by the domain allocator + someone from the labs or an individual who has been engaged with the compound community for some time, possessing both historical knowledge and a long-term perspective.\nI believe this step will benefit grantees, as it allows them to specify the kind of support they’ll need after the grant project concludes, such as code reviews, audits, marketing, rollout, maintenance, and more.\\nI’m a CGP 2.0 grantee that worked on the Compound 3 Wrapper project. This grant program was the first I participated in and I found it very smooth and straightforward. Overall, I am very happy with the program and I think it has led to significantly more developer activity around Compound. @harsha and @roohchill were very helpful and responsive.\nI haven’t yet voted any proposal with my COMP and I will gladly be voting for this proposal.\\n\nwhile neither I nor Labs requested for the proposal to be cancelled, I appreciate the gesture of cancelling to allow for more constructive discussions to take place before a renewal is made.\n\nThank you @kevin for sharing your comments. While we have been taking active feedback on CGP 2.0 throughout the last six months and on the renewal proposal very frequently, we want to ensure that we incorporate any additional feedback from all key stakeholders including the members of Compound Labs before proceeding for an on-chain vote.\n\nTo be fair, it does look like the majority of the participants in the temp check vote are either directly involved with CGP 2.0 or are grantees who got funding from the program.\n\nWe want to re-emphasize that we have invited active feedback and comments from different stakeholders very frequently throughout the duration of the grants program on Discord, developer community calls. Furthermore, we have invited feedback on the renewal proposal. While knowing the experience, feedback of stakeholders directly involved with CGP 2.0 is critical for further improvements, we have also explicitly requested and taken feedback from all the grant proposals both from the accepted and rejected groups.\n\nRespectfully, neither I nor Labs hold the authority to establish timelines here. My intention is solely to express the concerns shared by myself and some of my colleagues. Ideally, the service provider can consider these suggestions and address them before proceeding to a renewal.\nIf the community is happy with the grant program in its current form, there is nothing Labs can do to stop the renewal.\n\nWe believe that any additional inputs and feedback from Compound Labs is critical to the success of CGP 2.0 renewal. This will ensure that the funded proposals align with the protocol’s priorities and value creation, prior to advancing to an on-chain vote for CGP 2.0’s renewal.\n\nOne recommendation is to give weekly/monthly updates directly on the forums to convey which projects were funded and paid out. The Aave Grants program does a great job of communicating updates 1 and CGP should strive to do the same.\n\nWe appreciate your feedback and the work of Aave grants program to update the community. Going forward, we will be sharing updates on the status of each proposal across all domains once a week with the Compound community here. The Program Manager will ensure that the status of proposals (accepted, and funded) are known to the members of Compound Labs and the community once a week over forum posts and community calls. Questbook empowers anyone in the community to view the status of each proposal (even those that are rejected) along with the evaluation rubric, rubric scores and the feedback from each domain allocator. The Program Manager will include links to these proposals in the weekly forum updates, providing community members with the opportunity for a more comprehensive overview.\nWe extend our sincere gratitude for your valuable inputs and acknowledge that they will play a critical role in the impact and success of CGP 2.0 renewal. We are committed to incorporating additional inputs from key stakeholders including the members of Compound Labs before moving forward to the on-chain vote.\\nUpdate: We have returned all the unallocated funds back to the Compound treasury from CGP 2.0 2. Please find the link here 5.\\nUpdate\nThank you @kevin , @adam, community members and the members of the Compound Labs team for sharing your inputs on the CGP renewal proposal, more specifically on the RFPs. Without your critical inputs, the RFPs wouldn’t have shaped this way. Thank you @allthecolors , @Doo, and @cylon for creating the RFPs for each domain. Over the past two months, we’ve conducted several calls/sessions with each Domain Allocator to draft the RFPs, keeping in mind the feedback that we’ve received on the initial proposal and this thread. Additionally, we’ve included the RFPs for each domain in the updated proposal and have incorporated valuable inputs from community members engaged in this forum post to ensure that the RFPs are aligned with Compound’s roadmap.\nRFPs, acceptance criteria and specifications for each domain\n\nDapps and Protocol Ideas - Link 14\nMulti - Chain/Cross chain Strategy, Dev Tooling - Link 9\nSecurity Tooling - Link 5\n\nPlease find below the updated CGP 2.0 renewal proposal:\nTitle\nCGP 2.0 Updates and Renewal\nSummary\nAfter successfully running CGP 2.0 for two quarters and taking into consideration its impact, we propose to renew Compound Grants Program 2.0  with a budget of $970,000 spread across 3 domains. We have received great feedback and support from the community, builders, and domain allocators to renew the grants Compound Grants and are grateful for their valuable inputs.\nBackground  and Progress\nCGP 2.0 went live on Jan, 2023 with a grants budget of $800k spread across four domains. Over the past two quarters, CGP 2.0 domain allocators approved proposals requesting $670,000 in grants 2 and disbursed a total of ~$410,000 to accepted proposals from a pool of 100+ proposals. These domain allocators were elected from the community and by the community. The specific information regarding the accepted proposals and the funded teams can be found here 2. Please find below CGP 2.0’s funding breakdown, relevant metrics, insights, and proposed improvements going forward.\n\n\nFunding Breakdown\nCGP 2.0 Domain wise Funding Breakdown 1\n\n\n\nMetrics\n\nCGP 2.0 team has received exceptional feedback from the builder community. After conducting a survey to gather feedback from the proposers and grantees, CGP 2.0 received an impressive NPS score of 8.6/10 and experience score of 8.2/10. Below are some comments provided for reference.\n\n\n\nThe CGP 2.0 grants committee has accepted proposals from a diverse pool of over 100 submissions with an acceptance rate of 40%. This acceptance rate indicates high quality of proposal submissions and proposal pipeline. Throughout the grants program, CGP 2.0 grants team has consistently maintained a remarkable average communication turnaround time (TAT) of less than 48 hours and an average funding TAT of less than 2 weeks after the milestone has been completed.  Furthermore, the milestone completion rate for all funded proposal and accepted proposals stands at ~ 74% and ~44% respectively.\n\n\n\n\nCGP 2.0 Metrics1951×960 40.8 KB\n\nTotal number of proposals funded - 35\nNumber of funded proposals with new interfaces for supplying/borrowing/governance interaction - 14\nNumber of funded proposals supplying/borrowing on Compound - 3\n\n*Note: These figures exclude proposals that have not yet completed their first, initial milestones. The majority of the accepted proposals are actively progressing towards accomplishing their milestones, and we will keep the community informed about the progress of proposals in each bucket such as proposals supplying/borrowing on Compound, TVL  through community calls and reports.\n\n\n\nInsights and Feedback\n1. Specific domains made it easier for builders to understand the scope and structure their grant proposals\nIn the case of CGP 2.0, we took a fundamentally different approach by ensuring that each focus area (domain) is communicated clearly. We streamlined the program to four specific domains, providing builders with a more focused and structured framework.\nFeedback from proposer3434×110 55.4 KB\n\n2. Knowing the allocator’s name for their domain helped builders with a significantly faster query resolution and TAT\nThe public availability and accessibility of the domain allocator played a crucial role in maintaining a very low turnaround time (TAT) for CGP 2.0. Builders who applied to a specific domain could directly connect with the respective domain allocator, enabling them to ask additional questions/seek clarifications.\nFeedback from proposer3428×232 97.2 KB\nFeedback from proposer3426×374 90 KB\nFeedback from proposer3394×150 51.1 KB\n3. A transparent and objective review process significantly benefitted builders, enabling them to incorporate actionable feedback into their resubmissions\nIn addition to maintaining a communication TAT of less than 48 hours, domain allocators assessed proposals using a domain-specific rubric in a transparent manner. This objective feedback provided builders with a clear understanding of the areas that needed improvement, allowing them to revise and resubmit their proposals based on actionable feedback.\nEvaluation Rubric1894×2100 208 KB\nFeedback from proposer3426×172 58.1 KB\n\n\n\nProposer’s Experience and Comments\n\nGovernance Voting Participation and Decentralization Tracker  by Signal Corps\nAlcancía - Be your own bank for Latin Americans by Juan Diego Oliva Heinsen\n1Delta DAO - The decentralized margin broker by Achim and Kevin\nA Flexible Voting Money Market on Compound V3 by Ben DiFrancesco \nWido - Collateral Swap Extension by Roman\n\nTAT\nFeedback from proposer3428×232 97.2 KB\nFeedback from proposer1920×219 35.6 KB\n\n\n\n\nCGP Demo Day\nCGP 2.0 Demo Day\n\n\n\nChallenges and Expected Improvements\nWhile CGP 2.0 team has received great feedback from the builder community and Compound, there is still room for further improvement.\n1.Ensuring a consistent TAT for all proposals - Although the average communication TAT for all proposals was below 48 hours and the average funding TAT was less than 2 weeks, there were instances where proposers experienced delays in response or were unaware of the reasons behind the delay. Similarly, some grantees encountered delays in receiving funding. While these delays can be attributed to factors such as product bugs, going forward the CGP team will ensure a consistent communication and funding TAT to all the teams and ensure that these delays are communicated proactively.\n2. Addressing funding amount discrepancies - CGP 2.0 will be able to honor all funding commitments to all the accepted proposals. Despite that, there were cases where proposers did not receive the exact amount initially committed due to fluctuations in the value of COMP and funding delays. To mitigate this issue, the CGP team will aim to payout the grantees as soon as the proposer hits the milestone. We further propose paying out accepted proposals in stables rather than COMP to avoid potential discrepancies caused by COMP price volatility.\n3. Improving operational efficiency - Questbook is working with Synapse and Docusign to integrate their service and enable KYC and agreement signing directly from Questbook. This will significantly improve the operational efficiency and funding TAT going forward.\n\nProposal\nBased on the impact and insights derived from CGP 2.0, we propose renewing CGP 2.0 with a budget of $970k for two quarters. The domain allocators will utilize this budget to fund proposals that align with Compound’s roadmap. After researching, and gathering feedback from domain allocators, active community members, and builders,  we propose supporting the following domains:\n\n\n\n\nDomain\nDomain Allocator\nProposed Budget\nRFPs and Rubrics\n\n\n\n\nDapps and New Protocol Ideas\nallthecolours\n$450,000\nCGP 2.x Domain: Protocol Ideas and dApps - Google Docs 14\n\n\nMulti - Chain/Cross chain Strategy, Dev Tooling\nDoo from Stable Labs\n$200,000\nMultichain Cross Chain & Dev Tooling Domain - Google Docs 9\n\n\nSecurity Tooling\nMichael from Openzepplin\n$150,000\nCGP 2.x Domain: Security Tooling & Enhancements - Google Docs 5\n\n\n\n\nGiven that Dapps and New Protocol Ideas domain received more than twice the number of proposals compared to all the other domains combined, we propose merging Multi-chain and Cross-chain Strategy domain with the Dev Tooling domain.\nAdditionally, we propose increasing the allocated grants budget for Dapps and New Protocol Ideas domain to $450,000 and reducing the grants budget of the Security Tooling domain to $150,000 taking into consideration the number of proposals and allocated grant amounts for both domains during CGP 2.0 2\n\nRFPs, acceptance criteria, and specifications for each domain\n\nDapps and Protocol Ideas - Link 14\nMulti - Chain/Cross chain Strategy, Dev Tooling - Link 9\nSecurity Tooling - Link 5\n\n\n\nThe renewed grants program will maintain its dual-track approach, including both approaches i.e. outbound (sourcing based on the domain-specific RFPs) proposals to ensure alignment and value creation and inbound (open) proposals to invite creativity.\n\n\nThe Program Manager will share the status of proposals (whether accepted or funded) and ensure that the updates are effectively communicated to the broader community and members of Compound Labs through monthly forum posts and bi-weekly community calls\n\n\nSpecifications and Implementation\nSimilar to the model implemented in CGP 2.0, the renewed grants program will be run using Delegated Domain Capital Allocation Model 1. Each domain allocator will run their respective domain on-chain for full transparency using Questbook. The data and performance across key metrics will be visible to the community.\nThe disbursement of the grant will take place on-chain from a multi-sig wallet controlled by the program manager & the domain allocator. The domain allocator will approve or reject the application based on evaluation rubric. A Grants SAFE, with 3/4 multi-sig, between the program manager and 3 domain allocators will be setup. We will then have 3 SAFEs for each of the domains with a 2/2 between the program manager and the specific domain allocator. The funds for the grants program will flow from the treasury into the Grants SAFE. This SAFE will hold the funds related to operational costs, committee compensation, and the grants budget. Funds that will be disbursed to the proposers will reside in the domain-level SAFEs.\n\nAfter the end of two quarters, the grants committee and the Compound community shall evaluate the performance of each domain using publicly available data and decide to change the domain, change the domain allocator, or the program manager.\nCompensation\nSourcing, reviewing, funding, marketing, tracking, and nurturing proposals requires significant expertise and time commitment from the grants committee members and they should be fairly and competitively compensated for their efforts. Based on the learnings from CGP 2.0 specified above, we believe that a Program Manager is expected to dedicate approximately 25 hours per week, while the domain allocator is required to allocate an estimated 15 hours per week. However, these time commitments may vary depending on the number of proposals received for a domain and the domain allocators may exceed or work for less than 15 hours per week based on the proposal volumes. We propose keeping the hourly price the same for the Domain Allocators and the Program Manager as in the case of CGP 2.0\n\n\n\n\nRole\nPer Hour Cost\nHours Dedicated Per Week\nTotal\n\n\n\n\nProgram Manager\n100\n25\n$60,000\n\n\nDomain Allocator\n80\n15\n$90,000\n\n\nOperations Cost, Misc.\n\n\n$20,000\n\n\n\n\nTotal Amount Required\n$170,000\n\n\n\n\nQuestbook will provide the grants committee its grants orchestration tool free of cost.  We suggest that the grants committee continue with Synapse for KYC services and Docusign for all contractual agreements, as we have been using these services throughout CGP 2.0.\nHowever, for any specific asks from the grants team in order to run the process more smoothly, Questbook will charge for any additional feature requests based on the development overhead through a retrospective grant from Compound.\n\n\n\nCGP 2.0 Funding Overview\n(As on 4th October, 2023)\n\n\n\n\nDomain (A)\nAmount Available (B)\nGrant Amount Paid Out (C)\nGrant Amount Allocated (D)\nSurplus (E)\n\n\n\n\nDapps and protocol ideas\n~$144,433(3,293 COMP)\n$242,850\n$387,851\n\n\n\nMulti - chain and Cross Chain\n~$54,694 (1246.8384 COMP)\n$50,001\n$87,003\n\n\n\nDeveloper Tooling\n~$5663(129.06 COMP)\n$52,177\n$90,877\n\n\n\nCompound III Security Tooling\n~$40,759 (929.33 COMP)\n$64,550\n$105,800\n\n\n\nTotal\n$245,549\n$409,578\n$671,531\n0 ($20,000 in the parent Grants SAFE)\n\n\n\nAll unallocated funds have been returned back to the Compound treasury\n\nKPIs and Expectations\nContribution of Funded Projects to Compound\n\nNumber of users onboarded by the funded proposals onto their app/protocol\nTVL (if applicable) of the selected proposals\nNumber of projects increasing supplying/borrowing activity on Compound\nNumber of new interfaces for supplying / borrowing/governance interactions\nNumber of projects that have raised follow-on capital after getting a grant from CGP 2.0\n\nProgram Success\n\nIncrease in the number of contributors, proposals, and funded projects\nIncrease in milestone and proposal completion rates\nIncrease in NPS score from all proposers and grantees\nLower response turnaround time to delegates’ and community’s queries\nDiversity in projects being funded across technologies, geographies, and demographics, to name a few. We encourage the community members to review the proposals across different domains during community calls regularly\nTimely publishing of comprehensive monthly grants report, outlining the status, progress, and impact of the program, ensuring transparency and accountability\n\nEnhanced Community Involvement and Brand Awareness\n\nIncrease in the community members’ participation to keep domain allocators and program manager accountable (measured by the number of people looking at the dashboard engaging with the forum post updates)\nStrengthened contributors’ sentiment and word of mouth towards CGP and Compound  measured through frequent sentiment surveys/ polls to gauge satisfaction\n\nDomain Allocator Roles & Responsibilities\n\nAll Domain Allocators and the Program Manager will continue to uphold their designated responsibilities as outlined in the CGP 2.0 proposal.\nThe Program Manager will share the status of proposals (whether accepted or funded) and ensure that the updates are effectively communicated to the broader community and members of Compound Labs through monthly forum posts and bi-weekly community calls. This will increase visibility proactively and enhance community participation, giving the community an opportunity to voice concerns and seek clarity before milestone payouts are initiated.\nDomain allocators may request an audit for the considered/accepted proposals, particularly those that involve Solidity code being deployed into production and directly impacting Compound.  The audit requirements for each domain have also been detailed in each of the domain-specific RFP docs\n\nAbout Questbook\n\nQuestbook (YC-W21) is a decentralized grant orchestration tool, currently being/previously used by Arbitrum, TON, Polygon, AAVE, Celo & Solana.\nConsidering the achievements of CGP 2.0, as well as the time commitment and operational expertise necessary for running an effective grants program, Questbook will continue in the role of the Program Manager.  Ruchil from Questbook will assume the responsibilities of the Program Manager in place of Sriharsha Karamchati due to bandwidth constraints. Ruchil was the program manager for Polygon facilitating the disbursal of ~ $1M in grants. He works with Solana foundation and ecosystems within Solana on a frequent basis to help them design their grants program. He has received a grant of $250K from the Solana foundation for the same. He also worked closely with CGP 2.0 Program Manager and Grants team.\n\nWe welcome the community members to share their comments before proceeding with an on-chain vote.\\nThank you @robinnagpal for your kind words and the comments on further improving CGP 2.0. We’re actively working towards domain-specific RFPs, which we will soon be sharing with the community for their inputs. The renewal proposal will maintain its dual-track approach, including both approaches i.e. inbound (open) proposals to invite creativity and outbound (sourcing based on RFPs) proposals to ensure alignment and value creation.\nUpdates on the accepted, funded proposals across all domains will be frequently shared with the community on the forum. The Program Manager will share the status of proposals (whether accepted and funded) and ensure that the updates are effectively communicated to the broader community and members of Compound Labs through forum posts and community calls. This will increase visibility proactively and enhance community participation, giving the community an opportunity to voice concerns and seek clarity before milestone payouts are initiated.\\nWe’ve followed the discussion in the forum emphasizing the need for enhanced accountability and more effective monitoring of CGP statistics.\nStreamline CGP 3.0 reporting & accountability:\nIn response, think this idea could help meet the mentioned needs. We believe an additional accountability/reporting layer on top could increase transparency and effectiveness without downplaying Questbook’s contributions.\nI.e., having an external party taking over that function to let Questbook focus on their core skills. Exemplary set up would be sth like:\n\nLean KPI reporting for ventures and the Grant Program\nStandardized monthly reports that the community can access via the forum\nCollaborative working style with Questbook and ventures to gather data\nPerformance benchmark against other industry grant programs\n\nAny thoughts on this? Open to expanding on this idea further if there’s interest.\\nThank you @w3s for sharing your thoughts and comments. Really appreciate it!\nIn addition to our transparent approach that empowers the community members to get a deeper insight into the details of submitted, accepted, and funded proposals, it will be the responsibility of the Program Manager to share bi-weekly/monthly updates directly on the forum to communicate which projects were accepted, and funded as also recommended by community members in their response. The Program Manager will also update the community on the status of the proposals over community developer calls, and discord chat. These updates along with the comprehensive publicly available data on Questbook can serve as the foundation for creating customized dashboards and analytics tailored to specific needs based on the KPIs. However, we believe that such an endeavour may not be within the current scope of this proposal. It could potentially be a separate grant proposal once the program has been renewed.\nOver the past year, we’ve made significant enhancements to our processes, tools, workflows, and communication channels to ensure maximum transparency. Much like CGP 2.0, we will be sharing the breakdown, metrics, and impact of the renewed grants program with the community for their comments and feedback and we are open to improving further iterations of CGP based on the community’s feedback and the feedback of any external party.\\nThank you @harsha, all the Domain Allocators, and the broader community for your invaluable insights on refining the proposal and integrating RFPs to ensure a more transparent and efficient grant distribution process. As @kevin pointed out, we are dealing with a large amount of funds, and the community is collaborating to ensure their effective allocation.\nQuestbook and our team are in the final stages of preparing the CGP Renewal Onchain Vote Proposal, and we plan to submit it next Monday (Oct 30). Please comment here if you have any more concerns or suggestions. Thank you!\\nThe proposal is live for voting now!  4\nIf you hold COMP, please vote. The voting ends in the next 34 hours. Thank you. \\nUpdate: The on-chain proposal has passed! 6\nI (@ruchil ) from Questbook will be the Program Manager for the renewed CGP. Thank you everyone for your feedback and support. The CGP team will start inviting and reviewing proposals very soon. We will make sure to update the Compound community when the Program is live and share the status of accepted/funded proposals on a monthly basis.\nThank you all for the continued trust you have placed in us \\nAs @harsha noted, we have incorporated community feedback to develop revised domain descriptions with targeted RFPs and rubrics. Especially for proposals that were submitted to Questbook after the CGP 2.0 deadline, I encourage proposers to review the new program descriptions (linked in Harsha’s table above) to ensure their proposals and milestones remain well-aligned with the next iteration of the program as laid out here. If proposers want to make changes, they can use the tools on Questbook to revise/resubmit accordingly. We will also be increasing early-stage assessment of security risks for proposals involving smart contract integrations, so please be prepared to discuss security with us if your proposal involves smart contract development or integration, regardless of domain. Excited to be working with you all again to make CGP even more effective and impact-oriented!\\nThanks @harsha\nIn the Security Domain 3, I’ve provided the following RFPs based on community feedback and lessons learned from past security incidents. The three big categories are:\n\nGovernance Proposal Safety Checks\nUpgrade Rollbacks for the Timelock\nAsset Collateral Safety Checks\n\nThere is also still a general interest in other ideas to improve protocol security through CI/CD enhancements, incident response improvements, code refactors and security education materials but the three RFPs will receive priority funding.\\nHey @harsha ,the revisions to the program structure look good. The Rubrics, RFPs and KPIs provide a very robust framework for the program to start making impact from the get go. Appreciate the transparency from the DAs in the grant process as well.\\nI served as the Domain Allocator for Security Tooling as an extension of my role as OpenZeppelin’s Security Advisor to Compound.\nHaving felt the absence of a formal grant program prior to CGP 2.0, it’s resulted in a massive uptick in quality third-party contributions to the protocol. It’s also allowed us to better qualify and coordinate security audits for third-party contributions such as @gjaldon’s CometWrapper. In the security domain specifically, it’s allowed us to expand OpenZeppelin’s existing monitoring capabilities for Compound along with improvements in security documentation and tooling.\nI look forward to the next iteration of CGP to continue the development of better security for Compound through community contributions.\\nI am quite new to the Compound ecosystem and while we are working on a project with a CGP 2.0 grant, I do have a couple of thoughts / suggestions -\n\n\nSince CGP 2.0 was probably a few hundred thousand dollars worth of grants, and CGP 3.0 is even more ambitious, and since the average grant size is quite modest at $15-20k, this means domain allocators have to deal with as many as 50-75 grantees each, for example, the Dapp allocator will have a lot to do. As long as CGP is sure that 15 hours per week is good enough to deal with this workload, I guess it is fine, otherwise, TATs for project reviews and payouts may start to see some delays over time.\n\n\nAnother thought is it might be better for you to consider doing a survey among grantees at the end of every grant program where grantees anonymously fill up a simple survey form on what their experience was, and where they would like to see improvements. I first filled up such a survey during our work with the Balancer protocol last year and I thought it was a great idea to get feedback.\n\n\nAnd of course, I wish the CGP 3.0 program well and look forward to contributing positively to the Compound protocol and community.\\n@harsha, I appreciate your response and your acknowledgement of the areas where the CGP can be improved. I’d like to reiterate that ongoing discussions are taking place with other parties such as Alastor & w3s 4 and Alphagrowth 2 about improving the next iteration of CGP. It seems disingenuous to rush an on-chain proposal to allocate another ~$1M to the program before a) these discussions have finished and b) the community has had a chance to thoroughly access the value generated by CGP 2.0.\nAllocating ~$1M is a substantial decision, and the DAO should exercise caution and due diligence to understand the potential return on investment before going through with this renewal.\\nHi Kevin,\nI’ve been closely following the discussions around the Compound Grants Program and appreciate the depth of your insights. Addressing our Compound API Kit:\n\n\nDifferentiation from Compound.js: Our Kit stands apart from Compound.js. While the latter offers basic functionalities, our Kit is tailored to provide advanced features, such as leverage, collateral-swap, zaps by integrating flash loans and swap aggregators, bridging the gap between simple actions and more intricate, intent-driven operations on the Compound protocol.\n\n\nNaming and Clarity: The naming of our project might have led to some confusion. A more fitting name, such as “Compound Intent Kit,” would better convey our aim to assist developers in crafting intent-centric applications without the complexities of smart contracts.\n\n\nCommunity Collaboration: Echoing Harsha’s sentiments, we’re deeply rooted in the belief of community feedback and collaboration. Our primary goal is to empower and foster innovation within the Compound community, and we’re always receptive to suggestions to better align with its vision.\n\n\nThank you for fostering such an open dialogue. We’re committed to the growth and success of the Compound community and look forward to continued collaboration.\\n(post deleted by author)\\n\n\n\n kallolborah:\n\n1. RFPs vs Open calls\nAre you trying to outsource work or inviting proposals that can bring in fresh perspectives ? RFPs may be good to attract ‘service providers’ but Open calls will perhaps attract entrepreneurs and new ideas, some of which may add a lot of value to Compound.\n\n\nI’m not asking for the removal of open calls. My previous posts pointed out the issues with the current inbound-centric approach. I suggested a more RFP-focused model that still allows for inbounds to find a good middle-ground between the two approaches.\n\n\n\n kallolborah:\n\nFinally, cancelling CGP 2.0 may have been done in a hurry and somewhat unilateral way, unless we are not aware of the need to take that decision in a way.\n\n\nCGP 2.0 is not cancelled. As @harsha pointed out, the proposal to renew was cancelled by Questbook so they can take in the feedback and improve the program.\n@harsha and the Questbook team are doing a great job of taking feedback and working on an improved proposal. These types of discussions should not be rushed. A large amount of funds is being asked for the renewal and we should ensure these will be effectively allocated.\n\n\n\n kallolborah:\n\nThere are some 20-30 grantees in the CGP and many of them (including us) have discussed doing work across CGP 3.0.\n\n\nQuestbook already has the funds to pay out any grantee that was approved for the original scope of CGP 2.0. With GCP 3.0 not even being live yet, I don’t know why promises were made to grantees for funds that Questbook did not yet receive from the DAO.\nRegardless, I understand your concern and want to reiterate the goal here is to improve the next iteration of the grants program rather than stop the program altogether.\\nThanks @harsha for coordination and update. There are many opportunities for Compound community and believe the proposal will ensure further transparency and growth ahead.\nI will be helping with Multi - Chain/Cross chain Strategy, Dev Tooling Domain. This domain covers two main areas:\n\n\nMulti Chain Domain supports proposals related to features that help Compound on other L2s or L1s.\n\n\nDeveloper Tooling Domain supports proposals that make it easy to integrate Compound into other apps/protocols or easy to make Compound Web Extensions.\n\n\nAnd 5 Criteria to consider: Team experience, Novelty and quality of the idea, Relevance to Compound ecosystem, Completeness of the plan, Feasibility of goals and timeline. The criteria will apply a strictly high bar to ensure the quality and reputation.\nHere’s the RFP and Rubrics 5\\nUpdate:\nHi all, the renewed CGP 2.0 goes live on 30th November, 2023 with @ruchil (Questbook) as the Program Manager. As also specified in the proposal, the domain allocators  will review and fund proposals based on the domain specific RFPs listed below:\n1.Dapps and Protocol Ideas with @allthecolors as its domain allocator - RFP 3\nBudget : $450,000\nProposals within the scope of New Ideas and Dapps domain can be submitted here 2\n2.Multi - Chain/Cross chain Strategy, Dev Tooling with @Doo as the domain allocator for the combined domain - RFP 2\nBudget - $200,000\nProposals within the scope of Multichain and Crosschain can be submitted here 2 and those within the scope of Developer Tooling can be submitted here 1  2\n3.Security Tooling with @cylon as the domain allocator - RFP 2\nBudget - $150,000\nProposals within the scope of Security Tooling domain can be submitted here 1\nWe will continue with Synapse for KYC services and Docusign for all contractual agreements. If you or anyone you know is interested to make meaningful contribution to Compound in the relevant domains, we encourage you/them to submit proposals on questbook tool starting 30th November.\nIf you have any questions, please reach out to us in the #grants channel in the Compound Discord or DM me (roohchill) on Discord. Thank you everyone for your continued support and trust.\\nHi @kevin , we will withdraw the on-chain proposal. That said, we’d like to clarify that we have no intention of rushing this proposal. We have been taking active feedback from the community on every community developer call, and the feedback on the proposals through the discord grants channel very frequently throughout the duration of CGP 2.0. We have also explicitly requested and taken feedback from all the grant proposals both from the accepted and rejected groups 1.\nProposer Feedback1332×384 37.5 KB\nBefore submitting the renewal proposal for an on-chain vote, we also conducted a temp check vote to gauge the community’s sentiment towards the renewal proposal where we witnessed a clear majority towards the renewal. A similar process was followed for CGP hackathon proposal. Over the last 12 months, we have optimized the entire process, tooling, flow and communication to ensure every part of the grants program is as transparent as possible. We have always invited community participation on every community developer call, and discord chat. We intend to optimize for what’s best for Compound. If you as a representative of Compound Labs think the renewal of the proposal is not optimal at this point of time, we are open to withdrawing the on-chain renewal proposal to allow for further revisions before proceeding further.\n\na) these discussions have finished and b) the community has had a chance to thoroughly access the value generated by CGP 2.0\n\nWe respect these considerations and request you to provide specific timelines for concluding these discussions along with actionables. We assumed that we have taken detailed feedback from the community through demo day, forms, community dev calls, polls and replies on the forum post before submitting an on-chain vote.\nIf there are other parties who want to help with improving the grants program, we are always open to taking feedback or collaborating with them as long as the process is transparent and decentralized. We appreciate the involvement of Alastor & w3s 2 and Alphagrowth in improving the next iteration of CGP and are open to taking their feedback and collaborating with them. We invite them to engage in a discussion on this forum post for potential collaboration. Additionally, we are receptive to revising domains, and allocations, and re-structuring processes with inputs from Compound Labs and the community through discussions and polls. We encourage Alastor & w3s 2, Alphagrowth and other interested parties considering to contribute towards CGP within a new domain while taking on domain allocator roles, to actively participate in community deliberations. We will initiate a community vote and gather feedback during a community developer call before presenting the refined proposal for consideration. Your involvement is essential in shaping our collective direction.\\nHi, $8.5k mini grantee here (torque.fi 2).\nFirst, thank you to all for the opportunity. Many DAOs don’t make such an effort.\nTo the Compound community, your feedback is valuable & appreciated. Please feel free to leave your review of our work here, on Questbook, or if you’d like to do so privately, please email hello at torque.fi.\nIt’s been a wonderful experience thus far & the plan is to make Compound proud. We’re almost ready for audits & Arbitrum mainnet launch. It’d be great to connect with OZ & we’re working on others. I’m witness to @allthecolors responsiveness & professionalism. Managing the most popular category is no easy task.\nOur stablecoin update & related scope increase, although delaying us slightly, is a fresh attempt at an idea not so different from CompUSD years back. Actually, I don’t know what happened with it. Maybe someone can educate me. Since the announcement, there has only been validation, albeit not live.\nConsidering Compound has aligned with USDC, I’ll note we’re not in competition. USG is decentralized, returns earnings (if there is any) to holders via dynamic strategies, & is generally higher risk (comparable to FRAX or DAI). It’d be epic to discuss learnings from Compound’s research & development (possibly over Twitter Spaces)! Post-launch, Phantom 1 (the strategy admin of Torque) is considering a proposal to shore up the Arbitrum USDC.e incentives if they remain as current without new USDC market live.\nOf course, the market should aim to be profitable, but we need some spread like Polygon. If you don’t agree, do you propose we wait until the new USDC market is live? Thank you for the insights guys.\nScreenshot 2023-08-18 at 10.29.08 PM1744×315 36.3 KB\nLet’s grow.\nSincerely,\nCameron\\n\nFinally, cancelling CGP 2.0 may have been done in a hurry and somewhat unilateral way, unless we are not aware of the need to take that decision in a way. There are some 20-30 grantees in the CGP and many of them (including us) have discussed doing work across CGP 3.0. Although, our interests are not greater than the interest of the Compound protocol, giving some consideration to grantees show the maturity of thinking of the team behind the CGP/Compound. As a core contributor, we expect you to lead the way in these matters in a thoughtful and mature way.\n\nHi @kallolborah, thanks for sharing your comments. Really appreciate it! We canceled the CGP 2.0 renewal proposal’s on-chain vote to ensure that we incorporate any additional feedback from all key stakeholders including the members of Compound Labs before proceeding for an on-chain vote. Additional inputs and feedback from all key stakeholders is critical to the success of CGP 2.0 renewal. This will ensure that the funded proposals align with the protocol’s priorities and value creation, and contribute to the success of the next iteration. The budget for all already accepted proposals has been accounted for in the allocated funds section and the domain allocators will continue to review and make payouts for the accepted proposals/milestones upon the completion of respective milestones.\\nUpdate:\nAll domains of the renewed grants program are now accepting proposals on Questbook 2.\nDapps and Protocol Ideas - Proposal Submission 2, RFP and Evaluation Rubric\nDev Tooling, Multi - Chain and Cross chain - Proposal Submission (Dev Tooling) 2, Proposal Submission(Multi - Chain and Cross chain) 2, RFP and Evaluation Rubric\nSecurity Tooling - Proposal Submission 1, RFP and Evaluation Rubric"
  },
  {
    "number_of_comments": 10,
    "postid": "7ef77917-bdd9-4fdb-8868-fedbacfaecf3",
    "posturl": "https://www.comp.xyz/t/formal-process-for-new-collateral-assets/2124",
    "combinedcontent": "I encourage the Compound community to adopt a more formal process for adding new collateral assets. Each new asset adds existential risk to the entire protocol, yet in some cases the governance discussion has omitted important security questions.\nBefore creating a governance proposal (like #56) to add new collateral, at a minimum we should answer the following questions about the asset:\n\nWho audited the collateral token contract? What security issues were raised in the collateral token’s audit reports? Are any of these relevant to its use as collateral in Compound?\nCan the collateral token contract be upgraded? Who is authorized to make an upgrade? Can an upgrade happen instantly or is there a time-lock delay? Under what scenarios could an unauthorized upgrade occur? How many people or organizations would need to be compromised?\nDoes the collateral token contract have a fixed supply? If new tokens can be minted, are there any scenarios where tokens can be minted without proper authorization? (For example, centralized wrapped assets like WBTC and USDC may be vulnerable to insider collusion or external hackers.)\nAre there any large token holders? Who are they and what security procedures protect their accounts? If a single holder owns enough of the token to use it as collateral to borrow a significant fraction of Compound assets, then a hacker who steals these tokens may be able to “sell” them on Compound by supplying them as collateral and “borrowing” clean assets. The existence of this exit ramp may in itself increase the incentive to hack such large wallets.\nHow much CEX and DEX liquidity exists for the collateral asset? This liquidity supports the liquidation process when the collateral asset is seized (and probably sold). Deep liquidity also defends against market price manipulation with the intent of triggering a cascading liquidation event.\n\n\nFor reference, recent examples of collateral discussion:\n  \n    \n    \n    Add Markets: MKR, AAVE, SUSHI, YFI New Markets\n  \n  \n    With the oracle improvement done, the protocol ready to list new markets. I am proposing to add MKR, AAVE, SUSHI, and YFI. I consider all four coins to be blue-chip DeFi tokens and very logical additions to the protocol. Each token will have a zero collateral factor (initially), the same interest rate model (as COMP/LINK), and a reserve factor of 25% for each market (which is standard). \nCredit to @elee for doing the dev work. \nMKRPrice: $2600 \nFDV MCAP: $2.6B \nNote: 21% of MKR is the Goverance …\n  \n\n\n  \n    \n    \n    Add Market: LINK New Markets\n  \n  \n    Chainlink is arguably one of the most reputable, trusted, and valuable blockchain protocols, ranked the 10th largest coin by market cap at nearly $13B. \nI am proposing adding a market for $LINK as a collateral asset. \nHere are a few reasons why we should do so: \n\n\nUsed as collateral. Ideally, I think anyone should be able to borrow against any of their assets provided there is a market for it. The more assets we support, the greater financial freedom and capital efficiency we offer to DeFi users…\n  \n\n\\nsecond.     …\n\nI agree.\n\\n\n\n\n pyggie:\n\nAre there any large token holders? Who are they and what security procedures protect their accounts?\n\n\nWhat is the threshold for ‘large’?\nIt seems relative to the asset’s liquidity.\nIf the asset has lots of slippage on DEXs, a hacker may use Compound to exit.\nIf hackers use Compound, the hack may not greatly impact the price of the underlying asset. If there’s enough liquidity across exchanges, liquidators could exit without damage to Compound.\nLikelihood of hacker using Compound depends on DEX slippage.\nDamage of hacker using Compound depends on liquidity across all exchanges.\nSo, what is the threshold for ‘large’?\\nYou are right, what matters isn’t whether there are “large” holders but whether the Compound liquidation system can protect its suppliers if such a holder were hacked. The liquidation system is vulnerable to a price decline that is so large and so fast that borrower positions become insolvent before they can be liquidated. For example, if an asset has a collateral factor of 70%, then a sudden price drop of more than 30% can create underwater positions. These positions will be repeatedly liquidated until no collateral remains, but their debt will not be fully repaid.\nThe scenario I have in mind: A project treasury (or exchange hot wallet, etc) holding token XYZ is hacked. The hacker sells whatever they can via DEXes, causing a 30% price decline. The hacker supplies the remaining XYZ tokens as collateral to Compound, borrows ETH worth 70% of their value (at the old price), and walks away from the XYZ collateral.\nNow the question is how the market responds to the instantaneous 30% drop in price on DEXes. If the global market price is resilient, then liquidators will be able to repay all the XYZ-backed debts without needing to seize all the XYZ collateral. However, during this process sales of seized XYZ will cause further downward pressure on global XYZ prices, possibly creating insolvent positions and protocol losses.\nSo deciding what is “large” means guessing how the market would react to a cascading liquidation event that dumps XYZ on the market from both the hacker’s abandoned position and from innocent XYZ-backed borrowers whose positions are suddenly vulnerable due to the event. It could be that the market could absorb the hacker’s position alone, but is overwhelmed by the subsequent avalanche. This depends on both global market liquidity and patterns of collateral use in Compound (and other) borrowing markets.\nI don’t have a good answer. In the end I think it a subjective decision based on as much objective evidence as we can find.\\nOn Coingecko they list a market depth of +/-2% in price. For 70% CF, maybe we would care about +/-30% market depth.\nSomehow Coingecko (also other sites like coinmarketcap) has information for -2% market depth. Maybe they could tell us -30% market depth?\nThat may be impossible – they don’t know what new orders will be placed as prices drop. Most limit orders are likely near the current price, which would allow them to get an idea of +/-2% market depth.\\nPause Risk\nPausable tokens represent a risk to Compound because a pause of token transfers could prevent liquidation.\nIf pausing happens because of a vulnerability, the pause could coincide with a price decrease. During the pause, Compound would not be able to liquidate bad debt. This is a risk to all markets.\nThe size of the risk is the collateral factor times the current value of the paused market’s collateral. The risk is likely to be focused on stable coins because they are the most frequently borrowed. But, even though volatile assets may not be the primary assets borrowed, they could also be at risk if their reserves are low.\nPause risk might be limited by a Collateral Cap or Supply Cap.\nIf a market is paused during a downturn, the result would look like slow liquidation.\nThe price drops. Debts are not repaid. The difference between the value of the collateral and the borrowed assets represents the damage to the reserves. If the difference exceeds the reserves, this could lead to a bank-run.\nSee the reserves per asset to know the size of Supply Cap/Collateral Cap needed to mitigate this risk.\nWho can pause the asset?\nUnder what circumstances would they pause?\\nPersonally i think as long as the collaterals are based on cryptos then they are always vulnerable to hackers, therefore the risks you mentioned cannot be avoided entirely.  But what about collaterals based on real world asset (RWA)?  Many including our team believe RWA might be the next big opportunity to unleash the full power of DeFi liquidity.  It would be definitly an interesting and rewarding to see assets such as corporate bridge loans, pledges of equities, investment margins, etc to be enlisted on Compound.\\n\n\n\n M1CHEN_EntroFi:\n\nMany including our team believe RWA might be the next big opportunity to unleash the full power of DeFi liquidity. It would be definitly an interesting and rewarding to see assets such as corporate bridge loans, pledges of equities, investment margins, etc to be enlisted on Compound.\n\n\nLet me know if there’s any assistance I can offer for this one. I’ve been pretty hands on with RWA at Maker and also seen what a lot of the unexpected pitfalls are.\\nSounds good!  Our platform is called EntroFi and we launched last month; it is designed to connect DeFi capitals directly with loan borrowers who use real world assets (account receivables, real estate properties, pledges of securities, etc.) or existing NFTs as collaterals. We envision a world where liquidity and assets alike flow freely between on chain and off chain, and therefore would love to work with anyone team or platform who share such vision with us.\nThis could be the beginning of something truly extraordinary.\\nIt would be great to merge this with a few other suggested onboarding steps I posted here: Asset Onboarding Framework 11\n@TylerEther I saw you link to this thread on @stefan’s post, but I think answering both sets of questions might make decision making easier for voters. I haven’t gotten a chance to work on the framework too much since the initial post, but I would be open to helping another community member work towards a clearer process for adding new assets.\\nThanks for linking me to this; totally missed it!\nI have been working on extending Compound’s asset onboarding framework with Stefan’s help. The goal is to include as many data points as possible, then eventually find a way to grade the data and establish minimum requirements.\nIt would be great to collaborate with you @jmo and @getty on this!"
  },
  {
    "number_of_comments": 9,
    "postid": "41cc78c4-217f-4748-ab99-0cb4bce0024d",
    "posturl": "https://www.comp.xyz/t/grant-to-develop-a-futures-market-dex-for-ctokens-convexity/1847",
    "combinedcontent": "Hello, everyone! My name is Leo and I’m an ex-FX swap and interest rate derivatives trader 11.\nI am applying for a $50k grant to build a decentralised exchange for cToken futures. The grant will go towards set up costs and attorney fees.\nA cToken future is new kind of interest rate derivative that is a cash settled non-deliverable forward that also creates a fixed versus floating exposure to the Compound Supply Rate of any given market.\nThe first set of problems that this product will solve is enabling fixed rate deposits and stable rate borrowing using the existing Compound liquidity pools. The stable rate borrowing in this context is different from AAVE’s stable rate borrowing, because there is no risk of the rate being reset. However, if the utilisation ratio on average realises at a region that implies a wider spread between the supply rate and borrow rate, the borrower would be under-hedged.\nThe second problem that this product will solve is by creating a hedge instrument for crypto interest rates. As the market develops, my hypothesis is that overall level of crypto interest rates will become move correlated across protocols and CeFi lenders. With that assumption, an interest rate derivative that references the Compound Supply rate will move around with the general level of crypto interest rates, and therefore serve as a hedge instrument.\nOur team have built a proof of concept demonstrating one of the use cases (fixed rate deposits). It is only deployed on a local blockchain so I’m happy to host a demo and discuss what we’ve built so far. We are currently a team of 4 people (all met online) and have signed up for the HackMoney hackathon and plan on building a minimum viable product over the next 3 weeks. Concurrently I would like to push forward with making this idea a reality.\nThank you\\nI love the idea of adding derivative markets as I am an avid trader and derivatives/futures markets add to what is available to trade.\nAre you going to be on the call tomorrow morning? I think we need to see a demo showing exactly how this can help the Compoind Protocol.\nIf I am understanding correctly, your product would allow for someone to essentially lock-in an interest rate until the expiration of the futures contract?\nWhat are some negative effects of your idea?\\nHello CryptoCraig! Thanks for your reply.\nIt seems like every day I find out about a new team that is working on something similar. I guess that just shows that there is a collective awareness in the community that there is a need for such a solution. There are probably about a dozen or more teams out there doing something to do with interest rates - whether tokenising them or creating interest rate swap markets in CLOB or AMM.\nI think the biggest benefit to Compound is that it enables fixed rate deposits and stable rate borrowing. This is a feature unavailable in Compound right now. I’m sure a lot of people are happy with floating rates, but there probably are some who would rather have the option for fixed rates.\nI don’t really know how these money markets are going to develop over time. However, I think it is in the interest of COMP holders and the Compound community to support an interest rate derivative market that references the Compound markets because it creates another class of use cases. For example, if there was an active derivative market in Compound, it is possible that you could have crypto bond issuances referencing Compound supply rates (e.g. a 2 yr bond that pays Compound rate + 1% or something like that) In that case, both the bond issuer and investor could use derivatives to augment their exposures. But then again, this is DeFi, maybe people don’t want such exposure to a benchmark. Maybe a benchmark like LIBOR only has utility in a world where figuring out an appropriate interest rate for a loan was too cumbersome. I have no idea.\nNegative effects - I don’t know. There is definitely a risk of manipulation of the markets if the derivative markets grows sufficiently large. According to DeFi Protocols for Loanable Funds 3 by Gudgeon et al, a handful of addresses dominate the liquidity supplied to the market. They would have an outsized impact on the Compound rate fixings so they may be tempted to put on a huge sized derivative position and withdraw liquidity to benefit from the derivatives.\\nThe idea is interesting but would recommend lines of limited edition NFTS for certain contribution, activity or early adoption members.\nExclusivity and/or limited release NFTS is key to early interest and adoption.\\nHi Leo.  I’m curious about the specifications of this fixed rate product you have created.  Since there are many times tradFi products can be created differently (and potentially better) in DeFi, I have a possible solution to suggest.  Correct me if I am wrong in my understanding of cTokens, but when you look at the protocol math, the utilization ratio referenced is always the previous block.  If you took the utilization ratio from t-7days (or however many blocks that would be) you could create  cToken that has known utilization ratio (thus an implied borrow/lending rate) for the next 7 days.  It would be dynamic, but since its able to be calculated for the next 7 days it would be “stable”.\nThe creation of this token would require that being able to referencing a more distant utilization ratio instead of the previous block is trivial.  This would be its own market (e.g. cUSDC-7Day) and would reference its own utilization ratio, not the next block protocol.  Collateral and liquidation could function exactly the same as the current markets without issue.\nBuilding out a yield curve within the DeFi space will be an important milestone in the evolution of these markets and having a novel framework for interest rates could positively differentiate our protocol.  You can think of this set up as either looking back at earlier rates or that the current utilization is determining future rates.  This “stable” rate would only be stable for borrowers, not for lenders.  Interest paid to lenders is implied from the interest rate for borrowers and the total borrowed amount.  If the future rate is going known to be high, borrowers may pay off their debt leading to a high interest rate for the borrowers remaining but the interest paid to the lenders would be lower if all  lenders remained in the market.  Lenders would quickly leave for greener pastures and a new equilibrium would be reached.  Price Discovery\\nWas doing a little research on projects that are working on similar functionality as what the futures market would do. BarnBridge 3 (BOND) has already implemented similar fixed-interest rate products with Compound assets, USDC and DAI.\nNot only can users get set-term, fixed interest rates, they can also, with more risk, get a better variable interest rate with incentives. I’m not sure there is or will be a need for multiple / separate paths for entering this kind of market. The current TVL across all of Barn Bridge’s assets is less than $300M.\\nHello - @eggbagels\nSorry for the delay in response. I don’t quite understand what you mean by taking the utilisation ratio of the the block t-7. Does that mean that the cToken exchange rate for USDC t+7 is calculated by assuming the same Utilisation ratio that was snap shot from t-7?\nI’m not sure how that would work. Borrowing and lending, in my mind, is inherently forward looking activity. I’m having a hard time wrapping my mind around referencing historical Utilisation for the future in that way.\\nHello @CryptoCraig -\nYes I had come across Barnbridge too. What they have built is pretty cool and it is definitely a different approach to what I had in mind. I think we are currently in a stage of experimentation with regards to interest rate derivatives and structured products in DeFi and we will probably see a lot of different attempts at solving this problem. The use cases and user experience will probably determine which protocols gets used more.\nI think we have an elegant solution to the problem of fixed rate deposits and loans building on top of existing floating rate protocols. We’ll see how that translates to usage but I guess only time will tell.\nThanks!\\nIts our pleasure to have you with us\\nFuturiastic conclusion and will be a gr88888 help for both sides"
  },
  {
    "number_of_comments": 12,
    "postid": "b7e68008-9a2c-40a1-8757-21971a84a248",
    "posturl": "https://www.comp.xyz/t/trueusd-listing-proposal-stay-tuned/1490",
    "combinedcontent": "Hello, everyone from the Compound community. I am from the TrueUSD team. Our team would like to propose listing TrueUSD on Compound.finance 7.\nWe prepared 100,000 COMP and are ready to create the proposal at any time. However, we would love to hear the feedback from the community before we take the action. We also believe that compliance and transparency, the core value of TUSD will shine in the Defi realm with the help of the prosperity of the Compound ecosystem.\nWe need your support to help us to get listed!\nOverview\nTrueUSD (TUSD) is the first independently-verified digital asset redeemable 1-for-1 for US Dollars. The ERC20 stablecoin uses multiple banks, escrow accounts, and third-party attestations to reduce counterparty risk, provide transparency, and prevent fraud.\nTUSD offers liquidity on dozens of leading exchanges 3, DeFi protocols, and is supported by major OTC desks. TUSD also supports nearly instant minting and redemption speeds through the Silvergate Exchange Network (SEN) and PrimeX by PrimeTrust.\nTrueUSD has built a bank-level compliance department that in unison with TrueCoins escrow partners ensures that only approved users can mint or redeem TrueUSD.\nToken\n\n\nSymbol: TUSD\n\n\nBlockchain Type: ERC20, Avalanche, Binance Chain\n\n\nTrueUSD in DeFi\nHighly compliant and transparent stablecoin TUSD has now been listed on multiple DeFi platforms. Not only does TUSD have a high ranking among stablecoins in many rating agencies, but also in APR ranking on DeFi platforms.\nOn Curve.fi, there is a TUSD Metapool, by staking DAI, USDT, USDC, and TUSD, users can enjoy trading fee share and CRV mining rewards. The reward is currently around 30% and will reach 40% with an accelerator. On Uniswap, by providing liquidity to USD/ETH, users can gain a 17% APR. It can be a good choice when it comes to providing liquidity on Uniswap. As for SushiSwap, users can enjoy both trading fees and SUSHI mining rewards by providing liquidity of TUSD/wETH and the APY is up to 45.08% on Onsen.\nProposal Details\nThe details for TUSD on Compound are as follow:\n\n\nSet collateral factor as 0%\n\n\nSet reserve factor as 7.5%\n\n\nSet TUSD with unlimited borrow cap.\n\n\nupdates the Compound price feed to peg TUSD to $1\n\n\nUsing JumpRateModelV2 as the interest rate model.\n\n\nContract information\n\n\ncTUSD contract has been deployed at 0x12392F67bdf24faE0AF363c24aC620a2f67DAd86\n\n\nChange to a new oracle with TUSD support\n\n\nComptroller 5._setPriceOracle(0x4007B71e01424b2314c020fB0344b03A7C499E1A)\n\nAdd TUSD into the market\n\nComptroller 5._supportMarket(0x12392F67bdf24faE0AF363c24aC620a2f67DAd86)\n\nSet reserve factor of cTUSD as 7.5%\n\ncTUSD 2._setReserveFactor(75000000000000000)\nIf you like to know more about TUSD, you can find us here \nSocial Media\nTrueUSD.Website: https://trueusd.com/ 8\nTwitter: https://twitter.com/tusd_official 4\nMedium: https://trueusd.medium.com/\nTelegram EN: Telegram: Contact @TUSDofficial_EN 4\nTelegram CN: [Telegram: Contact @TUSDofficial_CN\n](Telegram: Contact @TUSDofficial_CN)\nYours,\nTrueUSD\\n\n\n\n Joyce:\n\n0x4007B71e01424b2314c020fB0344b03A7C499E1A\n\n\nWhat kind of tests have been done on the new oracle? Has the new token been simulated? I am seeing a lot of assets without prices currently UniswapAnchoredView | 0x4007B71e01424b2314c020fB0344b03A7C499E1A 10\nI’m excited to see you all moving forward with this!\\nHi, Jared\nThe contract 0x4007B71e01424b2314c020fB0344b03A7C499E1A 7 has been certified by Compound team\nWe will also discuss with the Compound team to launch the price feed of non-stable coins before it’s been completed\\nTrueUSD contract simulation progress update\nHello, everyone from the Compound community. After the last time we posted the proposal on Compound forum, we have been diligently preparing the contract and we consider it a good time to update progress.\nSimulation summary\nThe execution of the proposal and the TUSD market were simulated using ganache-cli with Alchemyapi for archived data. The contract forked at block height 12269785.\nContracts details\ncTUSD: 0x12392f67bdf24fae0af363c24ac620a2f67dad86\nNew oracle address: 0x4007b71e01424b2314c020fb0344b03a7c499e1a\nGovernance: 0xc0da02939e1441f497fd74f78ce7decb17b66529\nComptroller: 0x3d9819210a31b4961b30ef54be2aed79b9c9cd3b\nProposal execution\nIn the simulation process, the proposal could be successfully activated, voted, queued and executed.\nThe simulation process includes call the propose function in the governance contract and use bytecode below for transaction:\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\nNew TUSD markets test\n\nNew oracle is set up correctly with the address\n0x4007b71e01424b2314c020fb0344b03a7c499e1a\nNew market for TUSD is set up correctly\nCall markets(0x12392f67bdf24fae0af363c24ac620a2f67dad86) get:\n0000000000000000000000000000000000000000000000000000000000000001\n0000000000000000000000000000000000000000000000000000000000000000\n0000000000000000000000000000000000000000000000000000000000000000\n\nIt indicates that the TUSD market is approved, while it can’t be used as a collateral asset and receive COMP rewards.\n\nReserveFactor\nCall reserveFactorMantissa() and get：000000000000000000000000000000000000000000000000010a741a46278000 (75000000000000000) as the value\n\nNew TUSD market is also simulated for supply and borrow scenarios, with an account supplied COMP and borrowed TUSD from the cTUSD contract.\nThe testing includes:\n\nCOMP price (489655000) is successfully read from the new oracle contract when calculating the collateral value.\nTUSD can’t be borrowed when its total asset value exceeds 60% of the COMP value. TUSD could be borrowed when the asset value is lower than the amount.\nDuring the getAccountLiquidity function test, the value changed to liquidatable when the simulation time passed.\n\nSince the proposal has been created and been successfully simulated, we are now ready to activate and launch the 「Add TrueUSD Support」proposal.\nWe are happy to hear your voice regarding this proposal. Also, we strongly need your support to help us get listed on Compound!\\nGreat progress @Joyce! Is there somewhere the community can review the source of the simulation script you ran for verification (e.g. a github link)? I’m very excited to see this proposal finally get made \\nHi, Jared. Thanks for the support. Below is the GitHub link including the source of the simulation.\n https://github.com/rpunk/compound-simulation  17\\nThanks for sharing these and all the effort you put in @Joyce!\nSince I think its important we keep the quality bar high for changes to the core protocol, I want to point out some things that I think could be improved here. Its not 100% clear to me what these functions are doing (the tx data is hardcoded everywhere), and I think its unnecessarily difficult to run/replicate these tests (due to the timing concerns).\nI think the single-script simulations that we have many examples 4 of at this point in the repository should really be the norm. My team at Compound will be focused on making this easier and lowering the bar on contributions here over the next few months.\nHowever all that said, I do believe this sim is probably doing what it’s supposed to be doing, though I hope others will take a look in order to strengthen the overall confidence of the community.\\nCan the proposal team or other community members post a diff showing changes from Compound standard smart contracts and the ones deployed for adding TUSD? Even better would be including explanations with each addition or deletion to explain why it was needed.\\nTrueUSD is supported on Aave, the risk parameters and market overview can be viewed here for comparison:\n\nAave Risk Parameters listed as “True USD” 3\nAave TUSD Pool Dashboard 2\n\nTrueUSD is supported as collateral for DAI (MakerDAO/Oasis). Here are some resources for review.\n\nEnable TUSD, MakerDAO Blog, September 18, 2020\nDai Stats, ~26.6 million TUSD locked for DAI vault 1\n\\nThis is definitely one of the most important aspects of a proposal like this which True left out of the discussion. The contracts used are relatively old versions of the cToken delegate and delegator contracts—I was able to successfully match the cToken contracts to the code in commit bc3435f8d9a44e991e830b717b1062d732e0e50f 3.\nI’ll try to simulate the proposal through our customary methods and march the oracle tomorrow.\\nSecurity of tokens added to Compound is important as any vulnerabilities of supported tokens can directly impact Compound itself. Here are some security related articles for TrueUSD for our users to review.\nTrustToken has some TrueUSD related security resources posted online.\n\n\nTrueUSD Smart Contract Security Audit Report by SlowMist 1, June 20, 2018\n\nTrue Currencies Smart Contracts Audit by Jakub Wojciechowski, Sept 26, 2020\nTrueFi Bug Bounty by TrustToken\n\nTrustToken Releases Updated Security Practices, Dec 4, 2018\n\nTrustToken Announces TrueUSD Security Enhancements and Engineering Milestones, Dec 19, 2018\n\\nRegarding the Oracle code, the only change seems to be updating to the latest UniswapAnchoredView from Compound code repository here: open-oracle/UniswapAnchoredView.sol at master · compound-finance/open-oracle · GitHub 1\n\nOld Oracle 0x922018674c12a7f0d394ebeef9b58f186cde13c1\nNew Oracle 0x4007b71e01424b2314c020fb0344b03a7c499e1a 1\n\nHere is output from a diff of the Old Oracle and New Oracle contract code verified on Etherscan.\n<         if (config.isUniswapReversed) {\n<             // unscaledPriceMantissa * ethBaseUnit / config.baseUnit / expScale, but we simplify bc ethBaseUnit == expScale\n<             anchorPrice = unscaledPriceMantissa / config.baseUnit;\n<         } else {\n<             anchorPrice = mul(unscaledPriceMantissa, config.baseUnit) / ethBaseUnit / expScale;\n<         }\n---\n> \n>         // In the case of non-ETH tokens\n>         // a. pokeWindowValues already handled uniswap reversed cases, so priceAverage will always be Token/ETH TWAP price.\n>         // b. conversionFactor = ETH price * 1e6\n>         // unscaledPriceMantissa = priceAverage(token/ETH TWAP price) * expScale * conversionFactor\n>         // so ->\n>         // anchorPrice = priceAverage * tokenBaseUnit / ethBaseUnit * ETH_price * 1e6\n>         //             = priceAverage * conversionFactor * tokenBaseUnit / ethBaseUnit\n>         //             = unscaledPriceMantissa / expScale * tokenBaseUnit / ethBaseUnit\n>         anchorPrice = mul(unscaledPriceMantissa, config.baseUnit) / ethBaseUnit / expScale;\n\nI didn’t verify the configuration set in the New Oracle so hopefully the simulations are sufficient for that.\\n@rafael.cosman and @Joyce I see that the TUSD proposal was queued in the timelock within the last hour. In 48 hours it will be able to be executed. Before that happens we will need fresh prices posted to the new PriceFeed (Oracle). Can someone from your team post new prices between now and then? With the recent volatility we will probably want price updates right before the proposal executes as well.\n\n\nUniswapAnchoredView | 0x4007b71e01424b2314c020fb0344b03a7c499e1a 3\n\nThe current prices are over 42 days old according to the transaction history.\n\n\n\nCongratulations on the success of your proposal! "
  },
  {
    "number_of_comments": 13,
    "postid": "6aca6aad-5a3d-4acf-97d7-9751ce07e591",
    "posturl": "https://www.comp.xyz/t/comp-transfer-from-binance-to-kraken-with-wrong-network/1051",
    "combinedcontent": "Hello, I have an issue with a transaction.\nI made a transaction of COMP from my account on binance to my account on kraken using BEP20 network. However kraken does not support this network. So my COMP left binance and never arrived on kraken. It is still on the blockchain. How can I get it back, transfer it back to binance for example. Thanks for your help.\nRegards,\nJ\\nI am not sure what bep20 is?\\nThis is a network, its also called binance smart chain (BSC) and apparently Kraken does not support this network.\nHowever i sent the crypto on the correct adress but because of that network I’m not able to get them back. (either on kraken or even back to binance). which is not normal.\\nIn this situation, you would need Kraken to help you recover your tokens. Only they have the private key which can access the COMP on BSC. Unfortunately, the exchanges either will not cooperate, or will charge incredibly high fees to manually recover tokens like this.\\nThanks for this information.\nWhat do you call incredibly high fees ? 100 or 200 euros or even much more ?\nCan still be interesting for me since I have 10 COMP to recover.\nDo you know the step how to make this operation ?\nAppreciate your help.\nRegards,\nJulien\\nI do not know what the policies of Kraken are. You should contact their support line. Regardless, it definitely will be a hard process to go through because it is on BSC. This user posted 9 that Kraken would charge 0.25 BTC to recover incorrect tokens sent.\\nI am the fellow referred to in regards to the 0.25 BTC fee for recovering my cDAI/COMP transaction from Exodus(not Kraken) to Bittrex. They did not recover them for a 50/50 split that I offered, and to this day remain stagnant on a ERC20 contract. I lost my home and car from that goof I made. Be very wary of any of the altcoin transactions, it’s a dog eat dog world in the cryptoverse.\\nWow that sucks for you …\nI lost also a bit but it will be ok even if I’m still struggling to find a solution to get them back.\nActually I know that there is a way to get them back but need to private key from kraken.\nWill ask them but quite sure they will not give it to me…\nIndeed it’s a very tough world where mistakes are not allowed…\nHopefully one day the system will change and they will send it back where it comes from and we will get our crypto back. May be I dream a little bit.\\nFortunately, I am working with the Compound community to help return funds 13 accidentally sent to Compound smart contracts. DeFi has a learning curve, and we should try to help those who make mistakes. Good luck on your future DeFi ventures.\\nThanks for what you do. If you or anyone else have a contact to kraken that is willing to cooperate do not hesitate to let me know.\nI know it’s a 5 min job and not complicated one to get them back but only Kraken can do it I think.\nRegarding the expected fees for this operation, it varies so let’s see, step by step.\\nI appreciate your sentiment and well wishes.  All is not lost for me so worry not, I still have my cat, good health, a new home, and have bounced back financially. I hope you and I, as well as all others are able to regain control of the capital losses incurred from our missteps.\nIs the glass half full or half empty? It depends what was last done with it; if it was last filled it would be half full, whereas if it were last drank from it would be half empty. Let’s fill our glasses.\\nIf there is anyway for me to contribute at all whether it be coding, debugging, investments, or even just raw manpower please let me know and I will jump at the chance. I see such potential in the community and the comradery that is growing in it. Thank you for caring about other’s wellbeing!\\nHello, did you find solution to your problem? Having same  probleme . Thanks in advance\\nDear sir, my situation was I sent my ethereum coins to compound ethereum address from coinbase wallet. The coinbase customer service I need is seek compound customer service to help. Please advise. Thank you!"
  },
  {
    "number_of_comments": 24,
    "postid": "2163c970-a1fd-47b3-aee7-423fdce4f2a9",
    "posturl": "https://www.comp.xyz/t/lower-proposal-threshold-to-100-comp/282",
    "combinedcontent": "Currently it requires 100,000 COMP to propose governance actions (either directly owned or delegated). This puts governance proposals out of reach for all but a very small set of individuals and entities.\nAdditionally (my hypothesis) this creates selling pressure on COMP. There is little incentive to accrue COMP because meaningful participation in governance is out of reach.\nOne way to reason about this is: how much money do I need to make a proposal? Right now the answer would be ~$18 million (100,000 * 180). At 100 COMP to make a proposal the answer is $18,000 which seems appropriate. Within reach for highly motivated individuals.\nThere is also very little downside here. If we are flooded with proposals (a good thing!) we can 1) implement a COMP burn to make a proposal or 2) incremental raise the level.\nI’m looking for a sponsor to submit this!\\nGreat idea… also, if a proposal is ignored, it won’t reach quorum and just fail.\\nI love this idea as well, It is a pity that currently there is less than 10 entities able to initiate voting, it is more centralized that almost all countries (and all democratic) and most corporations. Strange notion of “Decentralized Finance”\\nGreat idea, but how are we going to pass this?\\nHi folx,\nI have been thinking about this recently as well. I wrote a medium post on Wednesday about a possible solution to allow proposals from smaller holders: https://medium.com/@monetsupply/p0-stake-based-decentralized-scrum-3eaef9331890 31\n\n\n\n lay2000lbs:\n\nIf we are flooded with proposals (a good thing!) we can 1) implement a COMP burn to make a proposal or 2) incremental raise the level.\n\n\nI personally think it would be prudent to start with a relatively high proposal threshold (maybe 10k+ COMP) and then slowly work our way down until the volume of proposals starts to become significant.\nWe may also want to consider a testing period, where proposal submission still requires 100k COMP, but superdelegates offer to sponsor lesser backed proposals that meet certain community criteria.\\nDefinitely onboard to lower the proposal threshold — I think it’s a great idea. @monet-supply’s plan for a gradual reduction makes a lot of sense too.\\nThinking out loud (and probably an implausible idea): can we have a tiered system.\ni.e. you can propose with 100 COMP, but for your proposal to pass, you need a higher quorum. you can also propose with 100,000 COMP, and quorum to pass stays the same.\nmore aggressively, we can have a formula\nlet x the number of COMP Alice has, such that x <100,000.\nlet y be the quorum requirement  for Alice’s proposal to pass, such that  0.04 < y <1\nthen the relation of x and y can be determine by a formula x*y = F, where F can be a constant or a formula.\\nI support this proposal. I’d love to participate more actively in COMP governance, but 100,000 is entirely out of my reach.\\nI thought your proposal was very thoughtful @monet-supply. I do think though the full scrum for governance is maybe a bit too much structure for what we need right now.\nAny reduction is better than what we have now. I’m going to continue to advocate for 100 COMP though as I think it’s a meaingful move that can shake things up in a good way.\\nThat’s a cool idea too! With 100 COMP proposal threshold there could be some sort of “governance sneak attack”… someone submits, no one pays attention and then someone who owns 4% of tokens votes it in right at the last minute.\\nI think @blck @brendan_dharma @rleshner would all be people who would put this forward.\\nI’m very supportive of lowering the threshold for COMP needed to submit a proposal. With that said, lowering the threshold will likely result in more proposals, and so I’d argue that if we lower the COMP-threshold, we should increase quite substantially the voting period (would suggest 2 weeks).\notherwise I fear it would become quite overwhelming to deal with the volume of proposals that will come.\ni’d also argue that if we’re going to lower the COMP-threshold, we may want to consider some sort of “time in wallet” threshold. basically an address must have held 100 COMP for X number of blocks in order to submit a proposal.\nin short, I’m open to the idea, but I’d like to see it approached with some caution\\nI’ve expressed skepticism around lowering the proposal threshold before and I’ll expound on my reasoning.\nMy main concern mostly applies to proposals which involve deploying new contracts. I’d be more open to a low threshold to parameter changes as opposed to new contracts, but 100 COMP still seems too low.\nThe Compound protocol is controlling hundreds of millions of dollars in crypto assets supplied by thousands of users. In its current state, it is one of the best audited and safest DeFi applications out there and I think most users want it to stay that way.\nWith this taken into account, in order to make meaningful contributions to the protocol, you need to take a deep dive into the protocol and along the way, you’ll meet most of the incredible community. Proposal ideas need to be well thought out, implemented perfectly, and extensively tested (and often audited).\nAfter getting community feedback and iterating to accommodate the needs of the community, you won’t have an issue finding someone to propose your update; I can tell you this from experience.\nAnother thing to take into account: we are only a few months into Compound decentralization out of the FOUR YEARS the whole process should take. Obviously this early on the community won’t have as much power as it will have in the coming years.\nOverall, I believe that the proposals thus far have been highly thoughtful, comprehensive, and usually cater to the requests of the community. I’m concerned that lowering the threshold so much will create lots of spam, half-baked proposals, and sometimes proposing without discussing with the community first. If anyone wants to get a proposal made, feel free to ping me on discord and we can get it done right—even without lowering the proposal threshold.\\nLET THE SILENT MAJORITY BE HEARD!\nThe current holders that are qualified to do proposals have ran the valuation of COMP below YFI !!!\nYou can still easily shut down our proposals, cause you own orders of magnitude more tokens than us, but at least provide us a chance to be heard?\nThanks for the proposing this, Cheers! Screen Shot 2020-08-28 at 4.47.05 PM2470×1676 698 KB\\nI agree with this proposal - hopefully it can lower the barrier to participating in governance.\nIf the throughput of proposals gets too noisy/low quality, we can easily vote the threshold up again.\nGenerally a fan of experimenting with high impact yet reversible decisions.\\nI’m thrilled by the enthusiasm to increase participation in Governance, and have some ideas on how the community can make this happen.\nFor background, these are the principles that guided the design of the Governance process (knowing that it can, and should, evolve over time):\n\n\nSafety first. As @arr00 mentioned, the protocol manages hundreds of millions of dollars worth of assets. While changing parameters is generally “safe”, introducing new code & contracts is a tremendous risk. An overview 8 to remind folks how proposals work.\n\nSkin in the game. The proposal threshold was set high to ensure that only stakeholders or groups of stakeholders banding together (through delegation) would be able to propose changes.\n\nOpenness. You don’t have to be a large COMP holder in order to participate in governance. You can receive voting rights by demonstrating intelligence or a commitment to the protocol or a proposal, and garner public support. Because of delegation, (theoretically) anyone can change the protocol if they adhere to safety first and win the support of those with skin in the game.\n\nWhile by some metrics, the protocol has succeeded wildly at Openness (see successful and pending proposals from @blck, @brendan_dharma, @arr00…), it’s clear from this thread and conversations in Discord that many folks feel like participation is out of reach.\nPrior to launching Governance, our team had the hypothesis that individual members of the community would build a base of delegated support. So far, this hasn’t happened. It could be a function of missing incentives 5–or other causes that the community can analyze.\nSolution 1 - Autonomous Proposals\nThe ability to delegate COMP opens up some very powerful opportunities that haven’t been fully explored.\nRather than delegating to a protocol politician or well-known community member (who, if they receive 100,000 COMP can create a proposal, act in their own interest, or vote for or against anything they want), what if you could delegate to a special-purpose smart contract–called an Autonomous Proposal.\nAnybody that meets a low threshold of ownership (e.g. 100 COMP as @lay2000lbs suggests) could deploy an autonomous proposal, with the title, description, and governance actions that they would like to build support for. Community members can delegate to the autonomous proposal, watch it rise up the leaderboard 4, and when it reaches the proposal threshold–the proposal is formalized and voted on.\nAutonomous proposals have the added benefit of providing additional visibility (and ability to audit and analyze) before going into the voting process–which meaningfully increases safety.\nSolution 2 - Deploy a new Governance Contract\nThe “alpha” Governance contract 5 was meant to be a starting point to test the parameters of the Governance process, and expand upon. A new contract could be written and deployed by the community, changing core parameters; lowering the proposal threshold (which itself would be much more attainable with autonomous proposals), increasing the vote delay, or reducing the quorum (voting requirement).\\n\n\n\n rleshner:\n\nCommunity members can delegate to the autonomous proposal, watch it rise up the leaderboard, and when it reaches the proposal threshold–the proposal is formalized and voted on.\n\n\nSeems like a good idea, but here we go again with the gas and incentivizing. If gas goes up to 300gwei, it’s almost impossible to vote without dumping 20$. Now it’s some incentive to vote, lets see what the community says about this\\nLet’s make it  N COMP tokens per proposal (not proposer) to avoid a SPAM attack on governance?\nThis shouldnt be too hard to implement, simply lock up N of the proposers tokens in the governance contract.\\nI like the idea of autonomous proposals. Seems like a nice hybrid between on-chain and off-chain voting. I also think the leaderboard / gamification of it could be good.\n@rleshner are you thinking anyone would be able to initiate an autonomous proposal? Or would there be a minimum cap on that (such as 100 COMP)?\\n\n\n\n rleshner:\n\nSolution 1 - Autonomous Proposals\nThe ability to delegate COMP opens up some very powerful opportunities that haven’t been fully explored.\nRather than delegating to a protocol politician or well-known community member (who, if they receive 100,000 COMP can create a proposal, act in their own interest, or vote for or against anything they want), what if you could delegate to a special-purpose smart contract–called an Autonomous Proposal .\nAnybody that meets a low threshold of ownership (e.g. 100 COMP as @lay2000lbs suggests) could deploy an autonomous proposal, with the title, description, and governance actions that they would like to build support for. Community members can delegate to the autonomous proposal, watch it rise up the leaderboard, and when it reaches the proposal threshold–the proposal is formalized and voted on.\nAutonomous proposals have the added benefit of providing additional visibility (and ability to audit and analyze) before going into the voting process–which meaningfully increases safety.\n\n\nYes, this!~ I think something like this can go a long way to incentivize a minimum amount of COMP ownership (e.g. 100) and increase the level of community engagement. Additionally, if farmers see value in COMP governance, they will want to keep a greater % of their rewards.\\nOnce again, I am working on having all delegations and voting paid for by the protocol. I’ll relay the txs in batches to save on gas.\\nI don’t think it’s really that great idea, as someone might imagine. It sounds like what we all need just to make limit lower and then suddenly community will be able to write code, test it and implement a proposals. Guess what, vast majority of community is just not technically capable of it no matter what. And if someone can write code, than surely that person is capable to publish it on github, link to it from forum and somebody could create a proposal if it’s really that good.\nThat’s a first thing. Nothing is preventing community from creating anything and coming with proposals after they actually have something, rather than just claim that we can’t implement something because not enough voting power.\nSecondly, what i personally think isn’t that there is that much lack of power for community, but rather lack of some simple utilities in Compound governance UI.\nI believe it’s road ending in dead-end trying to implement like every aspect of discussion on-chain.\nThere is no real need for that and hardly any benefit. I would suggest just adding simple voting interface and some sort of voting UI for proposing something like new listings, or changing parameters. There’s no need for on-chain governance for that, as it doesn’t trigger any actual change in protocol, just basically count votes from holders of COMP to monitor if something have support from community or not. And for that you don’t really even need a on-chain transaction, just check amount of comp at address and count it towards “yes”, or “no”. You can think of it just as sort of primaries, a tool for checking support for certain implementations, and for such system there’s indeed no need for high comp holder barrier. I’d say even double-digit COMP pretty much should be fine to create some vote-gathering  proposal.\\n\n\n\n Sirokko:\n\nI would suggest just adding simple voting interface and some sort of voting UI for proposing something like new listings, or changing parameters. There’s no need for on-chain governance for that, as it doesn’t trigger any actual change in protocol, just basically count votes from holders of COMP to monitor if something have support from community or not. And for that you don’t really even need a on-chain transaction, just check amount of comp at address and count it towards “yes”, or “no”.\n\n\nGreat idea. Snapshot Labs product has gotten a lot of traction recently, probably the simplest way to implement gasless signal voting.\\nIt’s super exciting to see the level of interest in lowering barriers to participating governance. I personally can’t wait to see this process continue to evolve.\nIn the meantime, to follow up on @rleshner’s previous message on Autonomous Proposals - these are now live  https://medium.com/compound-finance/compound-autonomous-proposals-354e7a2ad6b7 9\nIf you have 100 COMP, you can visit https://app.compound.finance/vote 7 and create an Autonomous Proposal. Once the Autonomous Proposal has >100,000 votes delegated to its address, any user can submit its proposal into the formal governance process. Try it out and share your Autonomous Proposal URL/address!\\n\n\n\n Sirokko:\n\nI believe it’s road ending in dead-end trying to implement like every aspect of discussion on-chain.\n\n\nAgree 100%!\n\n\n\n Sirokko:\n\nThere is no real need for that and hardly any benefit. I would suggest just adding simple voting interface and some sort of voting UI for proposing something like new listings, or changing parameters. There’s no need for on-chain governance for that, as it doesn’t trigger any actual change in protocol, just basically count votes from holders of COMP to monitor if something have support from community or not. And for that you don’t really even need a on-chain transaction, just check amount of comp at address and count it towards “yes”, or “no”.\n\n\nTo count votes could be as easy as pulling data from Etherscan’s API. You would just have to pull every address that has voted each time a new vote is made, or could limit the amount of pull requests by doing it every ‘x’ minutes."
  },
  {
    "number_of_comments": 12,
    "postid": "23a70912-ce2a-418b-a586-2ab77f2e0ed3",
    "posturl": "https://www.comp.xyz/t/cgp-hackathon-sponsorships-in-2023/4067",
    "combinedcontent": "\nCGP Hackathon Sponsorships in 2023\nAuthors: Adam, Harsha, CGP Domain Allocators\nCompound Grants Program seeks an additional $100k in grants budget to sponsor these ETHGlobal hackathons:\n\nETHGlobal Toronto\nETHGlobal New York\nETHOnline (Remote)\n\n\nCGP seeks quality builders\nCompound grants program has already accepted 12 proposals, amounting to a total committed amount of $200k to builders. All of the funding is happening transparently and can be tracked on https://questbook.app/ 11\nHowever, we need to attract the best builders to continue to make CGP an ongoing success. Particularly,\n\nIncrease awareness among top builders that Compound grants are back.\nIncrease inflow of builders for proposals in security, multichain-cross chain, and Dev tooling domains\n\nThe invite for builders can more formally be seen here in the form of well-defined RFPs :\n\nDevtooling 4\nSecurity 1\nMultichain strategy 4\n\n\nImpact\nUniswap and Aave regularly sponsor events. Their presence in hackathons inspires builders to build on top of their solutions.\nCompound has previously sponsored hackathons too. Speaking to the team, we learned that the impact of Compound sponsoring the hackathon was limited to brand recall.\nHowever, CGP aims to sponsor ETHGlobal events with the specific goal of getting more builders to build the RFPs laid out by the 4 domains.\nCase studies from CGP 2.0\n\nThe Wido team who will be working on a collateral swap extension for V3 3 won HackMoney 2021 and 2022 before organizing as Wido.\nGabriel built a prototype of his wrapper in a hackathon. He has gotten a grant from the Dev Tooling domain 3. His solution is already being used by another grantee.\n\n\nWhat we get\n\nPartner Table - to explain the RFPs and help builders lay out a development roadmap\nWorkshops (Recorded and in-person) - increase awareness of RFPs by building MVPs of desired projects\nConnected to the builders - Dedicated channel in the online ETHGlobal community and connect with teams building on the compound.\n\nWe should use sponsorships with a laser focus on inviting builders to the already listed RFPs.\nWe know what we want, we know who can build them, and we know where to find them.\nWe will pick winners only if they’re building something in response to the RFPs listed.\n\nWhat exactly will we be paying for?\n\n\nETHGlobal’s sponsorship fees for participation as a protocol in the three aforementioned events.\n\n\nCommunity member passes for protocol community representatives (ranging from $500 - $1,000 per person, per event). The number of community members that can represent varies from event to event. We decide this by self-nomination and polling on the forum posts for each event.\n\n\nCompound Grants hackathon sponsor bounty prizes ($5,000 - $10,000 per event).\n\n\nThis specific grant will NOT include stipends for travel and lodging fees for participants in the in-person events.\n\n\n\nCan I participate in a capacity beyond simply voting for this proposal?\nYes! We encourage community members to participate in the events. You may participate as a volunteer mentor, a judge, a hacker, or a presenter.\nAdam from Compound Labs will participate as a mentor, developer workshop presenter, and judge for the Compound Grants bounty prizes in each event.\nCommunity members may organize a presentation for the events that are based on the Compound protocol or Grants and must drive audience participation in the protocol community or Grants.\\nLet’s get a temperature check on this proposal. Please vote!\n I would vote FOR this proposal as is I would vote AGAINST this proposal and I will post a reply that shares my thoughts15votersResults\\nJust adding my two cents here that I believe this proposal is well-timed for driving talent and attention to Compound with the GCP 2 program hitting its stride and at the tip of the protocol’s multichain growth phase. CGP’s dapps and protocol ideas domain has some smaller, education-oriented proposals; where those proposals are complementary to this initiative, they may be supported directly within the protocol ideas CGP 2 domain.\\nI am the domain allocator for Multichain/Crosschain for CGP2.0 and I believe this is a great way to generate further support for the grants program and Compound.\nThis hackathon greatly complements CGP2.0 and can funnel participants to contribute to Compound via CGP2.0. With the amount allocated to CGP2.0, this will help raise awareness and attract talented contributors to Compound .\\nI’m the domain allocator for the Security Tooling Domain of CGP 2.0. I support this proposal as a way to build pipelines of applicants and interested parties that can build on Compound.\nMany Compound competitors are already very active in sponsoring hackathons and this ensures Compound is not left out of the talent growth.\\n[Update]:\nThe proposal is put up for voting here: Compound 5\nWe will be sharing the progress of the voting on this thread.\\nUpdate:\nThe proposal is live for voting now! 4\\nUpdate:\nThe proposal has been successfully passed and executed 2. Thank you everyone for the support.\nETH Global Hackathons can receive the amount only in USDC or USD. Hence will need to make the swap to USDC to transfer the amount. Please let me know if there are any concerns or suggestions.\nWe will be doing the swap in small portions over the course of the day on 10th March.\\nI am a grant recipient and working on the Compound Academy.\nhttps://compound.education 5\nOur goal is to provide users and developers with onboarding and education resources for Compound III. While we are currently developing guides, I believe that offering developer courses would be extremely beneficial. This would allow developers to be better educated before and during hackathons.\\nWhat is the update here? I can see on etherscan that the COMP was transferred to an EOA but nothing has been done with it. Additionally, it is highly preferable for COMP that is designated for community usage to not be in an EOA.\\nUpdate here:\nWe have performed the swap and received 87,128.483 USDC 5\nRequested the next steps for transferring the funds to the ETH Global Team. With the updated price, there are going to be few changes in the features offered. Will update the forum here once confirmed.\nThanks to @allthecolors and @madhavanmalolan for coordinating the execution of the swap.\\nI’m a Domain Allocator for Dev Tooling for CGP 2.0; I am in support for sponsoring EthGlobal hackathons. These hackathons will help us spread awareness about CGP and specifically the RFPs that have been created by the domain allocators. I totally agree that we should have a focus on these RFPs to ensure that we actually get some impact by sponsoring as against a wishful “brand presence” that is both subjective and impossible to prove/disprove.\\nHi arr00, thanks for writing. here is the update.\nThe 1962.7 COMP tokens were in the QB-Grants Wallet which is a signer on the grants SAFE. We are exploring ways to exchange it to USDC, since ETH Global team is accepting only in USD/USDC. For swapping the tokens, we have transferred to a wallet to connect to dex for easier execution. However, we appreciate the feedback from you to do it through a multi-sig maybe by using tools like wallet-connect, and totally agree with it. Hence transferring the tokens back to the Grants multi-sig SAFE. (Transaction Details 2)\nAlso - the delay is for the following reasons:\n\nSwapping through a dex like Uniswap has a huge slippage which is not leading anywhere close to $100K\nWith the SVB crash and Circle announcing deposits in SVB, we requested confirmation from the ETH global team if they still want the funds in USDC. They do want in USDC and got the confirmation yesterday.\nWe spoke to other community members who’ve had this problem in the past, and we were advised to find a liquid venue to sell the COMP like Coinbase.\n\nHere is the current problem we are facing. The COMP price is currently at $43. We have requested the funds when the price is $50. We wrote to ETH Global team regarding the same. If ETH Global can’t move forward with less than the full 100k USDC, we will take the feedback from the community to either request for more funds or return the funds to the treasury."
  },
  {
    "number_of_comments": 24,
    "postid": "2c3ec816-569a-45d8-814c-28a8e0050296",
    "posturl": "https://www.comp.xyz/t/improving-liquidation/1180",
    "combinedcontent": "Problem\nThe Comptroller contract stores a Liquidation Incentive parameter, which is currently 8%. This incentive is used to entice liquidators to perform liquidations.\nLately, “predatory” liquidators have begun burning the liquidation incentive in gas fees, earning no profit for themselves, and transferring all profit to miners (MEV).\n\nExample 1 48\nExample 2 19\n\nWhile this is effective at de-risking the protocol (in the short-term), it is driving community liquidators out of business. The quantity, and diversification of liquidators is what provides safety for the Compound protocol. If the predatory liquidators are successful at discouraging community liquidators, the health of the protocol will be jeopardized.\nImprovements to the liquidation process are needed.\nPossible Solution\nThe liquidation incentive can be split between the liquidator (5%), and the protocol reserves (3%). This brings multiple advantages:\n\nRegardless of whether the liquidator is friendly or predatory, the liquidation increases reserves, which benefits the users (safety), and COMP token-holders\nCascading liquidations pose less of a risk to the protocol; each liquidation increases the cushion for the next liquidation\nThe threshold at which the protocol suffers insolvency from a single address improves by 3%. This is because the “break-even” health of a user is ~ (Collateral Factor + Liquidation Incentive); the larger the liquidation incentive, the less collateral available to protect the protocol\n\nSecond, the gas costs of liquidations can be optimized; reducing the gas required to liquidate, and limiting gas battles – which will benefit community liquidators.\nTrade-offs / Risks\nDecreasing the liquidator incentive to 5% may reduce willingness to liquidate.\nCounter-argument: prior to the cascading liquidation / sell-off in March 2020, the Liquidation Incentive parameter was set to 5%; this was sufficient to always incentivize liquidation on a first-come first served basis. It was raised to 8% out of precaution (during an extremely volatile week).\nPotential Improvements\ncToken\n[core] In seizeInternal(), transfer 97% of the seizetokens to the liquidator, and 3% of the seizetokens to the Comptroller\n\nStore a third balance in memory for ComptrollerTokensNew\nEmit event for second transfer (to Comptroller)\nNote: this math would actually be 3.24% incentive going to the protocol, 4.76% to the liquidator (based on math!), which is OK - you could also calculate the right ratio\n\n[optional] Liquidation gas optimizations\nThe cToken and Comptroller have myriad opportunities for small iterative improvements to gas cost, including graceful failure info, duplicated checks (e.g. for freshness, borrower=liquidator), etc. There may or may not be low-hanging fruit.\nComptroller\n[optional] In liquidateBorrowAllowed(), require that tx.gasprice is below an arbitrarily (yet reasonable) threshold, e.g. 2,000 gwei; this would limit gas-racing to MEV\n[optional] In liquidateBorrowAllowed(), only check the closeFactor for borrow balances over $100,000; this would allow fully closing small borrowing positions, which is more gas optimal\nDeployment / Rollout\nThe core incentive changes would be at a cToken level, and would only apply to upgradable (2nd generation) cTokens, including DAI, USDT, UNI, and COMP, which would be upgraded through a governance proposal.\nThe major collateral markets, ETH & WBTC, would have to migrate to upgradable cTokens - which is a long-standing goal - for which these liquidation improvements are a good catalyst.\nFuture Improvements\n\nDynamically weight the incentive between liquidator and protocol; accounts that are deeply unprofitable could carry a larger weighting towards the liquidator (e.g. 8%), incentivizing the closure of the most at-risk accounts first.\nAdjust the incentive based on a function of time (similar to an auction model)\n\\n\n\n\n rleshner:\n\nThe liquidation incentive can be split between the liquidator (5%), and the protocol reserves (3%).\n\n\nI think the major disadvantage to this policy is that at minimum it creates the perception of misaligned incentives and may indeed created misaligned incentives.\nThink about all of the complaints about BitMEX profiteering from MEX liquidations. Even if there is no concrete evidence of bad behavior, at the very least, it creates the perception that the protocol might care a little less about liquidated borrowers, and might do a little less to ensure that liquidations are handled “above board”. I’d fear that the Compound brand would suffer as a result.\\nBy my understanding the issue with bitmex was the platform itself (supposedly) using proprietary information to hunt for liquidations and stops and then profit through the insurance fund, but this shouldn’t apply for on chain lending where positions are visible to everyone on an equal basis.\nLiquidation fees are a simple and coherent way to charge riskier users more than safe users.\\nPercentage\nReserving a portion of the liquidation fees into the contract makes sense to me. Cascading liquidations are indeed a concern.\nHowever, I don’t see how changing these percentages alters the balance between MEV liquidators and non-MEV liquidators. In either case there is a fixed amount of profit to be had for the liquidator, and some portion of that potential profit must be spent for gas. Simply reducing the maximum profit available will not make it any less favorable to MEV liquidate. Right?\nGas Limits\nI’m strongly opposed to a gas limit however. If my understanding is correct, Compound’s  liquidations only function correctly inside a band of prices. If the securing asset price has gone too low, or the borrowed asset price has gone too high, then it no longer makes financial sense to liquidate. It is essential that liquidation happen quickly when inside the band. In a case when Compound would be in most danger, with insane price moves on major assets, it is almost certain that gas prices would rise to astronomical levels as every trader franticly attempts to avoid losses, or  to profit from the changes. This would be the worst possible time to lock out liquidations.\\nThe current issue comes from the ability to be able post price by anyone, liquidators include the prices in the transactions where they liquidate and the gas price race is unlimited to the edge where there is no profit.\nExplaining the situation on the previous oracle when Compound Labs was posting the prices to the oracle:\nin this case the liquidator wanted to be in the same block where the oracle update tx was and the liquiadtion tx was limited by the gas price of the posted price tx gas price, so there was no gas price race in this case, and the winner was “random” accordig to how the transactions was indexed in the block cause they was all at the same gas price.\nI could see a potential fix of burning the profit in gas by disallowing calling the oracle update function from contracts meaning it would be forced to be a seperate transaction, and the liquidations could be limited again by the oracle update transaction gas price.\nThe incentive for the liquidator to update the oracle in this case is till there.\\nAbsolutely fascinating issue.\nThe problem with a gas limit is that in a liquidity crunch like in march, gas prices could go through the roof.\nMaybe having maximum gas as a function of a moving average of gas prices. i.e. 5x current gas prices?\nFront running in Ethereum is a noted issue.\nNot sure if it will ever go away, maybe community liquidators are gone forever.\nThis article is well written and explains this better than I will ever be able to.\n  \n      \n      ZenGo - Bitcoin & Cryptocurrency Wallet – 29 Dec 20\n  \n  \n    \n\nEthology: A Safari Tour in Ethereum’s Dark Forest 4\n\nA study of frontrunning bots, how efficient they are, how likely a transaction is to get hunted down and how can they be evaded.\n\n\n  \n  \n    \n    \n  \n  \n\n\nFew points:\nTalk to the the ones running the bots.\nAcademic/high level research is needed for this.\nGrants committee?\\nIt’s obvious, that this problem is caused by the chosen oracle design as liquidators can post the new prices themselves. Instead of trying to cure the symptoms, I think it’s a better idea to cure the problem at the root. This means by replacing the current oracle by a better solution. This can easily be done by using an existing, proven solution like Chainlink.\nThe oracle solution, which is discussed here 2, won’t solve this problem as it can be read in the protocol of the last developer call: “Getty mentioned that posting is somewhat subsidized by liquidators because it is in their best interest to post prices on chain in order to perform liquidations. In the early form of the Medianizer there will not be any new considerations to gas costs.”.\\nits worth to mention that this is also not a perfect solution, what i did explained above, if none of the liquidator manages to be included in the same block where the oracle update would be, then it would be open again for an unlimited gas price race in the next block, so this is just a slightly better case.\\nPlease don’t use quotes if you aren’t quoting the exact words I wrote or said.\n\n\n\n cryptix:\n\nIn the early form of the Medianizer there will not be any new considerations to gas costs.”.\n\n\nThis is not true. I have written and said that gas efficiency is important in the design of the medianizer.\\n\n\n\n getty:\n\nPlease don’t use quotes if you aren’t quoting the exact words I wrote or said.\n\n\nWell, perhaps you should have clicked on the link to the protocol I included in my message. I quoted the exact text of the protocol. If you think this is not correct, you should blame the person who wrote the protocol and not me.\\nPlease keep the conversation respectful and polite, and focused on the topic of the thread; namely the behavior of certain liquidation bots to deliberately spend the 8% liquidation incentive on gas.\\nWell if there is collusion between miner and liquidators, then are they really “spending” gas or wash trading? I think we need to dig deeper into individual front runners.\\n\n\n\n rleshner:\n\nrequire that tx.gasprice is below an arbitrarily (yet reasonable) threshold, e.g. 2,000 gwei; this would limit gas-racing to MEV\n\n\nthis will not necessarily decrease the MEV, as it will give rise to spam txs, namely, instead of broadcasting a single tx with high gas price, bots will send many txs with lower gas price.\nSuch behavior was observed when dydx price feed was back running friendly (as opposed to front running).\n\n\n\n rleshner:\n\ntransfer 97% of the seizetokens to the liquidator, and 3% of the seizetokens to the Comptroller\n\n\nImo here it is very important to define what is it Compound is trying to solve.\nFor example I think this would help a lot towards mitigating black thursday like events, where liquidations completely failed, as it will allow the COMP dao to use the accumulated fees are an insurance fund.\nBut will be less efficient if the future is to increase the collateral factor over time, and reducing the liquidation incentives will inevitably lead to less efficient liquidation process. And the community will probably be reluctant to use the insurance funds on a day to day basis.\n\n\n\n rleshner:\n\nthis was sufficient to always incentivize liquidation on a first-come first served basis\n\n\nImo the first come first serve approach (and in general, giving liquidation incentives without any commitment from the liquidators) is the problem of all DeFi liquidation approaches.\nAnd B.Protocol, the project I am working on, is trying to solve it.\\nJust wanted to add a +1 to this post and suggestion. I too believe that liquidators should not have the option to update oracle prices in the liquidation transaction.\\n\n\n\n blck:\n\nin this case the liquidator wanted to be in the same block where the oracle update tx was and the liquiadtion tx was limited by the gas price of the posted price tx gas price, so there was no gas price race in this case, and the winner was “random” accordig to how the transactions was indexed in the block cause they was all at the same gas price.\n\n\nI just wanted to pointed out that this gave rise to back running which results both in MEV and Ethereum spam.\nSo solutions like price kicks in only one block after its update, are imperative to prevent the spam (but of course give rise again to front running => high gas price).\\nCoinbase price feed is irreparable, simply that product is not valid (it is not the only one that is weak from their product suit).\n\n\n\n rleshner:\n\nThe liquidation incentive can be split between the liquidator (5%), and the protocol reserves (3%)\n\n\nHow come now “very important” liquidators will be disincentivize? To protect protocols and protocol users, it is necessary to eliminate MEV players and sophisticated bot liquidators completely.\nBy eliminating them, they would remain on rational liquidators, preferably the liquidators from the compound community.\nAs a user of the protocol, I am always in favor of increasing reserves, but the proposed solution is still a danger for lenders since we got stuck with the Coinbase product (changes mean nothing if DAI pump to 1.5$).\nA reasonable solution would be a combination of Bprotocol and Defi Saver (or equivalent in the Compound implementation)\nBprotocol offers a solution to keep asset value within the protocol using “jar rewards” and offer liquidators predictable profits without gas fee wars. On the other side, Defi Saver allows setup automatic liquidation without the cost of premiums, i.e. (users alone set-up repay or leverage threshold).\\nLarge mining farms and pools are against EIIP 1559 because predictable gas cost and better UX for users. I don’t think they protest because burning base fee. Large profits for miners are made through liquidations with gas fee wars.\nit is necessary to remove these parasites as soon as possible so that they do not have the ability to influence the protocol in this way.\nI will rather pay 10% premium to community liquidator then 1% to well designed manipulation scheme.\nIs that possible in technical sense?\nI think asset management project that i mention (bprotocol and defi saver) are already work on that so that it is feasible.\nMy business depend on this protocol and I don’t play farming games for quick profit.\nQuestion is which users Compound want on protocol? Clear speculators and arbitrage bots or real users with clear usecase?\nI also read that article (and all others about that topic),\nand I think situation is not good\\n\n\n\n dabar90:\n\nMy business depend on this protocol and I don’t play farming games for quick profit.\n\n\nClear speculators and arbitrage bots are a clear use case. We are all capitalists and are looking to maximize our wealth.\nWe are all wallet addresses on the blockchain, how could you differentiate?\\nI respect your statement, it’s okay to have a different opinion. However as far as the adoption of DeFi technology is concerned I think the ultimate goal is to fix the shortcomings of the traditional financial system, not to create a gambling place.\nWe are not wallet addreses, we own many wallet addreses and for differentiate that we try to build protocol which serves humans rather than bots.\nI have nothing against speculators and liquidation bots I just think there is a better way to solve account insolvency problem, protect user funds and protocol stability.\nThe protocol must attract real businesses and create real value, currently this is a leveraged farming bubble in which speculators with  50$ harm users.\\nAs the percentage earned by the liquidator decreases, it becomes unprofitable to liquidate smaller (“dusty” in makerdao parlance  ) debt positions.\nWould reducing the liquidator incentive from 8% to 5% create issues for Compound with dusty borrowing positions?\\n\n\n\n monet-supply:\n\nWould reducing the liquidator incentive from 8% to 5% create issues for Compound with dusty borrowing positions?\n\n\nAbsolutely; which is why removing the closeFactor (set to 50%; only allowing half a borrow to be liquidated) would double the economics of liquidating small position; shown from above:\n\n\n\n rleshner:\n\n[ optional ] In liquidateBorrowAllowed(), only check the closeFactor for borrow balances over $100,000; this would allow fully closing small borrowing positions, which is more gas optimal\n\n\n@monet-supply taken together, this should generally address your concern. Thoughts / alternatives?\\nCompletely missed this , removing the close factor should solve for dusty/small debt positions.\nOne other potential risk to think through, what if Compound provides delayed oracle prices during a downturn? With liquidators offered a price 5% rather than 8% below the oracle price, there’s a bit less margin of safety for potential oracle price/market price divergence. Have there been any cases in the past of liquidations stalling due to slow price updates?\\nNone that are known; the liquidation incentive was raised from 5% to 8% on Black Thursday in March 2020, as a precautionary measure, and never reduced afterwards.\\nmaybe I’m subjective because my borrowing position is “dusty”, does that mean that the borrowing position will be liquidated 100% - without closeFactor?\nWhile this is a logical move given the high gas fee, it puts “dusty” users at a disadvantage.\\nIntro\n\nThe original problem is posed as predatory liquidators who purposefully burn the 8% incentive as gas fees to miners. This is logical if the liquidator can cooperate with most of the miners, who would rebate some of the 8% back to the liquidator. However, any miner who isn’t part of the scheme will stiff the liquidator and keep all the profit. In today’s network I don’t think there are enough cooperating miners to make this approach profitable. (That may not last as there are already signs of miners harvesting MEV.)\nBut even without cooperation, another motivation for a liquidator to bid very high gas prices is to discourage competition. The natural equilibrium of a gas race ecosystem is a single, gas-efficient predator with a scorched earth strategy. The predator posts signs everywhere saying it will outbid any other liquidator without regard for its own profit, guaranteeing that anybody else who tries to participate will lose money on every attempt. Then, once everyone else gives up, the predator can enjoy dining in peace. This is bad for the protocol because it is now at the mercy of a single actor who may find other uses for its capital just when you need it most.\n\nAnalysis of proposals so far:\n\nThe first proposed solution reduces the incentive to 5% and adds a 3% liquidation fee paid to reserves. These two parts are unrelated. From the perspective of a liquidator, the only visible change is 8% becomes 5%. This does not change the structure of the liquidation market. It just reduces the maximum gas price that a liquidator will pay for a given liquidation.\nA smaller incentive increases the chance that all liquidators will pass on an opportunity due to a difference betwen the oracle prices and actual market prices that the liquidators use to value the repaid and seized assets. For example, if the collateral price is falling quickly, then the built-in oracle lag may lead to an oracle-market price difference of more than 5%, which would make any liquidation unprofitable even without gas fees. This might persist until the account is insolvent and lenders suffer a loss.\nA second proposal sets an upper limit on the gas price in a liquidation. Most miners use geth configured so that pending transactions having the same gas price are ordered in the block in the same order as they were first received by the miner (https://github.com/ethereum/go-ethereum/pull/21358 1). The consequence of this is that liquidators would race to be the first to submit a transaction at the maximum price into the mining pools. This competition will be dominated by a single winner: whoever has the most extensive global network of low latency connections to mining pools, combined with the fastest backend processing replicated around the world at mining pool ingress points or colocated with miners directly. This winner-take-all competition is not won by a “community liquidator”. And eventually, the miners themselves will absorb the liquidator function and run it internally.\nA third proposal (in the replies) separates the updating of oracle prices into a preceding transaction, whether this is done by the liquidator or someone else. In either case, all liquidators will attempt to tailgate or back-run the price update. In the past, this produced spam transactions because miners would randomly order pending transactions at the same price. But the new geth behavior means that whichever liquidator is able to transmit their transaction to the mining pool first wins the contest every time. The winner is the same as before: a globally replicated liquidator ideally colocated with all the mining pools, or miners running internal liquidators. In the event that the block with the price update doesn’t have enough space for all the tailgating liquidations, gas price auctions will occur for top space in the next block, with most of the incentive going to whatever miner publishes the next block.\nThere may be some benefits to each of these proposals, but none of them addresses the real underlying problem, which is that the protocol is paying an incentive (5% or 8% or whatever) that is intended to secure the protocol but instead is being skimmed, more and more, by miners. All of the proposals  drive the ecosystem toward a single winner who makes competition pointless and is eventually absorbed as an internal miner function.\n\nI’ve written about this before (Critique of Compound v2's liquidation system · GitHub 6) and suggested two possible designs:\n\nA dutch auction across multiple blocks with a gradually increasing incentive. As soon as the incentive reaches a value that is acceptable to at least one liquidator, they will use a high but reasonable gas price to liquidate in the next block.\nA fixed high incentive combined with a sealed-bid auction for the right to liquidate, say by using a commit-reveal scheme every 10-block epoch. Every 10th block the winners could perform the liquidations and would pay their bid amount into the protocol itself (split among the borrower and reserves perhaps), rather than to the miners.\n\nBoth of these approaches have the dual advantages of minimizing the cost of the incentive borne by the borrower and maximizing the fraction of the incentive received by the liquidator. The main disadvantages are complexity and the nonzero time duration of the auction, during which market prices can move further against the borrower.\nI’m sure there are better solutions and hope to hear some new ideas. This is the direction I think we should be working in."
  },
  {
    "number_of_comments": 21,
    "postid": "78193ab2-76f5-49e3-bdd9-52a202bab61a",
    "posturl": "https://www.comp.xyz/t/lower-proposal-threshold/1659",
    "combinedcontent": "\nBackground\nGovernor Bravo introduced many new ways for the compound community to modify the governance process. One of these new features is the ability to change the proposal threshold, which is currently set to 100k COMP. An address needs to have the proposal threshold delegated to their account to create a proposal.\n\nIdea\nI believe the community should consider changing the proposal threshold to about ~65k COMP. The 100k proposal threshold acts as a way to ensure that only the highest quality and vetted proposals make it to the actual governance process, but it may make it too hard. Many CAPs (Compound Autonomous Proposals) struggle to make it to the 100k threshold even when there is clear communal support.\nI believe that a threshold of 65k COMP will be enough to ensure quality, but make it easier for future ideas supported by the community to make it through the governance process.\\nGood idea, though i would love to see it go down to 50k as there are alot more delegated entities around that area, including Dharma who made nice proposals before.\\nWhat should be the minimum amount of COMP required to create a new Proposal ?\n 50000 COMP 65000 COMP 100000 COMP (no change)34votersResults\\nThe proposal threshold is a tool to set the “speed” of governance, and prevent toxic proposals (whether deliberately malicious, or too hasty). The 2-day review period created in Proposal 43 3 should further reduce the risk of a toxic proposal.\nLowering the threshold will generally increase the quantity of proposals (easier for CAPs to become live proposals, and more stakeholders who can directly create proposals).\nThis isn’t a parameter that should change often (governance should be as predictable as possible), but it seems that the timing is right following the launch of Governor Bravo. I support either @arr00’s suggestion of 65k, or @blck’s suggestion of 50k; whichever number we choose should have a bit of science behind it \\n@eddylazzarin would like to hear a16z’s opinion on the topic as you guys delegate 50k COMP to at least 5 university groups.\\nTime for the ones who believe crypto is the future to profit no matter how small or big the contribution. We all want to eat not just the big guys.\\n\n\n\n rleshner:\n\nwhichever number we choose should have a bit of science behind it\n\n\nInstead of hard-coding a set number, Compound could calculate this number by using 2 variables:\n\nTotal COMP in circulation\nNumber of COMP holders\n\nThis could bring some science into the threshold. Because at 100k COMP, looks like only 10 addresses can propose on what could potentially affect 158,000 holders.\nI would imagine a ‘decentralized’ finance project would have less decentralization at the beginning, but we are far from the beginning with Compound holding over $16B worth of supplied assets and with over $7B worth of borrowed assets.\nUsing the 2 variables above Compound could make the minimum amount auto adjust to allow more decentralization with time. Maybe a snapshot of the variables every 6 months or so. To start it off, 20k COMP would be fair in my opinion when looking where Compound is today. This would include less than 70 addresses, and not all 70 are probably going to want to rush to make proposals. Even if they do, the not-so-good proposals would be voted out.\nTo prevent accounts from spamming proposals, set a time limit/voting decay for accounts which created a proposal. If their proposal gets vetoed, then using a method like ‘slashing’ could be implemented. Where the user’s voting privilege is slowly restored. This should further push proposers to create rock-solid proposals that will launch Compound to the Moon!\\n@CryptoCraig thanks for weighing in; an important counterpoint is that the proposal threshold doesn’t determine how many addresses can propose changes; it determines the quantity of votes necessary for a proposal to enter the voting process.\nAddresses with 100,000  delegated to them can directly create live proposals.\nAny address with 100  can create an autonomous proposal; currently 476 addresses 4 can create proposals, and the proposal threshold itself won’t change this.\\nI understand now. So, a user would just need to have 100 COMP (either owned or delegated to them?) to create an autonomous proposal (CAP).\nThen, how does the community know to vote on the proposal at that point, with it still needing 100k votes? Sorry if this is answered somewhere, I am just very interested in Compound as this is where I’ve decided to park a good portion of my digital assets.\nProposal submissions and voting should happen on specific days, like when movies debut. This would allow voters the time needed to review a proposal and a day that they would need to come to the polls. This would also assist a CAP to reach the threshold of votes.\nI still think that more COMP holders should require less votes. At 188k holders, Compound will probably have a large percentage of votes coming from the largest holders. However, in 20 years, if Compound were to accumulate 4 million holders, then COMP balances will be more distributed resulting in a greater percentage of holders needed for proposals to get to the current threshold. You said it yourself, @rleshner , there needs to be some kind of science to it, which is exactly what I think.\nIf just picking a number is what Compound needs right now, I say do it. But, a simple formula should be able to handle this in the future.\\nFeel free to stop by Discord 1 to chat & ask questions & learn!\nA user needs to own 100  to create an autonomous proposal 5. These proposals are typically advertised on this forum, Discord, and Twitter to gather support. Sometimes this process takes hours, sometimes days, sometimes they never gather the requisite support needed.\nThere is a two-day review period before voting begins on a proposal (this is a recent upgrade 1).\\nI think lowering the proposal threshold to 50k is a great idea. I have been meaning to make this post, so I am thrilled someone else has. By lowering it to 50k, we’ll be roughly doubling the number of users that can create proposals, and I think those are good users to have involved.\nHaving passed two CAPs, I found it too challenging to get the 100k votes required when I knew the proposal would pass. Generally, getting 100k votes delegated to a user is quite challenging, and while 50k is still a lot, I think it is the next logical step.\nI think adjusting it to 65k would have a limited effect since only two users have those amounts.\\nI think there are two approaches to take here, the more conservative, and more ambitious.\n\n\nConservative - Decrease proposal threshold to 65k which will immediately allow 1 more EOA to propose and make the CAP process considerably easier\n\nAmbitious - Decrease proposal threshold to 50k which will immediately allow 11 more EOAs to propose and make the CAP process considerably easier.\n\nI would argue that the change in the ease of getting CAPs through is relatively small between 50k and 65k from empirical evidence. If I recall correctly, most CAPs that were successful got to 65k quite quickly while getting from ~65k to 100k was the harder part.\nSo what it really comes down to is how ambitious we want to be right now. Do we want to immediately add allow another 11 EOAs with varying levels of engagement in the community to propose, or do we want to be conservative and just make the CAP process much simpler?\nPersonally, I believe that due to the newly added 2 day delay, we can’t go wrong; however, I prefer to err on the side of caution and initially set the proposal threshold to 65k.\\nI think another viewpoint would be interesting to think of here, is the current limit PREVENTING good ideas from coming forward and being passed? Compound is by far the most active and productive governance out there, do folks feel like there are proposals that aren’t making it because of the limit?\nAlso, if there were to be more proposals making it to the floor, what sort of proposals do we think these would be? Requests for funds? Changes to the protocol? I think the difference is that changing the protocol is inherently risky to the entire project, while giving out money is fairly low risk (you can lose funds but the protocol is not in danger). I think considering what type of proposal we are more likely to incentivize is worthwhile considering as well as there might be another way to accommodate those proposals without changing the threshold.\\nI would love to propose several basic proposals and that I am sure would pass but trying to get 100k votes on a CAP is a real pain. Unless the CAP will make a big change, it is hard to get voters to delegate. In an ideal world, the CAP proposal threshold would be lower than the single user threshold, but that isn’t how the system works, and it would require a good amount of work to change it.\nI can’t be the only one who has a list of low-hanging uncontentious changes that should be made and struggle with the high threshold.\\n@getty would it be crazy to suggest that maybe for these kinds of proposals we did something like:\n\n\nIf the proposal is inherently good and benefits the protocol, then we can assume we want the proposal to pass. The problem, perhaps the social capital required to wrangle the votes is expensive.\n\n\nWhat if we compensate the folks with small amounts of COMP who delegate votes to a CAP that successfully passes a subsequent vote?\n\n\nIn this way, if the idea is strong, and folks likely think it will pass, they will likely support the CAP to pass the threshold, knowing that if the vote passes they will receive some small reward for doing the work of identifying a valuable proposal in advance?\nIt seems like folks wouldn’t really work to game this (although it’s crypto, so who knows) because the payout only happens if the vote passes. Most folks who would work to game such a vote don’t have significant COMP to make a difference at either threshold.\n(another edit)\nI don’t think we even need to make any protocol changes to make this work. We simply make the last instruction in the proposal a “compensate the CAP delegators with 1 comp split proportionally”\\nMaking the proposal threshold for CAPs lower than the normal proposal threshold would be a trivial task—however, it would doesn’t make much sense to me as users could just spin up CAPs and delegate to them instead of delegating to EOAs.\\nThat might be a neat idea but I think an incentive won’t move the needle for those holders. If you are someone with +10k comp getting a few COMP incentives doesn’t make much sense. In my mind, they are already invested and highly incentivized to support the proposal. The bigger problem in my mind is the high proposal threshold and the lack of voters set up to delegate.\n\n\n\n arr00:\n\nMaking the proposal threshold for CAPs lower than the normal proposal threshold would be a trivial task—however, it would doesn’t make much sense to me as users could just spin up CAPs and delegate to them instead of delegating to EOAs.\n\n\nTrue, I didn’t think it all the way through.\nThe biggest things I think the protocol could do to improve governance is lower the proposal threshold to 50k, move COMP rewards from the COMP borrow lend market to voters, and set up a system where users can use COMP as collateral and vote.\\nAgreed with both of your points here (lowering the threshold and allowing voting with COMP collateral).\nFWIW I noticed the recent CAP to add a LINK market 2 is sitting right at the 65k COMP mark, and it seemed to have widespread support.\\nThat 65k voter (0x0d8846e5d4af5dd24be37f460c07046fd80d96a3) has done an awesome job supporting CAPs. Without them it would be much much harder to get a CAP to a full proposal. We need more voters like them.\\nGetting delegation of voters is a problem we see elsewhere in other governances. It might be worthwhile running a program (maybe with yours truly Tally) to help get folks to delegate their vote to active participants. Maybe that would be worthwhile subsidizing.\\nThe reason for the high COMP threshold is to minimize the amount of sub-quality proposals right?\nLowering the threshold may increase the amount of sub-quality proposals, but at the same time, we could be preventing great improvements.\nThe threshold should certainly be lowered for this reason. The question is how much?\nI think the answer is in how many proposals the community and COMP holders can handle (review). After all, this is just the threshold to be eligible for voting. Whether or not the proposal passes is a whole different story. Quorum is currently at 400,000 votes.\nI propose we make a governance proposal to determine how many proposals voters would be happy with reviewing every week. We could make this a variable in Governor Bravo.\nWe then make an algorithm to programmatically adjust the minimum required to submit a proposal based on the number of proposals over the past month or so.\nWe’re currently at 1 proposal every 2 weeks for 2021 - much lower than I (most people?) can handle.\\nThere have been a couple forum threads and plenty of offline discussion around this with clear support for some reduction to the proposal threshold, so we should move forward with this.\nUnless there is any last minute dissent, early next week, Polychain is going to put forth the proposal initially suggested by @arr00 : reduce proposal threshold to 65,000 COMP.\nHopefully, this reduction makes the CAP process much more manageable for community contributors. I think we play it conservative and see how the 65k level plays out for a while, before considering further drops. This is likely a parameter that is much easier to reduce than increase."
  },
  {
    "number_of_comments": 92,
    "postid": "7da7d1ce-932e-4b82-aa82-372788184a85",
    "posturl": "https://www.comp.xyz/t/dai-liquidation-event/642",
    "combinedcontent": "DAI Liquidation Event\nThe Compound protocol uses Coinbase Oracle 95 for account liquidity calculations, anchored to within 20% of the Uniswap time-weighted average price. Any Ethereum address can post the signed/reported price on-chain, which allows for a permissionless and autonomous price feed 27 that rapidly de-risks accounts.\nFrom approximately 12:00am to 1:00am PT on Thanksgiving morning, the price of DAI on Coinbase Pro began trading at increasing prices and volume across the DAI/USDC, DAI/USD, and ETH/DAI pairs, reaching as high as $1.30. This coincided with a decline in ETH prices globally, of approximately 8% during the same time period.\nIncident DAI:USDC1038×858 36.2 KB Incident ETH:DAI1038×858 33.6 KB\nDAI/USDC; ETH/DAI; 24h volume 25m DAI\nDuring this period, 85.2m DAI were repaid by liquidators, who seized collateral from addresses borrowing DAI. In the largest liquidation 75, an address “yield farming” had 46.2m DAI repaid, and 49.8m DAI seized.\nIn total, 124 addresses of 225,793 were impacted; there are no under-collateralized accounts in the protocol; and all markets are healthy. However, addresses that were liquidated feel that the risk parameters & price feed were too aggressive / onerous.\nData\n\nList of addresses liquidated during event 164\n\nAnalysis\nFundamentally, the protocol and price feed performed as designed; real trading, on America’s largest exchange, was used to aggressively reduce the risk of borrowers in the protocol. What was unexpected was the adverse market condition that occurred, how quickly it occurred, and for many users, that it could occur at all.\nThe DAI market on Compound, with 1.6 billion DAI supplied and 1.3 billion DAI borrowed, has grown far larger and more rapidly than anyone had anticipated. The Compound DAI market eclipses both the underlying DAI market, the liquidity on exchanges, and the global trading volume of DAI by a vast margin. This exact risk is at the core of the Gauntlet Market Risk Assessment 64.\nNext Steps\nThis liquidation event should be a wake-up call to calibrate the protocol, which has scaled rapidly over the summer. I recommend that the community discuss what steps, if any, should be taken to prevent similar liquidations from occurring in the future.\nBroadly, potential changes could include:\n\nModifying the DAI market parameters, including the Borrowing Cap, to reduce the size of the DAI market relative to trading venues\nModifying the DAI price feed by either tightening the anchor bounds, capping the price (e.g. to 1.05), or utilizing additional reporters\nRemoving the reporter in some/all cases and relying only on Uniswap, or taking an entirely different approach\n\nAdditionally, impacted addresses have been vocal about being compensated for their liquidation costs–the governance process is capable of allocating COMP, and should analyze whether this is reasonable.\\nDo you think it would be a good idea that we use comp reserves to reimburse people? Then again, that is treating comp like cash.\\nIt seems disingenuous at best to say the platform worked as intended. The DAI price traded no higher than $1.03 anywhere else GLOBALLY. This was an exploit plain and simple. The adverse market condition was only on the single oracle that Compound used. The market did not price DAI at $1.30. The Compound protocol was attacked by someone who manipulated the DAI price up on Coinbase for the purpose of liquidating people at a discount.\nNot only should most of these liquidations not have happened, folks that had non-DAI collateral lost ~30% of value of their collateral in the liquidation process due to the erroneous oracle allowing the collateral to be seized at an adverse and improper exchange rate.\nI do hope the community at least partially reimburses the losses as this was clearly not intended behavior by the protocol, and borrowers following the UI “safe max” found themselves liquidated due to a protocol exploit. Not real market conditions that occur due to normal volatility.\\nUsers that got liquidated should be compensated imo. They borrow stablecoin because it’s the least volatile among all other assets but got liquidated anyway because of sudden 30% rise of borrowed asset. I’ve seen many platforms(comparably smaller) like pickle and harvest that were exploited, they come into a resolution to pay back the losses of their users. Hence, the responsibility falls on the platform.\nCompound is one of the biggest players in crypto space, if the smaller fish returned the funds, we expect compound can also do it. Otherwise, the community and the crypto world will view compound as greedy and incapable of accountability. We should add more source for pricing, especially decentralized ones. For now, we should avoid using coinbase price feed considering their recent status according to this tweet:\n@AdamScochran 11\n\"So:\nBinance had lag.\nByBit had liquidity crisis wicks.\nFTX had UI disconnect.\nUniswap had gas overload.\nCoinbase? The biggest player?\nThey had their 3rd major outage in three months, while also tweeting randomly about negative regulation and internal scandals.\"\\nYou were warned and you ignored the warning, People told you for months that this is a weak point in the design and you guys deliberately ignored it and now you get exactly what was predicted.\nHowever its not too late to throw the centralized Coinbase Oracle into the garbage and replace it with decentralized Chainlink ones. acknowledge your mistake or watch this sh*tshow happen again.\ngood luck you gonna need it.\\nMy previous comment got deleted here, and I am not sure why. I am a Compound user and I hold and farm COMP. Yet I can’t voice my opinion because someone doesn’t want to hear it? This is very unfortunate and not a good sign for a proper community communication. Fix your flawed design.\\nImagine not using link oracles in 2020\\nI lost well into 6 figures off this mistake. Naturally, losing large amounts of money has me researching the reasons behind this colossal error. In my pursuit of the truth, I discovered that compound received its price feed from Coinbase. Obviously this is a very risky strategy since Coinbase can have an error or other issues with their exchange, as we saw today. The thing that I am most curious about is why the compound team decided not to use chainlink, which pulls data from multiple sources and aggregates them, preventing mishaps like this\nI think the team has some explaining as to why they allowed such an easily preventable exploit to occur due to their own negligence. I feel that I should be personally reimbursed since this was no fault of my own, but the team itself’s fault due to either gross negligence or gross stupidity. Either way, it is unacceptable and I demand to be reimbursed.\\nDear GoldenBull,\nlike you I came to the same end that this was clearly gross negligence and preventable but I wouldn’t count on reimbursement, thats why we should all organize (all 124 affected) and join in on a Class Action Lawsuit against the Compound Protocol.\nSincerely,\nYehuda Leib\\nI’m posting this from the perspective of a programmer who has been in the business for 30 years with the past 5 of those years being spent working on blockchain/smart contract related projects. It is easy to see the “chinks” in your armor so to speak. The protocol is only as strong as it’s weakest point. The weakest point is glaringly the oracle system you decide to use. I don’t have the time to explain the intricacies of how and why as I am not a paid developer on this project but hopefully an event where the weaknesses in the protocol were so easily exploited will let your own team realize the flaws.\nIf your team chooses not to use a secure oracle (no coinbase does not provide a secure oracle) the attacks will continue.\nThere is good money in taking the funds from users in your protocol and you cannot blame the “smart contract hackers” who chose to do so.\\nI have no problem take responsibility when i am wrong but this event is market manipulation. Some people those who have not suffered losses are made smart but imagine in the future manipulation where price of DAI will be 2$. In that case every borrower will go through liquidation. I am using only compound.finance but if this case stay unsolved i will move to alternative. I am miner and i am using this dapp for business liquidity and when I pay  equipment 1 DAI is around 1$ not 1.3$…that is why it is called stablecoin. I would not complain that my positions were liquidated due to the volatility of ethereum or bitcoin, but due to stablecoin - that is unacceptable. This makes this product pointless and unusable.\\nI agree with you and i am in the same problem,  but please be smart and dont talk about racism because we dont see each other skins in this game - only public keys and usernames.\nGood luck\\n\n\nModifying the DAI market parameters, including the Borrowing Cap, to reduce the size of the DAI market relative to trading venues\n\n\nagree.\n\n\nModifying the DAI price feed by either tightening the anchor bounds, capping the price (e.g. to 1.05), or utilizing additional reporters\n\n\nI think that it is not good idea to set capping DAI level because it is designed to accept some volatility and balance its stability with the incentive mechanism.\nif we reduce anchored Uniswap bounds into 10% and it has not enough incentive for liquidators(considering 8% premium), it could bring the insolvent situation.\nso…in the addition to Coinbase Pro, using an average price from multiple price feed sources is better for reducing this kinds of risk.\nbtw, adding OKEx feed into designated reporter in UniswapAnchoredView is required to create new proposal? This is one of urgent action we need to avoid potential same types of issues.\n\n\nRemoving the reporter in some/all cases and relying only on Uniswap, or taking an entirely different approach\n\n\noperating only single point of price feed system is not a good idea. (in AMM, LP has no loyalty)\nI don’t have any good idea excepting increasing more reporters in the current open price feed model.\n\nFor compensation, I agree that we can support some amount of COMP for impacted wallet addresses.    As COMP holders, we had to take more interested to make enhanced price feed system rather than leaving only single point of external exchange reporter(coinbase pro).\\nChainlink is the way to go. If you want to see the difference to the open oracle, check here 13. The DAI/ETH price is delivered by 9 oracles and is aggregated (averaged), so large deviations like the one of Coinbase are ironed out. Integration is very easy.\nThis would also come with two other benefits:\n\nWhen adding new coins to Compound you are not limited to the coins which are offered by the Coinbase API. Chainlink offers prices for a much large range of coins.\nYou are no longer dependent on volunteers to post the current prices to the blockchain, which leads to outdated prices. With Chainlink prices are updated regularly automatically.\n\\nEarlier, I shared a brief summary of my thoughts on this matter in Discord. Below, I share an extended perspective.\nSummary of the Exploit :\nIt is clear that this event was a target exploit of the Compound protocol through a manipulated oracle, which the Compound system used to drive liquidation events. The broad market DAI price was roughly $1.03 on all major exchanges except very briefly on the two exchanges which Compound uses for its oracle input, where the price was manipulated to a value of roughly $1.30, causing this cascade of liquidations. The protocol functioned as written, however it cannot be said that the protocol was functioning as intended, as I do not believe that the intention of the Compound protocol was to have mass liquidations due to faulty inputs. In software engineering, this is called a bug and these bugs often lead attack vectors such as the one that currently exists in Compound.\nRemedies:\nCompound is a leader in this space. Accordingly, all eyes will be on the Compound community and the steps we collectively decide to take. Whether Compound remains a leader into the future, or fades, will depend on some key community decisions.\nCommunity:\nFor the long-term health of the Compound protocol (and by extension the value of the COMP token), it makes sense to compensate victims of this recent attack. The reputation of Compound—the goodwill and trust of users—is paramount to the success of the protocol. Damaging this relationship would be damaging to the protocol.\nPrecedent for providing a remedy has been seen in previous DeFi incidents where a faulty protocol weakness was exploited (e.g., Yearn, Harvest, etc). Historically, maintaining this goodwill strengthened these projects technically and from a community perspective. Accordingly, a minor dilution of the COMP token to maintain this goodwill is worthwhile in the long run. By way of example, maintaining a good community relationship effectively saved Harvest . These precedents can be used as a basis to kickstart compensation discussion.\nTechnical:\nThe oracle vulnerability is still a gaping hole that needs to be addressed. Can an attacker still manipulate the price of assets reported by the Compound oracle beyond their current fair market value? If an attacker can still perform this exploit, resolving this issue should be an immediate priority. Moreover, if this exploit is still performable, Compound should not be used by anyone. Additional price feeds are a band-aid solution but may be appropriate in the short term if diverse enough. This is a deeper discussion for the Compound community. If this exploit uniquely affects DAI, then disabling DAI is another option.\nA long-term solution requires investment into a robust oracle system, which is another deeper discussion for the Compound community.\nAnother wise investment may be an insurance fund which may be deployed in the case of unanticipated protocol attacks, however, this is also a deeper discussion.\nThe Compound community should act in the interest of the community . It is wise to proceed strategically, thinking of the long term and what will both remedy the technical issue and promote community health going forward.\\nThere is another aspect to consider here as far as reimbursements go. If all the affected people who are yield farming, kept their COMP, they would actually have decent amount of votes to vote for a good resolution for themselves. (Especially pretty large yield farmer with >30 million supply).\nHowever, if most of the affected people were selling COMP directly into the market they wont actually have any influence on the direction of the protocol, hence can’t really get the resolution that they want.\nThis is a good demonstration that if you are doing riskier things with Compound with large amount of capital, it is a good idea to keep the COMP that you are earning, because you will need it to resolve issues in your favor down the line. (This won’t be the first, or last large issue that protocol will run into)\\nI’m very sympathetic to those who were affected in this event.\nI sense three trends of discussions in the comments that were brought up already:\n\nWhat happened?\nHow do we prevent it?\nIncurred losses\n\nWithout fully understanding the event itself, it would be extremely tough to tackle the other two issues. For the interest of risk management, I recommend that we first focus the discussion on gathering data and fully understanding the event with the goal of turning the discussion into swift actionable items for governance to prevent this from happening again.\nIt would be wise to separate these two tracks to allow the discussion around incurred losses receive the attention it deserves while we can remain swift about risk management.\\nprice1824×590 156 KB\nPer the public doc, the pricing oracle should follow price within Coinbase oracle and Uni (20% weighted average), but seems DAI @1.3 u is highly likely to go beyond range of sanity check,  can u pls correct if any misunderstanding here.\nAlso, when is the potential change plan to take place?\nThanks\\nIn whose favor should COMP token holders vote? Systems like Compound are created so that everyone votes in their favor because they have “skin in the game”. (Correct me if I’m wrong)\nAs for sell pressure I agree with you - maybe some lock period for the affected address would be the solution - of course with COMP use case on Compound(collateral).\\nAbsolutely agree with everything written here. Defending Compound’s solvency is obviously a primary goal, but so should be defending the users of the protocol. The market price of DAI was never over $1.03. Coinbase Pro’s price was off market, and manipulated. What happened here was clearly an exploit. And Compound’s users lost. It is a small price for Comp token holders to take a bit of dilution and make this situation right for the users of the protocol. As 4d mentions, there is precedent for this. It is further salt in the wound that the Compound interface declares using 80% of borrowing capacity as “safe”, when clearly this exploit made that not the case.\nOn the technical front, in the event that there are disparate prices, I would argue the protocol should use the lowest liquid price available for borrowed assets and the highest prices available for supplied assets. Why? Because defending Compound’s users should also be a top priority in conjunction with defending the protocol’s solvency. If there were DAI available on the market for $1.01 or $1.03, then we would presume that these could be sourced to repay the loans. We wouldn’t presume a reasonable actor would wish to find the most expensive DAI in order to repay and make their position whole.\nIn this case, the protocol failed, and used an off-market, high, illiquid price to price the borrowed assets and this resulted in the seizure of assets at a 22% discount, plus the 8% penalty, resulting in a 30% loss of deposited collateral on positions that should never have been liquidated.\\nYou are nicely explain situation. I just add that I lost more than 30% of my collateral because price of gwei was 500+.\nI’m just wondering if the project will compensate for the addresses affected and to what extent. This move will say a lot about the quality and the direction in which the project is going.\\nthat part worked, the oracle rejected 1.3 DAI price, the highest that was allowed was 1.23~\\nCorrect, we must gathered data about losses. But about risk management situation I think that is clear. I mentioned before that in situation where my funds are liquidated because ETH or BTC volatility - it is my fault in risk management. But in situation when I deposit Ethereum and press SAFE MAX and after 2 hours come back and see liquidation on 100% (cca. 5000% -> 6000$) because of DAI manipulation - I don’t think it’s my negligence.\nI would still accept it somehow, but I didn’t have time to maneuver. As soon as I looked at the situation I started collecting ethereum on the accounts to defend myself from liquidation. It ended up that 50% of Ethereum was liquidated for peanuts, I paid a huge gas fee and after 10 minutes DAI became stablecoin again.\nMy opinion is that the purpose of stablecoin is to hold around the dollar however 30% of the volatility of stablecoin is totally unacceptable. That goes against the very purpose of stablecoin - why do we use them?\nImagine a situation where by manipulating DAI grows to $ 2? How many addresses would then be affected?\nI was already two times liquidated but those was my faults, but this one is not.\\nApologies about not being clear; I meant that managing the risk of these liquidations happening again due to this specific oracle issue should be prioritized, I was not making a comment on anyone’s financial risk management.\\nNo bad feelings  ,\nI admit that sometimes I have bad risk management, I commented on your post because everything you said makes sense. I followed up on risk management to present the situation I found myself in - helpless as on a roulette table.\\n\n\n\n dakeshi:\n\nI think that it is not good idea to set capping DAI level because it is designed to accept some volatility and balance its stability with the incentive mechanism.\n\n\nI’d argue the opposite side of this point. If DAI were to ever trade at a true 30% premium for any significant length of time (longer than a few hours), this would most likely result in emergency shutdown, and all outstanding DAI being settled for $1 worth of collateral.\nI don’t think it makes sense to allow for DAIUSD oracle prices above around ~$1.10 because the value for cDAI holders that Compound would be protecting with liquidations is basically fictitious.\\nI remember this in documentation too.  I assumed that the TWAP used an interval window that was a little longer- if the price registered at 1.23 like @blck noted it seems like the time window over which average price is calculated is a bit too short.  Might be good to have it a bit longer so that the window covers a peak volume time and/or integrate a volume weighted element to the feed [ie do a VWAP over the extended time window, or calculate the average price of the last $1mm in  transacted].\\nI’m generally more in the dakeshi camp on this one, but I see your point.  Given that one might be okay with neglecting the trade price for a few hours if it seems “nonsensical” [since we might be willing to cap the rate at 1.1 given that price really shouldn’t be above that level for more than a few hours], perhaps a more straightforward approach might be to use a longer time interval for the TWAP calculation.  This way you allow for the vol/stability of the trade price to get baked in as dakeshi advocates, plus you get some stickiness in price to help it weather some odd short term moves that might take some time to get arbed away through minting etc (may take especially long if network is congested), which is what the cap sort of gets at.\\nTo fix this problem at its core, Compound needs to integrate Chainlink Price Feeds. I am writing this as a daily DeFi user and as someone who only wants the best for the DeFi ecosystem as a whole, especially as the value secured rises. The false liquidation of ~$90M in user funds today was a serious issue that was directly caused by Compound’s centralized oracle solution which pulls market data from only a single exchange, Coinbase, with Uniswap TWAP used as a backstop. Compound’s price feeds provide data that only reflects a small subset of the total crypto trading market and fundamentally cannot provide sufficient market coverage. This in turn lowers the cost of market manipulation and exposes the protocol to inaccurate pricing from large trades.\nSpecifically, Coinbase has an extensive history of downtime and flash crashes, so I am surprised this was not immediately seen during development as being a huge single point of failure. Using Uniswap TWAP as a backstop is better than no backstop in this situation, but it introduces a false sense of security as it too can trivially be manipulated (as we saw during this event). This lack of market coverage allowed a malicious actor to manipulate just two exchanges to skew the price data delivered to the Compound protocol and falsely liquidate users and yield farmers using DAI as debt or collateral. The core issues of price feeds without market coverage are covered extensively in this blog post here 1 which provides context about the importance of data quality for oracles.\n1533×900 108 KB\nCoinbase was the only major exchange that experienced such a drastic price deviation, other major exchanges were unaffected.\nHowever, none of this information I mention above is new, as I have previously pointed out the numerous and specific vulnerabilities in the design of Compound’s oracle that were not and still have yet to be fixed. Here 2 is a tweet thread I wrote on July 21st 2020 on my concerns regarding the Compound oracle and the likelihood of Coinbase experiencing market manipulation/flash crashes, the ability to manipulate Uniswap TWAP, and why taking a simple median across pre-selected exchanges does not solve the issue adequately either. Compound’s price oracles are still highly vulnerable to these issues as we speak, leaving over $3B in user deposited funds at risk of further catastrophic losses, and needs to be fixed immediately. Compound’s price oracle simply does not provide adequate market coverage as it exists today. Moreover, because it requires exchanges to change their API infrastructure to provide signed data that is compatible with Ethereum, the Compound oracle will continue to be inherently limited in the amount of market coverage it can ever achieve.\nChainlink Price Feeds provide an immediate solution to this problem, allowing the Compound protocol to fully mitigate these oracle related issues going forward. Aave, another decentralized money market on Ethereum experienced no price oracle issues during this event or any false liquidations. There is a very simple reason for this; instead of rolling their own oracle, exposing them to wide range of nuanced attack vectors, they simply integrated Chainlink oracles, which has successfully provided Aave users with the true market wide price of both DAI and every other asset on the platform since launch, as well as during this Coinbase/Uniswap outlier flash crash. I implore you to consider the following sections as I describe how Chainlink is resilient to these attack vectors.\nChainlink’s Decentralized Price Feeds are highly accurate and resistant to exchange distortions because they provide full market coverage by using multiple layers of aggregation that smooth outliers and prevent manipulated data from being delivered to smart contracts. This ensures market manipulation on a select few exchanges have no effect on the final data point generated and delivered to contracts. Specifically, Chainlink has three levels of aggregation to prevent the exact issues Compound’s price oracles experienced today.\nimage1600×457 148 KB\n\nFirstly, Chainlink uses professional data providers (CoinGecko, BraveNewCoin, Amberdata, Kaiko, CryptoCompare, Alpha Vantage, CoinApi, CoinPaprika, CryptoAPIs, and more) who whose entire business model revolves around generating high quality data using refined aggregation methodologies. These data providers produce reference prices for cryptocurrencies that reflect the market-wide price by tracking hundreds of exchanges (both on-chain DEXs and off-chain CEXs), taking into account volume, liquidity, time, and other shifting differences across exchanges, preventing any single source of truth.\nSecondly, there are the security reviewed Chainlink node operators (T-Systems, LinkPool, Certus.One, Stake.fish, Chainlayer, Chorus,one, SNZ, Huobi, and dozens more) operated by professional DevOps and blockchain infrastructure teams who aggregate price data from multiple data aggregators and take the median off-chain before delivering the data point on-chain, preventing any single source of truth. These Chainlink nodes are paid for their services in LINK, not only covering their gas costs, ensuring timely and incentivized updates, but providing a source of profit. This creates crypto-economic security by creating a large opportunity cost for malicious activity. Additionally, multiple data providers already operate their own Chainlink oracle node and provide cryptographically signed data.\nThirdly, there are the Chainlink oracle networks (feeds.chain.link) which are on-chain reference contracts that aggregate data from multiple node operators, again preventing any single source of truth. Each Price Feed is updated based on a threshold deviation and a heartbeat frequency, ensuring fresh data that follows market volatility is always available to contracts. These Price Feeds are a shared public good funded by many DeFi projects and already secure over $4B in user funds.\n\nWhat I am proposing here is quite simple. By integrating Chainlink Price Feeds as the primary oracle solution for the Compound protocol, these market coverage issues simply disappear and users can be assured they will not be falsely liquidated (just as Aave can today). Chainlink already supports all of the price feeds the Compound protocol needs on mainnet and integration would be straight-forward, only requiring a few lines of code (docs.chain.link). Additionally, Chainlink Price Feeds can also be used in replacement of Uniswap as the backstop, providing a much more tamper-resistant solution, though being the primary oracle is ideal as it would completely stop these exploits from occurring and ensure there is no period without accurate data. I am writing this as a concerned DeFi user who does not want to see more user funds falsely liquidated due to entirely preventable oracle issues. We are all in this together and I believe that the DeFi community can come together to ensure all protocols are using oracle solutions that are sufficiently secure for the value they secure.\nPlease take what I say with consideration as the value locked in DeFi continues to grow in orders of magnitude. By fixing the issue at its source now, Compound development and governance can focus on and innovate around what assets should be listed and the risk parameters, rather than worrying about how to refund users in the wake of another price oracle exploit.\nI have created a discussion for this proposal here 46.\\nCoinbase looked at the data from many different angles and concluded that not only were there no price manipulation alerts in our trade surveillance software, but we did not find any evidence of collusion or single actors that pushed the price higher. We believe our books properly operated according to the availably liquidity at the time which, in stablecoin markets, may become thin as price moves away due to the collateralized nature of these markets. And, Coinbase Price Oracle accurately reported data throughout the DAI price increase.\nPete Elkins\nHead of Trade Surveillance and Market Health\nCoinbase\\nThank you, for the report, Pete. The thing is that even if we take as a given that price, reported by Coinbase oracle was accurate for the actual market conditions on particular Coinbase market and wasn’t a result of error or manipulation, it wasn’t relevant to actual price of DAI, as other market’s, in particular, Uniswap have not experienced and reported such price. Since it’s critical for Compound to have accurate price at every time, event proved, that Coinbase Price Oracle is not enough by itself  to achive that goal. That doesn’t mean that price reported by Coinbase isn’t important data source, but it looks like just by itself, it’s not going to do the trick.\\n@Elkins\nThank you for chiming in. While I find your discoveries hard to believe, I appreciate you coming here to share your perspective.\nFirst, it is widely understood that the market price was $1.03 during this incident. There were millions of DAI available for sale at this price across nearly every exchange in these moments. Can you please comment on how and why the Coinbase Oracle failed to reflect true market value despite three promised layers 5 of protection against off market data?\n\n\nData quality\nFor an oracle to provide a reliable price feed it is important to address various scenarios in which a data point to be signed does not reflect an actual market price of an asset. There are three layers in the Coinbase price oracle architecture designed to solve this:\n\n\nPrice source . We use the Coinbase Pro API as the source of the price data. Coinbase Pro is one of the most liquid crypto-exchanges in the world. There is already an ecosystem of oracles, market makers and traders that rely on an accurate data feed provided by the Pro API. As such, Coinbase is making continuous investments in the quality of the API itself, as well as the market, as measured by liquidity.\n\nOff-chain filtering . The Coinbase price oracle implements a filtering mechanism that rejects data points that significantly deviate from the expected volatility of each asset.\n\nOn-chain filtering. Compound open oracle’s [DelFiPrice](https://github.com/compound-finance/open-oracle/blob/master/contracts/DelFiPrice.sol) contract implements concepts of an ‘anchor’ source. Data points that significantly deviate from the last price reported by the anchor source are rejected.\n\n\n\nSpecifically I’d like your comment on point #2, off-chain filtering. Everyone knows DAI should trade near $1, and given that it was clear that in this moment, there was a liquidity crisis on Coinbase and Coinbase alone, why was $1.30 allowed to be printed on the oracle? Especially since it was only sustained for a few minutes. The oracle should be smarter.\nThis is completely unacceptable for Coinbase to be so cavalier about reporting local market prices on an narrow illiquid slice of the market when everyone knows the real purpose of the oracle is to report true market value. Your blog post says as much. Again it states “it is important to address various scenarios in which a data point to be signed does not reflect an actual market price of an asset”.\nWhat sanity checks did the oracle perform? Did it check to see if Coinbase was off-market relative to other exchanges? In this case, every other exchange. I’d like to know how this oracle failure happened. I’d also like a full disclosure of how many accounts participated in driving the price of DAI to $1.30 and keeping it there. Please disclose whether there was anything suspicious or any patterns amongst these accounts.\\nHi, Pete.\nThanks for this.\nWas there anything out of the ordinary with regard to the DAI-USDC designated market makers? Specifically:\n\nWere the market markets operating normally?\nWhat kept them from bringing back down the price by buying DAI on the Coinbase DAI-USD book and selling on DAI-USDC?\nThe price dislocation against the wider market lasted for about an hour, so I think market makers would have had time to source liquidity even from other exchanges. Why didn’t they?\nAre you planing on increasing market makers on DAI-USDC to prevent other such liquidity crunches? I analyzed the relevant trades for this event, and buying about 360,000 DAI with a total cost of about 21K USD (paying above $1) is what was required to move the price to $1.3.\n\\nI’m a long time Compound user, but not integrating Chainlink at this point would be a mistake. It’s been mentioned in this thread many different ways already, but if you aren’t getting full market coverage, us users are taking on unnecessary risk. This recent problem with the DAI price being manipulated is proof of that. What would be a good reason NOT to integrate Chainlink price feeds?\\nMakerDAO offered liquidity against USDC, TUSD and PAX with up to ~99% LTV (100x leverage) and no liquidations, so it seems like it should be impossible for the price to diverge this much if deposits and withdrawals were operating normally.\nCan you comment on the existence of any deposit/withdrawal issues during the time period covered? If there were issues, were they caused by technical faults or did Coinbase restrict transfers purposefully?\nThank you for your consideration.\\nEsteemed kybx86 has created a thread “Compensation Proposal: Distribute COMP to Affected Users in the DAI Liquidations” - I suggest everybody to move there, to that thread for the discussion of the specifics.\n\n  \n    \n    \n    Compensation Proposal: Distribute COMP to Affected Users in the DAI Liquidations Proposals\n  \n  \n    Note: I’ve deployed the CAP for this proposal . If you support this, you can delegate to this address: 0x2f04664b18fb9b6d49124fcc876b52a4ba797718 \n\nObjectives: \nFollowing the first proposal to increase the DAI reserve factor with the goal of de-risking the DAI market and mitigating against future improper liquidations, this next step outlines the mechanics to compensate users for funds lost in the liquidation events of 11/26 by distributing 55,255 COMP (0.55% of total COMP supply) to affected u…\n  \n\n\\nHi @cryptoguy123. I’ve been following your work, and I agree that Coinbase’s filtering was poor judgment and candidly frustrating. I was also one of the affected users and I’ve been working with the community on compensation. Not sure if you’ve seen it, but I deployed a proposal 2  to compensate users. It needs the community’s votes to pass. You can view the full details under this forum post: Compensation Proposal: Distribute COMP to Affected Users in the DAI Liquidations 14\\nIdeation for a new proposal that addresses objections with the initial has been occurring in the thread for the previous proposal Compensation Proposal: Distribute COMP to Affected Users in the DAI Liquidations 11\\n1- My thought is that it’s not the role of governance token to compensate for liquidations  or failed liquidations\n2 - In case there is a reserve build( the DAI reserve) it might be used after a vote to compensate for that day, even if it is not in this case a failed liquidation, but it should be voted whether or not the insurance fund should cover it\n3- in case the vote would be to cover it, the coverage shouldnt go higher than the insurance fund taken at the time of the event, should the loss be higher.\ninsurance fund(dai reserve) is there to insure for failed liquidation\ncomp token was there to distribute a governance power to the right persons, not here to be used as an insurance fund, if this changes, it must be decided in the future and not related to a specific event\nhere is my opinion\\nI’d say I agree that deployment of an insurance fund makes perfect sense for covering platform failure / false liquidations over distributing governance tokens. I’d prefer the former over the latter.\n(That being said my #1 preference would be returning the assets seized in false liquidation but I do not know the feasibility of that)\\nI don’t believe the return of the tokens is feasible because I’m pretty sure they’re liquidated by third parties who agree to pay the gas fee (not 100%).\\nI wonder if someone has done a more detailed analysis or researched DAI LIQUIDATION EVENT more deeply? (I do not mean individuals but providers of these types of services - ex. Gauntlet).\nI don’t think it’s more about “peanut compensation” than explaining to damaged users where, how, and why the error occurred.\nI think as members of the Compound community and protocol users we deserve some meaningful explanation. This event is ignored by the big token holders, who even claim that everything worked properly.\nGiven the current losses (ETH - $ 1000), an explanation would be a decent gesture.\n@rleshner - You and the team from Gauntlet have refrained from voting on the compensation proposal. Can we get some real arguments about the liquidation event?\nThank you\\nSo this is just being ignored? Nothing being done? No oracle changes, and no compensation to people who got screwed by Compound’s crappy price oracle?\\nWe can find that crappy oracle in new COMPOUND CHAIN WHITEPAPER. I don’t want to call out people but Compound.finance 1 is obviously in a strong partnership with Coinbase.\nI expected the team from Bankless and the Daily Gwei podcast to comment on the situation but everyone seems to avoid commenting on that.\nBull market is on the way and everyone is happy and euphoric and that distracts them from the real problems that exist in the DeFi sector (which is anything but DeFi).\\nAgreed, I have asked for some more details on how the oracle in the Compound Chain is going to operate (I know the node operators will be posting data, but I assume the reporter is still CB pro) but have not gotten a clear picture.\nIt would be a real shame if the lack of attention to fixing the oracle is simply because of the partnership you mention. I can’t imagine the community at large thinks that is a good idea. The silence is becoming rather frustrating, especially since I’m seeing DAI still prone to volatility on the CB pro market. Here is it today. Clearly this needs to be addressed ASAP.\n\\nCompound looks like an abandoned project. Nobody seems to care. Luckily there are other options out there.\\nAnother proof, that Coinbase is a really bad oracle source. Especially if it’s the only one.\n  \n      twitter.com\n  \n  \n    \n\nWilly Woo (woonomic) 1\n\n Buys on Coinbase are not completing. Coinbase is $350 lower than other exchanges right now, it's throwing off derivative indexes and likely impacting trade algos. https://t.co/m4GdHriGAzCoinbase Support @CoinbaseSupportWe’re investigating an issue impacting transactions on http://Coinbase.com  and the mobile apps. The record of a recently initiated transaction may be delayed in showing up in your Coinbase account. There may also be some issues with some buys completing on the platform.\n\n\n  8:37 AM - 11 Jan 2021\n    \n      \n        \n       754\n    \n    \n      \n        \n       166\n    \n\n\n  \n  \n    \n    \n  \n  \n\n\\nYeah, it’s painfully clear that Coinbase is an unreliable data source. The crashes, the price spikes etc. are becoming more and more frequent.\nWe need to swallow our pride and get integrated with Chainlink ASAP before the inevitable occurs.\\nHello, I hope everyone is having a good 2021 so far. Let’s keep the ball rolling on fixing the issues that were highlighted in the thanksgiving false liquidation event, while the attack vector itself is being fixed.\nHere I’m laying out a quick mock proposal for compensation for the victims of the exploit, which I wish to turn into a full proposal.\nSpecifically I’d like to get feedback from those that were against the previous proposal as this one seeks to address the issues that prevented its approval.\nThe two criticisms of the previous proposal were:\n\nThe distribution of compensation was in COMP\nToo much of it was going to whales\n\nSince it seems compensating in the assets liquidated is infeasible or unpopular, the first issue may be solved simply by drawing from DAI reserves which, based on conversation, seems to be an agreeable solution. It also may have positive secondary effects towards mitigating future exploits of the faulty oracle system assuming this proposal is merged before the attack vector is addressed.\nThe second issue may be solved using my previous computation of raw damages, found here:\nscript: https://pastebin.com/DcsXXxcJ 1 4\noutput: https://pastebin.com/y9s6997E 3 11\nThis is a more accurate computation of damages from the false liquidations than the previous proposal. A side effect of this is that same asset liquidations have drastically lowered damages as the manipulated DAI price was not a factor in these cases. As such the large whale receives a lower, but more accurate, proportion of the total compensation, whereas smaller users receive larger.\nI believe this should be sufficient to address concern number 2. Although I appreciate @wario’s hard work so far, I do not think it makes sense or is necessary to modulate compensation amounts arbitrarily based on user behavior.\nThis proposal would raise the USD compensation amount from ~$6.8M to ~$7.5M if 100% of damages were compensated. If those that were against the previous proposal have issues with this amount, a simple percentage of the total amount should be suggested and agreed upon! (e.g. 90% would bring it down to approximately the previous compensation amount of $6.8M)\nI plan to start working with the original proposal author, @kybx86, to draft a new proposal using this method. The next steps would be to use on-chain data to compute the actual numbers for damages  then encode it into a proposal.\nLet’s come together to make things right with Compounds users and improve the protocol!\\n\n\nToo much of it was going to whales\n\n\nI don’t think anyone in the previous compensation proposal stated that their reasoning for voting against it was because it was “going to whales”. To summarize the reasons stated, it was because most of the compensation would have gone to users egaged in recursive farming, and dumping of COMP rewards. As they were:\na) Gaming the COMP incentives\nb) Not aligned with the long-term interests of the protocol, given the dumping\nAn example to be precise, the largest liquidated account (0x909b), one of such industrial farmers has a raw total damage of 3.8M, according to your computations. So that’s close to half of the total budgeted compensation.\nPrevious thread here: Compensation Proposal: Distribute COMP to Affected Users in the DAI Liquidations 3\\nOkay well I think compensation is warranted even if you just ignore the addresses doing “recursive farming”. Folks who used the platform as intended and got liquidated due to oracle issues definitely deserve reimbursement IMO.\nMaybe just exclude that big address and that’s sufficient? I don’t think anyone wants to manually check the activity of all the addresses in that list to see what type of activity they were doing\\nIt doesn’t sound reasonable to me to exclude compensation for accounts just because they “were big”. I proposed a criteria to determine people doing recursive farming and published the results here Compensation Proposal: Distribute COMP to Affected Users in the DAI Liquidations - #72 by MasterofNonce 5\nI think this can be improved, but the basic idea is indeed to differentiate between accounts exploiting the incentives program and accounts using the protocol as intended.\nUsing such a criteria also results in a much lower compensation overall than what is proposed here and previously, with a total of 2.2M DAI.\\nI don’t totally understand the formula you used, but it’s lowering my reimbursement by like 75%. And I wasn’t farming at all, I just didn’t want to swap my stables for DAI so I was borrowing it instead.\nI should NOT have been liquidated under any realistic scenario, and should be entitled to full reimbursement.\nI’m all for trying to reduce compensation for people not using the platform as intended, but I was and your formula indicates that I was not\\n\n\n\n wario:\n\nIt doesn’t sound reasonable to me to exclude compensation for accounts just because they “were big”.\n\n\nIn the proposal I shared, this is not the case. It is simply a side effect of computing real damages that the DAI-DAI farming liquidations will suffer less damages and thus require less compensation (only the 8% penalty for the false liquidation). There is no artificial limiting of compensation due to the size of a user. Since the large users were behaving this way, the compensation ends up skewed less towards large accounts naturally.\nSorry for not making that clear.\\nNo, that was clear. I was responding to  @tob’s reply there.\nHowever, yo do state that a criticism for the previous proposal was:\n\n\nToo much of it was going to whales\n\n\nWhich I don’t think is correct, as I commented previously. The criticisms were related to what these large accounts were doing.\n  \n    \n    \n    DAI Liquidation Event \n  \n  \n    Too much of it was going to whales\n\n\nI don’t think anyone in the previous compensation proposal stated that their reasoning for voting against it was because it was “going to whales”. To summarize the reasons stated, it was because most of the compensation would have gone to users egaged in recursive farming, and dumping of COMP rewards. As they were: \na) Gaming the COMP incentives \nb) Not aligned with the long-term interests of the protocol, given the dumping \nAn example to be precise, the la…\n  \n\n\\nThe formula as suggested was looking for accounts lending and borrowing stablecoins, which is common practice with these recursive farming operations. This formula, however, can be revised. I think the point to focus on is if there is agreement on determining compensation based on what the accounts were doing. That is to say, to not compensate recursive farmers for the reasons stated when voting against the previous proposal. If there is, then a second step is agreeing on the best way to determine that.\\nThanks for kicking this off!\nIMO this looks like a good start. As far as I can tell the “real damage” is accurate, and I think compensating with Dai reserve seems fair, for both the effected users and the people in the community. The only thing is from what I can tell, there is only ~2M in Dai reserve right now, so the compensation may need to be distributed over time, or be discounted, or both.\nI think it would be useful to get consensus on whether the compensation should be discounted and if so, what the % should be.\nI also agree that it doesn’t make sense or is necessary to modulate compensation amounts arbitrarily based on user behavior. If people have issues with recursive yield farming, the right way to address this is to change the incentive so that yield farmers naturally move away; instead of randomly punish a subset of those people by not compensating in this or other similar events.\\n\n\n\n michjun:\n\nhe only thing is from what I can tell, there is only ~2M in Dai reserve right now, so the compensation may need to be distributed over time\n\n\nGreat idea, because DAI reserve is on the low level now for fair compensation.\n\n\n\n wario:\n\nI don’t think anyone in the previous compensation proposal stated that their reasoning for voting against it was because it was “going to whales”. To summarize the reasons stated, it was because most of the compensation would have gone to users egaged in recursive farming, and dumping of COMP rewards. As they were:\na) Gaming the COMP incentives\nb) Not aligned with the long-term interests of the protocol, given the dumping\n\n\nEvery attempt to minimize compensation lead to new protocol weaknesses. If such behavior is not aligned with the long-term interest of protocol, why it was allowed?\nAgain, I am NOT “stablecoins” farmer but I understand users which are. Stablecoins farming was allowed for the short-term interest of protocol? For pumping TVL and market cap?\nI think the short-term and long-term interest of protocol wasn’t aligned. and too much focus has been placed on stablecoins farming without giving users a tool with which to neutralize or mitigate such aggressive liquidation.\nI contacted Defi Saver (asset management dapp), which have an automated way to repay debt in case of volatility. They argue that even their solution would not stop the Coinbase oracle error.\nKain from Synthetix had a great statement that wasn’t aligned interest of protocol users and COMP holders. But, in DeFi space we shouldn’t have separate groups in that way. In Compound case there is huge bug/error in decentralized economic incentives.\\nWhat  is happening with this? I feel like it’s being ignored / forgotten\\nI think it isnt. Problem needs to be solved from root (COMP distribution, dumping COMP from exploitatiors,oracle fixing).\nBut if affected users will not be compensated due to obvious manpulation (some people said that there is not evidence for that - off course because Coinbase dont want to provide data) I will move all my funds to Aave.\n@rleshner always speaks that users need to be in governance of protocol but in practice we are hostage of early investors.\nOne reason for drop out proposal 32 is that users will be compensated in COMP token. Am I crazy?\n\nstatement - users need more governance power\nstatement (from VC funds-not @rleshner ) - we dont like idea to compensate affected USERS in COMP token.\nIts obvious that affected addresses = USERS, because if you are not user of protocol you cannot be affected by “DAI manipulation”.\nLots of contradictions and inconsistencies in the statements of the people who should keep the true users of the protocol. They really don’t do that.\nI expect the Gauntlet team and @rleshner  not to abstain from voting next time because we want to know in which direction this protocol will develop.\n\\nSo this is dead? Nothing is going to be done?\nThis really disappoints me, I don’t think I will use Compound again. Nothing has been done to address the exploit, and there’s just no accountability from Compound after providing an unsafe platform and causing folks to get liquidated even when using the “safe borrow percentage” recommended by Compound. Lending a stable and borrowing the “safe amount” of another stable should never lead to liquidation in the manner that occurred here.\nI find it totally ridiculous that much smaller, less successful platforms like Harvest and Pickle fully or partially compensate users who experienced an exploit, while Compound, a leader in this space, chose to do nothing and ignore the issue. It would not have been that hard to issue vCOMP that vests over time. Or providing some other kind of restitution (I liked 0xb1’s proposal here https://twitter.com/0x_b1/status/1356328468284387331 6)\nConsidering how successful Compound has been, purely based off of the historical activity & support of the user base, this really just feels like a HUGE let down. I’m extremely disappointed.\n@rleshner\\nI agree, users are not liquidated through their own fault, the error is by the protocol. Honestly, I am patiently waiting for the repair of the price feed oracle and next prosposal. If the losses of users (which have become very large with the increase in Ethereum prices) are not compensated, I will certainly transfer my funds elsewhere.\nIt is a shame that Coinbase gives a statement that everything worked properly and most WC funds (which have a protocol) support that.\\n\n\n\n tob:\n\nOr providing some other kind of restitution (I liked 0xb1’s proposal here https://twitter.com/0x_b1/status/1356328468284387331 )\n\n\nis this real? When will the intention be explained in the forum in a normal way?\\nNot sure, I was expecting it to get proposed by now\\nwas just reading this thread, it seems like in all the discussions here including this thread, we are just talking to our selves?  There seem to be zero input or response from dev, even when it’s crystal clear what a fix/solution is in this case and the serious nature of the issue.  Do devs just not read this forum?  Where can i find dev discussing the ongoing issues/fixes.\\nYep, they just decided to ignore this glaring issue and not provide any restitution for any affected folks. Pretty shitty if you ask me\\nBumping this thread again as it is imperative that the huge security hole in the protocol is fixed and that the users who were falsely liquidated receive adequate compensation.\nPersonally I’m surprised to see it remain unsolved for 5 months but it does seem there is ongoing discussion on addressing the faulty oracle system. Hopefully it is soon safe to use the protocol once more and we do not see the first billion dollar DeFi exploit.\nIn the meantime, what does everyone think of 0xb1’s compensation proposal? @kybx86 @rleshner @wario @mike-u410\\nagreed- we need a response from those folks. Can’t believe it’s been months and this still hasn’t been fixed\nRandomly saw this posted- clearly everyone seems to be aware of this issue. Seems like the only people putting large amounts of funds on Compound are huge whales that don’t borrow anything, or people that don’t realize the risk they are taking.\n\n  \n      \n      reddit\n  \n  \n    \n\nr/defi - Aave vs Compound? 20\n\n0 votes and 7 comments so far on Reddit\n\n\n  \n  \n    \n    \n  \n  \n\n\nThis needs a fix, and I think 0xb1’s proposal is totally fair\\nI agree, this has become a peripetia. First because the exploit happened when the price of Ethereum was around  400, so the losses to damaged users are very high when the price is  2100.\nSecond, we have not received an explanation or statement from individuals who are large holders or founders of the protocol. They are wisely silent and play a political game with users. I think all users should ask themselves what to expect when a similar exploit or hack occurs (both those that are damaged and those that are not).\nSmaller DeFi protocols were much more transparent and open to users when a protocol failure occurred.\nUnfortunately, the Compound.finance strategy is focused on the interests of early investors and is constantly patched up with Coinbase.\nI am personally considering other options and am waiting for a cheaper gas fee to move away from this CeFi project.\\nRecent examples of protocols taking responsibility for their faulty oracles leading to false liquidation:\nNexo on Twitter: “Due to a third-party service anomaly, the $USDT price deviated from its market value & some Nexo clients’ balances were incorrectly liquidated. All incorrect liquidations will be reversed & your wallets’ balances will be restored shortly. We apologize for any inconvenience.” / Twitter\nCelsius on Twitter: “Update - An error for $MATIC and $SNX price feeds from our 3rd party provider triggered margin calls earlier today. The issue is now resolved without any liquidations, and our team is working on resolving any margin calls that were issued as an error.” / Twitter 2\nLooks like the ball is rolling in terms of fixing the oracle in another thread. Hopefully Compound can step up and do the same. These protocols took responsibility the day of, yet here we are 7 months later.\\nWere there a DOT market on Compound… we all know what would have happened yesterday.\nCrypto₿uzz on Twitter: “#Coinbase glitch has $dot down 77% https://t.co/kH67Ey5awl” / Twitter 6\\nThis is just laughable that the Compound team are still ignoring this issue when it cost many users tens of thousands of dollars of losses, and that no fix has even been put in place preventing this from happening again.\nI can’t believe anyone (let alone billions of dollars) trust Compound with their assets when there are such obvious flaws that could cause anyone using the platform safely to get liquidated even if they are using it safely, and paying attention to what’s going on in terms of asset prices in the general market. Especially now that they know the team will ignore the issue and not do anything to make it right.\\n\ntens of thousands of dollars of losses\n\ntens of Millions. 10% of user funds were falsely liquidated ($100 Million of $1 Billion) and that is not accounting for the fact that it was right before the bullrun kicked off, so liquidations may have been at prices a fraction what of what those assets are valued at now.\\nNo report was made about that event. Ignoring this exploitation is a political decision of several VC funds while Gauntlet and founder wash their hands and  talk fairy tales about decentralization.\n\n  \n      \n\n      rekt\n  \n\n  \n    \n\nRekt - Home 5\n\n  DeFi / Crypto - Investigative journalism & creative commentary\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\\nI am very likely going to end up suing Compound due to the losses here and failure to protect user funds. If anyone wants to join the suit feel free to message me.\nIf the Compound team had done something at all to make this right, even if not fully rectifying the loss, I would have been fine with it. Reimbursing the 8% liquidation fees would have been totally reasonable in my opinion, despite “the longer term losses” folks suffered due to the rapid increase of the non stable assets since these liquidations took place.\nBut doing nothing and saying that things worked as designed, when there were clearly problems with the platform, and now making the changes 1 that we requested many months later while basically ignoring the damages we incurred that led to the changes is just infuriating and rubbing salt in the wound.\nNot to mention the Compound community furiously applauding the guy who put the proposal together and giving him $150k 8 for his hard work to “improve the platform”.\nThis is a total slap in the face to anyone who lost funds during this attack and was just “using the platform as designed”. I myself was borrowing DAI against other stables because I didn’t want to pay to swap them. It would have been (and still is) cheap to address this issue with:\n\nany of the various proposals put forth in this long thread (my preference is this one 3)\nan apology to the people who were affected\nand a thank you for bringing this serious problem to light that helped make the platform stronger & better in the long run\n\nInstead they just ignore it. Not sure why that is the chosen approach, when the negative publicity of a lawsuit far outweighs reimbursing the non-farming addresses in the list. Indeed, the largest wallet in this list of affected people seems to have stopped using it entirely. Reimbursing the rest is a tiny cost to Compound at this point.\nI am quite serious about this and hope that my comments bring this discussion back online so we can get a resolution in the near future and I don’t have to take any additional action to get reimbursed for my stolen funds.\n@rleshner @eddylazzarin @mike-u410 @hayesgm @franklin-pantera\\nThis was not ignored what so ever. Proposal 32 5 was made to reimburse users who lost funds and the proposal failed. The Compound protocol is run by a community not by an individual point of contact.\nIt is totally possible to revive this thread and get it done now that the oracle has been changed. @kybx86 have anything to say now?\\n\n\n\n tob:\n\nReimbursing the 8% liquidation fees would have been totally reasonable\n\n\nI would not agree with that. The redesign of the oracle price feed indicates that damaged users are not to blame for being liquidated (as some members have argued). What about  users with liquidated Ethereum positions? The difference between the current ETH price and the price from 7 months ago is quite large.\\nWell 8% would have been the minimum I’d expect.\nWhile it sucks that some of the assets that got liquidated went up in value that isn’t really Compound’s fault. You could have taken the DAI you ended up with and rebought your other token if you so chose to.\nConversely, if they had gone down in value after, would you reimburse Compound for it?\nI think reimbursing the liquidation fees at a bare minimum would go a long way towards appeasing people. Ideally they’d reimburse the liquidation fees + the amount of DAI that people got shorted since  we “bought” it at 30% above market value. I’m sure there is enough in the treasury to do that, and if not they ought to do it with COMP.\nHow anyone continues to have faith in a protocol that doesn’t protect it’s users and doesn’t take responsibility for it’s mistakes is beyond me\\n\n\n\n tob:\n\nisn’t really Compound’s fault.\n\n\nthe fact is that I would have those funds today in case the protocol worked properly.\nA protocol error caused that loss\n\n\n\n tob:\n\nConversely, if they had gone down in value after, would you reimburse Compound for it?\n\n\nI already did it, 7 months IR + 8%\\n\n\n\n tob:\n\nNot to mention the Compound community furiously applauding the guy who put the proposal together and giving him $150k  for his hard work to “improve the platform”.\n\n\nNo idea why you’re taking a swing at Getty. He took it upon himself to help improve the oracle situation, this is a benefit to all users.\n\n\n\n tob:\n\nI myself was borrowing DAI against other stables because I didn’t want to pay to swap them.\n\n\nThis does not pass the straight face test  Take some responsibility for being a leveraged farmer.\\nI was not a leveraged farmer. I was not even leveraged. I was borrowing DAI to farm on another site against my staked USDC. Totally ridiculous to get liquidated in this instance.\nI’m not taking a swing at Getty. Just pointing out that the platform seems to be rewarding him fixing this problem, but not doing anything at all to help those of us who actually were hurt by it\\nI am in agreement here. If this is not satisfactorily resolved I will join you in pursuing legal action, so feel free to loop me in.\nThat being said, I believe we should give those that were against the compensation proposal by reason of the fix to the oracle issue out prioritizing victim compensation to be true to their word.\nNow that the protocol failure has been resolved on the technical level we may revisit the topic of compensation for users affected in said protocol failure.\nI think your proposal is reasonable, however I would suggest using my computation of damages over the original flat 8% as it is more accurate. Other than that it sounds like the bare minimum that one would reasonably expect in a situation like this. Hopefully we can get @kybx86 back on board!\\nSince the Oracle issue is solved, I would suggest the Compound management would look into the compensation issue. Robert @rleshner please publicly tell your opinions about the situation? If a new proposal would be made would you be for the compensation or against it?\\nReintroducing the Compensation Proposal: Nov 26th, 2020 Oracle Issue Affected Users in DAI Liquidations\nHello everyone,\nThis post is intended to reintroduce and improve on the compensation proposal (“Prop 32”) in December 2020. To recap, on November 26th, 2020 an unexpected increase in the DAI price to $1.30 on Coinbase Pro led to 85.2 million in DAI being liquidated.\nThe original compensation proposal did not pass an executive vote 1, with 680k COMP voting against and 212k COMP voting for.\nOn June 21, 2021, the Compound community passed an update to Compound’s oracle system. This update implemented Chainlink Price Feeds over a custom oracle implementation that relied only on Uniswap and Coinbase Pro. While not the sole purpose, the oracle update was a byproduct of the DAI liquidation event on November 26, 2020, that liquidated DAI borrowers due to an adverse and unexpected increase in the price of DAI on Coinbase Pro.\nAdditionally, at the time of Prop 32, the DAI market’s reserves lacked the capitalization to properly compensate users. The prior approach of Prop 32 was to use COMP from the Reservoir, valued using a time-weighted average price, to compensate users affected by the liquidations. However, thanks to Prop 31, which increased the DAI market reserve factor from 5% to 15%, today, the 14 million DAI in reserves is sufficient to cover the losses as originally calculated in full. The total expected compensation amount is approximately 6.8 million DAI based on the protocol’s 8% liquidation penalty.\nThe oracle fix and the increased DAI reserves address three key issues 2 voiced by the community with the previous compensation proposal:\n\nReimbursement to users before clarity on when/how the underlying issue would be fixed.\nReimbursement denominated in COMP to affected users may not necessarily align with the objectives of COMP usage or COMP holders.\nSetting a precedent that tail-risk events should be subsidized with COMP.\n\nA new proposal would focus on using the DAI market reserves to compensate users (though using COMP to compensate users is still an option). No COMP will need to be distributed under this model. Further, the reserve functionality of money markets was built for situations like these, where those unintentionally liquidated can be compensated.\nCompensation is well within the capabilities of Compound governance today and will help give closure to a topic that’s still a point of ongoing discussion in the community and allow the protocol to move forward on stronger footing.\nMaybe we can move forward with a proposal that will use DAI from the reserves to compensate those affected by November’s liquidations in full. What does everyone think?\nDisclosure: For full transparency, I was one of the Compound users affected in the DAI liquidation event. I worked with the community to pass a reserve factor change 1 to the DAI market and led efforts for Prop 32 1.\\nI am fully in agreement with your reasoning and suggested resolution. It seems appropriate, and would go a long way towards rebuilding trust in the platform, and ensuring that Compound is a viable protocol that people can feel safe using going forward\\nCorrect me if I’m wrong, but isn’t the point of building reserves to protect lenders in the case of a bank run?  Since the reserves would likely not be withdrawn by governance during a crisis, the reserves help ensure that there is always market liquidity (reserves equal to the borrow amount means that all lenders could remove there liquidity without issue).  It also helps borrows by buffering the interest rates in extreme scenarios.  With that being said, I would suggest that if we compensate users, we should issue them COMP instead of touching reserves.  Building reserves is part of the long term health of the protocol and the reserve factor is the only way to capitalize them at the moment.  In the spirit of the name of the protocol, we should lean into the Compound interest and let the reserves grow untouched as long as we can.\\nWhy? What’s the reason that it will pass now rather than before?\n@eggbagels\n\nCorrect me if I’m wrong, but isn’t the point of building reserves to protect lenders in the case of a bank run?\n\nWell, yes, but also to let borrowers borrow more if there is not enough liquidity\\nHello fellow community members.  I am happy to see this topic being discussed again.  I manage an investment advisory firm in the US and I have been helping family offices and ultra-high net worth clients experiment with DeFi.  A handful of these clients were borrowing DAI on compound last November and were caught up in the liquidation.  None of them have computer science backgrounds, but all of them individually reached the conclusion that a $1.30 DAI price was the result of manipulation and not natural market forces.  Since this event I have been consistently fielding questions as to why I continue to label Compound as a blue chip DeFi protocol and why I continue to recommend its use to clients.  I want our firm and our clients to stay involved with Compound because I believe the community leaders at Compound are among the most innovative in the industry and that they have pioneered market designs and governance features that have been widely replicated by other projects in the ecosystem.\nI am comfortable with the thoughtful and slow moving approach that the community takes towards managing Compound.  However, the optics continue to worsen as we see other DeFi protocols suffer exploits that are very rapidly addressed and reimbursed by governance.  A few examples are Yearn (February, 10M DAI), Rari (May, $10M in ETH) and ThorChain (July, 4000 ETH).  Of course the list goes on and this sets a precedent of how users and governance tend to treat each other in this new industry.\nFor all of my affected clients, the money means very little as we were responsible in sizing these experiments.  The more important aspect is the tone this sets with those affected regarding Compound’s reputation.  On Wall Street, reputation is all you have and once it is gone it may not be recoverable.  I wanted to share this story and reiterate my admiration for this protocol and community.  I believe the 6.8M DAI expense of putting this event behind us is a good deal in comparison to the protocol’s reserves and the $1.4B in treasury funds controlled by governance.  Onwards and upwards!\\n\n\n\n kybx86:\n\nReimbursement denominated in COMP to affected users may not necessarily align with the objectives of COMP usage or COMP holders\n\n\nSo it is not in the interest of COMP holders (early investors) for Compound protocol users to be paid in the COMP token?\nSome users who were damaged in that event were liquidated positions in Ethereum, which are currently very large losses. Stablecoin compensation covers about 5% of the actual losses for these users. and represents only the good intentions of the community and the fact remains that the affected users were simply unlucky.\nNow the question is what can users expect if a similar situation occurs in the future?\nTo me this looks like a traditional casino and a protocol like Compound does not need such a reputation.\nIf community members think 8% is fair compensation I will respect that but if the liquidations happened with a DAI price of $ 1.3 do you really think that’s fair compensation?\nThese are protocol users who are also the owners of part of the protocol (probably according to the ideology used as promotional material) and such decisions will best show what kind of relationship as users we can expect.\\n\n\n\n TennisBowling:\n\nWhy? What’s the reason that it will pass now rather than before?\n\n\nBecause oracle has fixed now. If everenthing was worked properly, why we need other oracle solution?\\nThanks for everyone’s comments. I’m taking into account the feedback and planning to work on a more formal proposal along with the code in the coming weeks. I’ll keep this forum posted.\\nI will also post info from @kybx86 here. I ask the community delegate votes.\n\"The CAP has been deployed here  2.\nThis CAP needs 65k COMP delegate votes until it becomes a formal proposal.\nIf you support this proposal, please consider delegating to it.\nVIEW CAP & DELEGATE  2\"\\n@Dmitry Thank you.\nThe CAP is live and it needs votes to get to 65K so it can be made into a formal proposal.\nThe CAP already has 3 votes and ~12K COMP, please delegate to it to get it across the line:\n\n  \n      \n\n      compound.finance\n  \n\n  \n    \n\nCompound 5\n\n  Compound is an algorithmic, autonomous interest rate protocol built for developers, to unlock a universe of open financial applications.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n@dabar90 @4D_compound @monet-supply"
  },
  {
    "number_of_comments": 25,
    "postid": "c7bb732a-d2f4-445b-aa90-4fb71f252397",
    "posturl": "https://www.comp.xyz/t/legacy-market-migration-wbtc/1333",
    "combinedcontent": "Legacy market migration plan to begin with WBTC and continue with BAT, ZRX, USDC, and finally ETH.\n\nOld version of cToken contracts as mentioned above, are all immutable the governance cannot implement new features into them.\n\nSome of the features that are available and are under developement like:\n\n\nsweepToken 31 ( recover accidentally sent tokens )\nsupplyCap ( risk management )\nprotocol liquidation fee into reserves ( portion of the liquidation incentive could go into reserves )\n\nThe proposal will add a duplicated cWBTC market using the latest upgradeable cToken contract and implementation, which inherits the same propertys as the current cWBTC market.\n\n65% Collateral Factor\n20% Reserve Factor\nmodern interest rate model ( see below )\nCOMP distribution will be moved to the new cWBTC market, this incentivize the users to migrate into the new market.\n\nOpen Oracle\n\nNo changes needed, the current oracle already knows the underlying address of cWBTC\n\nDeployed and verified contracts for the proposal:\n\n\nCErc20Delegator 8 (cWBTC)\n\nCErc20Delegate 8 (Implementation for cWBTC)\nJumpRateModelV2 11\n\nThe new interestratemodel have  the following parameters:\n\nkink at 80% utilization\n2% base rate\n20% rate at 80% utilization\n40% rate at 100% utilization\n\n\nimage946×389 14 KB\n\nPrevious Discussion 22\nEdit:\nCErc20Delegator was re-deployed with correct initialexchangerate parameter\\nIts about time we move away from the old contracts. I would love to see a little more research/explanation for the IRM, but other than that lets go for it!\\nI’m also interested if there might be better IRM parameters.\nHowever, the proposed on is much better than the linear to 40% used by currently used by WBTC.\\n@blck will this resolve the recursive liquidate issue for wBTC too? I’m assuming so.\n\n\nIn-kind liquidation for older markets #37 12 (github issue)\n\\nyes it fixes that issue too\\nI just reviewed the newly deployed contracts. All were deployed correctly with the expected parameters. Below are screenshot from matching.\ncWBTC: 0xccF4429DB6322D5C611ee964527D42E5d685DD6a\n\nScreen Shot 2021-03-14 at 4.04.56 PM2880×116 165 KB\n\ncWBTC implementation: 0x24aA720906378bb8364228Bddb8CAbbc1f6Fe1ba\n\nScreen Shot 2021-03-14 at 3.30.22 PM1396×66 48.7 KB\n\nJumpRateModelV2: 0xF2e5dB36B0682f2CD6bC805c3a4236194e01f4D5\n\nScreen Shot 2021-03-14 at 3.31.40 PM2880×128 93 KB\n\\nI might be missing this - but to be clear - we’ll need to pay the gas fee to withdraw from the old contract and the gas fee to deposit into the new contract?\nWould this also mean that in order to migrate, if someone has borrowed against WBTC, they’d need to repay the loan, to withdraw the collateral to resupply to the new contract?\\nFor the time being, users do not need to take any action. Both WBTC markets will function normally for a while.\nBefore any further actions are taken, I hope we see a migrator contract which enables migration without having to payback existing loans.\\nI would hope so as well - considering gas prices - I’m afraid if there isn’t a migrator we might see a large egress of funds. Otherwise, we may want to consider a gas repayment program similar to Balancer’s.\nWith regards to the parallel contracts, what is the timeline for the comp distribution switch? I’m assuming immediately upon deployment?\nI only ask since I have quite a lot tied up in WBTC, USDC and ETH which would all need to be migrated if I want to keep earning Comp for voting/supplying.\\nThe COMP distribution switch is immediate to softly encourage migration to the new market. I will advocate for the introduction of a migrator with gas fees subsidized by the community before anyone is forced to move markets (decrease of collateral factor). Again, I don’t see this happening for a while, so no rush to do anything immediately.\\nthoughts on delaying the comp distribution switch till after a migrator contract is made? otherwise this is advantageous to those users who are comfortable interacting with smart contracts directly\\nalso, i’m slightly concerned that we’re doing our first legacy market migration for a large market like WBTC. why not start with BAT or ZRX?\\nCompletely agree that starting with a smaller market would have been ideal.\nI think that switching over COMP distribution now is ok. The new WBTC market will be shown on Compound frontends, so users will be able to switch their positions over using the normal interfaces as their leisure. Hopefully we will have a migrator soon, and its a must prior to any CF changes.\\nHi, I am wondering is it possible to develop a migration zapper so it won’t affect current loan collateralized by legacy cWBTC token.\nOtherwise, we need repay the loan first and then switch the WBTC markets. The gas fee could be too much. Thx a lot.\\nDoes this require an unstake and restake? (2 gas fees)\\n\n\nRedeem cWBTC for WBTC\n\n\nApprove new cWBTC for spending\n\n\nMint new cWBTC\n\n\\nyea this is 3 gas fees! But for what purpose?\\nSince this change is completely on Compound’s side, I think it is necessary to at least take measures such as having Compound pay for Gasfee.\nFrankly speaking, I think many of the recent changes, such as the last wbtc collateral rate change, are burdensome to users.\\nRight, there does not seem to be an explanation of anything anywhere.\\nthis couldn’t wait for L2? Given the gas fee …\\n\n\n\n brad5155:\n\nyea this is 3 gas fees! But for what purpose?\n\n\nIt’s a good question actually. You see, there is an actual value in an immutable contract. And it’s that the rules are set and can not be changed for whatever good or bad governance thinks. Frankly, for small user its pretty much always a better deal to have immutable code, rather than upgradeble.\nRemember story of personal income tax in US? It wasn’t there at beginning. Then it was pitched as temporary measure, and that pretty much nobody would pay it except super rich. And it was small like 1%. As soon as it was introduced it was quickly adjusted in following years, and now pretty much everybody pays it except super rich, who usually don’t really have that much personal income to be taxed  What that story teaches us?\nThat it’s much better to have rules set in stone, than in the hands of governance, which you really couldn’t control \nSo yea, thre might be some cherry on top which upgradebility allows, but i’m not sure it’s worth giving away control. But thing is, you don’t really have a choice. You can advocate different vision in forum, or can try to pitch your ideas, but in the end, i really doubt you can stop the transition to upgradeble cTokens. \\nTotally agreed. If it is unstoppable, a migrator is needed ASAP.\\nThank you for the reply, what are you suggesting? That I unstake and restake to new contract? or wait for a smoother transition?\\nIt depends how you are using the WBTC market. If you borrow WBTC for shorting btc and collecting COMP from it, than it makes sense to switch faster, as there is no COMP distribution on legacy market anymore.\nIf you, however supply WBTC and using it as collateral, than there’s no rush for you really. Both markets will exist for some time, and both will go the job. Eventually  governance will start pushing people out of legacy one by slowly decreasing collateral factor. It would take a time however (not days, weeks), and you will know it’s happening if you check governance from time to time for new proposals. If you using WBTC as collateral and your position is small enough for gas fees to matter, i’d wait for migrator solution. Or optimism supposed to land on ethereum somewhere next month, which should decrease gas prices too, as some projects will start to move to L2, which also helps.\\nHi there, any update on a migrator solution?\\nQuoting from a Discord discussion:\n\nthere is a community member built migrator that can do this\n\nhttps://goldenagellc.github.io/Compound-CERC20-Migrator/ 24\n\nread it carefully\nand validate by yourself that its legit if you can\n\nIt looks like one user has tried it out so far based on a review of the contract transaction history on etherscan."
  },
  {
    "number_of_comments": 10,
    "postid": "3fb148b5-0214-46e7-9fdd-28b1392f4991",
    "posturl": "https://www.comp.xyz/t/enable-cheap-private-transactions-with-compound-using-aztec/2997",
    "combinedcontent": "Introduction\nAztec is dedicated to bringing programmable privacy to Web3. The team invented PLONK — the paradigm-defining universal zk-SNARK — which is now used by leading zero-knowledge scaling projects. Over 45,000 users have deposited 15,500+ ETH and $11M DAI through zk.money 20, a private transfer protocol built on Aztec, since launching in 2021.\nThe team is launching Aztec Connect, which will offer earning, lending, borrowing, and swapping with gas savings and privacy by default via Aztec’s zkrollup and bridge contracts. With Aztec Connect, users can deposit into and borrow from Compound with 95%+ gas savings compared to mainnet and benefit from privacy by default.\nProposal\nMany users have been priced out of using Compound and are regularly asking for an L2 deployment. However, Compound is focused on building Gateway to offer cross-chain interest rates. Because Aztec interacts with existing L1 smart contracts, it can provide users with a scalability solution — without needing to redeploy any smart contracts or rebuild liquidity pools.\nHow it works: Users will deposit ERC20 into Aztec and initiate the Compound deposit or borrow request via zk.money. Aztec’s zkrollup will batch transactions across users and interact with Compound’s L1 contract via a bridge contract. Users will receive a cToken on Aztec - or whichever assets they borrowed, which can be used across other DeFi protocols on Aztec.\nAztec wants to make it seamless for users to deposit and borrow assets with Compound. Aztec is requesting a $100,000 grant to subsidize Compound deposits and loans. This subsidy will simulate the gas savings that will benefit users when the rollup is full and the gas cost associated with the rollup — kickstarting the network effects for this integration.\nAztec can drive substantial adoption and deliver an excellent ROI for Compound. With a $100,000 grant, there will be minimal transaction costs for up to 40,000 transactions on Aztec. The scenarios below show outcomes from 7-1000x ROI, depending on gas savings provided and user adoption. Importantly, the subsidy will only be spent as users adopt the network.\n\nScreen Shot 2022-02-24 at 12.56.11 PM1982×736 112 KB\n\nA successful outcome of this grant would be:\n\nReducing transaction fees by 95%+ compared to mainnet\nIncreasing Compound’s user base by 10% (30,000 users over the next 12 months)\nGrowing to >100 transactions per day so subsidy can be phased out\nEnabling institutions to deposit/borrow from Compound privately\n\nTimeline and milestones\nIn Q4 2021, Aztec collaborated with Adam Bavosa and Nick Martitsch on the spec for the Compound integration and launched its grants program 7 to encourage community development of bridge contracts. Joe (co-founder) and Lisa (head of business development) presented on the Compound community call and met allthecolors, who built the Compound deposit bridge.\nIn Q1 2022, the bridge will be completed and Solidified 8 will audit the bridge. Once the contract is audited, Aztec will integrate into zk.money on mainnet. The initial functionality will be depositing DAI.\nIn Q2, the borrowing contract will be completed, audited, and launched on zk.money. Aztec will also expand the scope of the grants program to encourage development of alternative interfaces with the Aztec SDK. In Q3, the team will focus on integrations with custodians and platforms focused on an institutional user base.\nWe propose earmarking a $100,000 budget with $50,000 distributed upfront and $50,000 released three months after launch. Aztec would like to collaborate to ensure the grant is optimally allocated across different assets and transaction types, promote the integration to users, and integrate Aztec into interfaces that provide significant volume to Compound.\nBefore the second tranche is released, Aztec will provide the Compound community with updates on user adoption, network performance, and subsidy spent. If the community is not satisfied with the metrics from the first three months after launch, there may be another governance vote deciding not to distribute the second tranche of funding.\nConclusion\nAztec aligns well with Compound’s current focus on strengthening its protocol on mainnet and building for new use cases, such as onboarding institutions to DeFi. Today, Compound users will benefit from the scalability benefits provided by Aztec’s rollup. Over time, we envision Aztec being a critical component of Compound’s institutional offering. This partnership will accelerate the user adoption and enable privacy as a new capability for the protocol.\nWe look forward to hearing your thoughts, feedback and suggestions in this forum.\nRelevant Links\n\n  \n      \n\n      aztec.network\n  \n\n  \n    \n\nAztec 11\n\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n  \n      \n\n      zk.money\n  \n\n  \n    \n\nzk.money · Private crypto payments have arrived! 20\n\n  Checkout zk.money by @aztecnetwork. You can now send crypto privately \uD83D\uDD75\uFE0F.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n  \n      \n\n      Discord\n  \n\n  \n    \n\nJoin the Aztec Network Discord Server! 2\n\n  Check out the Aztec Network community on Discord - hang out with 9,199 other members and enjoy free voice and text chat.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\nhttps://twitter.com/aztecnetwork\n  \n      \n\n      GitHub\n  \n\n  \n    \n\nGitHub - AztecProtocol/aztec-connect-bridges 5\n\n  Contribute to AztecProtocol/aztec-connect-bridges development by creating an account on GitHub.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nTeam\nZac Williamson (cofounder, CEO) - https://www.linkedin.com/in/zachary-williamson-b02b0192/ 4\nJoe Andrews (cofounder, head of product) - https://www.linkedin.com/in/joe-andrews-2783918a/ 3\nAriel Gabizon (chief scientist) - https://www.linkedin.com/in/ariel-gabizon-6b423a8/ 2\nLisa Cuesta (head of BD) - https://www.linkedin.com/in/lisacuesta/ 2\nJon Wu (head of growth) - https://www.linkedin.com/in/jonathanjwu/ 3\\nThanks @lisa - the opportunity to make Compound accessible again to smaller market participants is what pushed me over the edge to double down on Solidity skill-building so that I could draft this bridge. Even better, Aztec’s approach is privacy-oriented and, unlike most of the alternatives the community has discussed, it doesn’t require fragmentation of liquidity or reserves.\nFolks are welcome to review the draft PR for a Compound Aztec connect bridge 7; please post questions, suggestions, and other feedback as comments on the PR or to the development or ecosystem channels on Discord. Insights from those who have experience deploying contracts that interact with the protocol directly will be especially helpful here.\nIf the community agrees to funding a bridge subsidy, what do folks think about this proposal being funded with an allocation from the ETH that Coinbase returned to the timelock after the expiry of Compound’s Coinbase Earn program? The funds will be used to pay for the bridge’s gas, so funding directly in ETH would sidestep the need for either protocol to execute a trade. At current prices, the request amounts to well under 1% of ETH sitting in the timelock.\nOne barrier to funding directly in ETH is that the protocol currently can’t move ETH from the timelock, as pointed out by @arr00 2; however, he already has a PR to correct this technicality. I’m not sure if there’s time to merge this PR before the target go-live time for the first Aztec bridges, but we could ask @cylon and the OpenZeppelin team to prioritize if we felt this was a worthwhile direction to go for this proposal. If resolving that issue is still months out, then funding in COMP remains an option.\\ngood stuff. I think if we want to x scalability we should go with something privacy, don’t know enough about zk stuff to say which is better but I’m in\\nAppreciate you weighing in and your contribution! As you mentioned, we’d ultimately need ETH for gas so we’d be supportive of receiving the funding in ETH or swapping COMP for ETH. In terms of timing, we’re hoping to secure the funding in the coming weeks (aiming for mid March) - so I’ll let @cylon chime in on the timing for the PR.\\nHi, we’re still figuring out the appropriate pipeline for the next audits given some concerns that have come up around asset listings during our Comprehensive Protocol Audit.\nThe soonest we could expect to audit something next would be Mar 21st but there might be other items that will be prioritized. I’ll be sharing more in the forum this week.\\nScalable privacy! L2’s are getting interesting  Hoping goods thing come out of this\\nHey guys,\nCallen from Wintermute here. In full support of this proposal, $100k seems insignificant in contrast to the added value to the Compound Ecosystem.\nOne thing I would like some clarification on is the upside for Aztec, more specifically, are there fees associated with Aztec connect? Does Aztec capture value from users routing through them? If so what do these fees or the upside for Aztec look like?\nOverall, great proposal \\nThanks @Callen! Appreciate the support. There are no fees with Aztec Connect specifically, beyond the gas required to interact with the L1 contract, which Aztec doesn’t capture. Users will be charged a transaction fee to cover the proportional cost of submitting their transaction to mainnet in the rollup. Our goal is to have the network be sustainable over time, meaning there are no subsidies and users are able to benefit from 95% gas savings with the throughput on the network.\\nHi Lisa,\nSounds great, win win for users and the protocol. \\nA $100K subsidy is small in comparison to the value this could bring Compound.\nI’m supportive of this and can’t wait to see it in action!\\nBlockchain@Columbia supports this grant. Before the bridge and Aztec are fully production ready, it’s important to not only advocate for the benefits of private, cheap transactions, but also show users what it’s like. We think that $100,000 is reasonable to help cover gas fees on the network and start to drive adoption. Lastly, we believe that bringing privacy to crypto (through actual “ZK” rollups like Aztec) is essential to maturing the space and pushing it forwards. Compound should be at the forefront of this as the first lending protocol to integrate with Aztec."
  },
  {
    "number_of_comments": 10,
    "postid": "feb40c88-2b39-46fb-88cc-2d2e83e4762d",
    "posturl": "https://www.comp.xyz/t/gauntlet-compound-v2-deprecation-strategy/4596",
    "combinedcontent": "Simple Summary\nThe community has expressed interest in deprecating the v2 market. Recent Gauntlet recommendations have been aligned with this strategic initiative, with the execution of pausing of supply for v2 tail assets 17.\nTo further deprecate the v2 market, Gauntlet has the following goals in mind:\n\nEnsure enough borrowable USDC exists in v3 so that v2 users can migrate\nLimit poor UX for v2 users\n\nAs a first step in the deprecation plan, Gauntlet recommends the following changes:\n\nDecrease v3 utilization kink to 90%\nIncrease v3 Annual Interest Rate Slope High parameters\nAllocate v2 rewards to v3 Ethereum USDC suppliers\nIncrease v2 stablecoin reserve factors\n\nWe welcome community feedback and will create a poll to gauge community preferences on these options.\nAnalysis\nGoal 1) Ensure enough borrowable USDC exists in v3\nWhile deprecating v2, we want to ensure that users leaving v2 can migrate to v3.\nIn the past month, we are happy to see that the largest non-recursive v2 borrower with address 0xe84a061897afc2e7ff5fb7e3686717c528617487 2 has started to migrate towards Ethereum USDC.\nIn Ethereum USDC, this user supplies $53.78M WBTC and borrows $25.39M USDC.\nIn Compound v2, this user supplies $114.36M total collateral ($53.10M WETH, $43.63M WBTC, $17.63M BAT) and borrows $55.45M stablecoins ($53.95M USDC, $1.50M DAI).\nBelow are time series of this user’s supply and borrow balances in Ethereum USDC.\nScreen Shot 2023-08-08 at 2.20.35 PM1898×604 61.7 KB\nScreen Shot 2023-08-08 at 2.22.45 PM1894×620 78.9 KB\nThe Ethereum USDC Interest Rate proposal to reintroduce the kink at 95% and allocate rewards to USDC suppliers was executed on 7/17/23.\nScreen Shot 2023-08-08 at 3.05.11 PM1712×446 56.5 KB\nAs seen above, utilization in Ethereum USDC has remained high since the proposal was executed, often above 95%, despite appealing Net Supply APR. While large v2 borrowers can still partially migrate to v3 at high utilizations, they have limited borrowable USDC. They are also less incentivized to borrow above the kink due to the resulting higher borrow APRs.\nBelow are some options to increase borrowable in USDC.\nOption 1) Decrease v3 utilization kink to 90%\nPros\n\nIncreased borrowable USDC at lower equilibrium utilization.\nIncreased willingness for USDC suppliers to supply, knowing they have a greater chance to withdraw their USDC if equilibrium utilization is lower.\n\nCons\n\nAlthough decreased utilization yields more borrowable USDC, borrowers are empirically less likely to borrow past the kink at higher borrow APRs. In a way, this may result in less “feasibly borrowable” USDC.\nLess appealing equilibrium APRs.\n\nOption 2) Increase v3 Annual Interest Rate Slope High parameters\nPros\n\nMinimizes time spent at high utilization, either by suppliers increasing supply at greater post-kink supply APRs, or by borrowers repaying at greater post-kink borrow APRs.\nIncreased willingness for USDC suppliers to supply, knowing they:\n\nHave a greater chance to withdraw their USDC if equilibrium utilization is lower.\nWill receive higher supply APR during periods of high utilization.\n\n\n\nCons\n\nLess appealing for borrowers who are concerned about incurring high variable borrow APR.\n\nOption 3) Increase v3 rewards to USDC suppliers\nCurrently, the v2 protocol distributes 444.8 daily COMP rewards, split evenly to UDSC suppliers/borrowers and DAI suppliers/borrowers. Given the current COMP price of $55, this amounts to ~$25k daily COMP rewards. We could migrate all these rewards to Ethereum USDC suppliers, increasing the Earn Distribution from 0.50% to 2.72%.\nPros\n\nIncreases Net Supply APR, thereby incentivizing USDC suppliers.\n\nCons\n\nExpensive\n\nOption 4) Increase v2 USDC reserve factor\nPros\n\nIncentivizes v2 USDC suppliers to migrate to v3.\n\nCons\n\n\nThere are few remaining large non-recursive USDC suppliers in v2, as seen below:\nTop USDC supplier entire supply positions\nScreen Shot 2023-08-09 at 3.44.10 PM1688×1002 88.4 KB\nTop USDC supplier entire borrow positions\nScreen Shot 2023-08-09 at 3.44.59 PM1696×1000 97 KB\n\n\nGoal 2) Limit poor UX for v2 users\nBelow are levers we can use when deprecating the v2 market and the corresponding immediate effects and impact to v2 UX:\n\n\n\n\nParameter\nEffects\nUX rating (1 to 5, 5 = poorest UX)\n\n\n\n\nDecrease collateral factors\nPotential forced liquidations\n5\n\n\nPause supply for WETH, WBTC, DAI, USDC, and USDT (the only remaining unpaused assets)\nInability to top up existing positions or open new positions\n4\n\n\nDecrease borrow caps\nLimited ability to borrow in new or existing positions\n3\n\n\nAdjust IR Curves\nLower net supply APR, Higher net borrow APR\n2\n\n\nDecrease rewards\nLower net supply APR, Higher net borrow APR\n2\n\n\n\nDecreasing collateral factors is arguably the most aggressive lever that could cause the largest volume to flee the protocol. Too many users fleeing the protocol at once may be problematic, given the limited borrowable USDC in v3. Additionally, reducing CF can also result in poor UX, as some users may experience forced liquidations.\nPausing supply for existing users may also result in poor UX, as the only way they could avoid liquidations in a market downturn would be to repay their borrows.\nRecommendations\nGiven the tradeoffs of different options, Gauntlet’s recommends the first step in the v2 deprecation strategy should be to:\n\nDecrease v3 utilization kink to 90%\nIncrease v3 Annual Interest Rate Slope High parameters\nAllocate v2 rewards to v3 Ethereum USDC suppliers\nIncrease v2 stablecoin reserve factors\n\nAfterward, depending on community preference, we can introduce more aggressive levers, including decreasing collateral factors and pausing supply.\nPotential Risks\n\nUsers may leave v2 and not migrate to v3, resulting in lower net TVL across all Compound markets\nAs supply leaves v2, utilization may increase. The v2 max stablecoin borrow APRs are 32.50%, which are unlikely to cause any immediate forced liquidations. However, these high borrow APRs may result in a quick outflow from the protocol.\n\nNext Steps\nWe welcome community feedback and will create a poll to gauge community preferences on these options.\\nTo gauge community sentiment, we are starting a poll below. We encourage the community to select all the options they would like to be implemented in the first v2 deprecation proposal.\n66%Option 3) Allocate v2 rewards to Ethereum USDC suppliers58%Option 4) Increase v2 stablecoin reserve factors50%Option 1) Decrease v3 utilization kink to 90%41%Option 2) Increase v3 Annual Interest Rate Slope High parameters8%Abstain0%Against12voters27total votes\n                    \n                    Closed Aug 22\n                   \\nWe at Morpho Labs agree that supplies should not be paused on v2. The onchain and offchain ecosystem of v2 is still very large and it would undermine this ecosystem too much. The actions proposed on v3 as well as the “soft” changes in v2 seem to be a good step forward already.\\n[Gauntlet] Compound v2 Deprecation Strategy (Phase 1) (8/28/23)\nSimple Summary\nIn response to the results of the poll from our original Compound v2 Deprecation Strategy post, Gauntlet recommends the following changes for Phase 1 of the Compound v2 deprecation:\n\nIncrease v2 USDC Reserve Factor from 15% to 30%.\nIncrease v2 DAI Reserve Factor from 15% to 30%.\nIncrease v2 USDT Reserve Factor from 7.5% to 30%.\nAllocate all 379.8 v2 daily stablecoin COMP distributions to Ethereum v3 USDC daily COMP USDC supply distributions, broken down as follows:\n\n111.20 v2 DAI daily COMP supply distribution.\n111.20 v2 DAI daily COMP borrow distribution.\n66.20 v2 USDC daily COMP supply distribution.\n91.20 v2 USDC daily COMP borrow distribution.\n\n\nDecrease Supply Kink from 95% to 93%.\nDecrease Borrow Kink from 95% to 93%.\nIncrease Annual Supply Interest Rate Slope High from 0.76 to 1.5.\nIncrease Annual Borrow Interest Rate Slope High from 0.567 to 1.5.\n\nAnalysis\nReserve Factor & COMP Reward Changes\nScreen Shot 2023-08-25 at 1.43.55 PM2116×556 63.6 KB\nv2 USDC Earn APR is currently 2.88%, and Earn Distribution is 0.36%, resulting in a Net Earn APR of 3.24%. Increasing the v2 USDC Reserve Factor from 15% to 30% at the current utilization will decrease Earn APR to 2.37%, and removing all 66.20 daily v2 USDC COMP supply distributions will decrease Earn Distribution to 0%, resulting in decreasing the v2 USDC Net Earn APR from 3.24% to 2.37%.\nScreen Shot 2023-08-25 at 1.48.27 PM2128×562 62.8 KB\nv2 DAI Earn APR is currently 2.54%, and Earn Distribution is 0.86%, resulting in a Net Earn APR of 3.40%. Increasing the v2 DAI Reserve Factor from 15% to 30% at the current utilization will decrease Earn APR to 2.09%, and removing all 111.20 daily v2 USDC COMP supply distributions will decrease Earn Distribution to 0%, resulting in decreasing the v2 DAI Net Earn APR from 3.40% to 2.09%.\nScreen Shot 2023-08-25 at 1.48.14 PM2134×572 57.6 KB\nv2 USDT Earn APR is currently 3.57%, and Earn Distribution is 0%, resulting in a Net Earn APR of 3.57%. Increasing the v2 USDT Reserve Factor from 7.5% to 30% at the current utilization will decrease Earn APR to 2.70%, decreasing the v2 USDT Net Earn APR from 3.57% to 2.70%.\nScreen Shot 2023-08-25 at 6.15.39 PM2136×368 50.9 KB\nThe additional 379.8 daily COMP USDC supply distributions will increase Earn Distribution from 0.47% to 2.26%, resulting in a Net Earn APR of 5.08%.\nIR Curve Parameters\nGauntlet recommends the following changes to the Ethereum v3 USDC:\n\nDecrease Supply Kink from 95% to 93%.\nDecrease Borrow Kink from 95% to 93%.\nIncrease Annual Supply Interest Rate Slope High from 0.76 to 1.5.\nIncrease Annual Borrow Interest Rate Slope High from 0.567 to 1.5.\n\nBelow is the resulting IR curve:\nScreen Shot 2023-08-25 at 6.11.55 PM1712×456 51.4 KB\nThe decrease in utilization kink should increase the willingness for USDC suppliers to supply, knowing they have a greater chance to withdraw their USDC if equilibrium utilization is lower. Additionally, the increase in Annual Interest Rate Slope High ****parameters ****incentivizes equilibrium to reestablish more quickly when utilization increases past the kink.\nNext Steps\n\nTarget on-chain vote 9/5/23\n\\n\n\n\n Gauntlet:\n\nIn response to the results of the poll from our original Compound v2 Deprecation Strategy post, Gauntlet recommends the following changes for Phase 1 of the Compound v2 deprecation:\n\nIncrease v2 USDC Reserve Factor from 15% to 30%.\nIncrease v2 DAI Reserve Factor from 15% to 30%.\nIncrease v2 USDT Reserve Factor from 7.5% to 30%.\nAllocate all 379.8 v2 daily stablecoin COMP distributions to Ethereum v3 USDC daily COMP USDC supply distributions, broken down as follows:\n\n111.20 v2 DAI daily COMP supply distribution.\n111.20 v2 DAI daily COMP borrow distribution.\n66.20 v2 USDC daily COMP supply distribution.\n91.20 v2 USDC daily COMP borrow distribution.\n\n\nDecrease Supply Kink from 95% to 93%.\nDecrease Borrow Kink from 95% to 93%.\nIncrease Annual Supply Interest Rate Slope High from 0.76 to 1.5.\nIncrease Annual Borrow Interest Rate Slope High from 0.567 to 1.5.\n\n\n\nIn my opinion, we should be cautious about deprecating markets that don’t have a clear replacement in v3 yet. The goal is to drive users to v3, not other lending alternatives. The main assets that fall under this bucket are USDT and DAI.\nThere’s definitely interest in getting a USDT market up in v3 and Labs is working on it 4 (though we are also busy with other work so it may take a while). Perhaps we should wait until the USDT market is launched on v3 before starting to deprecate USDT on v2?\\nThanks for the feedback, @kevin . We agree that, given Labs is working on creating a USDT comet, it makes sense to wait to deprecate USDT on v2. We will create an on-chain proposal today which will not increase the v2 USDT reserve factor. Going forward, we can continue to discuss the progress of the USDT comet in relation to the v2 deprecation, and create polls to determine community preferences on USDT deprecation.\\nWe have decided to delay this current proposal until we get community alignment on deprecation of the v2 DAI and USDT markets. If the community prefers to keep the existing v2 DAI and USDT markets until new comets are deployed, we will shift v2 deprecation focus to USDC. Note that deprecating collateral assets may be difficult given multiple users borrow all 3 major stablecoins against the same collateral assets. We will create a poll tomorrow to gauge community preferences.\\nBefore the community votes on the v2 deprecation strategy for the v2 DAI and v2 USDT markets, we wanted to post some additional context on the specific user positions in these markets:\nDAI v2\nTop 20 DAI borrowers’ entire supply positions\nScreen Shot 2023-09-07 at 2.27.19 PM1814×1076 91 KB\nTop 20 DAI borrowers’ entire borrow positions\nScreen Shot 2023-09-07 at 2.27.33 PM1820×1078 94.6 KB\nTop 20 DAI borrowers’ DAI borrows relative to total v2 DAI borrows\nScreen Shot 2023-09-07 at 2.36.19 PM1818×916 111 KB\nTop 20 DAI suppliers’ entire supply positions\nScreen Shot 2023-09-07 at 2.26.46 PM1820×1080 82.2 KB\nTop 20 DAI suppliers’ entire borrow positions\nScreen Shot 2023-09-07 at 2.27.03 PM1822×1082 84.4 KB\nTop 20 DAI suppliers’ DAI supply relative to total v2 DAI supply\nScreen Shot 2023-09-07 at 2.31.30 PM1820×1080 108 KB\nAs seen in the above charts, the largest DAI supplier and borrower is a recursive user who supplies $90.6M DAI (47% of the total DAI supplied on v2) and borrows $57.1M DAI (42% of the total DAI borrowed on v2). If we were to slash DAI rewards and/or increase the DAI reserve factor, it’s possible this recursive user will leave the protocol.\nMost of the other top DAI borrowers, most notably the account whose address ends in fbe5ddc3, are non-recursive.\nMost of the other top DAI suppliers are also non-recursive and do not borrow against their DAI collateral.\nUSDT v2\nTop 20 USDT borrowers’ entire supply positions\nScreen Shot 2023-09-07 at 2.28.52 PM1818×1076 87.3 KB\nTop 20 USDT borrowers’ entire borrow positions\nScreen Shot 2023-09-07 at 2.29.08 PM1828×1078 88.1 KB\nTop 20 USDT borrowers’ USDT borrows relative to total v2 USDT borrows\nScreen Shot 2023-09-07 at 2.43.50 PM1816×914 119 KB\nTop 20 USDT suppliers’ entire supply positions\nScreen Shot 2023-09-07 at 2.28.11 PM1816×1080 90.2 KB\nTop 20 USDT suppliers’ entire borrow positions\nScreen Shot 2023-09-07 at 2.28.34 PM1818×1080 72.8 KB\nTop 20 USDT suppliers’ USDT supply relative to total v2 USDT supply\nScreen Shot 2023-09-07 at 2.44.36 PM1818×1080 119 KB\nAs seen in the above charts, the largest USDT  borrower is a non-recursive user with address ending in be389d04 who supplies $68.0M ETH & WBTC and borrows $40.6M USDT (20% of the total USDT borrowed on v2). The next 3 highest USDT borrowers borrow mostly against USDC collateral.\nSince USDT is not a collateral asset, most of the USDT suppliers do not have any borrows.\nNext Steps\nWe will move forward with deprecating v2 USDC, with an on-chain proposal next Monday 9/11. In the meantime, we welcome the community to vote on the below poll regarding USDT and DAI deprecation.\\nCompound v2 Deprecation Strategy57%4.Deprecate v2 excluding both USDT & DAI    29%2.Deprecate v2 excluding USDT only  14%1.Deprecate all v2 markets 0%3.Deprecate v2 excluding DAI only7voters\n                    \n                    Closed Sep 15\n                   \\nWe put up the on-chain proposal for deprecating v2 USDC here 13. Voting begins in 1 day and lasts for 3 days.\\nThe proposal has passed and been executed. We thank the community for their participation.\n  \n      \n\n      compound.finance\n  \n\n  \n    \n\nCompound 3\n\n  Compound is an algorithmic, autonomous interest rate protocol built for developers, to unlock a universe of open financial applications.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"
  },
  {
    "number_of_comments": 15,
    "postid": "7bef1d39-05be-44ef-a97e-e7bd5b7fcfe1",
    "posturl": "https://www.comp.xyz/t/draft-for-compound-growth-program/4582",
    "combinedcontent": "Compound Growth Program by Alphagrowth\nContents\n\nWhy Compound Growth Program\nBackground\nProposal Summary\n\nQualifying Chain integrations\nQualifying tokens on Compound\nPartnerships to increase TVL for Compound\nAssisting Compound Grants program\nGeneral Growth\nDeliverables\nTimeline\nFunding and Budget\n\n\nWhy Alphagrowth\nImpact and Transparency\nLinks and Research\n\nWhy:\nWe at AlphaGrowth believe that the Compound Protocol will benefit by expanding to more blockchains and supporting more money market pools. We plan to help Compound Community map out and execute a strategy to increase the utility of the Compound Protocol.\nBackground:\nOver the period of the last 3 months, we received feedback from various Compound Community members. Based on our initial interactions with the Compound community, we posted a discussion  2for the possible direction of the Compound Grants program, by means of RFP method as compared to the current Domain allocator method. Our initial focus was to use Compound Grants as the pivot point to foster the growth of the Compound ecosystem. But as we dived deeper into the requirements of Compound as a platform and came to understand that Compound needs a growth program concurrent to its Grants program. This belief was further solidified after discussions with the Compound delegates who gave us their feedback on possible avenues for Compound’s growth.\nProposal Summary\nThrough this proposal AlphaGrowth aims to foster growth activity on a series of Chain integrations, token listings and platform partnerships for Compound. The proposal will enable AlphaGrowth to engage in Growth and BD activities for the Compound ecosystem. The core aim is to map, strategize and execute more integrations with quality blockchains and quality token projects as well as opportunities that drive TVL, utility and increase revenue for the Compound protocol.\nAlphaGrowth will primarily focus on the following areas for Compound Business Development\n\nGrowth & Data Research\nQualifying Chain Integrations\nQualifying Tokens on Compound\nPartnerships to increase TVL for Compound\nAssisting Compound Grants Program\nMarketing Compound’s expansion(Content Creation & Distribution)\n\nQualifying Chain Integrations\nObjective:\nMap and score the pipeline of chains to integrate Compound.\nExecution:\nAlphaGrowth is currently tracking over 50+ EVM based chains as new opportunities for Compound. We aim to target the chains which are in the best interest of Compound to increase TVL, utility and increased revenue.\nIn this direction, some of the activities that AlphaGrowth will be actively working on are:\n\nResearch:\n\nConducting market research on the best potential integrations for Compound\nLeveraging the AlphaGrowth platform’s database containing over 31 million on and off chain data points and connections with 108+ chains. (Links to current research mentioned at the end of this post)\n\n\nOutreach\n\nReaching out to the identified targets, and advocating for Compound’s mission, vision, and proposition. This includes nurturing the prospects and pushing the conversations through the pipeline.\nManaging the dialogue with potential partners, and getting the prospects to and through the finish line.\n\n\nSales Engineering\n\nAlphaGrowth will work closely with both the prospective blockchains, token projects and the integration teams to analyze the viability of integration.\nAlphaGrowth will track technical infrastructure requirements to be able to launch Compound with new chains.\n\n\nNegotiations\n\nAlphaGrowth will work closely with the risk team and integration teams to facilitate, push and incentivize the process in a win-win-win scenario.\n\n\n\nQualifying Tokens on Compound\nObjective :\nGetting more tokens listed on the Compound money markets.\nExecution:\n\nMarket Research\n\nScan existing and trending tokens with deep liquidity and healthy metrics for the potential of listing on Compound\nUse the AlphaGrowth platform’s market tracking algorithm to scan through over 40,000 tracked tokens, identify the best token projects for outreach based on our suite of on and off chain signals.\n\n\nRisk Assessment\n\nPerform integration risk checks for target tokens, and collaborate with risk analytics partners to identify all variables associated with token listing\n\n\nOutbounds\n\nOnce the research and risk analytics phases are concluded, the real fun begins.\nAlphaGrowth team will reach out to the teams of shortlisted tokens, pushing the narrative for advocating the opportunity of getting listed on Compound\n\n\nNegotiate listing\n\nThe AlphaGrowth team will work closely with the token teams to ensure a proper timeline of listing\nAlphaGrowth team will negotiate Protocol Owed Liquidity to help bootstrap and make sure enough liquidity is available on Pool launch\n\n\n\nPartnerships to increase TVL for Compound\nObjective:\nIncreasing the TVL on Compound by means of integrations with other DeFi platforms\nExecution:\n\nMarket Research:\n\nThe AG team will conduct comprehensive research to identify and shortlist the top DeFi platforms for integration with Compound. The research will also consider DeFi platforms on chains that may see support for Compound\n\n\nOutreach and Networking:\n\nOnce the research and assessment phase is over, the AG team will initiate dialogues with the shortlisted DeFi platforms followed by nurturing the prospects\nAG will also leverage its own network of DeFi native team members who have a collective network of over 2700+ dApp founders.\n\n\nIntegration:\n\nThe AG team will collaborate with the teams of selected projects to ensure integration and deployment is on track\nThe AG team will also ensure adequate co-marketing campaigns are run with the integrating team to spread awareness for the integration\n\n\n\nAssisting Compound Grants program in Applications\nThe scope of business development for Compound will be incomplete without a close collaboration with the Compound grants program and its operations team (Domain allocators). In our research on Compound’s previous growth and grants initiatives, we identified that a higher quantity of applications can boost the performance of the grants program.\nHence the AG team will be working to closely assist CGP Domain allocators to bring more quality applications to submit for Compound Grants program for their review. It should be noted that the growth program will work in collaboration with the grants program without stepping in the scope of the Grants program. Key to this collaboration will also be granting free access to the domain allocators on AlphaGrowth platform and a deal-flow of 2700+ startups that is being used by 140+ Web3 investors.\nGeneral Growth, Data Research and Marketing\nApart from the core growth activities, AG will also engage in auxiliary growth and light marketing efforts to forward Compound’s cause. These will include\n\nTwitter AMAs: The AlphaGrowth team will work closely with DeFi communities to organize AMAs featuring Compound Integrations with the Compound labs team. In cases where the Compound labs team is unavailable, either a vetted Compound community member or AlphaGrowth will directly\nMarketing Materials: Create education materials for DeFi traders on the newest compound offerings. Identify content creators/video makers willing to create quality content in this direction, help them get funding under the CGP. Here’s an example of some of the AlphaGrowth marketing content Gravity Bridge Commercial - YouTube 2\n\nDeliverables\nOver the course of next two Quarters, the Compound Growth initiative powered by AlphaGrowth will aim at the following statistics\n\nChain integrations: Conversations and negotiations with at least 10 chains, starting the integration process and integrating with 2-3 chains.\nToken integrations: Nurturing partnerships with at least 20 tokens and their core teams. Proceed with getting these tokens supported on Compound subject to approval by Risk analytics partners.\nCGP Data feed: Support Domain allocators with applications, source 250+ applications from which the DAs can scan and select applications for the next stage.\nPlatform integration: Initiate a minimum 2 DeFi platform partnerships and integrations for the benefit of Compound\nAMAs: Organize/participate in at least 1 Twitter DeFi AMA per month, Min 6 in 2 quarters.\nContent Creation: Help create at least 1 video per month to help educate about Compound\n\nTimeline\nTo ensure that the Growth & BD operations for Compound are a success, the AlphaGrowth team will have to prioritize work into 3 major responsibilities divided in the period of 6 months -\n\nPhase 1: Planning and Research\nPhase 2: Outbounds and Conversions\nPhase 3: Continuity and Communications\n\nThe Initial 45 days will be heavily focused on Planning and research-related works with light outreach and discovery also being done in this time. For the remaining time, the team will primarily work on communications, conversions, negotiations and follow-ups. Between a team of 9 members, we will push nonstop to ensure the success of Compound’s expansion.\nFunding and Budget\nWe propose to initially allocate 18,000 COMP Tokens (0.825% of available COMP) to the Growth Program for 6 months. Out of the 18K allocated, 11K COMP will be transferred back to the COMP treasury if they remain unused. The estimated 6-month operational budget (not including potential performance bonuses) is as follows:\n\n\n\n\nSr No\nCategory\nCost\n\n\n\n\n1\nOperations, Research & Data (2 resources)\n2,000 COMP\n\n\n2\nBusiness Development Team (4 resources)\n5,000 COMP\n\n\n3\nMarketing (Educational Material Creation & Distribution)\n4,000 COMP\n\n\n4\nLiquidity Bootstrap Loans & New Chain Pools\n5,000 COMP\n\n\n5\nOne-off Reserve Buffer (i.e. Unexpected Expenses, Unexpected/Emergency Operational Expenses)\n2,000 COMP\n\n\n\nTotal\n18,000 COMP\n\n\n\nFor all incentives to be aligned, at the end of this 6-month engagement AG puts a proposal for a performance bonus to be evaluated and voted on by the community. The bonus will have a one year daily vesting schedule. This bonus will also be a signal for AlphaGrowth to proceed and take on further responsibility within the Compound ecosystem.\nNote 1: For the Funds allocated for categories 3, 4 & 5 - AlphaGrowth will be conducting a Post-program audit to list out all the expenses incurred in activities associated with these categories.\nNote 2: Funds in Categories 3,4 & 5 are returning funds. Any unspent funds assigned to these Categories will be returned back to the Compound Treasury at the end of the time period of the program. An exception to this is the liquidity bootstrap loans that unusually have to be deployed on timescales of more than 6 months, in which case the AG team will continuously update the Compound Community on the status of the funds.\nWhy AlphaGrowth\nWe believe decentralization allows for meritocracy. That holds true in the Compound ecosystem as well. We have firsthand experienced the problems that chains and protocols face in their expansion and integration. Our learnings will be pivotal to identifying the risks and benefits associated in various growth strategies for Compound\nOperating since 2018, AlphaGrowth has years of experience in DeFi and Crypto BD, We have successfully facilitated business development for multiple protocols including Kava, Sommelier Finance, NEAR, Aurora, and many others.\nIn our latest engagement with the Kava grants program, the Kava ecosystem witnessed integration with 78 projects and a TVL increase of $80+ Million. Our strategies have shown resilience and success throughout the crypto winter. Over the past year, our BD efforts have led to the deployment of over 150 projects on various blockchains. AlphaGrowth gets deals done, even in the bear.\nWe have interacted with over 100 ecosystems and 2700+ projects. To facilitate BD and grant deployment, our team built a crypto-native CRM and dashboard enabling us to efficiently evaluate and score opportunities, and report progress on all BD efforts.\nImpact and Transparency\nAt AlphaGrowth, we like doing business in public. We welcome any input or feedback from the Compound community on the proposal.\nTo keep the community informed on the progress, AG will implement the following tasks to guarantee transparency\n\nMonthly reports on the progress of BD activities so far (6 reports)\nQuarterly Progress reports highlighting various deals in the pipeline, successful integrations, closed deals and learnings (2 reports)\nA database that gives all the program specific information to the community so that the members can come, verify and inquire about the process.\nPost program analysis and report about spends on budget categories 3,4 & 5\n\nLinks:\nWebsite 4 | Twitter 2 | Linkedin 1\nSome of the examples of our research and data\n\nList of Blockchains tracked by Alphagrowth 5\nOur research on the list of top Defi Projects by TVL 1\nAlphagrowth’s research on new Projects that will be launched in the next six months 2\n\\nUpdate 1: On the Community call held on Aug 9th 2023, We had presented the proposal to the community\nWe were lucky enough to get the feedback from multiple community members. Among them here are the 3 points mentioned by @kevin\n\nEngineering side challenges to deploy compound to new chains\n\n\n\nSeeing more use cases and integrations with other Dapps ranther than getting on more chains\n\n\nBudget request for the program being too high\n\n\nAs time was limited, we could not complete the discussion. @kevin we are working on the proposal to address the points, any other point that you wanted to mention?\nIn order to make the proposal best suited for Compound’s growth and development, we look forward to get your feedback on it. Thanks\\nThanks @sharp and the Alphagrowth team for making this proposal and speaking at the Compound community call. Personally, I would love to see a third-party focused around growth and BD for the protocol. However, I do have some concerns about the current proposal which I will expand upon below.\nChain integrations and token integrations seem unnecessary\n\n\n\n sharp:\n\nAmong them here are the 3 points mentioned by @kevin\n\n\nEngineering side challenges to deploy compound to new chains\n\n\nSeeing more use cases and integrations with other Dapps ranther than getting on more chains\n\n\nBudget request for the program being too high\n\n\n\n\nThese three points cover the crux of my concerns. But I’d like to expand a bit further into the second point:\nCompound is in the fortunate position of being a protocol that most chains would probably pay to have as part of their own ecosystem. For many nascent chains, having a bluechip protocol like Compound launch on their platform is arguably a bigger benefit to the chain itself than Compound. Furthermore, with a presence on Polygon, Arbitrum, Base, and, in the future, Optimism, Compound will have a presence on most of the major chains/L2s. There seems to be very little upside for the protocol in this chain-expansion focused proposal. Not only that, the work that the DAO would be paying Alphagrowth $600K+ (11K COMP) for is already currently being done without cost by other chains that want to have Compound on their chain.\nA similar argument can be applied to asset listings. Compound has historically been conservative with approving new collateral assets because each new collateral asset adds new risks to the protocol. The bottleneck isn’t in finding new assets to support on the platform. The challenge is in identifying new assets that make sense as collateral for a market. With Compound v3 being out almost a year now, we are now starting to see other teams (e.g. Stader Labs, Rocket Pool) provide their own dev, BD, and growth resources to get their tokens listed on the platform.\ntldr: Several chains and assets are already committing their own resources to have Compound be part of their ecosystem. Paying a third-party more than $600K+ to do this (and without the dev resources) does not seem like a good use of the DAO’s funds.\nPotential growth opportunities\nThat being said, I definitely see value in the other suggested growth ideas, such as platform integrations and content creation. I think it makes sense to keep the scope of this proposal smaller.\\nHey @kevin , thanks a lot for the feedback. The first and foremost aim of our discussion was to make our proposal as detailed as possible. To put a spectrum of activities in front of the Community so that the community members give suggestions, approve or disapprove the parts they deem necessary and we make appropriate changes to the proposal based on the suggestions. In this regard your recommendations and honest feedback on the proposal have proven to be a valuable resource.\nFor the specific Chain integrations section of the proposal, based on the discussions, we are considering scaling down the scope of it. So for the particular topic of Chains, our scope of work becomes explorative in nature as against a previously collaborative nature. Reshifting priorities on research rather than action for this section, ultimately eliminating the cost factor for Compound by delegating research expenses to Chains.\nHere is an alternative approach without the Compound Community Bearing the cost.\nNew section for Chain Research Chain Integrations\n\nResearch - Alphagrowth research team lists and reaches out to the top chains in the market, and invites them for an integration viability check.\nThe integration viability check consists of 3 major parts\n\nRisk analysis: the possible risks that such an integration will present to Compound as well as the Chain.\nSecurity analysis: The security standpoints for any possible integrations\nRecommendations and reports by Alphagrowth: These reports will be shared with the Chain and the Compound Community.\n\n\nAlphagrowth will work with the current Risk analytics (Gauntlet) and security partners(OZ) for Compound (Pending approval from the partners)\nThe Viability check will only be conducted if the prospective chains are ready to incur the costs of the analysis. Thus the working mechanism will involve the first step of securing a research micro-grant from the requestor chain which will be used for the following purposes-\n\nThe grant will be used to cover the operational expenses of Risk and security partners of Compound along with expenses for Alphagrowth\nAny unused funds will either be deposited to the Compound treasury or be returned to the Chain treasury\nThe whole process will not use any monetary resources from Compound either directly or indirectly through the Growth program. The research grant or Viability report will not provide any integration Guarantee to the Grantor chain\n\n\nThe contents of the viability report and the recommendations will be advisory in nature and will not have a binding effect on either the Compound or the Chain. The report will list out the possible contingencies in the integration, along with highlighting the key parameters to help the Compound community and the Chain identify the positives and negatives related to the integration. These reports are supposed to be a guiding star of what the chain standards should be in order to consider integration with Compound.\nIrrespective of the contents of the report, the prospective chains will still have to follow the community guidelines and procedures if they wish to proceed with presenting a proposal for integration to the Compound community. The report recommendations may be used as factual evidence to approve or disapprove the Chain’s proposal, nothing mentioned in the report will constitute an endorsement for any particular direction of action.\n\nThe way we proceed now is either we completely reduce the Chain focus, or to make it limited to research like the aforementioned sections. @kevin would be glad to know your feedback?\\nHey @kevin, Bryan Colligan founder of AlphaGrowth here.  I have been thinking about how to respond for a couple days now.   You are a core community member. Without your support and feedback what we are proposing will likely not pass.  So, thank you for responding and voicing concerns.\nI will try to distill your perspective as best I can. It seems your perspective is Compound should not pay for expansion to new markets.  New markets should pay to have Compound expand to them.  Also there are concerns in the value to adding new markets at all.  Above all the perceived value of new markets is tiny.  I think I am understanding your viewpoint, correctly.\nIn having conversations with Delegates over the last 3 months what we found was that there is a desire for more.  More markets, attention and activity.  The common narrative among delegates said Compound community was apathetic, lacking momentum and losing market share.  From those conversations we had, we put together the best post possible.\nWe unfortunately have never had a conversation to understand what you would like to see. And would gladly do so to figure out a path forward.  But let’s try discovery in public.\nThere are 4 main objections in your response.\n\nCompound as a bluechip protocol should not need to chase Chains and Assets.\nChains and Assets should provide their own resources to integrate via BD, Dev and growth.\nEstimated value of new markets is low.\nPrice of Services is too high.\n\nCompound is valuable, I know 8 chains that would want compound tomorrow.  The main issue is that they do not know how to purchase compound.  I do mean purchase because what a chain is purchasing is a brand and distribution.  A brand known for code that works, fine tuned risk management and security.  The reason why I say they are purchasing a brand is that the code is open sourced and there are now over 100 forks.  From the new market perspective the brand associated with Compound is a symbol of trust.  Having Compound will motivate more capital and TVL to the new market.   Having a new markets listed on Compound is validating.  The brand association should have an associated premium, but what is the premium?\nOn the distribution side having a new markets listed on Compound creates awareness.  While there were only ~120 active users in the last 30 days these users have a large amount of capital.  Access to these users is very valuable for new markets.\nWe know Compound has value, new markets are will to pay and deploy resources for said value.  They just don’t know how.  The process is dense.  Purchasers at new markets are told to go to the forums and start a conversation.  That is simply not how heads of ecosystem and BD people operate.  The way Compound is packaged makes it too difficult to work with.  The TAM(total addressable market) of this complexity can quantified.  With a napkin calculation of 1.1 Billion of TVL in compound forks and 2.7% in generated fees per year there is $29 Million more fees that can be generated each year for compound users.(point #3)\nWhat we are suggesting is that Compound needs help normalizing and merchandising Compound.  The current selection process of new markets feels a bit arbitrary.  We believe it is important evaluation of new markets from a quantifiable approach.  What parameters matter for integrating new markets and what is the acceptance criteria. We are not looking to opening the flood gates for any and every chain integration with Compound, which seems to be a trigger point for the community.  With community feedback we will create a Go-to-Market(GTM) strategy and package Compound in a way which new markets can be evaluated and eventually integrate compound.  Currently chains and assets have resources but do not know how to start the process and where to begin. (point #2)  The persona acquiring Compound dApp is usually not a developer.  They negotiate over calls and after the negotiation they want to know where to send the check to get the job done.\nAs for our price.\n\nWe can slow down the process new market expansion make a research project to work with you and community on the GTM strategy.  Map requirements and customers journey on how new markets can effectively deploy resources to integrate Compound. Reduce the BD team to 1 resource.\nSeparate out Liquidity and Bootstrap Loans after the GTM strategy is approved by the community\nAdd clarity to the Marketing section on spend vs operation expenses\nLean in and Focus on increasing Compound usage and where to embed the protocol with existing markets.\n\nIs there something else we are missing? If this sounds acceptable we can adjust our ask to this plan.\\n@sharp, @bryancolligan thanks for your responses.\n\n\n\n bryancolligan:\n\nThere are 4 main objections in your response.\n\nCompound as a bluechip protocol should not need to chase Chains and Assets.\nChains and Assets should provide their own resources to integrate via BD, Dev and growth.\nEstimated value of new markets is low.\nPrice of Services is too high.\n\n\n\nYou hit the nail on the head. I’d like to re-emphasize that there is a lot of unexplored opportunities to get Compound integrated with other products in the ecosystem and that’s where I personally find the most value for Alphagrowth’s service. New chains and assets will still help to growth Compound, but I’m not sold on Alphagrowth’s value-add here given that Compound is already on most major L2s and Alphagrowth does not have the eng resources to develop any of these proposals.\nIf Alphagrowth came to the table by saying “we’ll identify the new chains/assets to add to Compound AND we’ll make sure the engineering work is done correctly”, then that seems more valuable of a service. If the offering is just to research these new chains/assets, then the value-add is more limited since the bottleneck for expansion is generally on the engineering side.\nThat being said, I still find it valuable for the DAO to have a service provider focused on growth. Is Alphagrowth open to a trial period for the DAO to figure out if this is a good fit?\\n\n\n\n kevin:\n\nwe’ll identify the new chains/assets to add to Compound AND we’ll make sure the engineering work is done correctly\n\n\nAligned. The demand is high from new and existing Chains. So we will identify new chains/assets to add to Compound and leverage external funding (chain/asset grants) to make sure the engineering work is done correctly.  While we learn this process, the attention to Chain integrations is projected to be 20-30% of the overall growth program.\nOur main focus (80%) remains on finding new distribution channels for existing markets. Then promoting the new partners through marketing and awareness programs with the new embedded channels.\nWe are also aligned on the idea of a trial period. Understanding that a ramp up time will take a couple weeks we recommend a minimum of a 4 month trial, we are open to suggestions.  If this sounds good we will adjust our ask to align to this.\\nCompound III has a very elegant code that has been audited by the top security team and has proven itself. I like the idea of coming up with a GTM strategy for the not widely adopted L2. All L2s would want to have at least one lending and borrowing solution and one DEX solution, so they would want to have Compound deployed on them.\nIt would be great to explore options for an additional revenue stream by licensing the protocol with a fee plus a profit-sharing model for lesser adopted L2s.\nSo Compound can be both a solution and a market provider for L1 and leading L2s. And the newer L2s can have their own team, or perhaps a Compound partner team, deploy new markets on their chains.\\nThank you for leaning in.  Because of the nature of the foundations with token most deals I have seen and put together consist of grants.  There are many types of grants and arrangements.\n\nIntegration Grants\n0% Loans\nLiquidity Incentives\nMilestone Grants\nToken swaps\nLiquidity Pooling\n\nA Goal will be to standardize how a new chain can integrate Compound after a risk & technical requirements analysis.  What kind of incentives, grants and or token swaps will be needed after risk assessment is a goal.\\nIt’s great to see quality ideas and proposals for expanding Compound’s reach, and I’d be happy to work with Alphagrowth as a CGP domain allocator if both proposals ultimately pass. I agree that Alphagrowth’s proposal is largely complementary to CGP; in defense of that claim, I want to highlight what I see as one of the areas that might be seen as an area of potential overlap, namely content creation.\nIn the most recent round, my CGP domain (Protocol Ideas and Dapps) approved a small handful of proposals focused on content creation, outreach, and/or education, cumulatively between 10-20% of the domain’s activity. Proposals in this area were not  explicitly among the high priorities listed in the program details 2 for this domain. I raise this example because it is offers an example of why the CGP 2.0 team felt it was important for the program detail documents to include examples of high-priority RFPs but at the same time allow for builders to bring their ideas and explore alignment with the relevant CGP domain. We also felt such content creation efforts were important for Compound but would not be eligible for CGP support if they could not be supported within this domain.\nI think it would be net-beneficial to see most content creation efforts shifted out of CGP Protocol Ideas and Dapps to become part of a more streamlined initiative under Alphagrowth’s proposal. There may still be value in supporting some high-quality but smaller, community-led education and outreach initiatives, so perhaps independent builders with ideas in this area could consult with both Alphagrowth and CGP to determine whether their ideas would make sense as part of the Alphagrowth content thrust or as a standalone effort under CGP.\nI agree with @kevin about the limited utility of research on chain and asset growth in the Alphagrowth proposal. While it’s possible in principle to deploy the Comet contracts on any EVM-compatible blockchain, the vast majority of them lack certain critical infrastructure to support a full Compound III deployment (Chainlink price feeds being the primary example). Between existing and in-development deployments, I don’t believe it’s an exaggeration to say that the cumulative TVL of chains on which Compound III markets are (or will soon be) deployed represents well over 90% of TVL across all EVM-comatible blockchains (excepting BNBchain and Tron, I suppose). The majority of value unlock in chain/asset growth would be development efforts to make more chains Compound-ready, which is really more up to the chains’ developer communities than it is to Compound.\nOn budget: a 1Q trial period with 500 COMP to ops/research/data, 1500 COMP to BD, 500 COMP to content creation, and 500 COMP of buffer for a total of 3000 COMP makes more sense to me. At current market rates, this USD equivalent of ~$126k to Alphagrowth for three months would amount to roughly 50% of the $252k in grants that CGP 2.0 awarded across its six months of operation. In other words, the funding rate to Alphagrowth would be roughly equal to that of all CGP-approved proposals combined. I hope that comparison helps place the proposal size in context and helps justify why the size I’m suggesting is 1/6 of what was requested (albeit for only half the requested period).\\nThank you for taking time review the proposal and sharing your valuable feedback. CGP and the Growth program both offer unique propositions which have the potential to enable the advancement of Compound as a platform and service. In this regard I find both the programs to be uniquely placed to have distinguised priorities yet closely alligned on a lot of topics. The complimenting nature of the two programs will offer a holistic opportunity for development & growth to the community.\nThe key constituents of the growth program are Research, BD and Marketing. The focus for Marketing will be to drive awareness relating to new Compound integrations, target onboarding of users of integrated platforms, and spotlight on methods to get targets to use the platform. I am aligned with your views @allthecolors on handling content and marketing. There might be some community driven educational content that may fit better with CGP due to their scope and KPIs. A close cooperation would guarantee the best results as you mentioned.\nWe are also aligned on reducing the scope of the growth program wrt chains as suggested by @allthecolors and @kevin in the previous replies.\nOn the topic of duration, I however believe a program duration of minimum 4 months would allow us to make any meaningful impact. As negotiations and finalization take months, even more in case of negotiations with other DAOs. Ideally we would have requested 6 months.\nThank you for the budget suggestions, the BD+Research budget requirements are in line with the suggestions in your earlier comment (~800 Comp/Month). The Marketing budget reflects calculations based on several factors including distribution channels and KOLs, which is why the budget extends to a monthly ask.\nWe have bifurcated the budget into two parts, based on our internal forecasts, analysis and previous engagements-\n\n\nThe BD+ Research Budget\n\n115 Comp/Month: Research, operations and reporting\n689 Comp/Month: DB, Partnerships and outbound campaigns\n\n\n\nMarketing budget\n\n689 Comp/Month\n\n\n\nThe Marketing budget will be publicly audited, any unspent marketing funds will be returned to the Compound treasury at the end of the program.\\nI actually believe that Alphagrowth’s proposal should be completely different. As they themselves noted:\n\n\n\n sharp:\n\nAs negotiations and finalization take months, even more in case of negotiations with other DAOs.\n\n\nTherefore, the proposal should be heavily reliant on commission based on successful large and important integrations rather than monthly fee. If not, Compound community will fall to issue of sunk cost and mismatched incentive.\n\n\n\n sharp:\n\nThe Marketing budget reflects calculations based on several factors including distribution channels and KOLs,\n\n\nThis is another important point that doesn’t match well with leading DeFi DAOs like Compound. KOLs are more for speculation focused. And one can argue many of these might even on promoting scams. How would approaching these help Compound? Also if the team believes that will actually drive users to use compound, any viable KOL example that you can share?\\nHey Doo, thank you for the recommendations.\nRe: Separate proposal\nInitially, we had a focus on Grants program. But as we shifted our focus to the Growth Program, we were already aiming for a separate proposal.\nRe: Commission-based approach\nI think this is a good idea. Alphagrowth follows a Commission based approach with the ecosystem partners. These still involve a monthly retainer style contract. What are your thoughts on a commission framework? The idea had crossed our minds before. The only blocker I see is that the retainer contracts include quick execution and close collaboration with the core team, but with no central executing authority in Compound, we did not find this the best working arrangement\nRe: KOL and marketing\nThe thrust of the KOL section is not on Compound education, but integration specific marketing, ie event-driven, action-inducing marketing (Not to be confused with in-person event marketing) Lets say, we get V3 integrated with Balancer (Event!). In this case, the focus will be on attracting Balancer native defi users to use the integration or use compound directly. Its not general compound marketing, everyone knows compound. But everyone is not aware about the new integrations of compound (esp outside compound community), the information about such events needs to be spread in the defi community to gain maximum effect.\\nSome edits to this proposal after taking in views of multiple community members\nCompound Growth Proposal[v3]\nThanks to the suggestions of all the compound community members and delegates including , @arr00, @pgov , @pauljlei, @JacobPPhillips, @ignaciorsg.eth, @michigan_blockchain etc. We are glad to present the revised and final version of the Compound growth Program.\nAn earlier version of the Growth proposal is also available on the forum. After conversating with more community members and taking into account the suggestions from community members @kevin, @joseph, @allthecolors, @robinnagpal. We made multiple revisions with the proposal and changed some focus areas and KPIs. With that in mind we believed it is best to post the final version of the proposal in a different thread. The earlier posts are linked below\nPost 1:Draft for Compound Grants program\nPost 2: Draft for Compound Growth program\nIntroduction:\nThe Compound Growth Program proposes to fill in the position of partner that enables the growth of the protocol. The program will secure dedicated personnel and resources for the Growth, BD related activities. The Key areas of focus of the growth program will include: increasing V3 integrations, users, wallets and TVL on the platform. Furthermore, there will be a focus on Marketing activities to enable the protocol to generate awareness and demand for the protocol products. The marketing and BD activities will be backed by robust research, which is also a part of the proposed program structure.\nCompound DAO needs a growth partner.  We have identified a couple of variables that can increase TVL and Users for protocol growth:\n\nCollabs & Co-Marketing with Partners\nIncreasing V3 integrations\nNew Assets research & process\nNew Wallet integration research & process\n\nOur focus will be on Marketing activities to enable the protocol to generate awareness and demand for the protocol products. Additionally focus will be on expanding V3 Integrations. The secondary focus of BD activities will be on emerging markets and new offerings that can create outsized risk returns.  A recent example of this was Compound DAO moving quickly by launching on the new untested Base chain.  The brave and bold insight is similar risk profile to an investor.  For each expansion Compound will need to analyze whether or not the brand risk is worth the investment. The path for growth is 100% aligned with the partners, integration, assets, wallets that the Compound DAO decides to associate with.\nBackground:\nOur initial intentions were to manage the next cycle of Compound grants program. However as we conversated with more delegates, it became clear that Compound needed a small grants program and a concurrent growth program to onboard new users with successive developments. A missing part in Compound’s operations is the focus on growth/ BD related activities. Compound already has other areas covered thanks to reliable partners in analytics, grants, and security.\n\nSecurity: Openzepplin\nRisk analytics: Gauntlet\nGrants: CGP\nGrowth: ?\n\nUsually ecosystems, and protocols have an internal BD team to manage the growth activities. Without a dedicated set of personnel and resources to focus on growth, Compound has not been able to keep up with its competitors, such as Aave. The growth program will ensure that Compound will keep up with the industry while still maintaining the conservative nature of the protocol.\nProposal Summary and Deliverables\nThe Growth program consists of 3 components - Research, BD and Marketing. Each of these components is crucial for the successful realization of the other two components. How we have devised the program, and research sets the stage for hyperfocussed BD and Marketing activities. BD helps identify the targets for marketing campaigns and Marketing helps multiply the impact of BD activities.\nResearch\nScope\n\nInitial Market research: Identify potential partners for dapp and collateral assets.\nData analytics activities: to help support BD and Marketing\nResearch on DeFi trends, suggestions to the community on incorporation\n\nDeliverables\n\nData research to enable a continuous and full pipeline for BD engagements.\nIdentification of Key marketing channels to partner with.\n\nBD\nAlpha Strategy\nAlphaGrowth Strategy heavily relies on Access.  We have access to existing data, tools, projects and relationships.  Part of what you are buying with our BD services is relationships and time.  The time others would take to build such relationships would take 2 years of full time work in the industry.\nWith that being said we have a pulse on the Defi industry, what’s being built, what narratives are spicy and what investors both professional and retail are looking for.  It simply comes down to what Robert Leshner recently said. Have the assets people want to trade.  For Compound the future heavily relies on making sure it has pools of assets that users need and want to supply and borrow.  If Compound does not offer the right assets users will simply go to competitors.  We have to be the the winners.\nScope\n\nOutbounds: Campaigns to open conversations with potential partners, leads.\nCompound V3 Integrations: Targeting V2 integrated Dapps, getting conversations and conversions for V3 integration\nGetting more collateral assets and TVL\nEvent presence: The team is present throughout major crypto/DeFi events. Better conversations on Compound integration through in person events.\n\nDeliverables\n\nCompound V3 Integrations\nWallet integrations\nCollateral assets\nTVL Increase\n\nMarketing\nAlpha Strategy\nThe aim of the Marketing activities is to leverage concentric circles of communities and partners in the DeFi landscape. Leverage other user’s communities and other communities users. A heavy focus for the marketing activities will be on co-marketing for every integration and collateral asset. Co-marketing is a strategy where you engage in marketing to your community and the partner community at the same time through event driven defi marketing- via Twitter spaces, KOL events, direct marketing, discord events and more.  In DeFi you must advertise Events not Yield (Events here are integrations and partnerships and not in person events).\nWe will heavily use the marketing spend to co-market integrations to partners and communities that will drive the most adoption for existing Compound Pools and new integrations. Adding existing DeFi users to Compound user base. Onboard old users to Compound who have migrated to other protocols. Additionally, the Marketing strategy will also utilize ‘Quests’ to incentivize partner community to start using compound.\nScope\n\nCo-marketing campaigns for v3 integrations, wallets, assets\nStandalone marketing campaigns for new user features/integrations\nAMAs\nDeFi KOL Marketing strategies\nIntegration marketing quests (Limited scope)\n\nDeliverables\nThe Marketing activities will be measured by\n\nTotal impressions on the campaigns\nTotal link clicks and increase in DAU, MAU (daily, monthly active users)\nNew user addition to Compound platform\n\nTimeline\nThe original growth program was to span for a period of 6 months. But as discussions on the proposal continued, a consensus was reached on starting first with a ‘trial period’. We plan the BD program to run for a time period of 4 months, separated in two phases.\n\nPhase 1: Planning and Research\nPhase 2: Outbounds and Conversions\n\nThe Initial days will be heavily focused on Planning and research-related works with light outreach and discovery also being done in this time. For the remaining time, the team will primarily work on communications, conversions, negotiations and follow-ups. Between a team of 9 members, we will push nonstop to ensure the success of Compound’s expansion.\nThe additional phase which focused on retention and post integration continuity has been removed as the trial period time will not be sufficient to have any meaningful impact via the activities of that phase.\n\nPhase 3: Continuity and Communications\n\nFunding and Budget\nFor the program, we have divided the budget into two parts. These are BD fund and the Growth fund. While marketing and BD activities of the program are closely related, we have kept these two funds separate as the growth fund is a returning fund. Any unused assets in the budget will be sent back to the Compound Treasury.\nBD Budget\n\n\n\n\nActivity\nRequirement/Month\nProgram Budget\n\n\n\n\nResearch\n120 Comp\n480 Comp\n\n\nBD & Outreach\n725 Comp\n2900 Comp\n\n\nTotal\n845 Comp\n3380 Comp\n\n\n\nGrowth Fund\nThe Growth fund is the external fund of the Growth program. While the internal activities, including research, BD, growth, outbounds, negotiation related activities are covered in the BD fund. The external activities are covered by the development fund.\n\n\n\n\nActivity\nDescription\nRequirement/Month\nTotal Requirement\nRemarks\n\n\n\n\nMarketing\nFund used for Marketing. Includes direct marketing, KOL, AMAs, co-marketing, etc.\n785 Comp\n2900 Comp\nUnused Fund will be returned\n\n\nDeveloper fund\nPaying protocols or developer teams to integrate products with Compound\n\n3000 Comp\nUnused funds will be returned to Treasury\n\n\nTotal\n\n\n5900\n\n\n\n\nThe growth fund is not a grant fund.  We envision to resort to using this fund as least as we can. To promote Compound integrations, we currently have 4  routes for financing/boosting integrations\n\n\nBootstrapping: For majority of the protocols wanting to integrate Compound, the primary thrust will be to negotiate a contract where the full burden of integration falls on the counterparty.\n\n\nCounterparty Grants: For bigger protocols like Balancer, the focus will be to get an integration from such protocols. The integration will be carried out by a partner developer team or anyone in the Compound Community interested in getting the grant and work on the integration.\n\n\nCGP: For protocols not fitting in the above two criteria, the key blocker becomes the cost of integration. In such cases we will help the protocols to apply for the Compound grants program to seal the integration commitment.\n\n\nDeveloper Fund: Should none of the above three methods provide a way forward for integration of the project. Most of the times we will not push ahead with the integration. But if we feel that a project has the capability of being a major asset as a partner with compound. We want to push ahead with the project and remove the financial bottleneck. For this we require a developer fund which will be able to provide small capital. This fund will be a reserve fund, used sparingly only in a few cases. The growth program will aim to be extremely frugal and conservative with the fund. Unused funds will be returned to the treasury.\n\n\nTotal Budget for the program: 9,280 (3380 BD in Services, 5900 Marketing and Development fund)\nWhy AlphaGrowth\nWe believe decentralization allows for meritocracy. That holds true in the Compound ecosystem as well. We have firsthand experienced the problems that chains and protocols face in their expansion and integration. Our learnings will be pivotal to identifying the risks and benefits associated in various growth strategies for Compound\nOperating since 2018, AlphaGrowth has years of experience in DeFi and Crypto BD, We have successfully facilitated business development for multiple protocols including Kava, Sommelier Finance, NEAR, Aurora, and many others.\nIn our latest engagement with the Kava grants program, the Kava ecosystem witnessed integration with 78 projects and a TVL increase of $80+ Million. Our strategies have shown resilience and success throughout the crypto winter. Over the past year, our BD efforts have led to the deployment of over 150 projects on various blockchains. AlphaGrowth gets deals done, even in the bear.\nWe have interacted with over 100 ecosystems and 2700+ projects. To facilitate BD and grant deployment, our team built a crypto-native CRM and dashboard enabling us to efficiently evaluate and score opportunities, and report progress on all BD efforts.\nImpact and Transparency\nAt AlphaGrowth, we like doing business in public. We welcome any input or feedback from the Compound community on the proposal.\nTo keep the community informed on the progress, AG will implement the following tasks to guarantee transparency\n\nMonthly reports on the progress of BD activities so far (6 reports)\nQuarterly Progress reports highlighting various deals in the pipeline, successful integrations, closed deals and learnings (2 reports)\nA database that gives all the program specific information to the community so that the members can come, verify and inquire about the process.\nPost program analysis and report about spends on Development fund.\n\nLinks:\nWebsite 1 | Twitter | Linkedin\nSome of the examples of our research and data\n\nList of Blockchains tracked by Alphagrowth 5\nOur research on the list of top Defi Projects by TVL 1\nAlphagrowth’s research on new Projects that will be launched in the next six months 2\n\\nOn the Compound community call held on October 4th, a few more community members made some suggestions.\nA couple of the suggestions were\n\nRemove new assets from the scope of Growth activities\nNarrow down and consolidate the scope of BD program\nMarketing program related suggestions\n\nBased on these suggestions, we held further calls with community members and are now working on a revised proposal to present to the community. We will keep the community posted\\nWe thank all the community members for their valuable suggestions on defining the Compound Growth Program. Below I have mentioned the finalized version of the proposal, please feel free to share your feedback\nIndex\n\nIntroduction\nBackground\nProposal\n\nScope\nOperations\n\nBusiness Development\nMarketing\nTimeline\n\n\nMilestones & Deliverables\n\nDeliverables\n\n\nFunding & Budget\n\nOperational Expenses\nGrowth Fund\n\n\n\n\nWhy AlphaGrowth\n\nTeam\nImpact & Transparency\nLinks\n\n\n\nIntroduction\nOver the past 6 months, we have chatted with multiple delegates and community members and aggregated the best ideas and strategies on how to grow the Compound protocol. Lack of regulatory clarity limits the ways in which the labs can market the protocol. This has hindered Compound protocol growth.  Therefore we propose the Compound Growth Program.  This program will secure dedicated personnel and resources for the Growth, BD related activities.\nCompound DAO needs a growth partner.  We have identified several avenues that will increase Users and TVL which will ultimately drive awareness for the protocol growth:\n\nCollaborations & Co-Marketing with strategic partners\nEducation Campaigns with KOLs (Key Opinion Leaders) around Compound products\nIncreasing the total number of V3 integrations\nIdentifying and targeting key opportunities for growth (ie Native USDC on Base)\nNew Wallets research & integration to increase the Compound footprint\n\nThe growth program will use marketing to publicize Compound products and attract new users to the platform, especially V3. The business development activities will increase the integrations and uses of the platform. The Key areas of focus of the growth program will include: increasing V3 integrations, users, wallets and TVL on the platform. Furthermore, there will be a focus on Marketing activities to enable the protocol to generate awareness and demand for the protocol products. The marketing and BD activities will be backed by robust research, which is also a part of the proposed program structure.\nBackground\nOur initial intentions were to manage the next cycle of the Compound grants program. However as we chatted with more delegates, it became clear that Compound needed a small grants program and a concurrent growth program to onboard new users with successive developments. A missing part in Compound’s operations is the focus on growth/BD-related activities. Compound already has other areas covered thanks to reliable partners in analytics, grants, and security.\n\nSecurity: Openzepplin\nRisk analytics: Gauntlet\nGrants: CGP\nGrowth: ???\n\nUsually ecosystems, and protocols have an internal BD team to manage the growth activities. Without a dedicated set of personnel and resources to focus on growth, Compound has not been able to keep up with its competitors, such as Aave. The growth program will ensure that Compound will keep up with the industry while still maintaining the conservative nature of the protocol.\nProposal\nThe Compound Growth program, a trial spanning over a period of 4 months will focus on key areas of growth for Compound Protocol. The Growth program has 2 main areas of focus, BD and Marketing. The AlphaGrowth team has spent the last 6 months discussing the growth requirements of Compound with various community members, delegates and other stakeholders. The program will be a trial run to identify the compatibility of a growth program for Compound. Additionally, this program will also help AlphaGrowth to test different strategies of sustainable growth for Compound\nScope\nThe overarching aim is to aim at the following-\nMarketing\n\nPublicizing Compound Protocol: Increasing Compound’s presence\nV3 Awareness: Increase general community awareness about V3 and its benefits over V2\nCo-marketing: For new integrations of Compound with other platforms, market the integration to both the communities and DeFi community in general to gather traction for the event.\nMarketing : Paid marketing campaigns\n\nBusiness Development\n\nIncrease V3 reach: Many V2 integrations are not on V3, push those integrations to V3 as well\nCompound Markets: Integrate Compound markets with different services. Wallets, Exchanges, custodians, etc.\nDapp integration: Defi applications that can promote activity, users or TVL.\n\nOperations\nBusiness Development\nHere are some of the outreach targets for the Growth Program\n\nV2 to V3 updating\n\nInfrastructure & Services: Applications and tools that ease the developer onboarding and deployment of Compound to other apps. Many of these have a V2 api/service live. Our aim will be to integrate V3 with them. Some examples are\n\nAnkr:\nCovalent\n\n\n\n\nExisting Pool integrations:\n\nhelp existing integrations migrate from V2 pools to V3 Pools\n\nArgent, which uses V2 pools\nCrypto.com\nPooltogether\n\n\n\n\nCompound integration in wallets & Exchanges\n\nPartner with wallets to provide connection to compound. The majority of wallets are already integrated with Compound, a thrust will be to integrate regional/centralized exchanges with compound markets\nInitiate conversations with certain key centralized exchanges like Nexo, OKX, Kucoin.\n\n\nCompound services to institutional custodians\n\nCustodians that provide services to high asset individuals/organizations (Typical sales deal for custodians is 18 Months, far beyond the trial period of the program, but a successful deal can drastically increase TVL)\n\nHex Trust\nBitgo\n\n\n\n\nStructured Products/Dapp Integrations\n\nNew/existing web3 dapps that increase the uses of Compound Markets/pools. Highly specific and targeted, again key targets will be dapps serving to regional users\n\nExample- Lemon App that caters to users in LATAM\n\n\n\n\n\nMarketing\n\nCompound Content Creation & Education\n\nCompound Labs being a US-based organization is restricted in its social media scope, so it cannot promote/market any activities on web3 Twitter (Apart from general information sharing through official handle). Compound as a Crypto Platform has no presence in Social media apart from Official handles. No tweet storms, no widespread sharing and retweeting of information by members.\n\nPromote key updates about the Compound by multiple channels on twitter\nPartner with DeFi KOLs to share information about Compound updates, spread the information about Compound in wider community\n\n\n\n\nV3 Awareness and Demand Generation\n\nSteady marketing of V3. Many community members are still unaware about the V3 launch, even more people outside Compound in the wider defi community are not yet aware about the enhanced safety and other features about V3.\n\nV3 Targetted ad campaigns to compound/aave/balancer follower base\nMerge this section with the first section on social media\nCo-marketing for V3 integrations with wallets, exchanges, etc.\n\n\n\n\nEvent Driven Marketing, Co-Marketing with with Partners and Integrations\n\nThe team will use events as a major driving factor for the Marketing campaign.  Whenever a partnership or integration occurs we will run an event.  Co-marketing grows the pie for both partners and the Compound Protocol. We will utilize both Compound’s audience and the Partner audience to drive adoption.\n\nCompound is integrated with a new dapp\nany new asset is added to the platform\nany new chain integration goes live\nany new pool is updated\n\n\nNative USDC comet on Base chain - marketing extensively to Base and L2 communities.\nLaunch of Compound on Linea (if it happens within the growth program timeline)\nIntegration of a new dapp with extensive user base\nIntegrations of Compound markets with new platforms\nThese campaigns provide the necessary push to acquire users from the partner’s user base.\n\n\nPaid Campaigns\n\nMultiple Campaigns through various channels to promote Compound products\n\nKOL promotion: Highly targeted KOL Campaigns for Defi users\nAMAs: involve high value partners\nSponsorships: i.e. defi edge with 24000 newsletter readers\n\n\n\n\n\nTimeline\nThe original growth program was to span for a period of 6 months. But as discussions on the proposal continued, a consensus was reached on starting first with a ‘trail period’. We plan the BD program to run for a time period of 4 months.  The main goal of a trial is to test and experiment on what activity will most help the Compound Protocol.\nMilestones and Deliverables\nThe program will focus on the following parameters to measure its success\n\nDiscover the of Cost of Marketing Spend to increase $1 of TVL\nDiscover User Acquisition Cost per User Persona (Whale +$100k, Shark 100-$10k, Minnow under $10k)\nDiscover User Acquisition Cost per Integration ( Wallet, dApp, Structured Product)\nDiscover which Integrations, Wallets, Partners drive metrics(Fees, Transactions, Users, Volume, TVL)\n\nDeliverables\n\n1 new V3 infrastructure service provider integration\n1 upgrade of V3 infrastructure service provider integration\nA minimum of 2 sponsored content posts to large audiences such as other defi platforms & newsletters\nA minimum of 5 Co-marketing campaigns with existing and new partners\nA minimum of 3 KOL Partnerships\nA minimum of 5 AMAs organized\nSales pipeline creation\nA goal of 4 meetings every week for integrations and partners, with funnel tracking to conversion\nConversations with key exchanges on integration\nConversations with 42+ Dapps for integrations\nTarget at least 1,000,000 impressions for marketing campaigns\nMinimum cumulative count of integrated dapp user reach at 1000\n2-4 new integrations\n\nAs we embark on a program to enhance Compound’s growth in the aforementioned metrics, we will be trying out a number of strategies for success. Which makes this an exciting yet challenging task. Not all growth strategies will work, the key aim of the growth program in its trial period will focus on identifying and gathering data on different activities and their rates of success.\na) Finding out which strategies moved the needle for Compound\nb) what activities had the highest ROI,\nc) identifying the plans that were able to create desirable behavior among target user base.\nAt the end of the program, we will present this data to the community to evaluate the data. The community should be able to identify, what works, and what doesn’t. What challenges did the growth program encounter during operations?\nFunding and Budget\nThe Compound Growth Program’s budget modeling is similar to Compound Grants Program. The funding ask of the growth program consists of two parts, Operational expenses and the Growth fund.\nOperational Expenses\nThis covers the Growth and BD activities for the program, similar to the Program manager fees on Compound Grants program. This will also include the research activities for the program. Earlier research formed a separate component of the Growth program but after taking suggestions from community members, we included it within the BD part and reduced the scope of research.\nThe operational expenses for the program are 690 COMP/month\n\n\n\n\nActivity\nRequirement/Month\nProgram Budget\n\n\n\n\nBD Operations\n690\n2760\n\n\n\nGrowth Fund\nThe growth fund will be the primary vehicle to boost Compound’s presence in the Crypto Communities online. This is similar to the fund of CGP which will be utilized to promote growth via partners.\n\n\n\n\nActivity\nDescription\nRequirement/Month\nTotal Requirement\nRemarks\n\n\n\n\nMarketing\nFund used for Marketing. Includes direct marketing, KOL, AMAs, co-marketing, etc.\n690 Comp\n2760 Comp\nUnused Funds will be returned\n\n\nDeveloper fund\nPaying certain integrations that can guarantee higher ROI\n\n2250 Comp\nUnused funds will be returned to Treasury\n\n\nTotal\n\n\n5010\n\n\n\n\nIt should be noted that the growth fund is not a grant fund.  We envision resorting to using this fund as least as we can. At times some of the integrations will need a development cost by the dapp, at those times\nTo promote Compound integrations, we currently have 5 routes for financing/boosting integrations\n\n\nBootstrapping: For majority of the protocols wanting to integrate Compound, the primary thrust will be to negotiate a contract where the full burden of integration falls on the counterparty.\n\n\nCounterparty Grants: For bigger protocols like Balancer, the focus will be to get an integration from such protocols. The integration will be carried out by a partner developer team or anyone in the Compound Community interested in getting the grant and work on the integration.\n\n\nCGP: For protocols not fitting in the above two criteria, the key blocker becomes the cost of integration. In such cases we will help the protocols to apply for the Compound grants program to seal the integration commitment.\n\n\nDeveloper Fund: Should none of the above three methods provide a way forward for integration of the project. Most of the times we will not push ahead with the integration. But if we feel that a project has the capability of being a major asset as a partner with compound. We want to push ahead with the project and remove the financial bottleneck. For this we require a developer fund which will be able to provide small capital. This fund will be a reserve fund, used sparingly only in a few cases. The growth program will aim to be extremely frugal and conservative with the fund.\n\n\nMatching Incentives Fund: Similar to the the Developer Fund an Incentives Fund can be used to help bootstrap liquidity to a new integration. This will also be used sparingly and to be considered for the best opportunities to bootstrap adoption.\n\n\nWhy AlphaGrowth\nWe believe decentralization allows for meritocracy. That holds true in the Compound ecosystem as well. We have firsthand experienced the problems that chains and protocols face in their expansion and integration. Our learnings will be pivotal to identifying the risks and benefits associated in various growth strategies for Compound\nOperating since 2018, AlphaGrowth has years of experience in DeFi and Crypto BD, We have successfully facilitated business development for multiple protocols including Kava, Sommelier Finance, NEAR, Aurora, and many others.\nIn our latest engagement with the Kava grants program, the Kava ecosystem witnessed integration with 78 projects and a TVL increase of $80+ Million. Our strategies have shown resilience and success throughout the crypto winter. Over the past year, our BD efforts have led to the deployment of over 150 projects on various blockchains. AlphaGrowth gets deals done, even in the bear.\nWe have interacted with over 100 ecosystems and 2700+ projects. To facilitate BD and grant deployment, our team built a crypto-native CRM and dashboard enabling us to efficiently evaluate and score opportunities, and report progress on all BD efforts.\nTeam\nAlphaGrowth consists a team of seasoned Crypto professionals with an extensive network in the industry.\nBryan Colligan: Program and Technical Lead Twitter, Linkedin 1\nJoe Bjornsen: Outreach and BD lead Linkedin\nShardul Pilley: Program manager LinkedIn\nKyril V: Operations and UIUX Linkedin\nMichael Borisov: Engineering Linkedin\nVitalii S: Partnerships manager and client success Linkedin\nJose Alfonso: Partnerships and BD Linkedin\nImpact and Transparency\nAt AlphaGrowth, we like doing business in public. We welcome any input or feedback from the Compound community on the proposal.\nTo keep the community informed on the progress, AG will implement the following tasks to guarantee transparency\n\nRegular reports through Community call on the progress of BD activities\nProgress reports highlighting various deals in the pipeline, successful integrations, closed deals and learnings (2 reports)\nA database that gives all the program specific information to the community so that the members can come, verify and inquire about the process.\nPost program analysis and report about spends on Development fund.\n\nLinks:\nWebsite 1 | Twitter | Linkedin\n\n\n\n\nResource\nLink\n\n\n\n\n\nDefi Projects Data\nCrypto Analytics & Market Analysis for Web3 | AlphaGrowth\nOur platform tracker to rank projects by multiple metrics\n\n\nCompound Overview\nCompound & COMP Token Crypto Analytics & User Data | AlphaGrowth\nA set of datapoints about Compound Protocol\n\n\nCompound Competitor Watch\nTop 5 Compound Competitors & Alternatives, And Similar Lending Projects | AlphaGrowth\nOur database of projects that Compares Compound and other Lending projects\n\n\nNews\nCompound News | AlphaGrowth\nNews about Compound\n\n\n\n"
  },
  {
    "number_of_comments": 27,
    "postid": "1d404763-56f5-4de8-8fa3-ac179e158755",
    "posturl": "https://www.comp.xyz/t/whitelist-of-addresses-that-can-create-proposals/1996",
    "combinedcontent": "A number of key community members lack sufficient COMP to create governance proposals. We want to keep the proposal threshold high to avoid spam but also want to create a smooth governance experience for those dedicating time and effort to improving the protocol. Gaining sufficient delegation to a CAP isn’t a huge issue for one-off proposals/contributors but is more of a hassle for frequent contributors.\nFor example, Compound is now paying @getty  indefinitely for contributing to the protocol but still requires him to rally 65k COMP to propose an improvement. Assuming Getty is the first of many contributors that have longer-term economic relationships with the Compound Protocol, it may make sense to grant these individuals special privileges in governance (namely, the right to propose).\nOne solution: create a whitelist of addresses that can create proposals. Rather than just checking to see if an address has 65k COMP delegated to it, we could also check to see if the proposer’s address is on a whitelist of trusted community contributors, allowing them to sidestep the COMP requirement.\n@arr00  took the lead on ironing out the implementation details of this. The code below allows addresses to be whitelisted for a specified period of time, after which the special authority expires.\nComparing compound-finance:master...arr00:whitelist-users · compound-finance/compound-protocol · GitHub 21.\nStill to decide:\n\nWho manages the whitelist — governance or a multisig?\nProposals from whitelisted addresses can’t be canceled for falling below the proposal threshold, so perhaps we allow a multisig to cancel proposals from whitelisted proposers?\n\nAdmittedly, this is a pretty minor possible improvement on the governance process (and there are some hacky ways to accomplish a similar result), but the bigger point here is that we should be empowering key community contributors with the tools they need to contribute actively to the Compound Protocol (a mix of funding, resources, privileges, etc).\\nAwesome idea! It can be frustrating trying to get the votes necessary to pass a CAP. This could be a great way to fuel Compound governance while we are still in the early stages.\nI think governance should manage who is on the whitelist, and the community multisig can have cancel power. Canceling should only be done at the proposer’s request.\nI am in the process of trying to get 65k votes (I have ~11k rn) so I can propose myself. There are many low-hanging items the protocol should be maintaining/adding, but due to my limited ability to create proposals, I tend to pick one big improvement/issue at a time. If I add the ability to create proposals, I would likely almost always have something going through governance.\nPS: Very cool to see Polychain and arr00 working together.\\nI think this is a great idea to help bolster community efforts and allow core contributors to improve the protocol with less overhead work. I’ll keep working on this and update the community along the way.\\nThe proposal threshold (now 65,000 COMP) was designed as a tool to prevent hasty or malicious proposals from entering the voting process.\nWhitelisting known / trusted / experienced contributors fits the spirit of governance, while maintaining the high standard that needs to be demanded of governance proposals broadly.\nThis is a creative idea that I am excited to support; in terms of implementation, empowering the community multi-sig to whitelist addresses (rather than the complete governance process each time) makes sense.\\nAre we thinking a change on the contract itself to whitelist addresses? And while I’m sure everyone that would be whitelisted is trustworthy, would adding this on the contract require some sort of security assumptions about the whitelisted addresses?\nWhat if we just rebuilt the Autonomous Proposal contract to be a single contract (not a factory) that accepts proposals from only a whitelisted group of individuals, but then the larger comp community can delegate their voting power to this contract.\nGovernance would be responsible for adding or removing whitelisted addresses, but the community would still be responsible for delegating their voting power to the contract itself. This way if the community were to lose faith in the white listed individuals, it would be easy enough to remove delegation power.\nI get that rounding up the delegated votes is tricky, but I think folks would be up for doing it once to a single contract, and I like the social dynamic more than making a chance on the governance contracts themselves.\\nI think @arr00 originally envisioned something not far off from this. I believe the idea was to create a sort of perpetual CAP with a whitelist of possible proposers that would allow anyone on the whitelist to use the COMP delegated to the CAP to create a proposal (so like Polychain could have its own whitelist, Robert could have his own, blck could have their own, etc. and then there could be community pools with separate whitelists).\nOne issue related to the current way CAPs work IIRC: I think certain community members should always have the freedom to propose things to the community, but we may not always agree with them (or would like token holders to have veto power), so letting them always use capital in a CAP to propose & vote would not be ideal.\nCreating the whitelist within the governance contracts seems to be the simplest way to do this and sufficient from a security perspective with the help of the community multisig.\\nWe at Blockchain at Berkeley are in support of this - Arr00 & Getty are fantastic contributors to the protocol, and I see this as a great way to reduce barriers for strong ideas to be put in front of the Compound community. Agreed with @JacobPPhillips here, I believe that leveraging the multisig and the format @arr00 put together strikes a balance by enabling trusted contributors to put forward proposals in a way that doesn’t require significant changes to existing governance smart contracts.\\nI think this is a great idea and was my original idea; however, the limitation on throughput is quite significant. A single proposer, as this contract and its subsidiaries would be considered, can only have one proposal at a time. As such, it is definitely a solution that does not scale. Once we start to try removing the restrictions for this one contract, we should go the more direct route currently being take.\\nThis proposal is a step forward to incentivize contribution to Compound. @Arr00 has made an invaluable contribution to Compound with Governor Bravo and other changes and it makes perfect sense for governance to allow him to add proposals w/o going through the tedious CAP process. On top of this, the risk of giving people these permissions is low - mostly just proposal spam, which is unlikely and further mitigated by gas costs.\nWhile its valuable for token holders to enable more progress within the protocol between votes, I think the impact this change is quite small at the moment.  Arr00 mentions this as well - “the limitation on throughput is quite significant”. We might want to consider other changes that would give power to the community, but would more directly target specific use cases. For instance, allowing a specific group of people (e.g. a subDAO) to change a subset of parameters. You could imagine an Interest Rate Group that could change IR curve for different assets (within some limits).  This would allow the the community to increase its pace of contribution, probably even more so than via address whitelisting.  At the same time, this would continue to enable the helpful restrictions on protocol changes that governance currently provides.\nOn @getty’s community call, @JacobPPhillips mentioned this would be hopefully the first of many changes to increase the speed of Compound governance. We’re looking forward to continue the discussion and work with the community to see this come to fruition.\\nI’m approaching completion with the engineering work for this project and would like to provide an update on the technical details of the implementation. This is the first update to the Governor Bravo implementation and acts as an example of how to update the Governance system.\nAt a high level, the new features allows for governance or the community multisig to set whitelist status expirations for any account. It was done with this format to introduce a concept of a term to governance. Users are granted special permission for a limited time and need to actively participate and perform to have their special status renewed. This term is set as an expiration timestamp after which their special whitelist status expires.\nDue to the whitelist status, whitelisted proposals can’t be canceled for falling below the proposal threshold, so as a safety precaution, the community multisig will be able to cancel these proposals. It is unlikely that this feature will ever be used. The community multisig does not receive unlimited veto powers—it is only able to cancel proposals from users with less votes than the proposal threshold.\n\nTechnical Details\nYou can view the code changes here 11.\nI have opted to create two new stored values:\n\n\nwhitelistAccountExpirations - an array of timestamp expirations for the whitelist status of each account. An account is whitelisted if now < whitelistAccountExpirations[msg.sender].\n\nwhitelistGuardian - an address which is in charge of monitoring and setting whitelisted accounts. The whitelistGuardian will initially be the community multisig. The whitelistGuardian is able to add and remove whitelistedAccounts and cancel proposals by whitelisted accounts. The guardian is unable to cancel a proposal by an account which meets the proposal threshold.\n\nThere are three new functions:\n\n\nisWhitelisted(address account) - isWhitelisted returns a boolean value of whether an account is whitelisted. This logic is isolated into its own function for readability.\n\n_setWhitelistAccountExpiration(address account, uint expiration) - setWhitelistAccountExpiration stores a new expiration for a given account’s whitelist status. An expiration of 0 removes the account from the whitelist. This is a permissioned function callable by governance and the whitelistGuardian.\n\n_setWhitelistGuardian(address account) - This function sets the whitelistGuardian. It is a permissioned function only callable by governance.\n\nThere is extensive testing done through scenario unit tests here 3 along with a forking simulation here 2. I do not consider this code frozen yet, but in the coming days I hope to finish and begin a bug bounty.\\nI took a look at the code overall it looks great and elegant. Why not allow the whitelist guardian to upgrade itself? Not a huge deal but just to understand the reasoning.\nI’m sure the Fei community would be interested in a feature like this as well.\\nThanks for taking a look at the code! I think generally we should restrict the power of external accounts  to allow only the intended powers. I don’t see a situation where the multisig should be transferring this power to another address. If the community decides to transfer this power to someone else, that should be another proposal.\\nplz give me whitelist, like I would create spicy proposals\\nI deployed the new governor implementation to kovan here. It is currently the implementation of kovan governance which is here 1. It is working as expected.\\nhi @arr00 , nice to meet you. I was wondering if I could get a letter of recommendation to get whitelisted for proposals. Please and thank you, Sam.\nattached is my resume and GitHub 6\\nI’ve created a PR 3 adding the code for this feature. Any feedback on the code would be appreciated at this point and bugs found will be rewarded. As of now, I am not planning on getting this change audited—It is fully covered by new unit tests and a forking simulation. The perceived risks from adding this feature is quite low.\\n@arr00\nwhen i was reading through the code, the if-else require statement in the cancel function sorta broke my\nhead cuz it was a really long hard to read line\nthe most confusing part to me was the fact that the require statement was like a || (b && c).\ni think this is functionally equiv and seemed a bit easier to read, correct me if im wrong tho\nif(msg.sender != proposal.proposer){\n\tif(isWhitelisted(proposal.proposer)) {\n\t\trequire((comp.getPriorVotes(proposal.proposer, sub256(block.number, 1)) < proposalThreshold) &&\t\n\t\tmsg.sender == whitelistGuardian,\n\t\t\"GovernorBravo::cancel: whitelisted proposer\");\n\t}else {\n\t\trequire((comp.getPriorVotes(proposal.proposer, sub256(block.number, 1)) < proposalThreshold),\n\t\t\"GovernorBravo::cancel: proposer above threshold\");\n\t}\n}\n\nanother thing that is equally pedantic so feel free to ignore me -  since you added the events in govbravoevent, maybe it would be good to add a fetcher for isWhitelisted in GovernorBravoValue?\neverything looks good to me, i hesitate to say otherwise cuz neither of these are really issues or problems haha\\nGreat points on both. I like your format better as it eliminates the double check for msg.sender == proposer. I’ll update the PR accordingly.\nThanks so much for taking a look and giving feedback!\\nIf the goal is to give more people the ability to submit proposals, why not simply lower the proposal threshold to 1000 or even 100?\nMaybe i’m missing something, but if Quorum is still 400k what’s the actual danger of a bad proposal?\\nIt could spam the UI, I think 1000 is too low of a bar for on-chain proposals, especially all at once. Signal votes are great for having super low barriers to entry. I do think proposal threshold could be lowered to something like 10-30k COMP though as a start.\nIt would be interesting to consider lowering quorum to be much less prohibitive perhaps 100-200k COMP. This would force large holders to play a more active role potentially leading to more failed votes than we currently see. @getty was mentioning on the community call he’d like to see more proposals fail which is an interesting heuristic that shows faster iteration on the governance side.\nThis is a bit off-topic so if there is general interest in the idea of lowering thresholds we should spin up another thread\\nThis prop should be ready to submit on-chain early next week.\nTo simplify and reduce bias in the decision process, here’s a possible framework for how the multisig would manage the whitelist:\n\nAny contributor who wishes to submit a proposal that has gone through the full off-chain governance process (community feedback, temperature checking, testing & peer review — to be more defined later) should be whitelisted for a short period of time (1 week) to submit a single proposal.\nActive contributors — defined as those who have been a core contributor on at least 2 successful proposals OR those who are salaried by the DAO — may be whitelisted for an extended period of time.\nExtended whitelisting will start at 3 months. Upon completion of this period, if the contributor has upheld community governance standards and continues to be an active contributor, they may have their whitelisting renewed for 6 months, and subsequently 12 months (max term) at the discretion of the multisig.\nIf a whitelisted contributor fails to uphold community governance standards, this contributor should be removed from the whitelist.\n\nThe community multisig is welcome to flex/update this framework as they see fit. This exact framework isn’t a fundamental part of this proposal; it’s merely meant to offer some initial guidance for the community multisig on how to use the newly granted power.\\nreviewed Add Governance Whitelist Users Feature by arr00 · Pull Request #149 · compound-finance/compound-protocol · GitHub 7, no obvious bugs stuck out to me, seems to do what it says.\nid personally vote yes\\nI’ve added post deploy 3 and post propose 2 simulations which sanity check the proposal. Everything checks out.\\nThis thread highlights some potential risks not clearly outlined in this proposal. It’s worth having a look.\n\n  \n\n      twitter.com\n  \n\n  \n    \nRomain - Paladin \uD83D\uDEE1\uFE0F\uD83D\uDCDC 9\n@Figue_me\n\n\n  DISCLAIMER : Paladin and our team are neutral and do not take positions in governance \uD83D\uDED1\n\nHowever I would like to point out some flaws in @polychain 's CAP-60 \n\nhttps://t.co/uQMJsvpNM5\n\n1/\n\n\n\n  2:03 PM - 14 Sep 2021\n\n    \n      \n        \n      \n      7\n    \n\n    \n      \n        \n      \n      2\n    \n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nI still feel confident that this is the right path forward, in light of these long-tail risks. However, if others feel differently, I just want to highlight the possible steps we could take to alleviate these concerns.\n\n\nHave a vote to remove the whitelist functionality (revert back to standard COMP gov bravo with only a 65k COMP threshold for proposals)\n\n\nChange the whitelist guardian to COMP governance, allowing token holders to control the whitelist\n\n\nRevamp and/or expand the size of the multisig to enhance it’s security\n\n\\nI am recommending @arr00 & @TylerEther to be added to the whitelist. @arr00 is a critical community member and has made significant contributions to the protocol and the entire DeFi ecosystem. @TylerEther is one of the few people to add an asset (LINK) to the protocol and is currently finalizing RFP 16 (split COMP rewards).\nBoth of them are “no-brainer” additions imo. I think the community multisig can add each of them; however, if people don’t like that, I will make an onchain proposal.\nIf someone speaks up and would rather a governance proposal add them, I’ll make it on Monday. Otherwise, the community multisig should add them on Monday.\\n++\n(20char20char20char20char20char)\\nGovernance delegate address verification:\n{\n  \"address\": \"0xc8A69971DAa3C3ADd85Ab0d0AF297515769ddfFC\",\n  \"msg\": \"To: Compound Community Multi-sig\\nDate: Monday, September 20, 2021\\nTime: 15:08 PST\\n\\nThis address is controlled by TylerEther for the purpose of protocol governance.\\n\\nTwitter: https://twitter.com/tylerether\",\n  \"sig\": \"0xd1eaeaec32cff436f30307b6557a17a6271585f0b81fa4ae60d4560a339addba4b544b4cae7f9b475890f71c53c67f8554e793f3ee73cc0aa8b58332fdfa5ff101\",\n  \"version\": \"2\"\n}\n\\nI’ll be using my existing governance address 0x2b384212edc04ae8bb41738d05ba20e33277bf33 6"
  },
  {
    "number_of_comments": 19,
    "postid": "0f7e7b44-5c07-477d-b942-236db1e7a54b",
    "posturl": "https://www.comp.xyz/t/gfx-labs-proposal-to-become-contributors/2924",
    "combinedcontent": "\nGFX Labs’ Proposal To Become Compound Contributors\n\nObjective\n\nMission\nCompound’s objective is to provide a market where users can lend and borrow against their assets in a permissionless, trustless, and secure way.\n\nFeature\nA core tenet of Compound is that participants can use the protocol to enter and exit without seeking the approval or validation of anyone. It is available to anyone, anywhere, is completely open-source, and preserves liquidity so users can utilize the protocol for an instant or a year without a difference in pricing. In contrast to a central coordinator/underwriter of loans in traditional finance, Compound’s loan terms are the same for all users and available to everyone – with no regard to work history, geography, or even credit score.\n\nCOMP\nThe COMP token and its holders represent the governance of the Compound market. The contracts that define on what terms participants interact with Compound are owned by the protocol and upgradable by governance. Those contracts include everything from the interest rate model, which selects the supply and borrow rates, to the Comptroller, which contains the core protocol logic. In addition to upgrading existing contracts, governance can add new assets (contracts) to the protocol and develop additional products.\n\nGovernance\nFor Compound to succeed, the protocol needs to support assets from which the market wants to earn interest or to borrow for a fee, and provide competitive interest rates to both sides of the market. Supporting new assets and providing competitive rates is ultimately a risk management task. Governance needs to maintain the existing supported assets while continuing to add new assets as the market evolves. It is the responsibility of governance to manage those risks while continuing to innovate.\n\nRevenue\nThe reward for COMP holders (the governors of the protocol) for successfully performing risk management is the prospect of fees paid by users to the protocol. The oft-forgotten Reserve Factor (RF) defines the portion of the borrower’s interest that is paid to the protocol. The Reserve Factor is moderated by governance and can be anything from 0% to 100%, but typically is 25% for altcoins. \nPrice, Reserves and Dollar value of Reserves1200×742 55.9 KB\n\nToday the protocol reserves are worth about $41m and consist mostly of DAI (RF: 15%) and USDC (RF: 7%). Assuming that the protocol usage remains constant, the reserves will increase by approximately $17m over the course of a year. However, the protocol spends roughly $100m/year on COMP incentives (one of our prior posts COMP Distribution Speeds - The End of CompSpeeds? 2 discusses COMP incentives). We believe the protocol can significantly reduce (or end) the current COMP incentives and become substantially more efficient.\nThat brings us to the shortcomings of governance and the current state of the protocol. The protocol is largely unattended. While this is helpful from a decentralization perspective, it isn’t sustainable if Compound is to survive, much less thrive. Crypto and DeFi are still-nascent industries that will continue to evolve rapidly, and Compound must evolve with them.\n\nTo-Do\nWe have developed a To-Do list for the protocol to return to a dominant market position and set forth on a path to profitability.\n\nEnd COMP rewards. There is ~$1B of capital in Compound providing little value to the protocol beyond printing a higher TVL at the cost of $100m in COMP a year. While the original intention of COMP incentives was to disperse COMP to its users, the incentives have brought in mercenaries that claim and sell COMP.\nRefactor COMP incentives to be used to bootstrap liquidity in new markets and scale back as liquidity enters.\nSeek liquidity from other protocols like MakerDAO via their Direct Deposit program. GFX Labs has successfully pushed forward a Dai Direct Deposit Module for Compound 9 that will conservatively bring to the protocol a 100 million DAI credit line that is insensitive to liquidity mining rewards. GFX Labs plans to propose a similar program for USDC and USDP, though they are more complex from MakerDAO’s perspective and require more time and collaboration to prepare. Compound currently spends $38m in COMP incentives on the DAI market and another $38m on the USDC market. Sourcing liquidity from MakerDao can lead to substantially reducing stablecoin incentives.\nAdd supply caps. While the protocol currently has borrow caps to lower governance hijacking risk, the protocol can similarly add supply caps to limit the amount of an asset that can be deposited to the protocol. Similar to MakerDAO’s debt ceilings, supply caps provide governance with an additional tool to manage asset risk. Adding a supply cap mitigates the protocol’s concern of an infinite-mint attack and empowers the protocol to onboard assets it might not otherwise support.\nAdd more assets. The protocol makes money by supporting markets that are in demand. With the addition of supply caps, the protocol can support more assets, since it can better manage risk.\nImprove the oracle system to support more assets. The existing oracle system only supports assets with a Uniswap v2 market. GFX Labs has been working with Chainlink to develop UniswapAnchorView (UAV) v3 contract, which switches the protocol’s anchor to Uniswap v3. While this does support more markets, it is still somewhat limiting. Adding Balancer and other markets as available anchors will grow the range of assets Compound can safely support. In addition, the protocol should develop tooling to support other popular assets such as LP tokens and token derivatives. There are a number of large (by MCAP) assets the protocol could support that the market wants to use as collateral, such as wSTETH, that could significantly increase protocol revenues.\nDeploy Compound v2 to L2s and sidechains. Interacting with the protocol is expensive on mainnet and can crowd out users who do not have the scale of capital to justify the gas expenses. Deploying Compound on Polygon, Optimism, and other networks would grow the protocol beyond Ethereum and put competitors at bay.\nTransition off Legacy CTokens. The ETH, USDC, ZRX, & BAT markets are all running on a non-upgradable legacy ctoken contract. While WBTC has already migrated, the other markets have not. The protocol is currently missing out on additional revenue by not being able to configure those markets with the improved interest rate model, set a kink in the interest curve for ETH, ZRX, & BAT, and protocol liquidation fee. Generally, this change is easy and only has upside for the protocol.\nInterest rate curves. Every interest rate curve utilized is the original interest curve chosen when the cToken was first configured (except DAI, which was updated on July 28th, 2020). This is a critical piece of Compound’s borrow/lend market, and yet the protocol has done little to research alternative curves or test the interest rate curves already utilized.\nDeprecate old markets. Compound v2 is nearing 1000 days old. The crypto landscape has changed significantly over the last few years, and just as Compound needs to adapt to what participants want today, governance also needs to offload what isn’t required or safe to support. Depreciating REP & SAI, closing down legacy markets, and managing risk on older markets such as BAT & ZRX is important to maintaining the protocol.\nUpdate the liquidation system. The efficiency of the close factor and liquidation system as a whole hasn’t been researched meaningfully or iterated upon. As a critical component of the protocol, it needs more dedicated resources.\nUpgrade testing and developer code base. While Compound’s codebase and testing was state of the art at its launch, it has since fallen behind. Without a clear, incentivized, and responsible party to improve and maintain the tools, they have become hard to work with and outdated, which has hurt development efforts.\nClean up the existing protocol. The Comptroller needs to be cleaned up and possibly entirely refactored. Similar to the prior point, the codebase was innovative at launch but has since fallen behind without incentivizing someone to improve the system.\nSeparate revenue generation and risk management. Reserve factors should be optimized to balance protocol revenue and liquidity.\nRebalance reserves. The stock of reserves should be adjusted periodically to minimize risk. Compound is not in the business of taking positions and should seek to offload the surplus of volatile assets for stablecoins or even COMP.\n\nThe v2 protocol has a lot that could be improved. While it has made progress with community members contributing individual improvements, it lacks a concerted, focused effort.\n\nProposal\nGFX Labs is offering its services to improve the protocol and build a concerted effort for the community. We have in-depth experience in Compound, the competing money markets, DeFi, and crypto markets as a whole. At GFX Labs, we are building a team with a wide range of skill sets and using those resources to develop products and improve DeFi. One of the most valuable things about our team is that we genuinely believe in DeFi and want to build a better financial system.\n\nAbout\nGFX Labs isn’t a fund. We don’t hold positions. We are a team of builders who love DeFi. Our participation in governance is rooted in a desire to see DeFi overtake legacy finance. We promise to be active participants who improve the protocol by action and input. We’ll promote innovation and provide constructive comments on proposals we disagree with. Our door will always be open to those who share our passion for DeFi & Compound.\nGetty and Eddy, the two founders of GFX, are active contributors to Compound and have already made several proposals.\n\n24: The first Compound Autonomous Proposal which raised the WBTC CF to 60%\n36: Raised the WBTC CF to 75%\n47: Oracle Improvement: lead a long-term effort to substantially upgrade the oracle system.\n51: Getty became the first contributor to earn a streaming grant for his work on the oracle improvement.\n53: Added MKR and organized a new price oracle to support MKR, AAVE, SUSHI, & YFI. Note: MonetSupply sponsored the proposal.\n54: Added AAVE, SUSHI, & YFI. Note: Polychain sponsored the proposal.\n56: Set collateral factors for MKR, SUSHI, AAVE, YFI, & LINK.\n\nNotably, Getty started hosting the Compound Community Call last March and has been hosting them bi-weekly as a casual place for the community to talk about Compound.\nIn addition to Compound, GFX Labs spends material time on MakerDAO & Uniswap. We have worked on the last three proposals at Uniswap, including the proposal to introduce 1bp fee tiers, which has significantly improved stablecoin trading at Uniswap and taken market share from Curve. At MakerDAO, we have successfully pushed forward a Dai Direct Deposit Module for Compound 9 that will conservatively bring a 100 million DAI credit line to the protocol. GFX Labs is planning to propose a similar program for USDC and USDP, though they are more complex from MakerDAO’s perspective, and require more time and collaboration to prepare.\n\nCompensation/Incentives\nThe issue of compensation for contributors has been discussed in the community a few times, and is often the pain point of a proposal. Ultimately, it is incentives that drive resources or the lack thereof in the instance of many protocols’ governance/improvements. While “prestige” or the joy of interacting with the protocol is certainly there, it isn’t a scalable or long-term plan for innovation. Much of what is required to maintain a protocol isn’t sexy work. As a decentralized protocol, Compound needs to continue to experiment with incentives.\n\nOpportunity Cost\nThe existing incentives in crypto/DeFi are such that if you are a team with skills, you should raise a round and launch a project. The next best thing is probably to join an existing team. Or, if you know DeFi well, you can go the finance route and join/start a trading firm. Governance compensation has hardly begun to compete with the opportunities available in the rest of the industry. However, it has been remarked upon many times how governance improvements are greatly accretive, because if a small parameter change can improve the token price by even 1%, that can represent tens of millions of dollars of value created. The value that could be created by building out a product could be immense for the protocol.\n\nKPIs\nKPIs are regularly used to guide a subjective process towards an objective one. By assigning clear and measurable goals, the protocol can incentivize contributors to work towards the desired outcome. We propose a base compensation and performance bonuses that are paid if specific KPIs are met.\nIncentivized KPIs sometimes have adverse effects due to short-term interest. A CEO has significant power and lateral ability to achieve the KPIs they are incentivized to hit in a traditional company. While they might report to a board, they still have relatively little oversight. However, GFX Labs is still at the behest of Compound governance, and every change that we propose must be explicitly authorized by an on-chain vote. We thus believe that KPIs will allow us to strive to meet the high bar of safety, efficiency, and innovation that governance demands.\nPossible KPIs include:\n\nToken price: a lot of noise but ultimately the thing everyone cares the most about.\nRevenues: a clear indicator of the efficiency and sustainability of Compound.\nTVL: helpful to see how trusted a product is but manipulatable by rewards programs.\nDollar value borrowed: represents usage and affects revenues but manipulatable by rewards programs.\nAssets supported: While more choices are good for users, they might not be good for the security and safety of the protocol.\nMarket share: the percentage of market share Compound has in money markets.\nGovernance proposals: might be a good low bar but easily exploited.\n\nWhile each of these options has its pros and cons, we think protocol revenues and token price best align us (the contributor) and the protocol. Revenues are something we are confident we can grow, and it is hard to manipulate. The token price is ultimately what holders care the most about. In regards to the external risks of using the token price as a KPI, the trade-off is simple: we can work really hard, and the price might not move, and thus we lose, or we could work really hard, and the price might go up and thus pay us more than what governance believes is justified. In the bear case, GFX takes the hit, and in the bull case, everyone wins. While progress and innovation should drive the token price up, we think most will agree that crypto is wild and not everything is easily explainable. We believe the other KPIs are more manipulable and less inefficient as metrics of our performance.\nIn addition to the KPI compensation, we also ask for a base compensation. In a sense, this is to guarantee that we have enough resources until what is hopefully the largest part of the compensation is realized after we hit the KPIs.\n\nToken/Stablecoin\nAn interesting point of debate in governance compensation is what to pay with: governance tokens or stablecoins. Below is a simplified view of the trade-offs from the protocol’s and contributor’s point of view. We think it is in Compound’s best interest to pay contributors in stablecoins and not push more COMP into circulation. However, if governance feels strongly that it makes sense to pay in COMP, we would ask for a premium relative to the stablecoin price.\n\n\n\n\nProtocol paying\nPro\nCon\n\n\n\n\nGovernance Token\nThe contributor’s payment is tied to the token price. Efficient if the protocol thinks their token is overvalued.\nThe protocol is putting more tokens into circulation and diluting the existing holders.\n\n\nStablecoin\nExisting holders aren’t diluted, and the cost/payment is exact.\nThe contributor isn’t tied to the protocol.\n\n\n\n\n\n\n\nContributor collecting\nPro\nCon\n\n\n\n\nGovernance Token\n\nUncertain as to what the value is.\n\n\nStablecoin\nKnows exactly what they are getting paid.\n\n\n\n\n\nCompensation\nBelow is our proposed compensation structure. Since we think we can increase protocol revenues by 50% over the course of a year, we wish to let our base-fee cover up to a 50% increase. We earn bonus payments when we improve the revenues beyond 50% and the price of COMP increases. Both KPIs have to be met to earn the bonus. For example: if protocol revenues were to increase by 200% (very difficult in our opinion), and the token price was above $600, our bonus would be $30m. If the token price were above $600, but revenues had only increased by 100%, we would earn a $10m bonus. If revenues were to increase to 200% but the token price was $400, we would earn a $20m bonus. This model ensures that we only get paid if protocol performance has increased, and we do not become reliant solely on price. While certainly not a perfect model, it has good trade-offs which we believe properly align us with the improvement of the protocol.\n\n\n\n\nRevenue Increase\nCOMP Price\nBonus\n\n\n\n\n75%\n$100\n$5,000,000\n\n\n100%\n$200\n$10,000,000\n\n\n125%\n$300\n$15,000,000\n\n\n150%\n$400\n$20,000,000\n\n\n175%\n$500\n$25,000,000\n\n\n200%\n$600\n$30,000,000\n\n\n225%\n$700\n$35,000,000\n\n\n250%\n$800\n$40,000,000\n\n\n275%\n$900\n$45,000,000\n\n\n300%\n$1,000\n$50,000,000\n\n\n\nWe propose $5m USDC streamed to us for our base fee over the period (1 year). While a guaranteed income would be preferred, we are willing to trust Compound governance. We will commit to completing/quarterbacking at least 6 of the items on the above to-do list. At the end of the period, if we have met any of our KPIs (revenue and token price), we’ll ask Compound Governance to award us our bonus. While a year may be a long time in crypto, we realize the community may be interested in us participating for another term or committing to a longer period. We’re open to exploring that and are submitting this proposal as a starting point for governance to consider.\nWe think Compound and GFX Labs can have a symbiotic relationship that is profitable for both parties. We look forward to hearing everyone’s thoughts on this unique proposal.\n\nCompound v3\nAs long-term Compound participants, we are looking forward to seeing Compound Labs bring Wednesday’s announcement  36of v3 to life. We look forward to assisting with v3 as more information is shared about the new protocol’s timing, progress, and requirements. However, the v2 protocol differs significantly from v3. The announced protocol only has one borrowable asset, such as USDC, and thus one interest rate curve. While the protocol is excellent for participants who want to supply multiple assets as collateral and only borrow USDC, it is not a general money market like v2. Until the full vision of Compound Cash is built, we think v2 should be continued to be maintained, tuned, and improved.\\n100% support this initiative. The base price is very fair given the importance of the todo items and the benefits they would have for keeping Compound up to date. Ultimately someone will have to spearhead these items and they will need a budget, and GFX labs has shown good faith in this area for a long time. The risk of not doing this is too high to ignore.\\nThis is an awesome proposal, and I am totally on board with the to-do list articulated here. I have some specific questions on the compensation structure/KPIs, which I think can be refined, but generally speaking think it’s well designed. My biggest question would be how to best align v2 initiatives outlined here with some of the v3 roadmap that was laid out earlier this week. For example, is it going to get confusing or muddled from a user perspective (and result in even more fractured liquidity) if a Compound v2 protocol is launched on something like Arbitrum, and then a Starport with only one borrowable asset is launched on Arbitrum a month later? How much of the work here will be relevant & reusable in v3? And realistically is the goal of the v3 roadmap to end in the deprecation of the v2 protocol?\nOverall very supportive of this initiative.\\nA developer team investing meaningful labor/resources into a protocol like Compound deserves clear terms and a contract with a runway sufficient to engage in more than short-term planning. This proposal seems like a solid step in that direction. Of course it also helps that GFX Labs and its leadership have been such active long-term contributors!\nHere are a couple of questions to consider:\n(1) Is it in Compound’s best interests for GFX Labs, or any one organization, to tackle all 15 of the items on the suggested to-do list? I think it’s a solid list, and Getty and Eddy are clearly well-qualified to tackle most items on it. But I wouldn’t want the adoption of this proposal to preclude the protocol from hiring other teams to work on items that may be a bit more outside of GFX Labs’ usual wheelhouse. An example that comes to mind is the interest rate curve research: that work strikes me as benefitting from a very different skill/experience set than what’s needed for most of the other items and could be tackled without deep knowledge of the complete protocol codebase. Maybe some detail on how GFX Labs envisions prioritizing these items and/or how we would make space for others to come in and tackle a subset of these items would help address this question for me.\n(2) Can the protocol afford the services as proposed? Over the past year interacting with governance, I’ve learned that my barometer for the market value of this work is way off, so to be clear, I’m not questioning whether the pay rate is reasonable. Rather, I’m asking whether the protocol can afford to pay GFX Labs its base fee and bonus in the most optimistic scenario, or even in the base case scenario:\n\nI’m assuming the base fee and bonus would both come from protocol reserves, because that’s the protocol’s only source of the requested payment vehicle (USDC) unless it sells off COMP, which would go against the proposal’s intent to “not push more COMP into circulation”.\nUsing the numbers in the proposal, the protocol currently holds 29.4% of ~$41M, or a little more than $12M USDC in its reserves. Assuming constant protocol usage as in the proposal, the protocol accrues 29.4% of $17M, or $5M USDC annually to reserves.\nSaid another way, for their base fee, GFX Labs is requesting all new USDC reserves accrued over the one-year contract in a no-change-in-revenue scenario (= $5M).\nAssuming GFX Labs is successful at increasing revenues by 50%, the protocol will retain this extra renevue (across all markets, which is the silver lining) at the expense of 100% of the USDC revenue it would have accrued without this proposal.\nBut then we have to consider where the funds for the performance bonus would come from. Let’s take the 75% revenue increase, $100 COMP price example (= $5M bonus). In this scenario, the protocol would take in an extra 0.75*$5M = $3.75M USDC to reserves, but would owe GFX labs $5M USDC in bonus payments. The extra USDC revenues aren’t sufficient to cover the performance fee, so we would need to tap into prior years’ accrued reserves and/or swap other stablecoin reserves for USDC just to pay the bonus.\nAny improvements in the price of COMP would exacerbate this problem, as the price of COMP doesn’t directly affect available reserves (yes there is probably some correlation, but it is indirect and sublinear). The only way the protocol would be able to pay the bonus would be to sell COMP, again contradicting the positioning about not diluting token holders.\nThe situation just gets more untenable as we go up the KPI chart.\n\nRegarding point (2), in short, unless my math is wrong here, I think the bonus incentive needs to be paid in COMP (or the size of the bonus significantly reduced) to make this proposal viable without risking a full drain of the protocol’s USDC reserves.\\nGetty and GFX Labs have been key players in contributing to the protocol over the past year. They’ve\n\nAdded markets,\nCoordinated with Chainlink to improve the oracle system,\nAdjusted CFs and COMP speeds,\nFacilitated and provided discussion,\nHelped me diagnose and disclose these COMP speed bugs, and\nWere one of the top responders to the proposal 62 bug, partaking in damage control, diagnosis, and recovery\n\nCompound Labs has done a phenomenal job in creating such an excellent product. Hats off to them and all of the developers (I may criticize the codebase and the state of things a lot, but please know I love and respect all of you)! Now, while I believe Compound protocol is great as is, it’s important to realize how early we are in the adoption of DeFi.\nI think DeFi, in general, has so much room to grow - Compound as well. In order to continue growing Compound and to retain our position as an industry leader, much effort is needed. While Compound Labs is building Compound Treasury and Gateway, I think there will still be demand for the existing protocol and its future iterations. With that being said, GFX Labs has done an excellent job in outlining the TLC the protocol needs.\nGFX Labs’ TODO list is extensive and it’ll take a lot of resources to accomplish all of those high-value items. It certainly won’t be cheap - they’ll need to hire a significant amount of people in a variety of different roles, and in a rapidly growing and competitive industry.\nI plan to also submit a proposal similar to GFX Labs’. Together, our firms will work together to complete the items on GFX Labs’ TODO list and more. We’ve already been working together for almost the past year - listing additional markets, building and improving processes/requirements, oracle improvements, and more. With our combined skill sets (for reference, I created a product and bootstrapped a business about 12 years ago which I exited last year and is still profitable under different ownership), I believe working together on the protocol will have a Compounding effect. But this proposal isn’t about that, so back to GFX Labs’ proposal.\nI think we can all agree that Getty and GFX Labs has already provided value to the protocol, that their TODO list is of high importance, and that GFX Labs consistently executes. So the big question is what is the right amount of compensation?\nI think the requested base fee of $5M for a year will provide GFX Labs with sufficient financial security to commit resources in maintaining, growing, and improving the protocol. I think the work GFX Labs has already done and is planning on doing will well exceed $5M in long-term value for the protocol.\nHowever, I have some quims about the proposed bonuses. There can be many reasons other than GFX Labs’ work that can cause the price of COMP to increase - macro sentiments and changes, Compound Labs’ work, BTC correlation, overall industry growth, marketing and PR, TVL changes, COMP distribution changes, other key players’ work, etc. The same can be roughly said for revenue increases.\nThen, of course, there’s the question of whether Compound has the resources to pay those high bonuses, and where to draw the money from.\nI think it would be best to drop the bonus compensation and work with only the base fee. At the end of the 1-year term, GFX Labs can then renegotiate a new work contract using the past year’s work as justification for any increases or decreases in compensation.\\n\n\n\n hzlkmp:\n\nMy biggest question would be how to best align v2 initiatives outlined here with some of the v3 roadmap that was laid out earlier this week.\n\n\nWe would love to tailor the to-do list more towards v3; however, we thought it would best to reference the existing protocol with the limited information available. GFX’s services are tied to the productivity of the overall protocol. We’ll work on the improvements/issues that we believe are most important to the protocol.\n\n\n\n hzlkmp:\n\nAnd realistically is the goal of the v3 roadmap to end in the deprecation of the v2 protocol?\n\n\nFrom our perspective, the v2 (current version) and v3 (announced last Wednesday) can co-exist because v3 isn’t a general money market like v2. As v3 develops, we’ll certainly look to contribute to it.\n\n\n\n allthecolors:\n\nA developer team investing meaningful labor/resources into a protocol like Compound deserves clear terms and a contract with a runway sufficient to engage in more than short-term planning. This proposal seems like a solid step in that direction. Of course it also helps that GFX Labs and its leadership have been such active long-term contributors!\n\n\nThank you!\n\n\n\n allthecolors:\n\nIs it in Compound’s best interests for GFX Labs, or any one organization, to tackle all 15 of the items on the suggested to-do list?\n\n\nWe would love to see other contributors participate in protocol development. There is a ton of work to be done, and while I wish we could do all of it, we don’t have the resources to act as an external core team. In an ideal world, the protocol has a team of contributors improving the protocol.\n\n\n\n allthecolors:\n\nCan the protocol afford the services as proposed?\n\n\nThe base fee can come from the existing protocol reserves and be streamed via Sablier. As for the performance bonus, the protocol will have three main choices: USDC reserves (or a mix of stablecoins), borrow USDC using COMP, sell COMP for USDC (the protocol or us). If we hit a KPI, we’re confident the protocol will be in a position to pay the bonus.\n\n\n\n Tyler Loewen:\n\nI plan to also submit a proposal similar to GFX Labs’. Together, our firms will work together to complete the items on GFX Labs’ TODO list and more. We’ve already been working together for almost the past year - listing additional markets, building and improving processes/requirements, oracle improvements, and more. With our combined skill sets ( for reference, I created a product and bootstrapped a business about 12 years ago which I exited last year and is still profitable under different ownership ), I believe working together on the protocol will have a Compounding effect. But this proposal isn’t about that, so back to GFX Labs’ proposal.\n\n\nLove it! The more the merrier!\n\n\n\n Tyler Loewen:\n\nI think it would be best to drop the bonus compensation and work with only the base fee. At the end of the 1-year term, GFX Labs can then renegotiate a new work contract using the past year’s work as justification for any increases or decreases in compensation.\n\n\nWhen we think about allocating resources, we think about the risk/reward, and when we offer our services to a protocol, we expect voters to think similarly. Voters want to know they are getting the most for their money, and the servicer wants to get paid the most possible. We chose the base+bonus model because it is a tried and tested one. We manage some risk by taking a base and putting the rest on the bonus, whereas the protocol gets more bang for its buck because we only get paid the bonus if the protocol is improving. Without the bonus, we would have to increase the base to meet the risk/reward we want, and in turn, we believe it would hurt the voters’ risk/reward.\\nAt a high level, Gauntlet believes that contributors are important to the growth and success of a leading money markets protocol like Compound. GFX has been a great contributor to the community and we are excited to see this proposal. We could be supportive of the proposal, but suggest a few changes to scope below.\nWe believe that GFX Labs can drive an impact on handling the operational overhead of protocol development. For example, upgrade testing and refactoring the developer code base are areas where GFX can make a highly valuable contribution to Compound.\nAt the same time, it’s really difficult to weigh in on a proposal with:\n\nSuch a broad, heterogeneous scope\nNo information on the intended methodology for these various initiatives\n\nWe suggest the following changes:\n\n\nRemove items that duplicate work by other contributors. For instance, Gauntlet has already committed to managing the reserve factors and has published a detailed methodology here 2. OpenZeppelin and Gauntlet have already agreed to evaluate risk (market and smart contract) for new assets - but it would be great for GFX to do the dev work to create these proposals and set up oracles, etc.\n\n\nProvide basic information on what success looks like for each initiative\n\n\n\nFor instance - COMP reward management. It’s hard to support this in the current state as there’s no info on what this would entail.\n\n\n\nReduce the work to two key programs (a and b below) and allow the community to vote on them separately, as they are two very different services in nature. You could group the 15 work items into a couple of buckets:\na) Protocol development work\nb) COMP reward management\nc) Laundry list of parameter changes and management\nGFX has strong experience with a), and it’s high priority for the protocol to address b). However, the addition of c) adds risk by increasing scope and it’s not clear why GFX might be successful with this work, as @allthecolors and other community members have noted. Even if someone had deep experience here, we’d hope they would provide more info on methodology and what infrastructure they have built to support the ongoing management of IR curves and similar quantitatively driven optimizations.\n\n\nProvide more info on how GFX will work with the community to ensure they are building the right solutions for the protocol. For instance, if you want to list new assets, there might not be a need for supply caps at all. How will you prioritize these initiatives over the next year? How will you work with other community developers as well?\n\n\n\n\n\n\n\nInitiative\nBucket\nNext Step\n\n\n\n\n1\nEnd COMP rewards. There is ~$1B of capital in Compound providing little value to the protocol beyond printing a higher TVL at the cost of $100m in COMP a year. While the original intention of COMP incentives was to disperse COMP to its users, the incentives have brought in mercenaries that claim and sell COMP.\nCOMP Reward Management\nProvide more info & methodology\n\n\n2\nRefactor COMP incentives to be used to bootstrap liquidity in new markets and scale back as liquidity enters.\nCOMP Reward Management\nProvide more info & methodology\n\n\n3\nSeek liquidity from other protocols like MakerDAO via their Direct Deposit program. GFX Labs has successfully pushed forward a Dai Direct Deposit Module for Compound 3 that will conservatively bring to the protocol a 100 million DAI credit line that is insensitive to liquidity mining rewards. GFX Labs plans to propose a similar program for USDC and USDP, though they are more complex from MakerDAO’s perspective and require more time and collaboration to prepare. Compound currently spends $38m in COMP incentives on the DAI market and another $38m on the USDC market. Sourcing liquidity from MakerDao can lead to substantially reducing stablecoin incentives.\nCOMP Reward Management\nN/A, this seems pretty straightforward\n\n\n4\nAdd supply caps. While the protocol currently has borrow caps to lower governance hijacking risk, the protocol can similarly add supply caps to limit the amount of an asset that can be deposited to the protocol. Similar to MakerDAO’s debt ceilings, supply caps provide governance with an additional tool to manage asset risk. Adding a supply cap mitigates the protocol’s concern of an infinite-mint attack and empowers the protocol to onboard assets it might not otherwise support.\nDevelopment Work\nCut. Happy to discuss further but this just does not appear to be an additive risk lever\n\n\n5\nAdd more assets. The protocol makes money by supporting markets that are in demand. With the addition of supply caps, the protocol can support more assets, since it can better manage risk.\nDevelopment Work\nN/A, Sounds great\n\n\n6\nImprove the oracle system to support more assets. The existing oracle system only supports assets with a Uniswap v2 market. GFX Labs has been working with Chainlink to develop UniswapAnchorView (UAV) v3 contract, which switches the protocol’s anchor to Uniswap v3. While this does support more markets, it is still somewhat limiting. Adding Balancer and other markets as available anchors will grow the range of assets Compound can safely support. In addition, the protocol should develop tooling to support other popular assets such as LP tokens and token derivatives. There are a number of large (by MCAP) assets the protocol could support that the market wants to use as collateral, such as wSTETH, that could significantly increase protocol revenues.\nDevelopment Work\nN/A, Sounds great\n\n\n7\nDeploy Compound v2 to L2s and sidechains. Interacting with the protocol is expensive on mainnet and can crowd out users who do not have the scale of capital to justify the gas expenses. Deploying Compound on Polygon, Optimism, and other networks would grow the protocol beyond Ethereum and put competitors at bay.\nDevelopment Work\nN/A, Sounds great\n\n\n8\nTransition off Legacy CTokens. The ETH, USDC, ZRX, & BAT markets are all running on a non-upgradable legacy ctoken contract. While WBTC has already migrated, the other markets have not. The protocol is currently missing out on additional revenue by not being able to configure those markets with the improved interest rate model, set a kink in the interest curve for ETH, ZRX, & BAT, and protocol liquidation fee. Generally, this change is easy and only has upside for the protocol.\nDevelopment Work\nN/A, Sounds great\n\n\n9\nInterest rate curves. Every interest rate curve utilized is the original interest curve chosen when the cToken was first configured (except DAI, which was updated on July 28th, 2020). This is a critical piece of Compound’s borrow/lend market, and yet the protocol has done little to research alternative curves or test the interest rate curves already utilized.\nMisc Parameter changes and Management\nCUT\n\n\n10\nDeprecate old markets. Compound v2 is nearing 1000 days old. The crypto landscape has changed significantly over the last few years, and just as Compound needs to adapt to what participants want today, governance also needs to offload what isn’t required or safe to support. Depreciating REP & SAI, closing down legacy markets, and managing risk on older markets such as BAT & ZRX is important to maintaining the protocol.\nMisc Parameter changes and Management\nCUT\n\n\n11\nUpdate the liquidation system. The efficiency of the close factor and liquidation system as a whole hasn’t been researched meaningfully or iterated upon. As a critical component of the protocol, it needs more dedicated resources.\nDevelopment Work\nProvide more info & methodology\n\n\n12\nUpgrade testing and developer code base. While Compound’s codebase and testing was state of the art at its launch, it has since fallen behind. Without a clear, incentivized, and responsible party to improve and maintain the tools, they have become hard to work with and outdated, which has hurt development efforts.\nDevelopment Work\nN/A, Sounds great\n\n\n13\nClean up the existing protocol. The Comptroller needs to be cleaned up and possibly entirely refactored. Similar to the prior point, the codebase was innovative at launch but has since fallen behind without incentivizing someone to improve the system.\nDevelopment Work\nN/A, Sounds great\n\n\n14\nSeparate revenue generation and risk management. Reserve factors should be optimized to balance protocol revenue and liquidity.\nMisc Parameter changes and Management\nCUT\n\n\n15\nRebalance reserves. The stock of reserves should be adjusted periodically to minimize risk. Compound is not in the business of taking positions and should seek to offload the surplus of volatile assets for stablecoins or even COMP.\nMisc Parameter changes and Management\nCUT\n\n\n\nWe’re looking forward to working with GFX and the greater Compound community to land a scope and move forward here.\nSeparately, using KPIs to drive a performance bonus sounds like a great way to align costs to the protocol with value delivered to it. However, as @TylerEther and others have pointed out, the proposed bonus structure is not tied to the work GFX is doing any more than it is tied to the overall market recovering. Hopefully by creating a clearer, more focused scope with defined success metrics, another KPI to incentivize success will become an appealing option. The fact that the COMP price is an input to the bonus formula (and the bonus is paid in COMP!) begs the question - how is this bonus structure better at incentive alignment than just paying the team in COMP that vest over a long period of time?\\n\n\n\n pauljlei:\n\nCut. Happy to discuss further but this just does not appear to be an additive risk lever\n\n\nIntrigued why gauntlet doesn’t think supply caps will be beneficial. Can you please expound?\\nThank you, Paul & Gauntlet, for your comments. We’re glad Gauntlet thinks GFX has been a great contributor and can make a meaningful impact at Compound.\nThe proposal is purposely flexible in avenues to add value but concise with incentives. The objective isn’t to do any one of the items on the to-do list, but instead to holistically improve the protocol to drive COMP appreciation and higher cash flows. Crypto & DeFi are rapidly evolving environments. We plan to adapt to the protocol’s needs to reach the goals of Compound governance.\nAs a participant since the early v1 days of Compound, completing the first CAP, and fixing the oracle after the DAI Nov 2020, Getty and GFX Labs has already established a record of value creation at the protocol. If this proposal passes, GFX will become the tip of the spear and push efforts everywhere necessary to re-establish the protocol in a dominant market position.\\nIt is clear that GFX Labs is a valuable contributor to the Compound Ecosystem and should be paid in accordance with some of the work that is outlined in the above proposal. As a Compound token holder, the largest issue I have with the above proposal is the lack of clear KPIs and commitments from GFX Labs. i.e.\n\nThe proposal is purposely flexible in avenues to add value but concise with incentives. The objective isn’t to do any one of the items on the to-do list, but instead to holistically improve the protocol to drive COMP appreciation and higher cash flows\n\nI have never seen a contract (in my traditional line of work) that simply agrees to pay someone $5 million dollars without clear deliverables and KPIs to measure the deliverables against. Price appreciation in the Compound Token is too nebulous to benchmark a proposal against and does not provide the community with an accurate data point to reevaluate future contracts on.\nIn the vein of DAO work and lack of OKRs, the Maker Community seemingly when through a contentious vote around offboarding a Core Unit, in part because they did not provide KPIs and ways to measure the team’s success against - MIP41c5-SP3: Facilitator Offboarding (MKT-001) - Archive - The Maker Forum 8.\nDAO governance and contracting are difficult, and I don’t want to make this process onerous to discourage future companies or individuals from proposing similar things as above. However, I believe DAO contracting proposals require a greater degree of granularity around commitments and relevant KPIs than this proposal provides. It is important as community members that we spend the protocol’s money as effectively as possible and are able to analyze service providers’ performance in a quantitative way.\\nI like where this is going. I have some follow-up feedback in two areas:\nBalancing contributor flexibility and accountability to governance\nThe recommendations from @pauljlei and @compound101 seem very reasonable to me. At the same time, GFX Labs’ contributions to this point have largely been real-time responses to evolving needs (as opposed to pre-established objectives with associated KPIs), so I can understand the inclination to push back a bit against specific KPIs that may sound sensible today but may not be in the best interest of the protocol 2 or 3 months down the road.\nWe can likely strike a balance where GFX Labs has some degree of autonomy in prioritizing deliverables while remaining accountable to governance during the streaming period. Even so, I still think that an initial categorization and prioritization of goals, along the lines of @pauljlei 's table, would be a worthwhile exercise for @GFXlabs to engage with and share with us, ideally prior to the passage of of this proposal.\nTo balance the community’s desire for clarity of objectives and deliverables with GFX Labs’ request for a greater degree of discretion over project management, we might consider establishing a regular schedule for GFX Labs to report on its progress and next steps. For example, we could request a quarterly report and set up a formal community evaluation/comment period at the 6-month mark as an opportunity for governance to provide feedback and request course correction if deemed necessary. The goal here would not be micromanagement, but rather to ensure (likely as it may already be) that GFX Labs is listening and communicating consistently with the developer/user/governance communities as it carries out its work. Provided the community upholds its part to provide that feedback (which admittely can be hard to get when things are going well and nothing’s on fire!), this structure would allow GFX Labs to course-correct and/or defend decisions around goal prioritization before any potential disagreement over the focus or quality of their work would escalate to the point of an offboarding discussion.\nStablecoin payment\nOn a separate note, I’m still feeling uncertain regarding the state of discussion on stablecoin payments. I believe this would be Compound’s first significant direct expenditure of stablecoin reserves outside of Proposal 59 (DAI liquidation reimbursement) and the first time reserves would be used directly to compensate contributors for their labor.\nI think of these reserves as a safety net against small underwater / unprofitable-to-liquidate positions in the protocol, which are miniscule at the moment but likely to grow over time (unless gas prices were to temporarily collapse, I suppose). In this light, doesn’t spending down such a large fraction of reserves increase insolvency risk to the protocol? Won’t it be harder to attract and compensate other contributors when each new agreement creates (what appears to me to be) a superlinear increase to insolvency risk? Should we set a strict lower bound on what fraction of assets we commit to leave untouched in reserves in each market? The principle of whether or not the protocol should pay contributors from stablecoin reserves feels under-discussed to me; perhaps folks with a better handle on the risk management side can refute or corroborate this perspective.\\nGreat discussion and insightful suggestions to improve transparency and accountability.\nHowever, the root of the problem, as @compound101 has noted, is that “DAO governance and contracting are difficult”. More specifically, it’s the utter lack of vendor and project management disciplines on the part of Compound (perhaps the case with most DAOs) mostly because there’s no centralized entity or leadership to enforce such disciplines.\nIt’s obvious that GFX Labs is highly regarded by the community. However, it’s hard to fathom that a $5 Million contract would be awarded in the traditional world without extensive documentation (business case and cost justification), clear deliverables, milestones etc. For software development contracts, even Big 3 consulting firms (such as Accenture) provide a clear breakdown of resources that will be used (1 Architect + 3 developers etc.) and their hourly rates. Additionally, most outsourced contracts (whether software development or other services) do not involve profit sharing at all, because it’s not difficult to source services through competitive bidding. Granted certain skills are hard to come by, in that case, a vendor can quote a high hourly rate.\nHere’re my thoughts on how to proceed:\nInterim solution:\nGFX Labs and other similar vendors can provide the following to the community:\n\nProjects they want to work on\nJustification and benefit to the DAO\nDeliverables\nNumber of resources and hourly rates (by each project)\nEffort and overall cost estimates and Timeline for delivery (by each project)\n\nDepending on the complexity, Compound can negotiate hourly rates and award projects on an “a la carte” basis.\nLong-term solution:\nIt’s become clear to me during the past few months that lack of vendor and project management disciplines will have an adverse impact on the survivability of the “DAO” model. Being a leader in the DeFi space, it is highly incumbent on the Compound community to consider the legal structure / entity models discussed in this A16Z paper 4, so that Compound will have a better ability to enter into and manage contracts.\\nThis proposal oozes chutzpah. At Reverie, we like ambitious proposals by extremely capable individuals and teams. And as everyone who has interacted with @GFXlabs and @getty knows, these guys can walk the walk. There should be little doubt about that.\nBefore we share our thoughts on @GFXlabs’ proposal, we’d like to be transparent about conflicts of interest. First, Reverie is a small investor in GFX Labs (<2.5%). Second, Getty is a personal friend of mine. Finally, Reverie holds a position in COMP (our exposure to COMP is worth significantly more than our investment in GFX Labs). In short, on paper, there’s a heck of a lot of conflicts of interests. In our opinion however, independence is overrated in matter such as these — when we make decisions, we want to be “dependent” and share in both the upside and also the downside of a decision. To us, a wholly independent opinion is a wispy one.\nNow for our thoughts on the proposal.\nMeasuring Success\n\n\nHaving too many KPI’s is not ideal: it gets in the way of focus (groups of people working on complicated projects can only focus on a few core objectives at once) and makes it difficult to focus on the most important things. We think there should be 3-5 KPI’s, tops.\n\n\nIn our view, there are three core KPI’s worth focusing on:\n\n\nProfit growth. Protocol profits is the money left over after the protocol pays all of its expenses, which today, come primarily from COMP-denominated incentives. Profit rather than revenue growth is a more accurate measure of success for a few reasons, with the main one being that it’s more difficult to “juice” profits than revenues. We can look at this by way of a simple example: Compound could very easily juice revenues by paying borrowers and lenders more COMP incentives; although revenue would grow due to an increase in outstanding borrow, profits would fall because the revenue would growth slower than the COMP token-based expenses.\n\n\nMarket share growth. Protocol market share can be calculated in several ways, including borrow-share (Compound’s percentage of all outstanding borrow on DeFi), revenue share (percentage of all money market revenue), and profit share (percentage of all money market profits). We believe all three of these metrics should be actively tracked.\n\n\nToken price. Over the long run, price follows fundamentals. So if the above KPI’s are doing well, this last KPI should take care of itself.\n\n\n\n\nCompensation\n\n\nThere’s a reason why the top employees make significantly more money than average employees: one top employee can contribute 100x more value to an enterprise than an average employee (who can be net negatives to an enterprise). That’s a long way of saying that we see no issue with paying truly exceptional people a lot of money for a job well done.\n\n\nAll at once, we’re not in favor of paying people a lot of money before a job well done. In our view, $5M cash is a dazzling amount of money for the protocol to pay over the course of one year. On the cash side, we think the protocol would pay @GFXlabs their labor and resource cost plus something extra on top to make sure they’re motivated by a healthy profit margin. For example, if GFX assigns six employees to Compound and each employee costs the company $250k per year, we believe Compound should be willing to pay 150% of labor cost to GFX, or $2.25M (thereby giving them a profit of $750k and a profit margin of 33%).\n\n\nWe think most of @GFXlabs’ compensation should be performance-based and tethered to the aforementioned KPI’s, along with one other requirement: that significant COMP-based upside be granted only if the KPI performance exceeds that of a predefined benchmark. In this case, we think the benchmark should be a basket of competitors/DeFi projects. More specifically, we think GFX Labs should only significant capture upside if their work makes Compound grow KPI’s faster than a basket of competitors. This would make sure the protocol is paying GFX for a job well done rather than paying the them for the job done by a bull market. To use a simplified example of how this could work, we could say GFX Labs gets paid $20M if their work leads to growth in profits, market share, and token price in excess of growth exhibited by Aave, Uniswap, and dYdX. This compensation scheme makes sure that GFX Labs can’t ride on the coattails of a bull market to achieve growth.\n\n\nWe’re lucky to have @GFXlabs and @getty. But we think it’s important to structure things the right way before we rush to pay people life-changing sums of money.\nI’ll also mention that it can be tedious to structure compensation over forum posts. If tokenholders are in favor of the proposal but aren’t sure about the compensation, our suggestion would be to form a “Compensation Council” or something of the like with the authority to (i) come up with compensation details and appropriate benchmarks, and (ii) review performance and award a performance-based bonus.\\nI think it might be best to assign an importance score or value score to each ticket item and pay bonuses based on those. There could be KPIs for each ticket item, then bonuses could be calculated as KPI * value score == [0, 1] * value score.\nThis would be very beneficial for one of the most important aspects of software engineering in respect to the longetivity and growth capacity: technical debt.\nAddressing technical debt usually doesn’t have immediate payoffs such as profit growth, market share growth, token price increases, etc., but if neglected, technical debt can really drag down a project. After all, the majority of the work spent on a software project over the long term is on maintenance. The easier a project is to maintain, the better.\\n@sukernik’s response is very well-reasoned and closely aligns with how we view compensation scenarios like this one.\nMy takeaway from the broad scope of @GFXlabs’s proposal that this should be viewed less as a decision on “tasks to be done” & how we measure the value of these tasks to the protocol - and more as a decision on “people to be hired” for the general benefit of Compound. To be clear - this means that the proposal should be taken with more weight, not less.\nAs a general principle, empowering specific individuals with a broad mandate is something that should be minimized as much as possible within any DAO, given the risk of scope creep and social capture over time.\nI’d suggest that this proposal add more detail on the resources to be committed - in terms of time, headcount or other applicable metrics - as well as a reasonable process for regular reporting on objectives, workstreams, and output / outcomes. This would help contextualize the base compensation.\nOn KPI-based compensation, @sukernik’s benchmarking approach is right on point.\nAdditionally, there should be a clear process for identifying under-performance and off-boarding, with a reasonable time window for @GFXlabs to remediate any issues and for the community to cut the engagment short if the remediation is unsatisfactory. In practice, this may take the form of scheduled polls and evaluations (either community-wide or by a select committee) at pre-defined intervals, where these issues can be raised + a 6-month window for remediation.\n\nour suggestion would be to form a “Compensation Council” or something of the like with the authority to (i) come up with compensation details and appropriate benchmarks, and (ii) review performance and award a performance-based bonus.\n\nThis is something we’ve frequently suggested as one of the most high-impact initiatives for the community to kick off. There’s a depth of topics that need to be addressed, including vendor evaluation, RFP development / oversight, and performance measurement.\\nI can’t just lurk anymore, because this conversation is following a pattern I see at all the protocols I hang out at. Someone who is a trusted community member comes with a proposal (and maybe it’s not perfect!) and then get told they’re not worth paying. People need a sanity check here.\n\n\n\n sukernik:\n\nIn our view, $5M cash is a dazzling amount of money for the protocol to pay over the course of one year.\n\n\nHow can people even say this? Didn’t these folks literally just get one of the biggest DAOs to agree to provide unlimited liquidity to the Dai market? How many tens of millions will that one item save Compound now that there will be a depositor who won’t remove a single coin when liquidity mining is turned off?\n\n\n\n allthecolors:\n\nSaid another way, for their base fee , GFX Labs is requesting all new USDC reserves accrued over the one-year contract in a no-change-in-revenue scenario (= $5M)\n\n\nSee what I wrote above. There’s ways to cut costs that we’re all eager to do cough Comp rewards cough. And look at who just worked at another protocol to bring liquidity that was 1) going to our main competitor for months now 3, and 2) is managed based on interest rates inclusive of rewards 1 so that the amount Maker provides is actually lower with rewards!\nWe would be brain dead not to phase out rewards on Dai deposits once this goes live.\n\n\n\n RogerS:\n\nHowever, it’s hard to fathom that a $5 Million contract would be awarded in the traditional world\n\n\nIn the traditional world, there would be a contract that would protect the contractor, too. Compound lives in a world where Justin Sun can borrow 100,000 Comp, send it to Binance to obscure it 3, and make some proposal to do god-knows-what to the protocol. What is to stop rando billionaires from cutting off any payment to Gauntlet or OZ or GFX or some other company providing services? There’s no job security here. If we don’t want to pay a lot for services, we need to escrow the money with a third party who can be bound legally not to pull the rug.\n\n\n\n pauljlei:\n\nhe proposed bonus structure is not tied to the work GFX is doing any more than it is tied to the overall market recovering.\n\n\nIsn’t Gauntlet asking $1.9 million 9 in exchange for just repeatedly saying “allow more leverage” and then collecting a fee based on hypothetical increases in borrowing? Don’t throw stones. Especially when you seem to add risk to other protocols you work on 18.\n\n\n\n compound101:\n\nAs a Compound token holder, the largest issue I have with the above proposal is the lack of clear KPIs and commitments from GFX Labs. i.e.\n\n\nSo let’s move beyond the question of pay – $5 million base pay is cheap and already paid for by work they did for free before proposing this big giant menu of stuff. This clear KPI thing is the real problem. Let’s get @franklin-pantera and the other VCs to each put a person in a room, lock the door, and let them out when they have a list of 5-10 things they want to see done in the next 12 to 24 months. Slide a pizza under the door if it takes a while.\nThink there’s actually an army of people ready to do those KPIs? Do like with auditing and ask for competing bids.\nI’m just a humble farmer and don’t know what Compound needs to make token holders like me lots of money, but you gigabrains go figure it out and then tell GFX and Tyler and ar00 and anyone else willing to roll up their sleeves to go do it. There’s a shortage of people willing and able to work here. Stop acting like we can’t pay a premium if anyone comes in here and delivers more than they cost. Know what? It’s not like we can’t kick them to the curb with a simple vote if it’s not working in a couple months.\\nWelcome @DefiDegen - fwiw, I think we’re actually saying the same thing. The ask for clear KPI’s is indeed the problem because it’s not directly addressing the point of @GFXlabs’s proposal aka:\n\nThe objective isn’t to do any one of the items on the to-do list, but instead to holistically improve the protocol to drive COMP appreciation and higher cash flows.\n\nThis isn’t a decision about a project list or a contractor, it’s about elevating the GFX team to the role of a core contributor. It’s clearly stated in the thread title, so @GFXlabs, let me know if I’m off base here.\nTo be clear, I think this is extremely worth considering. I hope to see many others follow GFX’s path of establishing themselves as credible, high-impact community members and then proposing a sustainable path for the community to bring them onboard in a long-term capacity. That’s why I’m more interested in getting greater precision on the inputs than the outputs.\nI also know that Getty & team can take feedback, it would be impossible to function in this environment without tough skin, so I’m not worried about hurt feelings here - especially since I see a thread of mostly constructive & reasonable points here. There’s real ways we can improve this proposal, so that we set a strong blueprint for more people to follow this path.\nI also believe that paying a premium for contributions is well worth it in this stage of growth and has long-term benefits in attracting more talent.\\n\n\n\n DefiDegen:\n\nSomeone who is a trusted community member comes with a proposal (and maybe it’s not perfect!) and then get told they’re not worth paying.\n\n\n@DefiDegen that’s not how I read any of the responses. It’s a constructive and well-meaning discussion to find the right balance. For example, @sukernik’s example of $250K per year per employee assumes an hourly rate of $125. While that’s just an example, I am sure that community would be open-minded to consider a higher rate for very skilled people, say senior architects. The question here is, how do we keep track of the actual hours dedicated by such a resource to Compound?\nNow, @franklin-pantera has pointed out that, perhaps, GFXLabs is intending this as a comprehensive protocol improvement outsourcing deal, and not to be treated on per project basis. This is somewhat akin to a Fortune 500 company outsourcing entire IT operations to a vendor. However, I can guarantee, from close experience, that even such deals are not awarded without extensive documentation, justification, cost estimates, estimates of vendor’s profit margin, and metrics to monitor.  Profit sharing arrangements based on the stock price of an outsourcer are almost non-existent.\nI can see three models, but first some definitions:\n\nActual cost: Cost based on an estimate of effort involved, Vendor resources, and resources’ actual compensation\nProfit Margin (Cash): A guaranteed profit if certain base metrics (deliverables related, not protocol performance related) metrics are met.\nProfit Sharing: Compensation based on protocol performance (earnings, COMP price etc). KPIs discussed by @sukernik are great.\n\nCompensation Models:\n\nActual cost + Profit Margin (Cash)\nActual cost + Profit Sharing\nActual cost + Profit Margin (Cash) + Profit Sharing\n\nModel 1 easily makes sense. Model 2 requires vendor to have some skin in the game, and they can be nicely rewarded for assuming such risk.\nHowever, Model 3 doesn’t make any sense at all. It’s all reward and no risk to vendors. It can be fine-tuned, but even with a very small Profit Margin(Cash) portion, it practically removes all risk to vendors. I hope Compound doesn’t take this route. If it ends up accepting this model, then it only shows that DAOs (at least, Compound DAO) are not ready for prime time.\\nCompound definitely needs to go multichain and I support ending with the rewards the way they are at present\\nHi, I’m new here, but I was curious whether this conversation had any progress lately outside of this thread.\nAlso, I was curious ask if more stock option-like / equity-like compensation models have been considered.\nFor example, if the compensation was $2m USDC as the yearly base but then had a $10m COMP component that vests over four years. It would still be significant compensation to @GFXlabs ($2m USDC + $2.5m COMP for the next 12 months), while not being too much stables as concerns raised from @allthecolors, and GFX would have a good balance of covering costs + capturing the upside potential that they seek, no?\nThere’s a lot of conversation that treats this engagement more like hiring a contractor. In that mental model, all the questions are reasonable: if you’re hiring Accenture for a Salesforce implementation, yes you defined every little detail of the deliverable and Accenture is expected to justify every detail of their cost/invoicing. But it’s my read that the intention of GFX’s proposal here is to hire them as a “full-time” team, being on the same side as the protocol not an outside vender.\nWhen you hire full-time employees at a startup you indeed do not define detailed deliverables. Instead, you define general roles and responsibilities and try to assess whether you would entrust the individual in consideration to make the decisions required in that role and execute. Then you load up a fair amount of equity compensation on top of base salary to align incentives for the long term success of the startup. In that mental model, I feel like what GFX is suggesting here is totally reasonable (detailed enough; there’s already demonstrated trust & demonstration of ability from prior involvement) if indeed thinking of them as a “full-time” long term committed team is their intent and they are willing to receive COMP that vests over time as big chunk of the compensation.\nOf course there is a question of whether $10m over four years of COMP is reasonable for the protocol to spend. But relative to $100m a year its spending on rewards, I feel like a $10m COMP spend that vests over four years could be a reasonably high ROI bet to buy the attention commitment from a team like GFX? If anything I feel like the shortage is on GFX caliber non-mercenary teams that the protocol could fund not the other way around."
  },
  {
    "number_of_comments": 9,
    "postid": "e37a6c92-1050-4788-a1ad-e0aee4d237e0",
    "posturl": "https://www.comp.xyz/t/compound-finance-community-calls/1224",
    "combinedcontent": "Every other Wednesday at 9:30 am PT there is a Compound Finance developer call in the Discord chat. That call is typically focused on development talk and not much else. A few days ago, the Blockchain at Berkeley club tweeted 7 that Compound Finance should have a goverance/community call on Clubhouse.\nI think it is a great idea to have a space to talk about Compound and governance regularly and try to grow the community further.\nI propose hosting a biweekly Clubhouse call on the weeks there are not development calls. We all have a busy schedule, so I have posted a poll below to figure out what day works for most people. Once we pick a day (the poll lasts 1 week), I’ll add a new poll to pick a time. I think the best time would be Wednesday late-afternoon PT, so people associate Wednesday with Compound Finance and the evening so people who can’t participate in the morning development call have an opportunity to participate.\nPlease vote for the days that work for you. Multiple votes are encouraged.\nWhich day should we have the Clubhouse call?47%Wednesday        23%Sunday    11%Friday  11%Saturday  5%Thursday 0%Monday0%Tuesday17voters17total votes\n                    \n                    Closed Feb '21\n                   \\nWhy clubhouse? couldn’t we just do it in the compound-live channel?\\nI’m an Android user so I can’t participate\uD83D\uDE22.\nI’d like you to use the Discord audio streaming channel \\nHosting it on Clubhouse will help with the discoverability factor, as it notifies people when new events are occurring (whether it be for people or genres you follow) via push notifications. I’ve stumbled upon many cool talks I would have otherwise not found because of this.\nIt is becoming the go-to platform for freeform conversation in the crypto world, and hosting at least some of our discussions their will certainly attract more eyes and eclectic conversation (TBD whether this will be good or bad).\nSo, yeah I think this is a good idea to at least test out…if it is unproductive then we can always just revert back to what we are doing now!\nthe main drawback is that like @tatsuzou12 said - Android users will not be able to join. Hopefully this is worked out soon.\\nThanks @getty, I think it’s a great call - especially if the focus is on casual / educational aspects of governance as well as “activating” non-voting COMP holders in an organic (and cost-effective) manner. Also would want to be conscious of relaying any key points or follow-ups from the discussion to this forum, since we should avoid over-fragmenting the governance layer when possible.\nHappy to help “host” the room from the #DEFI channel if others aren’t able to do so - that’ll add an extra layer of notifications / discoverability for the channel’s 15k followers.\\nThe community has chosen Wednesday. I recommend 6:00 pm PT because it is after most people’s working hours and generally far enough from the 9:30 am PT developer call that people should be able to attend at least one. Pick which times work for you. The poll will close Sunday night.\nWednesday Pacific Time30%18:00:00   20%10:00:00  20%19:00:00  10%20:00:00 10%21:00:00 10%6:00:00 0%11:00:000%12:00:000%13:00:000%14:00:000%15:00:000%16:00:000%17:00:000%22:00:000%23:00:000%24:00:000%5:00:000%7:00:000%8:00:000%9:00:0010voters10total votes\n                    \n                    Closed Mar '21\n                   \nPS: the poll can only have 20 options, so I removed the 4 most unideal imo.\\n@TennisBowling @tatsuzou12, I think Clubhouse is ideal for the points @MasterofNonce made. I know that Clubhouse is coming to Andriod soon and I look forward to seeing you and other Andriod users online.\n@franklin-pantera, neat idea. Send me a DM pls\\nIf anyone needs a CH invites they can DM me on Discord or Twitter\\nThe first Compound Finance community call will be this Wednesday at 6 PM PST on Clubhouse in the DeFi room (thank you, @franklin-pantera). Compound Finance : Community Call - Clubhouse 6\nWhile these calls will be less formal than the developer calls, we will still have the goal of discussing active forum posts, governance ideas, and ongoing development.\\n3/3 6 PM PST Comomunity Call Agenda:\n\n\nProposal 39: raise ZRX & BAT CF, lower WBTC CF.\n\n\nCompound Grants Program (CGP)\n\n\nOracle Infrastructure: Chainlink Proposal & Medianizer\n\n\nGovernor Bravo Development\n\n\nGateway\n\n\nOpen Q&A\n\n\nJoin: Compound Finance : Community Call - Clubhouse 6"
  },
  {
    "number_of_comments": 68,
    "postid": "cd4e6a70-e5bf-4938-aa39-981067fd27f5",
    "posturl": "https://www.comp.xyz/t/add-market-link/1516",
    "combinedcontent": "Chainlink 10 is arguably one of the most reputable, trusted, and valuable blockchain protocols, ranked the 10th largest coin by market cap at nearly $13B.\nI am proposing adding a market for $LINK as a collateral asset.\nHere are a few reasons why we should do so:\n\n\nUsed as collateral. Ideally, I think anyone should be able to borrow against any of their assets provided there is a market for it. The more assets we support, the greater financial freedom and capital efficiency we offer to DeFi users around the globe. This is especially important for the unbanked.\n\n\nOTC desks can quickly borrow at market rates to sell large amounts of tokens to their clients without much delay. Borrowing this way is much less risky for them to operate than for them to constantly hold large amounts of various volatile tokens.\n\n\nAllowing for strategic governance voting using borrowed tokens.\n\n\nIt is true that there is a risk when adding any asset as a collateral asset. While there is a risk, offering more collateralizable markets increases the value of the compound protocol, increases the utility of DeFi and crypto, and provides more financial freedom and capital efficiency to users (especially the unbanked).\nTo ensure the safety of the Compound protocol, we must carefully manage risk. In this case, we can do this by fine-tuning the collateral factor, reserve factor, and borrowing limits for $LINK.\nI’d like to open up a discussion as to what the optimal initial values of these risk-related factors should be.\nOnce we come to a majority agreement on this, I will deploy the contracts and create an autonomous proposal to be voted on by everyone. $LINK is already in our price feed, so this is fairly simple to do.\nSo with respect to $LINK, what do you think are optimal initial values for:\n\nCollateral factor\nReserve factor\nBorrowing limit\n\nThanks for reading,\nTylerEther / TRiLeZ\\nWill just link to this related post\n  \n    \n    \n    Adding LINK as a supported asset New Markets\n  \n  \n    As the Open Price Feed already supports LINK, I think we should consider supporting it. LINK has of the highest volume for ERC-20 tokens and has a limited supply cap. I believe it is well suited for being added to Compound.\n  \n\n\\nIncredibly excited about this. LINK seems like a great asset to add to Compound.\\nThe most recently added markets were cCOMP and cUNI.\n\n\nCollateral Factor for cCOMP and cUNI are both set to 60%.\n\n~35% of LINK (~$11.3billion USD value) is stored in a closed-source contract which is unknown whether there is a malicious distribution risk (see risk notes later in my post here).\nI think we should consider starting cLINK with a low Collateral Factor to help offset this risk.\n\n\n\nReserve Factor for cCOMP and cUNI are both currently at 25% (proposal 31).\n\nBorrow Cap appears to be Token-Specific. I’m not sure what this should be or even if it needs to be set for cLINK market? Need feedback from others on this.\n\nUNI Borrow Cap is currently set to 11.25 Million UNI\nCOMP Borrow Cap is currently set to 90,750 COMP\nThe Community MultiSig has power to adjust borrow caps\nFrom the Crypto Briefing article below it seems that LINK is NOT currently used for Chainlink governance. Maybe the borrow cap is less important if there is no voting-power borrow risk?\n\n\n\nFor the Interest Rate Model, cLINK might want to use the same one used for cCOMP JumpRateModelV2 | 0xd956188795ca6f4a74092ddca33e0ea4ca3a1395 (etherscan.io) 1\n\n2% APY borrow base rate\n20% APY borrow rate at kink\nKink at 80% utilization\n100% APY borrow rate at 100% utilization\nNo need to deploy a new IRM contract, just point to this one if using exact same rate parameters,\n\n\n\n@jmo from Gauntlet has been doing some risk modeling for Compound. I would love to have some input from Gauntlet for ideas on initial values for these market parameters for adding the LINK token.\nHere is the ChainLink Token (LINK) Security Audit Report 7.\nToken Security is important when adding asset markets to Compound. Consider the following from OpenZeppelin Compound Audit.\n\n“if a malicious or poorly-designed token is added to Compound, it could allow someone to steal all funds entrusted to Compound. For example, if anyone can arbitrarily change the totalSupply or account balances of a listed ERC20 token faster than the price oracle can adjust the price, an attacker could use those newly minted tokens as collateral to borrow all Compound assets.”\nLINK token audit above shows totalSupply at 1 billion.\n\nThe LINK contract code 2 is immutable (no upgrades) and the totalSupply is a hard-coded constant.\n\n\nHow are LINK tokens distributed to users from the total supply? If LINK tokens can be distributed arbitrarily faster than price oracle can adjust that would be a concern. If the distribution is predictable then this is less of a concern.\n\nI do have a security concern about this. ~35% of all LINK (~350million) is owned by a Smart Contract here which does not have its code published on EtherScan!\n\nContract Address 0x98c63b7b319dfbdf3d811530f2ab9dfe4983af9d | Etherscan 7 (closed-source contract holding 35% of all LINK valued today at ~ $11.3 Billion USD)\n\nIf this was distributed all at once or stolen by a malicious actor, it would be enough to take a large chunk of the liquid non-borrowed assets currently in Compound. Collateral Factor for cLINK could help mitigate this. With a 60% CF, this would pose ~$6.78 Billion USD risk. Starting with lower CF for cLINK might be a good idea due to this closed source LINK contract.\n\nSee the comment from rleshner in this discussion regarding using Borrow Cap’s on other Compound Markets (WBTC, ETH, etc) to enforce some liquidity safety buffer in each market.\n\n\nBefore adding LINK to Compound, I would love for this smart contract code to be published on EtherScan by the contract writer or other person with access to the code. Maybe someone with Chainlink contacts can push to get that contract code posted on Etherscan?\n\nI think this may be the Chainlink Node Operators incentive pool which appears to be centrally controlled by the company “SmartContract Chainlink Ltd.” Since the company controls these assets they can distribute in any manner they decide.\n\n\n\n\nThe LINK token control and distribution is a popular topic of discussion. Here is a recent article from Crypto Briefing.\n\nIs Chainlink Centralized? A Breakdown of Token Distribution | Crypto Briefing 5\n\n\n\n\n\\nGreat to see the community rallying around the addition of new assets.\n\n\n\n mistertom:\n\n\nI DO have a major security concern about this. ~35% of all LINK (~350million) is owned by a Smart Contract here which does not have its code published on EtherScan!\n\nContract Address 0x98c63b7b319dfbdf3d811530f2ab9dfe4983af9d | Etherscan (closed-source contract holding 35% of all LINK valued today at ~ $11.3 Billion USD)\n\n\n\nThis risk (an attack or movement of that contract’s LINK into Compound, in order to borrow Compound assets) could be offset by activating borrowing caps for Ether, WBTC, and the other collateral assets (BAT, ZRX, UNI, COMP).\\nI like the possibility of using Borrow Caps to help ensure some safety liquidity margin in other Compound Markets (ETH, WBTC, etc). I think Gauntlet had suggested something similar maybe in combination with their thoughts on automated governance parameter adjustments.\nI think another easy way to offset some of the risk of possible malicious or just unpredictible massive distribution from that 35% LINK holding contract is to start the cLINK market Collateral Factor lower. This is a parameter that is easy to start at zero or low, but difficult to smoothly reduce later (as we saw with WBTC reduction of CF recently).\\nI think part of my security concern is related to the central control by the Chainlink company of a massive amount of LINK tokens which at one point was ~ 65% of the 1billion tokens (at the time of ICO). They control several contracts and wallets the largest of which is the 35% one mentioned in my earlier post. Any safety decisions we make when adding LINK to Compound can help limit losses if these Chainlink treasury contracts or wallets become vulnerable to exploit or theft.\\nThanks mistertom for the reply! Non-circulating tokens and the creation of additional tokens were not things I considered before. I’ll think about these sort of attack scenarios and then loop back on this.\nI did some investigating into the contract at 0x98c63b7b319dfbdf3d811530f2ab9dfe4983af9d. The bytecodes generated is almost a 100% match to this contract: https://github.com/gnosis/MultiSigWallet/tree/4b9a417b63e433e353527ba73ef687e0eedc0d11 4.\nSo it’s safe to classify this contract as an old Gnosis MultiSig wallet.\\n@mistertom I think adding LINK with a collateralFactor of 0, like USDT is a good path forward.  Of course, we’ll want to raise that later. This would:\n\nPunt on any concerns with token centralization\nAllow the protocol to test if the interest rate model is a good fit for LINK\nGive Gauntlet some time to run stress tests and pick collateral requirements for LINK\n\nWe can’t commit to anything right now, but I think it would be great to get LINK on the platform in the meantime and start generating revenue with it to demonstrate the value it provides.\\nIf LINK starts with a collateral factor of zero and reserve factor of 25% I think I would be voting “yes.” That would be a super safe way to introduce it and give some time to deliberate on next value for the Collateral Factor while testing out the new market.\\nSo here’s the current issue with adding new markets as described above.\nSome tokens, such as LINK, are highly concentrated in a small number of wallets. These tokens are not in circulation. Price discovery of such token is based upon the circulating supply.\nAssuming demand for the token stays the same, a large increase in the circulating supply will likely pull the price of the token down.\nLet’s examine the specific case of the Chainlink founders dumping all of their dormant tokens into Compound protocol.\nNow, just having LINK sitting in our protocol likely won’t have much of an effect on the price as its still not really circulating. A large amount of LINK would either have to be liquidated or borrowed and then subsequently be sold in exchanges for the price to drop.\nLet’s examine large amounts of these dormant tokens being borrowed and sold. This would be a great way to short LINK - borrow a large amount, sell all at market price, then buy back at lower price points. Since Compound requires over-collateralization, this would be costly and risky for a user. We also have a borrowing limit to prevent this. This doesn’t seem like too big of a concern to me.\nNow let’s examine a scenario where all of these dormant tokens are used as collateral to borrow WBTC, then eventually all of the LINK tokens are subject to liquidation. Assuming all of the assets are seized, they would then be in the possession of a number of different owners. If these new owners were to sell all of their newly acquired LINK, the price could very well nosedive. This could cause a ripple effect of liquidations where LINK is used as collateral. This is what we want to avoid.\nSo there’s a clear risk when it comes to the collateralization rate of various tokens.\nBut this is not just some token, this is a token of one of the largest and most reputable blockchain projects. It’s highly unlikely that the Chainlink founders will perform such an attack. It’s more likely for such an attacker to be someone who’s hacked Chainlink’s Gnosis MultiSig wallet(s). But the risk is still there.\nAs a solution to this problem in general, we could introduce deposit caps or max collateralizable amounts based on trading volume for a period of time, for each and every market. One of us can make a new proposal for this.\nFinally, I’d like to mention this risk isn’t specific to LINK. Such an attack could be performed by whales with any asset.\\nI’m supportive of adding LINK, above were my attempts to investigate into the token and see what market parameters we might set to sell voters (and myself) on passing the proposal to add the new market. We need to sell the COMP voting whales on the initial parameters to reach that 400,000 votes needed for the proposal to pass. jmo from Gauntlet above mentioned there were similar central control concerns with adding USDT which resulted in setting the initial CF to zero. Our voters might be more trusting of Chainlink company than Bitfinex (Tether associated company) in which case they could approve initial Collateral Factor above zero. Choosing that initial CF value is a gamble, too high and the token add might fail to pass a vote. If the CF is set low enough I think we can almost guarantee that the proposal to add LINK market will pass and then future proposal to raise the CF can be voted on later with additional trust as users have seen the LINK market in operation.\\nAnother market parameter that hasn’t been discussed here, COMP distribution rewards for the LINK market. We could start the market out without COMP distributions. I believe when cUNI and cCOMP markets were launched, the COMP distribution was automatically adjusted with the size of each market. Now that COMP distribution rates are set manually, it can cause a mad rush to enter a market which starts empty and receiving a COMP distribution. Take for example the recent “upgrade” of the WBTC market. This created a new empty WBTC market with massive COMP distribution allocated (moved from old WBTC market). Compound users made a mad-rush to enter the new WBTC market to take advantage of the COMP distribution which was very massive for the first few users to enter the market. If we don’t mind this mad-rush then maybe we could set the COMP speed to match cUNI which was set in proposal 35: Comptroller._setCompSpeed(\"cUNI\", 1950000000000000). This distributes ~ 25.62 COMP per day to the UNI market split 50/50 between suppliers and borrowers. LINK would be the first market added since manual COMP speeds were implemented other than the WBTC migration.\\nI think using the distribution to improve the liqudiity flow into a new contract is great, this helps establish a great liquidity market for borrowers very fast.\\nThat’s a good point, the COMP distribution helps build up the new market with users quickly. I say we copy the COMP speed from cUNI for this new LINK market.\\nwe would have to “decrease” some amount from other market and give the decreased amount to the new market, to keep the “same” comp speed in total\\nI think there’s a buffer because total comp speeds are less than 0.5 COMP per block coming from the Reservoir. But I guess that other COMP is designated for other protocol purposes (grants, etc).\nWhich market(s) do you think we should reallocate the COMP Distribution from?\n\nComptroller compRate is 0.176 * 2 = 0.352 COMP per block (split 50/50 supply/borrow)\nReservoir drip rate is 0.5 COMP per block, the difference is reserved for grants, etc.\n\nTable has been updated with suggested COMP reallocation from rleshner. reallocate from cUNI/cBAT/cZRX to cLINK. My math needs to be double-checked for rounding / floating point errors.\n\n\n\n\nMarket\ncompSpeed\ncompSpeed*2\nNew compSpeed\nNew compSpeed*2\n\n\n\n\n…\n(supply) /block\n(supply+borrow) /block\n\n\n\n\nUSDC\n0.067\n0.134\nNo Change\nNo Change\n\n\nDAI\n0.067\n0.134\nNo Change\nNo Change\n\n\nWBTC\n0.01075\n0.0215\nNo Change\nNo Change\n\n\nETH\n0.01075\n0.0215\nNo Change\nNo Change\n\n\nUSDT\n0.00965\n0.0193\nNo Change\nNo Change\n\n\nCOMP\n0.005\n0.01\nNo Change\nNo Change\n\n\nUNI\n0.00195\n0.0039\n0.0014625\n0.002925\n\n\nZRX\n0.00195\n0.0039\n0.0014625\n0.002925\n\n\nBAT\n0.00195\n0.0039\n0.0014625\n0.002925\n\n\nLINK\nN/A\nN/A\n0.0014625\n0.002925\n\n\nTotals\n…\n…\n0.176\n0.352\n\n\n\nThe following link shows the COMP Distribution APY estimates which take into account market size. This can help us determine which market(s) we can reallocate some COMP distribution from.\n\nCOMP Distribution: https://compound.finance/governance/comp 6\n\n\\n1%~ from all market, that would be 0.00352, but also the idea of yours that actually we can afford giving it a comp distribution rate without decreasing any could be a solution too.\\nMy table has been updated above with the suggested 1% reallocation to LINK. My math needs a double-check though. I think this works well and then we don’t need to take any COMP from other treasury usage. One disadvantage of reallocating this way is the proposal will have a large number of function calls but that shouldn’t keep us from doing it .\\nThinking it more deeply this is actually an issue cause the governance contract can handle max 10 operation in one proposal \\nOoh, I didn’t know that! This would be 10 operations just calling _setCompSpeed. I’m guessing Governor Bravo doesn’t increase the number of operations?\n\nSpeaking of Governor Bravo do we know if the Compound Governance UI and comp.vote are supporting Bravo yet? @arr00 do you know if the governance UI’s are supporting Bravo now?\nFor testing deploying new LINK market and/or Governor Bravo proposals, which testnet would be best to use?\n\nMaybe for the comp speed updates (10 operations) we make that a separate proposal and do them back to back?\\n@TylerEther we’re getting closer to having values for all the market parameters to add the LINK market. We also need to review which contract code will be used for each smart contract used for the new market.\n\n\ncLINK proxy contract (stored cToken state and delegates functionality to implementation)\n\n\nCErc20Delegator, recently deployed for cWBTC2 market 2\n\n\n\nConstructor Argument\nValue\nNote\n\n\n\n\n\nunderlying_ (address)\n0x514910771af9ca656af840dff83e8264ecf986ca\n(LINK)\n\n\n\ncomptroller_ (address)\n0x3d9819210a31b4961b30ef54be2aed79b9c9cd3b\n\n\n\n\ninterestRateModel_ (address)\n0xd956188795ca6f4a74092ddca33e0ea4ca3a1395\n(shared with cCOMP)\n\n\n\ninitialExchangeRateMantissa_ (uint256)\n200000000000000000000000000\nCopied from cCOMP\n\n\n\nname_ (string)\n\"Compound ChainLink Token\"\nor \"Compound LINK\" ?\n\n\n\nsymbol_ (string)\n\"cLINK\"\n\n\n\n\ndecimals_ (uint8)\n8\n\n\n\n\nadmin_ (address)\n0x6d903f6003cca6255d85cca4d3b5e5146dc33925\nTimelock\n\n\n\nimplementation_ (address)\n0x24aa720906378bb8364228bddb8cabbc1f6fe1ba\n(shared with cWBTC2)\n\n\n\nbecomeImplementationData (bytes)\n0x\n\n\n\n\n\n\n\n\n\ncLINK Implementation contract (implements functionality used by the proxy contract)\n\n\nCErc20Delegate (from arr00: implementation can be shared with cWBTC2)\n\nMost-recently deployed for cWBTC2 implementation 2\n\nIncludes sweepToken enhancement for governance to collect accidentally sent tokens received by the proxy contract.\n\n\n\n\n\nThe current PriceOracle contract used by the Comptroller already is configured for LINK so I don’t think any changes are needed there. The important thing is that the underlying address for LINK matches the value in the oracle (the price lookup uses this).\n\n\nUniswapAnchoredView\n\nDeployed for the implementation of cUNI\n\n\n\n\n\nI’m sure there are things I’m missing but I wanted to get the discussion started on details needed to make the LINK market live. This might be something which can be tried on a testnet, or a test environment like ganache cli or hardhat (something to fork mainnet for testing). Help is available on the #development channel of Discord for any technical questions.\\nThanks @mistertom\nIt looks like we won’t need to update any COMP distribution speeds. They’re all calculated based on utility. They’ll be automatically updated when the new market is added. \\nThanks for taking the lead to getting this done. I’m quite supportive as long as the parameters are acceptable. I will try to catch up on the conversation soon and have some more constructive input.\nSomething that caught my eye and I’d like to point out:\n\n\n\n mistertom:\n\nCErc20Delegator implementation should be deployed first\n\n\nWe actually can, and should, use the exact same implementation already deployed for cWBTC2. It is at this address 0x24aa720906378bb8364228bddb8cabbc1f6fe1ba.\\nThat was changed in proposal 33. We still will have to do it manually but those are details for the proposal, don’t need to be figured out for first steps of making the new market. blck said we have up to 10 operations per proposal so maybe we could just reallocate the comp reward from less markets or allocate it from the excess accruing.\n\nRemove automatic COMP claims and COMP speed refresh 1\n\nIt was causing too much gas usage for users interacting with Compound and there was also a need to reduce the size of comptroller contract for some future features being considered.\\nAh good point. Because it doesn’t store any data/state then it can be shared by multiple proxy contracts. Thanks for this detail, I’ll update the post with the same implementation address used by cWBTC2.\\n\n\n\n jmo:\n\nI think adding LINK with a collateralFactor of 0, like USDT is a good path forward.\n\n\nAgree; starting with a 0% collateralFactor is a “no risk” approach to add the asset. Based on @jmo and Gauntlet’s recommendations, there can be an immediate follow-up proposal to raise the collateralFactor, after the market develops.\n\n\n\n blck:\n\nwe would have to “decrease” some amount from other market and give the decreased amount to the new market, to keep the “same” comp speed in total\n\n\nI think the simplest (and most fair?) approach, would be to draw it from “similar” assets; UNI, BAT, ZRX; set LINK and these to 0.0014625, which would preserve the total COMP per day distribution, both of collateral assets, and for the total distribution.\\nI think we’re ready for the proposal.\nHere’s the script to deploy the cLINK contract:\nnpx saddle -n mainnet script token:deploy '{\n  \"underlying\": \"0x514910771af9ca656af840dff83e8264ecf986ca\",\n  \"comptroller\": \"$Comptroller\",\n  \"interestRateModel\": \"0xd956188795ca6f4a74092ddca33e0ea4ca3a1395\",\n  \"initialExchangeRateMantissa\": \"2.0e26\",\n  \"name\": \"Compound ChainLink Token\",\n  \"symbol\": \"cLINK\",\n  \"decimals\": \"8\",\n  \"admin\": \"$Timelock\",\n  \"implementation\": \"0x24aa720906378bb8364228bddb8cabbc1f6fe1ba\",\n  \"becomeImplementationData\": \"0x\"\n}'\n\nThis is using an updated version of token:deploy: https://github.com/TRiLeZ/compound-protocol/tree/update-deploy-token 8. Tested and works nicely.\nI need someone to verify that those parameters are correct.\nFor the governance proposal:\n\nCollateral factor: 0%\nReserve factor: 25%\nComp speed: 0.0014625 (set for LINK, BAT, UNI, and ZRX)\n\\nMy apologies for screwing up the initialExchangeRateMantissa_ … I was copying from the cWBTC2 contract deployment rather than doing the math myself. Unfortunately this one for LINK is different because underlying decimals for ChainLink Token are 18. I’ve corrected my earlier post with this new value.\nThe standard cToken initial exchange rate for Compound is 0.02 scaled. The goal is to have 50 cTokens = 1 underlyingToken when the market is first deployed. In our case for cLINK, the math is:\n\n0.02 * 10^(18 + underlyingDecimals - cTokenDecimals)\n= 0.02 * 10^(18+18-8)\n= 0.02 * 10^28 … floating-point representation …\n= 2e26 … unsigned scaled integer representation … (value used for constructor)\n\n\nYour math looks good to me!\\nThe math works well with reallocation of COMP distribution from UNI/BAT/ZRX. I’ve updated the post above to indicate rebalancing those three for equal distribution with LINK.\\n50 cTokens = 1 underlyingToken really clears up the confusion. Thanks for this! \\n\n\n\n rleshner:\n\nI think the simplest (and most fair?) approach, would be to draw it from “similar” assets; UNI, BAT, ZRX; set LINK and these to 0.0014625 , which would preserve the total COMP per day distribution, both of collateral assets, and for the total distribution.\n\n\nI don’t think it’s “most fair” distribution. I suggest we keep UNI,BAT and ZRX distributions where they are. They are modest and actually kind of fairly distributed, getting only their share of 10% from total daily distribution equally distributed between all existing markets, without any additional initiatives unlike stable coins and some other markets like COMP (which actually getting unfair portion, due to existense of COMP market CAP). There is no real reason to decrease distributions to minor markets. Instead decrease DAI and USDC distribution in equal portion to create 0.00195 distribution for LINK. Which will be decrease of 0.000975 for both DAI and USDC. Distribution is still massively shifted towards stable coins at the expense of everything else.\nAlternatively, distribution for LINK could be completely drawn from COMP, which is distorted market for a long time, producing unfair distribution to early borrowers, actually paying them to borrow COMP and at same time blocking new borrowers from joining. And since governance is adamant in not raising CAP at COMP market, to let market fix overinsentivisation of borrowing, that is actually a way to kind of apply a patch, by removing extra distributions and directing it to LINK market.\nRemoving 0.00195 from COMP probably wouldn’t completely fix COMP market, but will make it less ugly then it is now. with it’s -7% APY for borrowing (yes, minus, you getting paid for borrow COMP) But that minus 7% isn’t for everybody, but only exclusevely for those, who already borrowed. New people can’t borrow, because CAP \\nThank you for outlining this problem. 0 COMP able to be borrowed but the utilization rate is reported to be 16.16%.\nUtilization rates need to be updated to take borrow caps into effect.\nA new thread/proposal should be created to address this.\\nAre the simulations of the proposal somewhere (e.g. GitHub) where we can review them?\\n\nDeployments\nThese use the updated token:deploy script found here 7.\n\nRopsten\nnpx saddle -n ropsten script token:deploy '{\n  \"underlying\": \"0x8fa29bece84a731008f585aac8f47a08bf4e33c6\",\n  \"comptroller\": \"$Comptroller\",\n  \"interestRateModel\": \"0x2341Ba42Eb00c63CF03559c9A2295a23ace7E4aD\",\n  \"initialExchangeRateMantissa\": \"2.0e26\",\n  \"name\": \"Compound ChainLink Token\",\n  \"symbol\": \"cLINK\",\n  \"decimals\": \"8\",\n  \"admin\": \"$Timelock\",\n  \"implementation\": \"0x0295a48b76bc68662bd15bfaecedca075a4f568f\",\n  \"becomeImplementationData\": \"0x\"\n}'\n\n\nMainnet\nnpx saddle -n mainnet script token:deploy '{\n  \"underlying\": \"0x514910771af9ca656af840dff83e8264ecf986ca\",\n  \"comptroller\": \"$Comptroller\",\n  \"interestRateModel\": \"0xd956188795ca6f4a74092ddca33e0ea4ca3a1395\",\n  \"initialExchangeRateMantissa\": \"2.0e26\",\n  \"name\": \"Compound ChainLink Token\",\n  \"symbol\": \"cLINK\",\n  \"decimals\": \"8\",\n  \"admin\": \"$Timelock\",\n  \"implementation\": \"0x24aa720906378bb8364228bddb8cabbc1f6fe1ba\",\n  \"becomeImplementationData\": \"0x\"\n}'\n\n\nProposal\n\nRopsten\nComptroller._supportMarket(0x92c27d1E28dFa05662a46fCc70C5F4aDc392b3E9)\nSupport cLINK\nComptroller._setCollateralFactor(0x92c27d1E28dFa05662a46fCc70C5F4aDc392b3E9, 0)\n0% collateral factor\ncLINK._setReserveFactor(250000000000000000)\n25% reserve factor\nSee here 4.\n\nMainnet\nComptroller._supportMarket(0xface851a4921ce59e912d19329929ce6da6eb0c7)\nSupport cLINK\nComptroller.setCollateralFactor(0xface851a4921ce59e912d19329929ce6da6eb0c7, 0)\n0% collateral factor\nComptroller._setCompSpeed(0xface851a4921ce59e912d19329929ce6da6eb0c7, 1462500000000000)\ncompSpeed(cLINK) = 0.0014625\nComptroller._setCompSpeed(0x6C8c6b02E7b2BE14d4fA6022Dfd6d75921D90E4E, 1462500000000000)\ncompSpeed(cBAT) = 0.0014625\nComptroller._setCompSpeed(0x35a18000230da775cac24873d00ff85bccded550, 1462500000000000)\ncompSpeed(cUNI) = 0.0014625\nComptroller._setCompSpeed(0xB3319f5D18Bc0D84dD1b4825Dcde5d5f7266d407, 1462500000000000)\ncompSpeed(cZRX) = 0.0014625\ncLINK._setReserveFactor(250000000000000000)\n25% reserve factor\\nLooks good to me. The contract deployed to 0xFAce851a4921ce59e912d19329929CE6da6EB0c7 matches the expected value.\nWould like to see some simulations prior to actual proposal, but once those are in, I’m very excited for this to be proposed. Good job.\nYou can base simulations on this one 3 I wrote a while back. You will need to migrate it to work with governor bravo, but that shouldn’t be much of an issue.\\nWith @arr00’s help, I’ve written these simulations: compound-protocol/hypothetical_clink_integration.scen at clink-integration · TRiLeZ/compound-protocol · GitHub 6\nThe simulations pass!\nGive me the go-ahead and I’ll submit a CAP \\n@blck would you be willing to fast track this to full proposal with your 100k+ voting power? CAP always takes so long and painful to gather the 100k delegation.\\n@sukernik is it possible to grant something to @TylerEther for work on making this LINK market happen. I don’t see it in the list of funded grants so maybe hasn’t been applied for yet.\\ni have to wait till the active one is gone, cannot have more than one active\\nUnderstood I was asking too early, sorry about that. Excited to see the active proposal pass soon with almost unanimous support. \\nGreat suggestion @mistertom.\n@TylerEther - your fantastic effort on adding LINK to Compound is definitely eligible for a grant from the Compound Grants Program 1! Is that something you’d be open to?\\nI’ve just applied for a grant. \nThanks everyone for the support!\\nFantastic work @TylerEther - excited for this to become a proposal \\nNow that there are no proposals active, I’ve submitted a CAP: Ethereum Transaction Hash (Txhash) Details | Etherscan 9\\nEDIT: The full proposal to add link was directly proposed by blck who has enough COMP delegated. Delegations to the Autonomous Proposal (CAP) need to be re-delegated to another voting address before voting starts on the full proposal in order to vote for the full proposal!\n\nhttps://compound.finance/governance/proposals/46\n\\n\n\n\n mistertom:\n\n0xb3768415df87eba2d2019e89f9a31bbb313f8c7b\n\n\nTo delegate your comp, one has to do\n\ngo to compound app\ngo to vote\nyou’ll see delegating to - click change\ndelegate to CAP address 0xb3768415df87eba2d2019e89f9a31bbb313f8c7b\n\n\nto save fees you can go to:\n  \n      \n      comp.vote\n  \n  \n    \n\nComp.Vote 9\n\nGas-less voting and delegation for Compound Finance governance. Sign and relay your transactions for free.\n\n\n  \n  \n    \n    \n  \n  \n\n\\nInteresting discussion here: Lower Proposal Threshold 6 regarding the high COMP delegation requirement for making full proposals. Specifically comments from Arr00 and Getty who have personally been through the pain of trying to gather 100k COMP delegation for CAP’s.\\nThanks for sharing this thread. Coincidentally 65k COMP is currently delegated to the proposal 12 (exactly what @arr00 was suggesting we set the threshold to).\nI definitely support lowering it to at least 65k, and possibly 50k, to get these CAPs through to a formal vote, as they appear to have widespread community support.\\nThere’s an “easy” way to fast-track popular proposals past the CAP and delegation hurdle. If we have a community member with 100k voting power already delegated (currently there are 8 such voters) we can try to convince them to submit the proposal as they already meet the delegation threshold for full proposal. The discussion to lower the delegation threshold proposal both increases the number of community members with enough power to immediately submit full proposals, and lowers the delegation hurdle for Autonomous Proposals. This has been discussed before and we will probably see a proposal or CAP coming to lower the threshold  in the future.\nIn the case of adding LINK, we’ve already reached over 65k delegation so we’re gaining momentum toward the 100k needed for the CAP. It will be interesting to see how gathering delegations goes, 35k is still a large amount of COMP to get re-delegated!\\nAgreed! I know @blck has indicated willingness to delegate (but is currently tied up in another live proposal).\nAny other takers?\\nIf nothing changes i will do it at the end of sunday, so the review and the voting period will be on normal working days and not weekend.\\nWell the CAP for adding LINK lost its 65k delegation. Maybe would you consider proposing the LINK token-add this coming Sunday evening @blck ? @TylerEther received a grant for this so you two could probably coordinate offline for any compensation of gas fees or other work items, or even directly with grants committee if you want separate payments.\nThere’s another proposal inbound here Safety and Gas Patches but it appears they’re not done with the code reviews and changes yet. If that proposal does come out before this one then I guess we can use the new cToken implementation (part of each liquidation goes to cToken reserves).\\nTUSD proposal is now live so we’ll have to wait until after that one. https://compound.finance/governance/proposals/45 5\\nhttps://compound.finance/governance/proposals/46 12\\nAwesome, I didn’t even know we could have two proposals pending at the same time!  @TylerEther you can get that COMP back from the cap for voting on this proposal by using the terminate() option on the cap. Thanks so much for posting the proposal @blck ! \\nyou can have more it just a limit / proposer\\n@tonyotonio delegations to the CAP will no longer be votable for adding LINK because we were able to get a full proposal posted by blck (community member with 100k delegation). Please update this post to remove the cap address and request users to re-delegate to their preferred voting address (self or community member).\nWe are currently in the two-day review period so there is still time to re-delegate!\n\nFull Proposal 46 to add LINK 23\n\\nUpdate: some benevolent user has queued the proposal in the timelock, thank you for that! \nNow that the proposal has passed the vote (success), it can be queued in the Timelock. There will be a 48 hour delay between when it is queued and when it can be executed. Any address can queue the proposal.\n\n\nGovernorBravoDelegator | 0xc0da02939e1441f497fd74f78ce7decb17b66529 2\n\nWrite as Proxy Function “queue”\n\nProposal ID: 46\n\n\n\n\n\nUpdate: Proposal has been queued!\n\\nExcellent! Thanks for the update.\nHappy to see all the recent progress here in the Compound community, I feel like we are in a groove. Congrats again to @TylerEther on getting this proposal passed!\\nCongratulations everyone! \nWhat a great community! Thanks everyone!!\\nIs this TRiLeZ from Tribot?\\nI’ve been spotted   \\nGood to see talent in the crypto space!\\nLINK:\n\nTraded at a large number of exchanges: Binance, Okex, Coinbase, FTX, Houbi. Between them and legitimate others, the 24hr volume looks to be +$400m\nDEX liquidity: Uniswap v3 ~$83m volume 7-day, Uniswap v2 $30m liquidity 1, Suhsiswap $63m liquidity\n\nNotable: There are a few wallets with considerable LINK holdings. I think they are related to the team. I’ll try to find out.\nFDV: $24B\nInitial collateral factor: 50%\n\nFor reference: COMP\n\nTraded at a large number of exchanges: Binance/US, Okex, Coinbase, FTX. Between them and legitimate others, the 24hr volume looks to be around $210m\nDEX liquidity: Uniswap v3 ~$17m volume 7-day, Uniswap v2 $3m liquidity, Suhsiswap $14m liquidity 1, Balancer v2 $7m 1\n\nNotable: The treasury holds 31.5% of minted COMP.\nFDV: $4.5B\nCollateral factor: 60%\n\\nSomething very important to consider about link is the concentrated holdings. 45% of link is held in unverified contracts (I think by link team). This is $10+ B which would essentially allow the link team to take all of Compound held funds. I think limiting the amount of collateral Link in the protocol would be a good idea.\\nI am not concerned but I appreciate you bringing up the point. It would be great if we had supply caps.\\nA related scenario to consider is if a hacker gained control of the Chainlink project wallets containing this $10B worth of LINK. Today that hacker might be able to liquidate $100M via DEXes. But Compound offers a much wider exit ramp: by depositing the $10B LINK you could “borrow” $5B of liquid assets.\nSo allowing unlimited collateral means the Compound protocol inherits the security quality of the Chainlink treasury. About all you can say about that is “seems good so far”. But, by creating the exit route for funds, we would increase the incentive to hack their wallets by 50x or more.\nAt a minimum for our due diligence, we should know how many individual people would need to be compromised to allow this to occur.\\nHere are a few ideas for mitigating the risk of a collateral exploit (this applies to any vulnerable collateral, not just LINK, especially those with centralized minting such as WBTC):\n\n\nsupply cap - this is a sure way to limit the damage of a collateral exploit, but has the big disadvantage of putting a hard limit on the size of the protocol.\n\nsupply inflow rate limit - for example, constrain the daily inflow of new collateral assets to some dollar value, possibly with a governance multisig override. This defends against the “hacker trying to quickly liquidate stolen assets” attack, while allowing unlimited gradual growth of the total value of protocol assets.\n\nsegregated markets - for example, create a separate set of pools where riskier assets could be supplied as collateral, and where stablecoin lenders would most likely be rewarded with higher interest rates to reflect the higher risk. Some downsides: fractured liquidity and complexity for users.\n"
  },
  {
    "number_of_comments": 9,
    "postid": "a622088a-b9a9-4bb7-9d04-e546a4906cbb",
    "posturl": "https://www.comp.xyz/t/gauntlet-compound-renewal/3541",
    "combinedcontent": "\nSummary\n\nA proposal to renew Gauntlet’s 12-month engagement with Compound on continuous market risk management to maximize capital efficiency while minimizing the risk of insolvency and liquidations to create long-term sustainable growth. Gauntlet’s current engagement with Compound runs through September 27, 2022.\n\nBackground\n\nFor the past three years, Gauntlet has worked with Compound to maximize the protocol’s capital efficiency given an acceptable level of market risk.\nOver the past year, Gauntlet has delivered the following:\n\n\nRecommendations: Provided 16 sets of parameter recommendations, including 45 total parameter changes to 13 total assets. Gauntlet also provided initial risk parameter recommendations to support the launch of Compound III 6.\n\nCommunity Updates: Built Risk Dashboard 25 to provide insight on risk and capital efficiency for the community. Updated the community on risk developments during Compound Developer Calls. Published educational resources including VaR/LaR Deepdive, Model Methodology 1, Parameter Recommendation Methodology 1, and CMA/ES 1.\n\nAnalysis: Continuously monitored market risk including publishing 2 Market Downturn Reports (May 2022 1 and January 2022 1). Provided analysis and recommendations on critical initiatives including ETH Merge 1, Reserve Factors, Asset Listing Framework, and MKR Borrow Cap. Provided analysis for Compound’s S&P rating 1, the first credit rating in DeFi history.\n\nIn the two years prior to this past year, Gauntlet worked formally and informally for Compound to perform market risk assessments 3, contribute to treasury management 6, optimize incentives 3, calibrate risk parameters 3, and upgrade 1 the protocol 1.\nResults\nOver the past year’s engagement, Gauntlet increased collateral factors for the majority of assets while incurring no major insolvencies despite large market crashes. As a result, borrowers increased their utilization, which generated an additional $5.15m of borrow interest income and an additional $96m+ in total borrow. For more details and further reading, click here 16.\n\nProposal\n\nScope\nGauntlet’s Risk Management platform quantifies risk, optimizes risk parameters, runs economic stress tests, and calibrates parameters dynamically. We use agent-based simulation models tuned to actual market data to model tail market events and interactions between different users within DeFi protocols. Our agent-based simulations are constructed analogously to how transaction-level backtesting is done in high-frequency and algorithmic trading. Gauntlet’s platform provides similar statistical power in these actuarial analyses by modifying these techniques to handle the idiosyncrasies of cryptocurrencies.\nContinued support for Compound II\n\nCoverage of all markets except Legacy (e.g., WBTC) and Deprecated (e.g., SAI, REP)\nSupported risk parameters: Collateral Factor, Close Factor, Borrow Cap, Reserve Factor, and Liquidation Incentive\nMarket conditions will determine the frequency of updates. For that reason, no SLA will be preset\n\n[New] Gauntlet will support Compound III, integration is in progress\n\nCompound III introduces new mechanisms that pose opportunities and challenges as they relate to managing market risk and optimizing capital efficiency, such as a different composition of user positions and risk profiles, updates to reserve size due to the new liquidation mechanism, and new parameters to manage risk with greater granularity.\nCoverage of base asset (USDC) and all collateral assets (WBTC, WETH, LINK, UNI, COMP)\nSupported risk parameters: Borrow Collateral Factor, Liquidation Collateral Factor, Liquidation Factor, Borrow Cap, Collateral Safety Grade, [New] Supply Cap, [New] Target Reserves, [New] Store Front Price Factor, [New] Liquidator Points\n\n\nOut of scope\n\nProtocol development work (e.g., Solidity changes that improve risk/reward)\nFormalized mechanism design outside of the supported parameters\nGauntlet will not look to manage the following at the outset: enabling or disabling a currency for borrowing, optimizing COMP emissions\n\nDuration\n1-year engagement (Sept 28, 2022 to Sept 28, 2023)\n\nExpectations\n\nOutcomes\nGauntlet aims to improve the following target metrics without increasing the protocol’s net insolvent value percentage:\n\n\nValue at Risk: conveys capital at risk due to insolvencies when markets are under duress (i.e., Black Thursday). The current VaR in the system is broken down by collateral type. Gauntlet computes VaR (based on a measure of protocol insolvency) at the 95th percentile of our simulation runs.\n\nLiquidations at Risk: conveys capital at risk due to liquidations when markets are under duress (i.e., Black Thursday). The current LaR in the system is broken down by collateral type. Gauntlet computes LaR (based on a measure of protocol liquidations) at the 95th percentile of our simulation runs.\n\nBorrow Usage: provides information about how aggressively depositors of collateral borrow against their supply. Defined on a per asset level as:            \nwhere U is the utilization ratio of each user:\n\nGauntlet aggregates this to a system level by taking a weighted sum of all the assets used as collateral.\n\nCommunications\n\nRisk parameter change steps: forum post, community discussion, on-chain vote\nParticipation in community calls with explanations of risk parameter changes and any anomalies observed, including but not limited to Discord Developer & Twitter Spaces community calls\nRisk Dashboard (refer to the next section)\nMarket Downturn Risk Reviews to provide a detailed retrospective on market risk\n\n\nDashboards\n\nAs part of this engagement, Gauntlet will update the Risk Dashboard 25 for Compound III to provide key insights into risk and capital efficiency for the community. The dashboard focuses on both the system-level risk in Compound and the market risk on an individual collateral level. Our goal is to help convey our methodology to the community and provide visibility into why Gauntlet is making specific parameter recommendations. Updates to the current dashboard include an updated UI and historical views of protocol statistics, including (but not limited to) total supplies and borrows, collateral usage, and customer acquisition and retention metrics.\n\nCost\n\nGauntlet charges a service fee that seeks to be commensurate with the value we add to protocols and provides a strong signal of our alignment with the protocol.\nThe service fee structure will be the same as the previous year’s engagement. The quarterly performance fee is calculated per the following formula: log(Number of Assets, 10) * Total Borrow * Marginal Base Fee tier bps / 4\n\n1242×468 22.9 KB\n\n\nTotal Borrow is calculated as the 30-day average and rounded down to the nearest $1B. Given that Total Borrow adjusts based on market volatility (e.g., Compound’s total borrow is down ~88% YoY), this metric provides strong alignment with our clients as we are both incentivized to grow this metric, and it allows our service fee to adjust with our clients’ growth.\nWhen Total Borrow < $2B, there is no basis point fee. In this case, the formula is log(Number of Assets,10) * $1,200,000 / 4 ) — this is effectively the “minimum fee.” As of this posting on August 30th, there are currently 16 assets and ~$1B in Total Borrow. To provide an example - if the quarterly fee were calculated today, it would be $361k (~$1.45m annually).\n\n[New] Insolvency refund: In order to increase our alignment with Compound and put actual “skin in the game,” we will refund a portion of our payment should our risk parameter optimizations incur losses for the protocol during the engagement. Our ultimate goal is to protect the protocol - we stand behind our work and want the community to have confidence in our recommendations.\nHow this works:\n\nA portion of our payment (30% of the minimum fee) will be transferred from Compound in a lump sum COMP transfer to an on-chain Polygon vault. Funds will be converted and actively managed by Gauntlet to manage the potential backstop.\nLosses are defined as any new insolvencies related to market risk or oracle failure.\n\nExclusions: Issues related to smart contract bugs or related to an underlying asset that is smart contract related and dust accounts (defined as accounts with borrow less than $1,000). Refund does not apply if any Gauntlet risk parameter (excluding Reserve Factor) proposal fails during the engagement.\n\n\nShould losses occur, Gauntlet will share an update with the community, and send funds back to the DAO in a timely manner.\nAt the end of the engagement, any remaining funds in the vault will be removed, realized by Gauntlet’s Finance team, and no longer eligible for a refund.\n\nPayment Method:\n\nPayment currency will be the same as our prior engagement - denominated in COMP at 30d VWAP. Gauntlet has yet to sell any COMP, but note that we may do so in the future for tax, operational, or other company requirements.\nFor the insolvency refund, 30% of the minimum fee will be paid in lump sum fund transfer, at the start of the engagement and deposited in a vault.\nThe remainder of the minimum fee will be paid via a year-long Sablier stream.\nAny additional fees beyond the minimum fee will be calculated at the start of each quarter and will be paid via lump sum fund transfer, at the start of the quarter, following a governance vote.\n\n\nNext Steps\n\nPlease share any comments or feedback below. We are targeting to submit a governance proposal by Sunday, Sept 18.\n\nAbout Gauntlet\n\nGauntlet 6 is a simulation platform for market risk management and protocol optimization. Our prior and current optimization work includes engagements with Aave, MakerDAO, Sushi, Synthetix, BENQI, and many others.\\nAs I have closely followed Gauntlet’s offering during the past year, and interacted with them on several occasions so that their engagement brings more value to the community, here’re my thoughts:\n\nGauntlet presents quantitative rigor to most of their analyses. They frame the problem in terms of a math equation, which is quite insightful. Latest example 12.\nHaving a party dedicated to risk management is very desired for a leading DeFi protocol like Compound, and I believe that Gauntlet has done a good job during the past year.\nTheir dashboard is unintuitive and not friendly for users not well-versed in risk management. My understanding is that they are working aggressively to make improvements. I’m looking forward to the upgrades.\nMy biggest concern has been with respect to the transparency of their models. I urge them to look for ways to explain their models, processes, simulation sequences with inputs & outputs better. Move from the current engagement model that is mostly predicated on “Trust”, towards “Trust, but Verify”.\nEven so, the “Trust” placed by the community didn’t fail us the past year - the protocol didn’t incur insolvencies despite market volatility.\n\nIt’s nice to see that they will provide risk management for Compound III too, and will also have some “skin in the game”.\nBased on the above, I would vote “yes” to renew their engagement.\\nWe (Blockchain@Columbia) agree with @RogerS’s statements above. Gauntlet’s commitment to solving parameter optimization problems by generating models specifically for the Compound protocol is quite valuable.\nFor a student group like ours, where leaders often change and new entrants must be quickly brought up to speed, Gauntlet’s posts with clear rationale and models for each recommendation are quite helpful. We also benefit from the availability of the Gauntlet team to communicate on proposals/questions when they arise.\nWe plan to vote “yes”\nTo comment on the one criticism raised above, we do agree that more transparency & higher levels of community understanding for most of the models would be positive. There is material that exists on this front (as Paul dove into in the Background–>CommunityUpdates section) but most of that is fairly technical. It would be beneficial to release some type of content (videos, shorter posts, threads) that would be more easily accessible to a wider group of potential community members.\\nWe Blockchain at Berkeley (calBlockchain.eth) are super excited to have Gauntlet advising governance on risk during the launch of Comp III. We believe the service Gauntlet provides to Compound is critical to the safety and solvency of the protocol.\nWe are excited about the possibility of Gauntlet making their risk assessments more accessible and understandable by governance. More clarity on the inputs/conditions of models, reduces trust when the analysis is too complex and unclear for voters with to understand fully, comprehend, and verify the relevance of Gauntlets simulations. We hope this can be done in an accessible way where most users can begin to understand the nature of the models.\nWe believe Gauntlet’s insolvency refund mechanism will additionally align Gauntlet with the success of Compound, and makes us much more comfortable voting “yes” to continue this partnership.\nWe are happy to support.\\nThank you, @RogerS, @blockchaincolumbia, and @devenmatthews / Berkeley, for your feedback and support. We are looking forward to increasing transparency and community understanding. Recently, we launched the below in light of those efforts and will keep the community posted on our updates:\n\n\nCompound Analytics Dashboard 6: The Analytics Dashboard shows key metrics and insights on supply, borrow, usage, and liquidations\n\n\n1864×647 110 KB\n\n\n\nDashboard FAQs 2: to address common questions about market risk and Gauntlet’s Risk Dashboard.\n\nAs a reminder, we will publish an on-chain vote on Sunday, 8/18, and voting will begin on Tuesday, 8/20. One day before the on-chain vote (Saturday), we will return to the forums with the latest market data to calculate the service fee, which will be used for the on-chain proposal.\\nThe metrics as of Friday, 9/16/2022 are:\n\n30-Day Average Total Borrow: $948,996,468\nAssets: 16\nCOMP 30-Day VWAP: $50.8\n\nUsing the table above and the minimum fee, the annual fee is $1,444,944, which equates to 28,444 COMP Tokens using the 30-Day VWAP. 30% (8,533 COMP) will be paid in lump sum fund transfer. 70% (19,911 COMP) will be paid via a year-long Sablier stream. We will publish the on-chain proposal later today.\\nHi @pauljlei ,\nThe following dashboard: Metabase 3, includes Compound VIII or only V2 at the moment?\nThanks in advance, and I find it really helpful.\\nI look forward to voting for Proposal 125 19.\nOver the past year, Gauntlet has set the bar for a professional organization performing services for a protocol, and I expect that their services will continue to improve.\n\nThe Compound protocol has benefited from their risk management: across the board, collateral factors have increased, without accruing bad debt or losses–even during extremely volatile market environments.\nThe quantity (and quality) of their reporting and dashboards is improving.\nThey are helping the community safely transition into Compound III on Ethereum (and beyond).\nThe renewed service fee is competitive, and by including an “insolvency refund”, significantly more aligned with the success of the protocol. This could (should) become a best practice for all protocol services.\n\nDisclosure: in my personal capacity, I work closely with the CEO of Gauntlet in a venture fund that we manage, Robot Ventures 4. Robot Ventures holds a small stake in Gauntlet. I have campaigned for and against Gauntlet proposals in the past, and do my best to stay objective when evaluating Gauntlet, especially as it relates to compensation.\\nThank you, @rleshner, for your support and feedback. We look forward to supporting Compound III’s continued growth and success.\nWe appreciate your feedback, @dcota. The dashboard currently includes Compound II, but we look forward to adding Compound III in the near future.\nAs a heads up, voting for our proposal begins ~12 PM PT today.\\nGauntlet has been a valuable partner for Compound, and their risk management strategies have proven to be helpful across a broad spectrum of market conditions. I’m looking forward to seeing the updated Risk Dashboard and continued engagement for Compound III moving forward. Additionally, the Insolvency Refund is a creative way to align incentives in the event of downside risk.\nOverall, I’m excited for what’s to come and appreciate Gauntlet’s efforts in improving their dashboards and reporting standards as they continue to grow alongside their partners."
  },
  {
    "number_of_comments": 22,
    "postid": "3a64025e-1d58-4a45-a80c-6d80266e780f",
    "posturl": "https://www.comp.xyz/t/compound-improvement-proposals-cip/3722",
    "combinedcontent": "\nSimple Summary\nFollowing our Post-Mortem recommendations to create a Working Group, OpenZeppelin has decided to go further in proposing the creation of a Compound Improvement Proposal system to better capture and coordinate initiatives for the Compound protocol. Following a similar format laid out by other decentralized protocols, we’ve drafted a foundational CIP-1 to define the process for CIPs and the role they can play in the Compound ecosystem going forward.\nWe will be holding an inaugural Working Group call on Oct 26th at 9:30am PST in the Compound Community Discord to discuss feedback from the wider community and plan next steps.\n\nCIP-1: Purpose and Guidelines\nAuthor: Michael Lewellen\nStatus: Draft\nType: Meta Process\nCreated: 2022-10-20\n\nOverview\nA Compound Improvement Proposal (CIP) describes standards, processes and enhancements intended to improve the Compound Protocol. They can take the form of governance processes, on-chain enhancements, off-chain tooling or other goals that help Compound achieve objectives as a leading DeFi lending platform.\n\nCIP Rationale\nCIPs are intended to be the default mechanism to improve Compound as both on-chain code and off-chain processes.\nIt’s important to note that CIPs are strictly separate from Compound Governor Proposals 4 and should not be considered a required prerequisite for submitting an on-chain proposal. Instead, CIPs are intended to define better processes for certain types of Governance Proposals in addition to serving as a way to solicit community feedback and acceptance on certain protocol enhancements before they are submitted on-chain.\nCIPs also serve as a place to suggest improvements that any community participants can use to find contribution opportunities and even seek funding through grant committees and other support groups.\n\nCIP Types\nThere are three types of CIP:\n\n\nMeta Process - describes any process to be adopted or modified to coordinate Compound governance, development, or community efforts\n\nProtocol Enhancement - describes any change to the smart contracts that make up the Compound Protocol and Governor\n\nTooling & Support - describes any additions or improvements to off-chain infrastructure, tooling, documentation or other components that support the usage of the Compound protocol.\n\n\nCIP Lifecycle\nCIP Authors will start out with an idea, draft that idea into a CIP document, request a peer review and then move through an approval process with the Compound Working Group. Ideas can be filed as issues on a commonly shared GitHub similar to Ethereum PM 1. CIPs can be drafted as PRs and then request review to be added to the agenda for the biweekly Working Group meetings.\n\n1024×297 10.6 KB\n\nCIP Editors will be selected from among core community contributors that will attend meetings regularly and provide feedback to CIPs. If any objections are raised to a CIP in its first meeting, it must address that feedback and then request another review in a future meeting. In a CIP’s second meeting, if there are no objections OR a majority of Editors are in favor, the CIP is approved and then remains in Last Call for 14 days before the implementation process begins.\n\nOnce past the Working Group, a CIP’s implementation path will diverge depending on its type. Meta Process CIPs will be approved by a majority Signal vote by COMP holders with no quorum after which they will become officially adopted as part of the Compound governance process that all COMP holders are expected to uphold. Other CIPs are left to the community to implement which can include funding to be approved by a Grant Committee or any other entity willing to support. Protocol Enhancements will also need to pass through a Compound Governance vote just as any other smart contract change would.\n\nWhat belongs in a successful CIP?\nEach CIP should have the following parts:\n\n\nPreamble - RFC 822 style headers containing metadata about the CIP, including the CIP number, a short descriptive title (limited to a maximum of 44 characters), a description (limited to a maximum of 140 characters), and the author details.\n\nAbstract - Abstract is a multi-sentence (short paragraph) technical summary. This should be a very terse and human-readable version of the specification section. Someone should be able to read only the abstract to get the gist of what this specification does.\n\nMotivation (optional) - A motivation section is critical for CIPs that want to change the Compound protocol. It should clearly explain why the existing protocol state is inadequate to address the problem that the CIP solves. This section may be omitted if the motivation is evident.\n\nSpecification/Process - The technical specification should describe the syntax and semantics of any new feature. For Meta Processes, this should explain the processes in detail step-by-step. The specification should be detailed enough to allow for the implementation of either a technical implementation or governance process should the CIP be approved.\n\nRationale - The rationale describes what motivated the CIP and why particular design or process decisions were made. It should describe alternate designs/processes that were considered and related work, e.g. how the feature/process is implemented in other protocols. The rationale should discuss important objections or concerns raised during discussion around the CIP.\n\nSecurity Considerations (optional) - While not always required for “Meta Process” or “Tooling & Support” CIPs, every “Protocol Enhancement” must contain a section that discusses the security implications/considerations relevant to the proposed change.\n\n\nFAQ\nMany similar systems have already been created for protocol governance so it’s important to understand that while CIP may borrow ideas from structures such as EIP 3, MIP 4 or AIP 3, it is intended to serve Compound’s specific needs and may therefore differ in several key respects.\n\n\nDo I need to create a CIP and/or get it approved in order to create a Compound Governance Proposal?\n\nNo! Compound Governor has operated for several years without CIPs and can continue to vote on Proposals independently of them. A CIP involving on-chain improvements could eventually lead to a Governance Proposal but the CIP is still separate and focused more on coordinating off-chain community consensus.\n\n\n\nDo I need to create a CIP in order to receive funding from Compound?\n\nTechnically no. Compound funding is provided through either the Compound Governor or a delegated grant committee (there is currently a grant committee proposal 1 pending) that remain independent of CIPs. However, CIPs can provide a pipeline for important ideas to receive community feedback and support that increases the chance of receiving funding.\n\n\n\nWill the Compound Governor be bound to follow Meta Process CIPs for its governance processes?\n\nFrom a smart contract perspective, COMP token holders will always be the final decision-makers for Compound Governor Proposals. However, COMP Signal Votes will be used to establish support for certain governance processes that voters should abide by. If an On-chain Proposal is submitted in violation of a CIP Process, it will be up to COMP voters to reject the Proposal or effectively deprecate that process. CIP Editors will be responsible for making COMP voters aware if a Proposal has not followed an established CIP Process during the two-day review period.\n\n\n\n\nNext Steps\nWe ask the community to read through CIP-1 and share their feedback here in the forum and attend the Working Group call on Oct 26th at 9:30am PST in the Compound Community Discord. During the community call, we will push forward to achieve the following:\n\nAddress immediate feedback from the community to improve CIP-1\nEstablish a group of CIP Editors that will shepard the CIP process\nCreate a GitHub repository to contain CIP drafts and track progress\nMove to finalize CIP-1 and hold a COMP Signal Vote to mark its official adoption\nEncourage the creation of additional CIPs to capture existing processes and begin working on new improvement ideas. OpenZeppelin will specifically be focused on our  security recommendations from the cETH Price Incident Post-mortem.\n\nIf you want to share direct feedback or get involved, please free to share your feedback below or reach out directly to me on Discord, Telegram or email:\n\nEmail: michael@openzeppelin.com\n\nTelegram: @cyloncat\n\nDiscord (in the Compound server): Michael L#3462\n\\nThis makes a lot of sense - a formal way to determine what goes into the protocol (&community) specifications \\nGreat job @cylon. This is much needed. This provides a streamlined process for bringing forth creative ideas / offshoots of Compound protocol and gain community feedback. This can pave the way to think in terms of Compound “ecosystem”, not just the base protocol.\\nGreat job putting this together @cylon. The proposed structure of CIP is very well defined, appreciate the efforts. I’d also suggest an optional section on Legal Compliances to be added to the CIP, that might come along with the proposed CIP. This way, we would have a comprehensive CIP.\\nHi @harsha - Thanks for the suggestion.\nLegal Compliance is a tricky topic depending on the subject-matter of a CIP. It’s also important to note that CIPs don’t represent binding decisions beyond a consensus that is left to the community to self-enforce. However, if legal compliance is an important topic for a specific CIP, it could fall under the existing Rationale section as a factor considered in the design/process decisions.\nFor example, in CIP-1 I explicitly state that CIPs are strictly separate from Compound Governance Proposals in the Rationale section. While not my primary intent, this statement could also help address any legal concerns that a CIP contributor might have about the legal responsibilities behind drafting a CIP as opposed to making a DAO governance proposal.\\nMichael, this is a fantastic idea and the details you’ve laid out here are very promising from my perspective. Thank you for taking initiative on this! I have a question and a comment.\nWhat is a Signal vote? Can you give more details on that?\nMy comment is that I want to remind you and everyone in the community that the name of the game for the protocol and DeFi generally is decentralization. Getting as many helping hands as we can and as many opinions on how to proceed is best!\nHere is a link to the discord event of the working group kick off: Compound 4\\n@adam Thanks for the support and setting up the Discord call!\nThe Signal Vote would be a process that allows COMP tokenholders to signal their support or opposition to a CIP through off-chain voting such as Snapshot. CIP-1 specifically states that Meta Process CIPs go through a Signal Vote to indicate that there is support from DAO stakeholders to uphold those processes for certain types of governance proposals. This would also include the adoption of CIP-1 itself.\nMy reasoning for using Signal Voting is:\n\nCompound Governor Proposals cost voters gas and would add friction to the process of passing Meta CIPs\nMeta CIPs represent non-binding governance processes so the on-chain execution of proposals is not necessary\nReaching quorum threshold for Compound Governor requires 400K which I personally feel is excessive for Meta CIPs.  CIP-1 recommends no quorum threshold be necessary for a Signal vote to pass although I’m open to feedback here.\nMost importantly, CIPs should remain strictly separate from Compound Governor Proposals to avoid confusion.\n\nIn short, the Approval of a CIP by CIP Editors vs its Implementation by Compound Governor and the community should always remain separate processes. Signal Voting through Snapshot helps to emphasize that separation while still giving COMP token holders a voice in the adoption of Meta CIPs for governance processes.\\nthanks for opening this topic.\nAbout CIP structure, it looks good for me but I think that it is also important to mention its impact as separated section. and how was merging motivation and rationale section instead of making motivation as the optional?\nlt would be this structure:\n\n\nMotivation\n\nwhat is the problem?\n\n\nProposed Solution(or Proposal Details)\n\nproposal details including tech spec, steps to achieve this goal.\n\n\nAlternatives considered\n\nExpected impact\n\n\nexpected results: modeling result\n\n\nusers should take actions before this proposals get activated\n\n\n(Ex: Liquidation, deposit stop for the current market, the current market will be closed)\n\nSecurity Considerations (optional)\\nHi @dakeshi - Thanks for the feedback.\nI made the Motivation section optional just because it was an extension of the structure I adopted from EIP-1. I agree that there are good reasons to make it required.\nI think Proposal Details is also a better way to represent the Specification/Process section. We could make Alternatives considered a required sub-section of Rationale. I also like the idea of including Expected impact.\nOne thing to note is that I do hope to make CIPs as accessible as possible to new submitters so I’m hoping to keep proposal requirements minimal. I still think your suggestions make sense but that’s the only reason I hesitated to add more initially. In general, I would hope that its easier for folks to submit and get a CIP approved than an EIP but there will still need to be a minimal level of detail to provide.\\nHi everyone - Reminder that we have a Working Group Call in the Compound Discord 1 to discuss CIP-1 tomorrow at 9:30am PT\nIf you’d like to suggest changes, ask questions or become involved in the CIP process, please be there! If there are no further objections or feedback to CIP-1 in this first call, I’ll submit a finalized version of the CIP-1 to be reviewed in the Community Call next week before placing it in Final Call stage and announcing a list of CIP Editors.\nAlso, if you’d like to be a CIP Editor, please attend tomorrow or message me to let me know.\\nThanks for your feedback. I totally agree that CIP should be easy to access for everyone with the minimal requirement + some optional parts(as a template) to accept various types of opinions.\nTo make slim structure, I’m still considering which is the best candidate for the section title among Motivation, Rationale and Description based on research results.\n\nIn addition to references you mentioned(EIP, MIP, AIP), I did take research about these kinds of template formats including swift language 1, gatsby, react js and rust\n\nIf we choose Rationale, I think that it doesn’t need to be separate Motivation section because it is possible to mention its background and motivation in Rationale section.\\nReally helpful to hear you go through the proposal and potential changes on the call today.\nI do wonder whether it’s worth specifying in any way what criteria the editors and working group will use to assess whether proposals can pass on to a vote (if a vote is called for) – since the group will serve in a kind of gatekeeping capacity. Perhaps also whether editor/working group approval indicates an endorsement of the proposal, or whether it indicates only that it has met xyz criteria and is presented to the community neutrally.\nI raise this in part because of what I’ve been seeing with Optimism governance: the Governance Committees (which are experimental) are under a great deal of scrutiny, and it’s been contentious. Very different situation, I know. But it may be worth thinking through some of these issues at some point during the initial phases of this new process (if you haven’t already).\\nHi @duncand - Great question. I appreciate you bringing it up and sharing your perspective from similar initiatives.\nAt its core, I think CIP Editors should give approval based on similar criteria as EIP Editors 2\n\nRead the CIP to check if it is ready: sound and complete. The ideas must make technical sense, even if they don’t seem likely to get to final status.\nThe title should accurately describe the content.\nCheck the CIP for language (spelling, grammar, sentence structure, etc.), markup (GitHub flavored Markdown), code style\n\nOverall, the Editors are there not to pass judgment on CIPs but merely do the administrative & editorial part. They may still voice either support or opposition to the CIP’s claims of improving Compound but that should not factor into their descion-making criteria to approve a CIP. That will come down to the implementation phase where a CIP is left up to a Snapshot, Governance or Grant decision.\\nHi everyone,\nThanks for attending the CIP-1 Working Group Call this Wednesday. You can find the recording here 4.\nI’ve finished incorporating all of the latest feedback into CIP-1 here 8. I’ve added several new sections outlining CIP Editor responsibilities, History, Copyright, touched up the template structure and addressed some minor wording changes.\nNext steps are to review it in the community call next week and form a founding team of CIP Editors. Once a group of CIP Editors has been chosen, I intend to push for CIP-1’s approval followed by a Snapshot to be conducted two weeks after.\\nAhead of the community call today, I’ve established the following list of community members who have graciously volunteered to act as the founding group of CIP Editors. They have also been added to CIP-1 here 2.\n\nMichael Lewellen (@cylon)\nSriharsha Karamchati (@harsha)\n@Arr00\nJared Flatow (@jared)\nGeoffery Hayes (@hayesgm)\nKevin Cheng (@kevin)\nDuncan Dobbelmann (@duncand)\n@Dakeshi\n\nWhile these members may still change with additional members that wish to join in the following weeks, this group will act as the final approvers to move CIP-1 into the Final Call stage during the community call.\\nAfter reviewing in yesterday’s community call, we’ve got no objections to moving CIP-1 into the Final Call stage. You can see the finalized draft here 4.\nPer the CIP Lifecycle 3 process, CIP-1 will be in the Final Call stage for the next 14 days before going through a Snapshot vote of COMP token holders, which would start on November 16th. If the vote passes with a simple majority, CIP-1 will be adopted!\nDuring this time, I’ll continue to solicit feedback and coordinate with the current CIP Editors. I also intend to start work on a GitHub repository for managing the CIP content ahead of the vote so we will be ready to accept new CIP submissions in anticipation of CIP-1’s adoption.\\nThanks so much for kicking this off. A two questions come to mind:\n\nThe process behind the off chain voting is very vague. Is there a recommended quorum, duration, and venue (https://snapshot.comp.vote 1 for example)?\nHow should the shared repository be managed? Possibly have a compound dao org with CIP editors as members?\n\\nHi @arr00 - Great questions and something good to address now that we’re in Final Call.\nI’m intending the process of voting to be:\n\nUse Snapshot through this existing Compound page 3\n\nSetup the vote with no quorum per the CIP-1 process and use single choice voting with a simple majority.\nSet the duration of the vote for one week to start Nov 16th and end on Nov 23rd.\n\nWe can see an example of another Snapshot vote conducted by Gauntlet here 2. I can take the lead in setting up the Snapshot vote and over the voting process in the next Community Call.\\n@arr00 - Regarding the repository, I think we should do it under the existing Compound Finance GitHub Org 3 but have the repo itself be managed by the CIP Editors.\n@adam What do you think about this approach?\\nSounds good, I like that approach.\\nHi everyone,\nToday is the day we start the Snapshot vote for the adoption of CIP-1. The only change the document has received since going into Final Call status was the welcome addition of @CL_Michael to the CIP Editors list.\nThe vote will be conducted on Snapshot here 10 and is scheduled to end in one week. There is no quorum threshold but I would still kindly ask that all Compound contributors vote in support to send a strong signal for CIP-1’s adoption. After all, there are no gas fees \nPlease note that you do need to have your COMP delegated to your wallet before you can vote on Snapshot. If anyone has any issues with using Snapshot for voting, please reach out to me or @arr00. I’ll also be available to answer any final questions on today’s community call.\\nHi everyone,\nI’m happy to share that CIP-1 4 has passed with 201K votes in support and <1 against. Starting next week, I’ll begin the process of setting up a new CIP repo and begin working to prepare additional CIP proposals alongside other community contributions.\nThank you to everyone that contributed and supported this initiative!\\nHi everyone - An initial setup of the CIP GitHub repository is now live here: GitHub - compound-finance/cip-pm: Compound Improvement Proposal PM Repository 16\nI’ll review the current status of this repo in our upcoming community call."
  },
  {
    "number_of_comments": 19,
    "postid": "62f00373-2ace-4cbf-8622-2531f1c06f37",
    "posturl": "https://www.comp.xyz/t/oracle-contributor/1887",
    "combinedcontent": "After 6-months, it is great to see the protocol running on the new oracle system 26. A couple of total ground-up reworks, countless edits, and a lot of convincing, but I am thrilled to see it implemented. The improvement will help prevent another Dai November 2020 event and enable the protocol to list a breadth of new markets.\nWhile the major update has been made, more work is still needed. We still need to transition the Uniswap anchor from v2 to v3, support additional markets, ongoing monitoring 6, and further research the system’s efficiency. The recent milestone is the beginning of a more vigilant and efficient Compound.\nI am requesting a 0.000214 Contributor Comp Speed grant 54 from the protocol. Over the last 6-months, I acted as the project manager for the oracle improvement. I researched a myriad of options, worked closely with the Chainlink team, managed community feedback and input, championed the project, and most importantly, got it through governance. The ongoing contributor grant will be for the work I have done to get the oracle improvement in place and to manage it going forward.\nThe contributorCompSpeed grant works similarly to the compSpeed parameter for cTokens. The speed is set to a number of COMP rewarded each block, and the grant receipt can claim it like regular COMP rewards. However, unlike a typical grant, it will be distributed over time rather than immediately, and the grant can be ended at any time by governance if the community feels I am not doing a good job maintaining and developing the oracle system.\nA contributorCompSpeed of 0.000214 is ~500 COMP a year using an average block time of 13.5 seconds, and the address associated with it will be my governance address 0x9B68c14e936104e9a7a24c712BEecdc220002984 24/gettyhill.eth.\nIf this is successful, I hope this encourages other community members to apply for ongoing contributor grants. There is a lot of work that needs to be regularly happening to improve and manage the protocol. Feel free to message me on Discord if you are looking for a way to get involved in the development/research of the protocol.\\nGetty was instrumental to Proposal 047, which took months of coordinating and development, to create a price feed which is likely the most robust /safest in the entirety of DeFi, and met the highest possible standard demanded by the Compound community. He knows the system as well as anybody, and has built a great relationship with both Chainlink and Compound governance.\nIn addition, Getty has been organizing the bi-weekly general community call on Clubhouse, to communicate development and broaden the Compound community.\nSimply put, I strongly believe Getty is the ideal person to carry on the work of the price feed, and serve as a guardian of the protocol.\nThis would be the first streaming COMP grant, and well deserved.\\nGreat work @getty, this was a tough change to make and I was very impressed with your ability to get it done. Personally, I am in support of your request and hope to see others follow in your footsteps \\nGreat job on the proposals, and thank you for your involvement in the community. I definitely believe that you are deserving of the grant.\nThat being said, why can’t this be done through the grants committee? As much as the streaming grant seems like a nice idea, it’s much easier to implement through the grants committee and much less of a strain on the community.\\nI think the grant program is ideal for one-off grants and projects with defined deliverables. In contrast, a contributor grant is more of a long-term investment in an individual/contriubtor. The grants program provides capital to help make an idea a reality, but an ongoing grant doesn’t come with upfront capital, making it more directed at skills/efforts.\\nImpressive how Getty lead the whole Chainlink integration on Compound. This could also be an excellent first step in adding more tokens to Compound. On behalf of Instadapp, I’m in support of this grant.\\nI can’t think of a more deserving community member for a streaming grant than @getty. On top of facilitating a hugely substantial protocol upgrade over the past 5 months w/ Prop 47, he has also played a pivotal role in helping to foster continued community discussion, most notably with his hosting of the community Clubhouse events.\nGiven the broad scope of his involvement and dedication to improving the Compound protocol, I fully support this grant.\\nstreaming comp? cool idea, all for it.\\nI am strongly in favor of providing @getty with a streaming grant of 500 COMP. It would accomplish two things at once:\n\n\nRewarding a top contributor. Getty led the charge on upgrading the oracle system, which took months of research and coordination. The new oracle system does a lot of good for Compound, and we should reward the person who made it happen.\n\n\nSetting up streaming grants. Unlike grants coming from the Compound Grants Program, streaming grants come directly from Compound’s treasury. While the grants program functions well for one-off grants, it’s not the ideal infrastructure for recurring work. Setting up a streaming grant for Getty sets a new precedent for paying contributors who do great recurring work.\n\n\nI hope to see more contributors applying for streaming grants in the future!\\nStrongly supportive of this.\nHaving seen Getty in action on both the oracle initiative and the Compound community calls, he’s added enormous value to the protocol and displayed an ability to collaborate with different stakeholders to iterate towards the best solution. If the goal is to inspire 100 more Getty’s to work on improving Compound, then this is a great step.\\nsince its the first stream, maybe some of the comp can go to paying chainlink for whatever fees they may charge. Just spitting ideas…\\nNot much to add that hasn’t already been said. COMP is fortunate to have community members like @getty who have stepped up to solve core issues. I continue to think that paying/incentivizing these contributors is probably the highest-value use of treasury funds.\nPolychain will support this.\\nI think this is a great idea, and Getty would be a great hire for the protocol! The grants program can help fund short term projects, while long term contributors with a proven track record can have a more stable arrangement.\nOnly concern I have is paying out 100% in COMP could cause some issues. If the price tanks, we can’t expect contributors to work for below market rates so governance could be forced to increase the award. On the other hand if the price of COMP increased, we wouldn’t want to reduce an existing payment stream as this will make contributors less willing to work with Compound in the future and reduce trust in Compound governance overall.\nI think an ideal solution would be to pay out 60-80% in stablecoins (could be taken from DAI/USDC/USDT/TUSD reserves at governance discretion), with the remainder in COMP. This ensures that contributors will earn at least an acceptable rate of pay in a stable asset, while preserving the incentive benefits and upside participation of COMP payments.\\nI appreciate the endorsement!\nAs for funding, overall, I like your idea and think it has merit. That being said, I also think it is personal to the contributor. I would rather have the upside and downside than receive a portion in stables. The COMP rewarded for my efforts will be treated as an investment rather than a salary. As well, the protocol isn’t set up for an ongoing stable grant, but I think it is something that we could put together for future contributors.\\n@getty What is your thought on using Sablier 12 as opposed to the native stream grant? I think that Sablier is much better suited for this usage.\\nInteresting idea, I think using Compound’s system is easiest for now, but I’ll look into it.\\nCopied from the Discord:\n@jared asked, “what are actually the stopping criteria for this streaming grant? there’s a ton of support for this proposal fundamentally (myself included), but when do we turn it off? are we expecting specific future contributions or this is all backpay for completed work?”\nGood question. The goal of the grant is to reward 6-months of work already done and future work. I think it should run for at least 6 months. That being said, I’m not going anywhere. This is just the beginning; the current solution is far from ideal (best available solution), and I don’t think we’ll ever reach a perfect solution but I want to strive to have the safest and most robust oracle system. If the community ever feels I’m not providing the value needed to support the streaming grant, by all means, it should be canceled.\nAdding on: The current priorities for the oracle system are to get more assets on Compound and work on transitioning the Uniswap anchor from v2 to v3.\\nI fully believe that you deserve the grant and am happy to see this happening; however, I don’t think an indefinite grant from governance is good. Creating a proposal to stop the stream (be it in 6 months or 6 years) will be a significant annoyance. Imagine you decide in a year that you want to pivot to something else—do you want to have to spend weeks organizing a proposal to stop your pay?\nOverall, I see zero upside and significant downside of creating a contributor streaming payment directly through the comptroller rather than using Sablier.\\nI’d like to echo this thought.  Perpetuity probably not the best idea for any of our developers, just from a first principles standpoint.\\nI have the same type of concerns @arr00 pointed out. it doesn’t have clear standard how to evaluate it and when to stop it. if some dev who got received COMP through governance proposal don’t want to make any update for his work, someone should create new proposal to stop streaming comp.\nand if its result didn’t meet Compound Community needs how to handle this issue?\nI agree that improving oracle task deserves to get received the grant but we need to take more consider whether or not on-going task should be handled through streaming COMP type + goverance proposal system.\nI think that Compound grant program is much better form to support this kinds of task\n\n\n\nSetting up streaming grants. Unlike grants coming from the Compound Grants Program, streaming grants come directly from Compound’s treasury. While the grants program functions well for one-off grants, it’s not the ideal infrastructure for recurring work. Setting up a streaming grant for Getty sets a new precedent for paying contributors who do great recurring work.\n\n\n@sukernik is it possible to operate series of grant on each milestone in Compound Grant Program?\nex) total grant budget: 500 COMP.\nit could have several milestones depends on implementation levels.\ninitial idea: 20%\nimplementation: 30%\ntesting phrase: 30%\nfinal product: 20%"
  },
  {
    "number_of_comments": 47,
    "postid": "a8465bea-2d72-4e96-a57a-8f0efd067320",
    "posturl": "https://www.comp.xyz/t/hkust-epi-labs-blockchain-delegate-platform/3959",
    "combinedcontent": "Information\nDelegate Address: 0xE4594A66d9507fFc0d4335CC240BD61C1173E666\nForum: @HKUST_EPI_BLOCKCHAIN\nEmail: epi@ust.hk\nExternal Website: EPI Lab 1\nCommunications Thread\nThis thread has been created for the sole purpose of communication of the decisions made by HKUST-EPI on Compound Finance’s proposals and governance matters.\nThis thread can be treated as HKUST-EPI’s Delegate Platform.\nOverview\nIn Hong Kong University of Science and Technology (HKUST), the visions and ideas which new technologies are developed from should be embraced and nurtured. Students are encouraged to explore and trailblaze on any potential new frontiers.\nAs blockchain technology is still maturing and adoption is rapidly increasing, continuous education and development should be established. For mainstream adoption to be achieved, strategic planning as well as proper decision making and marketing is required.\nEPI Lab (Research Lab) would like to contribute to the growth of Compound Finance by being a delegate. The primary angle which EPI lab considers each governance matter is of a risk based approach, other considerations are also involved.\nParticipation in decentralised finance on a blockchain, would provide greater insights and experiences which can be shared to the students and collaborating academies.\nThe research lab will be solely responsible for Governance matters to Decentralised Finance protocols with assistance from other Faculty members on other matters such as, Research, Education, Business Development and Web3 developments.\nHKUST-EPI has exposure to governance participation in other protocols:\n\nMakerDao\n\nHKUST-EPI sincerely encourages all Compound Finance stakeholders to delegate with the research lab to garner more voting power in the decision making process.\n\nWaiver of liability\nBy delegating to Hong Kong University of Science and Technology, you acknowledge and agree that Hong Kong University of Science and Technology will participate on a best efforts basis and will not be liable for any form of damages related to participation in Compound Finance.\\nPoll #142: Voted Against\nComments:\nThe conservative nature of the adjustment of the wBTC parameters do not justify the detriments to the borrowers during this bear market.\\nPoll #144: Voted For\nComments:\nIn hopes of increasing the utilisation rate for Compound V3 onboarding staked ETH as a collateral would encourage more borrowings of USDC.\\nPoll #145: Voted For\nComments:\n\nBase on the minor rise in asset price, and the increase demand of COMP\nWe are in support of the increase in supply of COMP from 600 to 900k\n\\nPoll #147: Voted For\nComments:\n\nWe agree with the risk assessment made by gauntlet\n\\nPoll #146: Voted Against\nComments:\n\nDespite having the opportunity for having more revenue from collateralised borrowings\nIt would be wise to stick to a lower limit for centralised assets as collateral\n\\nPoll #148: Voted For\nComments:\n\nIn view of the first mover’s advantage, it would be wise to purchase the domain name before it is taken up by a secondary person and misrepresent the protocol.\n\\nPoll #149: Voted Against\nComments:\n\nDespite having the opportunity for having more revenue from collateralised borrowings\nIt would be wise to stick to a lower limit for centralised assets as collateral\n\\nPoll #150: Voted For\nComments:\n\nKeeps Compound in touch with the community.\nIn touch with talents available in the community.\nPublicity for Compound and shows the industry that it is of a going concern.\n\\nPoll #151: Voted For\nComments:\n\nAllows Compound to access the liquidity of approximately 700m USDC (wrapped) from the polygon ecosystem\nWe trust that risk on the bridge and wrapping contract for USDC has been assessed.\nMore streams of revenue for the protocol.\nOur only concern is the bridging and wrapping contracts used for this deployment.\n\\nPoll #152: Voted For\nComments:\n\nV3 had superior risk mitigation to V2, and we voted for from a risk based approached where collaterals will be isolated to a single collateral type being borrowed.\n\\nPoll #153: Voted Against\nComments:\n\nWe would have agreed with the BTC parameters change to promote the use of V3 but with the inclusion of cbETH we voted against due to or stance on centralised volatile tokens being used as collateral\n\\nPoll #154: Voted For\nComments:\n\nRestart business operations\n\\nPoll #156: Voted For\nComments:\n\nWe voted For\n\nThis is in line with the progression of migrating V2 positions to V3.\n\n\n\\nPoll #157: Voted For\nComments:\n\nWe voted For\n\nAs the agreement is valid and inline with the disbursement\n\n\n\\nPoll #158: Voted For\nComments:\n\nWe voted For\n\nNo objections to bridging COMP tokens to Polygon for vault reward emissions.\n\n\n\\nPoll #159: Voted For\nComments:\n\nWe voted For\n\nNo objections to bridging COMP tokens to Polygon for vault reward emissions.\n\n\n\\nPoll #160: Voted For\nComments:\n\nWe voted For\n\nNo objections to initialization on Arbitrum\n\n\n\\nPoll #162: Voted For\nComments:\n\nWe voted For\n\nNo objections to adjustments of the interest rate parameters to provide the protocol with a more competitive edge.\n\n\n\\nPoll #163: Voted For\nComments:\n\nWe voted For\n\nNo objections to engaging Gauntlet to perform an incentive optimisation exercise.\n\n\n\\nPoll #164: Voted For\nComments:\n\nWe voted For\n\nNo objections top-up proposals by gauntlet within their scope of work on incentive optimisation.\n\n\n\\nPoll #165: Voted For\nComments:\n\nWe voted For\n\nNo objection on the contractual payout to OpenZeppelin\n\n\n\\nPoll #166: Voted For\nComments:\n\nWe voted For\n\nNo objection on the migration procedures\n\n\n\\nPoll #167: Voted For\nComments:\n\nWe voted For\n\nNo objection on the migration procedures\n\n\n\\nPoll #168: Voted For\nComments:\n\nWe voted For\n\nNo objection on the change of interest curve, as similar to other protocols, and the split of COMP rewards to suppliers.\n\n\n\\nPoll #169: Voted For\nComments:\n\nWe voted For\n\nNo objection on assessments made by Gauntlet for the increase of the various token supply.\n\n\n\\nPoll #170: Voted For\nComments:\n\nWe voted For\n\nNo objection on the pausing of minting cTokens for V2 markets to mitigate against tail end risk from liquidity crunch, in addition these tokens are also available on the v3 market in which the protocol is attempting to migrate collaterals and users over.\n\n\n\\nPoll #171: Voted For\nComments:\n\nWe voted For\n\nNo objection on expanding Compound to Base chain with Gauntlet’s risk parameter and reward mechanism assessment.\n\n\n\\nPoll #172: Voted For\nComments:\n\nWe voted For\n\nNo objection on onboarding maticX as collateral on polygon to USDCv3 market.\n\n\n\\nPoll #173: Voted For\nComments:\n\nWe voted For\n\nNo objection on Gauntlet’s incentive optimisation revision on arbitrum.\n\n\n\\nPoll #174: Voted For\nComments:\n\nWe voted For\n\nNo objection on expansion to BASE as other protocols has also implemented plans to do so.\n\n\n\\nPoll #174: Voted For\nComments:\n\nWe voted For\n\nNo objection on with Gauntlet’s assessment, with the introduction of eDSR on Maker. Compound is required to make its parameters more attractive to attract a different/new demographic of userbase.\n\n\n\\nPoll #176: Voted For\nComments:\n\nWe voted For\n\nNo objection on the parameter adjustments in order to stay relevant and competitive in a market trying to draw as like ETH LSDs onto the protocols.\n\n\n\\nPoll #177: Voted For\nComments:\n\nWe voted For\n\nNo objection on increase in supply of BTC on polygon due to demand.\n\n\n\\nPoll #178: would have Voted For\nComments:\n\nWe voted For\n\nNo objection on deployment of USDCv3 onto arbitrum\n\n\n\\nPoll #180: Voted For\nComments:\n\nWe voted For\n\nNo objection on the encouraged migration from and deprecation of the v2 markets.\n\n\n\\nPoll #181: Voted For\nComments:\n\nWe voted For\n\nNo objection on the continued service from Gauntlet which has proven to be beneficial to the protocol.\n\n\n\\nPoll #182: Voted For\nComments:\n\nWe voted For\n\nNo objection on the increase of daily COMP emission as rewards on arbitrum as suggested by Gauntlet.\n\n\n\\nPoll #184: Voted For\nComments:\n\nWe voted For\n\nNo objection on the onboarding of stMATIC on polygon markets, stMATIC is one of the core assets for the polygon market.\n\n\n\\nPoll #185: Voted For\nComments:\n\nWe voted For\n\nNo objection on the continued engagement of a service provider for the protocol.\n\n\n\\nPoll #186: Voted For\nComments:\n\nWe voted For\n\nNo objection on the continued deprecation of the v2 markets and using a smoother approach on the reduction of the collateralisation factors.\n\n\n\\nPoll #187: Voted For\nComments:\n\nWe voted For\n\nNo objection on investment of assets in a more volatility market using aeras but one should always be vigilant about potential losses.\n\n\n\\nPoll #188: Voted For\nComments:\n\nWe voted For\n\nNo objection to the wBTC cap increase on arbitrum the utilisation is nearing the cap.\n\n\n\\nPoll #189: Voted For\nComments:\n\nWe voted For\n\nNo objection to funding of the grant programme to encourage more innovation and hopefully provides protocol integrations to new protocols.\n\n\n\\nPoll #190: Voted For\nComments:\n\nWe would have voted For\n\nNo objection to depreciation plans for V2.\n\n\n\\nPoll #192: Voted For\nComments:\n\nWe voted For\n\nNo objection to adding rETH as collateral to remain competitive amongst the other protocols.\n\n\n\\nPoll #193: Voted For\nComments:\n\nWe voted For\n\nNo objection to phase 4 v2 market deprecation procedures\n\n\n\nPoll #194: Voted For\nComments:\n\nWe voted For\n\nNo objection to the funding of more COMP tokens to v3 for user incentives.\n\n\n\\nPoll #195: Voted For\nComments:\n\nWe voted For\n\nNo objection to change of interest rates to remain competitive. though it should be noted that Gauntlet is the risk assessor for various other protocols, hence the “competitiveness” of the interest rates are judgemental due to similar changes performed to other protocols.\n\n\n"
  },
  {
    "number_of_comments": 14,
    "postid": "9629f224-db0e-4732-a0c5-5bdbdb026fbb",
    "posturl": "https://www.comp.xyz/t/listing-ethx-on-compound/4730",
    "combinedcontent": "Hey folks! Kranthi this side from Stader’s ETH staking product, ETHx with a proposal to add it as a collateral asset on Compound’s ETH & USDC markets\nBackground\nStader is a non-custodial liquid staking platform with $180Mn+ TVL 1 across 7 PoS blockchains (Ethereum, Polygon, Bnb, Hedera, Fantom…) with Ethereum being the latest to launch in early July. Stader’s LST on Matic (MaticX) is quite popular on Compound USDC v3 Polygon with 6M Matic collateralized\nETHx is created to improve accessibility & support decentralization of Ethereum by allowing anyone to spin up a validator with just a 4 ETH bond; the lowest in the industry and 80% lower than solo staking. This coupled with the unique multipool architecture of permissionless & permissioned Node operators furthers decentralization without compromising scalability. Users can deposit any amount of ETH with Stader and receive proportionate ETHx, an ERC-20 C-token which represents staked ETH; that appreciates in value over time due to the staking rewards and is represented in the exchange rate\nETHx is currently at $72M TVL (40K ETH) 1 across 800+ validators (90% permissionless) spread over 170+ Node Operators who bonded over $7M to cover for slashing / penalties. In terms of liquidity depth, ETHx is at $25M across Curve, Balancer, PanCakeSwap, Wombat etc. ETHx is also live on Ledger & OKX with more integrations to follow\nReferences/Useful links\n\n\nWebsite: https://www.staderlabs.com/ 1\n\n\nETHx Dapp: Ethereum Staking - Stake Ether & Earn Eth2 Rewards | Stader Labs 1\n\n\nETHx Litepaper: https://www.staderlabs.com/docs/ETHx%20Litepaper.pdf\n\n\nETHx Contract: $1,800.09 | ETHx (ETHx) Token Tracker | Etherscan 1\n\n\nDocs: https://staderlabs.gitbook.io/ethereum/ 1\n\n\nGithub: GitHub - stader-labs/ethx\n\n\nAudits: Audit Reports - Stader - ETHx 1\n\n\nBug Bounty: Stader for ETH Bug Bounties | Immunefi 1\n\n\nDune: https://dune.com/stader_labs/dashboard-catalogue 1\n\n\nPrice feeds\n\nChainlink ETHx Price Feed 8\nRedstone ETHx Adapter 4\nRedstone ETHx Price Feed 4\n\n\n\nCommunities\n\nTwitter: https://twitter.com/staderlabs\nTelegram: https://t.me/staderlabs\nDiscord: Stader Community\nBlog: Unlocking DeFi Power: Stader Labs' Liquid Staking Insights\n\n\n\nMotivation\nLSTs are the perfect use case for the Compound ETH market which is quite evident from all the success it has seen with other LSTs. From Compound’s perspective any new asset is a source of additional revenue and expands the ecosystem as a whole. And a relatively risk free yield-bearing ETH correlated asset should be a straightforward proposition\nRisks\nThe introduction of ETHx provides an opportunity for diversification and broadening the collateral base while mitigating concentration risks. However, we do acknowledge the concerns around the potential risks associated with ETHx too. They can be broadly segmented into three buckets and are addressed as such\n1. Security\nWhile Stader might be new to Ethereum, we have built LSTs on 6 chains prior to this and security has always been paramount for Stader. All our smart contracts across every chain have been audited at least twice. The ETHx smart contract in particular has been triple audited 1 by leading smart contract security partners Sigma Prime, Halborn & Code4rena; complete with Forta on-chain monitoring and a $1Mn bug bounty 1 on Immunefi\n2. Governance\nStader’s governance is led by the Stader DAO - a wide variety of $SD holders who participate in key decisions pertaining to the protocol. Here are the Stader Governance Forum & Snapshot 1\nETHx has an Oracle Node Operator Genesis Committee of 7 distinguished community members composed of top validators and esteemed Ethereum community members. You can learn more about the ETHx ONO committee here 1\n3. Centralisation\nStader is a non-custodial & decentralized liquid staking solution based on DAO governance. $SD is Stader’s native governance token. ETHx is based on the foundation of decentralizing Ethereum with permissionless node operators getting the majority of the TVL (currently at ~90%).\nThe ETHx contract upgrades are managed by Admin time-lock contract with a 6 on 9 Multi sig (2 Stader members and 7 external members) as the proposer. Composition of the committee can be found here 1\nYou can learn more about how Stader envisions embracing the path of decentralization here\n4. Redemptions\nETHx, like all of Stader’s prior LSTs went live with redemptions enabled from Day 1. So, the ETH is not locked and can always be redeemed by unstaking & withdrawing on Stader Dapp anytime. Withdrawal requests are processed either from the deposit pool or by exiting validators. Users can also swap their ETHx for ETH on any of the DEXs or aggregators for instant liquidity\nParameters\nETHx currently has $72M in TVL 1 and total liquidity of $25M on ecosystem DEXs. My proposal is to set the parameters for ETHx on Compound ETH market as\nCollateral Factor: 90%\nLiquidation Factor: 93%\nLiquidation penalty: 3%\nand the parameters for the USDC Market as\nCollateral Factor: 65%\nLiquidation Factor: 70%\nLiquidation penalty: 12%\nYour support and consideration of our proposal is greatly appreciated! Look forward to hearing from you soon and happy to provide additional resources and support to the community if needed\nBest regards,\nKranthi\\nI support the proposal.\nIt is great to have a highly decentralised LST as collateral in Compound.\nCongratulations on your 160+ node operators. This is 5x Lido’s size and orders of magnitude bigger than most of LSTs out there.\\nAs a Compound user, I’ve been waiting for new LSTs to be listed for a really long time. Compound approach prioritizing conservative parameters on the safest assets has always resonated with me, but I think we have been falling behind lately.\nShanghai upgrade has already been live for half a year, so it’s already safe to affirm that it has been a complete success. With withdrawals implemented into the core protocol, staking solved one of its biggest tail risks, so listing newer LSTs seems a sound move (with reasonable parameters according to their market conditions).\nAfter doing some research about ETHx, I feel comfortablr about their security. They weren’t cheap on auditing their product nor in their bug bounty offer. They also launched with withdrawals already implemented, which is a huge green flag for me (see Swell, which has been depegged for a month already, due to not having withdrawals implemented).\nI have even found that they had a 2k ETH withdrawal last week, which given their TVL is a decent sized unstake, and it had 0 impact on their peg. They even recovered that TVL days after, so they seem to be doing good.\nI’d be happy to see this proposal going forward, specially regarding the addition of ETHx to the USDC Ethereum Market, as it doesn’t have any LST listed there yet. Parameters seem conservative and a low initial cap could be set up and gradually increased along ETHx growth (if applies)\\nThis is an exciting proposal from @gonemultichain. We support bringing ETHx to Compound.\nThe recent discussions in the staking ecosystem around dominance and self-limiting have generated a lot of interest around finding ways to distribute the powers of validation among a larger set of providers in a more equal manner. It’s our opinion that this starts with diversifying the opportunities for providers to benefit the holders of their LSTs in DeFi.\nFranklin DAO is a delegate in both Compound and Stader DAOs. We believe this would benefit both ecosystems. ETHx is a great example of an incoming token to expand these opportunities for Stader as a provider. Stader’s many innovations around ETHx particularly their 4ETH bond requirements help make staking more accessible and increase the security of their staking token.\nWe would like to see @Gauntlet opinion on parameters for this market, albeit we support this proposal and moving forward with listing ETHx.\\nWe also support this proposal. The integration of decentralized LSTs such as ETHx and rETH stands as a formidable strategy to amplify lending operations within Comet. This initiative is particularly timely and pertinent given that AAVE has yet to embrace this integration, and there exists a substantial market demand for it.\nOther delegates have addressed the counterparty risk associated with ETHx. From a market risk perspective, a $6M transaction involving ETHx/WETH results in an approximate 1% slippage. This would leave more space for the supply cap.\n\\nStader is becoming one of the most well-recognized LSD protocols in terms of ETH descentralization. ETHx TVL growing substantially with almost $50M needs protocols in the Defi space like Compound to better give utility to the LSD ETHx token and allow ETH staking to be naturally integrated within the Defi space.\nSome of the relevant data under ETHx can be seen here: https://dune.com/stader_labs/ethx-general-metrics 1\nCheers and hopefully I can deposit my ETHx as collateral in Compound soon.\\n[Gauntlet] ETHx Initial Parameter Recommendations: Ethereum v3 WETH Comet (11/3/23)\nSimple Summary\nGauntlet recommends the following parameters for listing ETHx on the Ethereum Compound v3 WETH comet.\n\n\n\n\nParameter\nValue\n\n\n\n\nCollateral Factor\n90%\n\n\nLiquidation Factor\n93%\n\n\nLiquidation Penalty\n2.5%\n\n\nSupply Cap\n5,000 ($9M)\n\n\n\nAnalysis\nETHx has a circulating supply of 37k, with a market cap of $68M. As seen below, 5k ETHx can currently be swapped for WETH at 7.79% slippage.\nScreen Shot 2023-11-03 at 1.32.27 PM926×758 43.4 KB\nSufficient DEX liquidity for LSTs in USDC comets is very important since liquidators have to quickly swap the LST into USDC with minimal slippage to realize an arbitrage. However, in the WETH comet, a liquidator can purchase ETHx using WETH and then unstake ETHx at a later date to realize the arbitrage, assuming no breaches in the LST’s smart contract.\nThe larger concern for low DEX liquidity may instead be price manipulation risk, depending on the type of ETHx price oracle Compound uses.\nTo start conservatively, we recommend setting an initial supply cap of $9M, with the standard WETH comet LST collateral asset parameters.\n\n\n\n\nParameter\nValue\n\n\n\n\nCollateral Factor\n90%\n\n\nLiquidation Factor\n93%\n\n\nLiquidation Penalty\n2.5%\n\n\nSupply Cap\n5,000 ($9M)\n\n\n\nNext Steps\n\nWe welcome community feedback\n\\nI also support idea of listing ETHx on Compound, however i suggest that we take USDC Comet as priority one, rather then WETH Comet, which is frankly a very niche market. To illustrate that lets take a look at USDC Comet on Base. Clearly, when users do have a choice of collateral between staked Eth or plain Eth they prefer staked one, to extend that there is 10x more demand, as staked ETH is almost at the cap, while plain eth hardly at 10% of the cap, where caps are approx $15M and $20M respectively.\nHowever, more options are better, so it makes sense to launch it on both USDC and ETH Comets, but due to liquidity limitations i’d suggest we could start with $4M initial cap for each one. That can also potentially show us which market will have more usage and demand for it.\\nGauntlet team and Compound community,\nThanks a ton for your support and for sharing the ETHx parameters. Would love to proceed further. Kindly let us know the next steps / any info you need from us.\nLastly, ETHx on-chain liquidity has improved significantly since the assessment. So, just want to add the current snapshot for reference\nAs of today (7th Nov), ETHx has a liquidity depth of at $29M across Curve 2 ($18M), Balancer 1 ($4M), PanCakeSwap ($3M), Uniswap ($2M), Wombat ($2M) etc which translates to a 6.5K ETHx swap at ~7.7% slippage, 6K at ~3.7% and 5K at ~0.6% respectively.\nimage2537×695 74.8 KB\nWould @Gauntlet team like to consider the updated liquidity?\\nHello everyone,\nMatt from RedStone Oracles here. We’re glad to see a positive reception of the proposal by the Compound community and we can only vouch for Stader’s professionalism. Following up on @Gauntlet recommendations, here is a bit of context from our perspective on the ETHx Price Feed:\nETHx currently has ~$29M on-chain liquidity distributed across trading venues such as Balancer, Curve, Pancakeswap and Wombat Exchange, which is enough for us to have a robust market Price Feed. The liquidity and slippage are under constant supervision via our dedicated monitoring (already well battle-tested during Terra, FTX, SVB crashes and more) to make sure each of the included data sources is secure at all times.\nOnce we detect a sudden rise in slippage and/or sudden drop in liquidity we automatically detach the source eliminating the risk of price manipulation.\nThe price is updated onchain each time it deviates 0.5% (deviation threshold) or if 6 hours have elapsed (heartbeat).\nWe are excited to see this proposal come to life and we remain available for Delegates and Risk Analysis Partners for any oracle-related discussion. At the same time, we are happy to share opinions and referrals of working with RedStone from our partners such as Sommelier, Enzyme, Venus, Angle, Mento, Alchemix, Raft, Gravita and a number of others.\\nVery happy to see ETHx whitelisted and to witness their growth.\nSince my last comment in this post 17d ago, I can see in their Dune Dashboard:\n\nETH staked is up from 22k to 40k.\nNode Operators are up from 160 to 182.\n\\nHi @Sirokko,\nWe assessed the potential inclusion of ETHx as collateral in the USDC comet and concluded it doesn’t align with the community’s risk preferences. $4M supply cap for ETHx is minimal in comparison to the blue-chip tokens like WBTC ($488M supplied) and is minimal given the risks ETHx could pose. There’s a substantial amount of USDC at stake in the Ethereum USDC comet—$367M—that could be exposed to price manipulation risks due to ETHx’s lower liquidity. Additionally, liquidators face a risk being unable to arb ETHx if liquidity decreases. It’s essential for us to prioritize highly liquid assets as collateral in the comet with the largest TVL to safeguard the interests of our users and the protocol.\\nHi @knight_mayr ,\nWe appreciate the feedback. Since the majority of our analysis focuses on worst-case scenarios, we still prefer to start using conservative numbers. A temporary increase in the ETHx/WETH pool’s DEX liquidity doesn’t guarantee its longevity, and decreasing supply caps is challenging once they’re fully utilized, especially if liquidity conditions shift. If our recommended ETHx supply cap becomes fully utilized and DEX liquidity remains robust, we are happy to reassess the ETHx supply cap.\\nGreat to see ETHx improvements in terms of TVL and liquidity depth during these last weeks. Seems like that 5k ETH swap price impact has lowered to -0.5%.\nAlso happy to see this post getting traction, hope to see both ETHx and rETH as new collaterals onboarded soon.\\nFair enough @Gauntlet. Will get started on the PR with these parameters and we can monitor the liquidity scenario"
  },
  {
    "number_of_comments": 19,
    "postid": "c9126080-13db-42db-918f-afec6ec46f38",
    "posturl": "https://www.comp.xyz/t/compound-iii/3351",
    "combinedcontent": "A few months ago, we began the discussion for a new multi-chain strategy 243; a version of the Compound protocol that can be deployed and run on all EVM compatible chains.\nToday, Compound Labs is excited to release a code repository to the Compound community, which we hope can form the basis of a multi-chain deployment strategy: comet 400, which the community has been referring to as Compound III.\nCompound III is designed with borrowers in mind, to be capital efficient, gas efficient, safe, and simple to govern.\nThe repository uses a business source license 96, which Compound governance can grant usage to, as it sees fit, by making changes to compound-community-licenses.eth, a new ENS domain owned by the community.\nDevelopers can begin planning integrations with Compound III, and auditing / suggesting improvements to the codebase.\n\nChangelog\nThe following is a summary of the major changes from the existing protocol:\n\nCompound III deployments feature a single borrowable (interest earning) base asset. All other assets are collateral. This reduces risk, and can improve capital efficiency.\nCollateral size limits can be set for each collateral asset (a.k.a. supply caps).\nThere are separate borrowing collateral factors, and liquidation collateral factors. This protects borrowers from early liquidation, and can improve risk management.\nThe risk management / liquidation engine has been entirely redesigned, to increase the safety of the protocol while preserving liquidator incentives.\nThe price feed doesn’t expect a custom price oracle; instead, it is designed to use Chainlink directly, which is portable to EVM chains beyond Ethereum; governance can modify this decision in the future.\nSupply/borrow interest rate models can be decoupled from one another; governance has full control over economic policy.\nAdvanced account management tools, which can enable new UX patterns and applications on top of the protocol.\nAn abstract incentive metric is built natively into the core contract, to enable rewarding user activity from day one of the protocol. A rewards system is elegantly added on top to provide incentives similar to v2, but flexible enough to be extended by governance in new ways.\nA code repository which includes sophisticated tooling for managing and testing deployments, based on years of experience and feedback from prior versions of the protocol.\n\n\nNext Steps\nOver the coming weeks, we look forward to working with the community to finish auditing the protocol; learning from the current testnet; releasing an initial deployment on Ethereum, with interfaces, liquidation bots, and tooling; and beginning deployments across other EVM chains with tools for governance to manage those deployments.\nIf you have any questions, please join the next Community Developer call in Discord! \\nThanks for sharing comet plan.\n\n\n\n jared:\n\nThe price feed doesn’t expect a custom price oracle; instead, it is designed to use Chainlink directly, which is portable to EVM chains beyond Ethereum; governance can modify this decision in the future.\n\n\nI was wondering it contains Chainlink’s proof of reserve product for bridge assets. I think that we need it to reduce potential risk like bridge hacking event(Harmony Horizon bridge exploit. Consequences to Aave V3 Harmony - Governance - Aave 27)\\nWill be huge for devs.  Could definitely see synergies with new Polygon tools – should be a main chain we interact with.\\nI’m really digging this approach. In my opinion, it’s the perfect example of how two protocols can exist in perfect harmony.\nCompound is now chosing the streamlined, simple route, albeit very powerful, whereas Aave has tagged on one feature after another at the expense of other factors.\nEspecially in a multi-chain/L2 world, I can definitely see this thrive in a big way. Do we have any idea around the base asset itself yet, or is this being kept secret as part of the magic sauce?\nI honestly am contemplating of liquidating my entire Aave position to enjoy the simplicity and safety of this new model, you guys really nailed this in my opinion, and I’d personally be very much looking forward to the actual formal proposal detailing the next steps to get Compound to this new version!\nBig thumbs up, the long wait was 100% worth it!\\nI’m not sure exactly what you’re asking but we definitely agree, safety first! The core protocol doesn’t rely on any bridges, but for multi-chain governance / cross-chain liquidity that’s always our primary concern for any bridge we would propose to introduce.\\nPersonally, I’d love to see this deployed on Polygon! It will be entirely up to governance, but we will work with @hamzahkhan to pave a path forward and help guide the community \\nThanks! We can’t wait to get to a mainnet proposal too. I would expect a proposal similar to what’s on testnet, e.g. at least a USDC base market and an ETH base market on Ethereum. Probably similar on other chains using a local stablecoin and/or native asset to start.\\nSuper excited to see this come to fruition. Been wanting to see compound go multi chain for a while now. Personally I think Polygon would be the best first step, only question remains is should we go for the POS chain or wait for the zk roll up Hermez which is coming soon I’ve heard\\nCan we PLEASE get a vote for polygon asap\\nit’s actually oracle issue rather than a bridge itself. in the case of recent harmony case, there was the gap between real backed assets on bridge and reported asset value from the oracle.(and this gap made potential insolvency issue using over valued collaterals) That’s why I mentioned proof of reserve feature to avoid similar cases when Compound protocol operate on multichain env.\\nThis probably needs to be dealt with on an asset by asset basis by governance? Bridged assets are basically at least as weak as the bridge itself, I’m interested to understand how exactly the proof of reserve product gives further protection.\nMore generally, I think there’s a lot to consider/reconsider about the price oracle(s), they are still one of the biggest (if not the biggest) attack vector(s) and being unanchored is a bit scary.\\n\n\n\n jared:\n\nCompound III deployments feature a single borrowable (interest earning) base asset. All other assets are collateral. This reduces risk, and can improve capital efficiency.\n\n\nCompound Cash? Stablecoin? or any asset that needs to be single borrowed?\n\n\n\n jared:\n\nThere are separate borrowing collateral factors, and liquidation collateral factors. This protects borrowers from early liquidation, and can improve risk management.\n\n\nI don’t know why this isn’t earlier implemented? Aave has that almost 2y\n\n\n\n jared:\n\nThe risk management / liquidation engine has been entirely redesigned, to increase the safety of the protocol while preserving liquidator incentives.\n\n\nWhere is possible to find in detail about Compound III?\\nCurious if there’s a possibility of multiple assets of the same type being borrowable within a single market? Eg. a USD stablecoin market with several collaterals (not borrowable), and several USD stablecoins (borrowable but not usable as collateral).\nOn first look, it seems like this would be fine from a risk management perspective:\n\nCollateral assets would have generally the same variance with respect to each borrowable asset\nAny upside variance in a borrowable asset that causes a short squeeze would only affect solvency of that particular borrowable asset (and not other borrowable assets or collaterals)\n\\nFrom a risk management perspective yes, but code-wise not really. The contract is seriously optimized for a single base asset, and also the ERC20 wrapper might get weird, unless they all had the same interest indices, which I think would also be weird since the suppliers of each base / liquidity would be different.\\nIf I understand correct, supplying assets will yield 1 global cToken that generates interest?\nI’m building a dapp (uses Compound) that could profit heavily of the lower fees on other chains.\nHow will the redeeming happen and amount to be redeemed get calculated?\\nKind of, a positive supply of the base asset will look like an ERC20 balance and accrue interest. There’s no exchange rate (or you can think of it as internal), so the balanceOf is the amount of the base asset you can withdraw.\\nhey jared \nwhat’s the blocker for not having an anchor in place? something tellor can help address? porting to any evm is not a problem for us either. (already on polygon, arbitrum, optimism)\nhappy to chat about it at the dystopia hack summit in sf. looks like we’ll both be there.\\nOver the past six weeks, considerable effort has gone into reviewing the Compound III testnet deployments, codebase, and documentation. Thank you everyone for your feedback, edits, audits, and testing.\nAudits were completed by OpenZeppelin 14 and ChainSecurity 16. These are significant documents that everyone in the community is encouraged to review.\nFull documentation of Compound III 127 is available for developers to prepare new application integrations, and for liquidators to begin building bots 24 to protect the protocol.\nCompound III is now ready for production!\nThe first deployment of Compound III is a USDC market on Ethereum. The release candidate contracts have been deployed; Compound Governance controls these contracts, and is solely capable of activating the market.\n\nDeployed Contracts\n\ncUSDCv3: 0xc3d688B66703497DAA19211EEdff47f25384cdc3 46  2\n\nThis is the main proxy contract for interacting with the new market. The address should remain fixed and independent from future upgrades to the market. It is an OpenZeppelin TransparentUpgradeableProxy contract 6.\n\ncUSDCv3 Implementation: 0x42F9505a376761b180e27a01bA0554244ED1DE7D 12  2\n\nThis is the implementation of the market logic contract, as deployed by the Comet Factory via the Configurator.\n\ncUSDCv3 Ext: 0x285617313887d43256F852cAE0Ee4de4b68D45B0 7  1\n\nThis is an extension of the market logic contract which supports some auxiliary/independent interfaces for the protocol. This is used to add additional functionality without requiring contract space in the main protocol contract.\n\nConfigurator: 0x316f9708bB98af7dA9c68C1C3b5e79039cD336E3 8  1\n\nThis is a proxy 2 contract for the ‘configurator’, which is used to set and update parameters of a Comet proxy contract. The configurator deploys implementations of the Comet logic contract according to its configuration. This pattern allows significant gas savings for users of the protocol by ‘constantizing’ the parameters of the protocol.\n\nConfigurator Implementation: 0xcFC1fA6b7ca982176529899D99af6473aD80DF4F 3  2\n\nThis is the implementation of the Configurator contract, which can also be upgraded to support unforeseen changes to the protocol.\n\nProxy Admin: 0x1EC63B5883C3481134FD50D5DAebc83Ecd2E8779 1 \n\nThis is the admin of the Comet and Configurator proxy contracts. It is a ProxyAdmin 2 as recommended/implemented by OpenZeppelin according to their upgradeability pattern.\n\nComet Factory: 0x1C1853Bc7C6bFf0D276Da53972C0b1a066DB1AE7 4  3\n\nThis is the factory contract capable of producing instances of the Comet implementation/logic contract, and invoked by the Configurator.\n\nRewards: 0x1B0e765F6224C21223AeA2af16c1C46E38885a40 13 \n\nThis is a rewards contract which can hold rewards tokens (e.g. COMP, WETH) and allows claiming rewards by users, according to the core protocol tracking indices.\n\nParameters  5\n\nThe risk parameters for the USDC market were configured based on recommendations by Gauntlet. Supply caps are set to 0, and require a governance proposal to activate the market, before users can supply collateral or borrow USDC.\n\n\n\n\nCollateral Asset\nBorrow CF\nLiquidation CF\nLiquidation Fee\n\n\n\n\nWETH\n82.5%\n89.5%\n5.0%\n\n\nWBTC\n70.0%\n77.0%\n5.0%\n\n\nLINK\n79.0%\n85.0%\n7.0%\n\n\nUNI\n75.0%\n81.0%\n7.0%\n\n\nCOMP\n65.0%\n70.0%\n7.0%\n\n\n\n\n\n\n\nParameter\nSupply Rate Model\nBorrow Rate Model\n\n\n\n\nBase\n0.00%\n1.50%\n\n\nSlopeLow\n3.25%\n3.50%\n\n\nKink\n0.80\n0.80\n\n\nSlopeHigh\n40.00%\n25.00%\n\n\n\nTarget Reserves: 5,000,000 USDC\nStorefront Price Factor: 50%\nMinimum Borrow Size: 100 USDC\nSupply Reward Speed: 0.00 (usage is not tracked for rewards)\nBorrow Reward Speed: 0.00 (usage is not tracked for rewards)\n\nNext Steps\nEveryone has a chance to review the contracts and parameters of the market, in preparation for its activation. Compound III is now just a governance proposal away from being live!\\nFantastic news! Congrats @kevin, @jared, and the entire team who put in the massive effort to make Compound III possible.\nWe’re thrilled that Chainlink continues to be a part of Compound’s journey and look forward to helping Compound grow on Ethereum and beyond. Plus, there are emerging opportunities for strong collaborations like Proof of Reserve, as @dakeshi suggested.\nExcited for the governance vote to officially launch CIII!\\nBen from QiDao Protocol\nWould love to see this on Polygon! We can certainly support Compound through liquidity deposits (kind of like what DAI does with Aave but with our stablecoin, MAI).\nMAI is the first and main stablecoin on Polygon, so we have plenty of liquidity and market penetration. We recently got added into Aave V3. MAI is also used in all major DEXs on Polygon."
  },
  {
    "number_of_comments": 27,
    "postid": "3679b978-6e94-45fa-abc6-cfa72741bfa4",
    "posturl": "https://www.comp.xyz/t/compound-grants-program/1292",
    "combinedcontent": "tl;dr: We (Larry, Jesse, Getty, Aparna, Monetsupply, Ken, Sam, and Ryan) want to start a grants program to provide funding to Compound’s community. The purpose of this thread is to share our proposal and receive feedback from the community. If the feedback from the community is positive, we will share an address you can delegate votes to.\n\nSummary\nWe propose to start a program called the Compound Grants Program (“CGP”), which will provide funding to projects, ideas, and events that benefit Compound and its stakeholders. If approved, funding for the program will come from Compound’s treasury 88, which currently holds ~200k COMP (~$95mm as of 03/01/21). As a reminder, the treasury currently accrues 0.5 COMP per block, spends 0.352 COMP per block on liquidity incentives, and saves the balance of 0.148 COMP per block for governance by tokenholders.\nThe program will be a pilot. For this reason, we believe it’s prudent to limit the program’s dollar value to $1mm per quarter and the length to two quarters for a cost of $2mm (4,444 COMP) over six months.\nSince it’s not practical to solicit a community vote for every disbursement, we propose forming a small and nimble committee that has the power to administer the grants at its own discretion (limited by the aforementioned dollar and length caps). We suggest forming a committee of eight members: one lead member to head the program and seven reviewing members to review the lead’s work and assist with program operations. The committee will operate with a 4 of 7 multi-sig (only reviewers will be part of the multi-sig).\nAs compensation for administering the program, we propose the lead be paid $5k upfront and $100 per hour thereafter with a cap of 30 hours per week for a maximum compensation of $83k (184 COMP). In total, we are asking for a maximum of ~5,000 COMP to fund the grants (4,444 COMP), program setup and operational costs (444 COMP), and compensation for the program lead (184 COMP). All unspent funds will be returned to the community treasury at the conclusion of the CGP.\nPurpose\nDecentralized projects are living and breathing communities with a variety of stakeholders. These stakeholders include project team/contributors, tokenholders, users, partners, and for certain projects, liquidity providers. The goal for the CGP is to nurture Compound’s ecosystem to benefit all of these stakeholders. To be more specific, the grants program aims to:\n\n\nGrow Compound’s ecosystem by funding development happening on top of it. Funding development focused on helping Compound grow is critical to the project’s long-term success.\n\n\nFund ideas that benefit Compound that would otherwise not receive funding. Many good ideas are left unexplored because they fail to receive funding. We intend to make sure as few good ideas as possible are underfunded or unfunded.\n\n\nStrengthen goodwill by providing funding for community-led ideas. Funding projects, ideas, and events brought forth by community members will encourage more active participation by the community. It will have the added benefit of nourishing goodwill. A well-nourished goodwill keeps community members loyal and happy, which in turn, encourages new members to join what they see is a thriving community.\n\n\nProgram Scope\nThe CGP was heavily inspired by the Uniswap Grants Program 57 (“UGP”), which received approval from the community to deploy a maximum of $750k per quarter for two quarters. It’s difficult to deploy a meaningful amount of money to ecosystem grants without compromising on quality. While we believe all of these ecosystems will be enormous in the future, it’s important for us to be practical today by matching the grants budget to the size of the ecosystem.\nTo that end, the pilot program will deploy a maximum of $1mm per quarter and run for two quarters. We have no way of knowing whether this amount of money is overshooting or undershooting the needs of the ecosystem — we will only find out after running the experiment. For example, if we find out $2mm is not enough money to fund all of the high quality opportunities, we as a community may decide we need to create a larger allocation for grants. On the flipside, if we find out $2mm is too much, all unspent funds will be returned to the community treasury for use at a later time.\nAs part of the program, our intention is to fund projects, ideas, and events that directly benefit Compound and its stakeholders. While there may be opportunities to fund projects, ideas, and events that indirectly benefit Compound, these opportunities fall outside the scope of this program.\nNot all opportunities applying for grants will benefit the Compound ecosystem equally. To help us prioritize which ideas to fund, we propose the following buckets:\nHigh priority\n\n\nProtocol and parameter development. Apart from acts of stewardship and generosity, there is little to no incentive for community members to propose technical updates to the protocol. With no carrot with which to motivate community members to propose changes, the protocol isn’t able to innovate as quickly as it should be innovating in a dynamic and competitive market. We should note that in a minority of cases community members did receive payment for work done (for example, see the work done by Gauntlet on the COMP Contributor Grants 30 proposal), although here too, the incentive to contribute was not well-designed since it required Gauntlet to front audit and development costs before knowing the proposal to pay them would pass. To encourage community members to propose changes, the CGP will fully or partially pre-fund development and audit costs. In doing so, we hope to encourage more proposals, which will lead to more innovation and as a result, a far better Compound for all of us.\n\n\nCode audits. Making technical updates to the protocol is risky business: smart contracts are immutable and control billions of dollars in user funds. An error in a technical update can have serious consequences. Because of this, it is considered best practice to have an auditor review the proposed update for soundness prior to its submission. Unfortunately, these audits are expensive 4, particularly for individual contributors who need to pay for them out of pocket. We intend to provide grants that pre-fund audit costs for soon-to-be proposals. We hope this will encourage more individual contributors to propose technical updates to the protocol.\n\n\nBusiness development / integrations. A greater amount of liquidity makes Compound a better product for all users. To grow liquidity, Compound should be integrated with as many applications as possible. To that end, we aim to fund integrations that grow usage of Compound. In funding integrations, we will effectively be funding the business development function for the protocol.\n\n\nAdvertising and sponsorships. It will be important to get the word out there about this program. The more people there are that know about the CGP, the more applications we should expect to receive. To spread the word about the program, we will spend funds to advertise the CGP on podcasts, newsletters, and other mediums that attract the audience we want to attract.\n\n\nMedium priority\n\n\nHackathons. It’s very likely that there exist uses for Compound that haven’t been explored yet. Hackathons are a fantastic way to explore design spaces, and the CGP intends to sponsor them.\n\n\nBounties. This one speaks for itself: bug fixes and minor protocol updates will be covered by the CGP.\n\n\nLow priority\n\n\nMiscellaneous improvements. It’s difficult to know ahead of time all of the grant applications that will come through. Just because an application does not fit neatly into one of the above buckets doesn’t mean it’s not valuable to the Compound ecosystem. So long as an application benefits Compound directly, we will consider it for a grant.\n\n\nApplications for miscellaneous improvements to Compound fall into this bucket. Because the scope of this bucket is broad, we consider it to be low priority compared to the narrowly scoped buckets above.\n\n\nWhile we did our best to prioritize items among each of the three buckets, we are confident that the above list is not all-inclusive. We expect to receive grant applications for phenomenal ideas that we simply can’t think of today. The committee asks the community for the right to exercise discretion to fund ideas that are beneficial to Compound but are not part of the scope outlined above.\nProcess and Timeline\nIf approved, the program will begin shortly after this proposal passes and end six months following the start of the program. (In other words, if the proposal passes on 3/15/2021 and begins on 3/22/2021, it will end on 9/22/2021). The program will run on a rolling process: we will welcome applications at any point in time during the program length! We will stop accepting applications two weeks before the end of the program (during these two weeks, we will start wrapping the program up, which will include evaluating the last of the applications and returning unspent funds to the treasury).\nWe will source potential grants via an applications process. (We will be sharing the application soon after this proposal passes).\nOnce an application is received, CGP committee members will discuss the application and evaluate it in the context of its benefit to Compound and its stakeholders. If the committee approves the application, funds will be paid out to the receiving party on the timely basis. If the committee does not approve the application, the soliciting party will be notified as to why the application was not approved and, if applicable, what steps need to be taken to have the application approved in the future. All approved grants and their amounts will be disclosed to the community publicly and on a timely basis.\nA simplified diagram of the grants approval and disbursement process can be found below.\n\ndiagram image1906×214 52.1 KB\n\nFinally, a member of the CGP committee will participate in the bi-weekly Compound community developer calls. We will attend the call, listen to what the community thinks should be funded, and present some of the newly funded grants.\nCommittee Members\nThe CGP committee will consist of eight members: one lead and seven reviewers. We believe the best committees share two features: first they must be capable, and second they must be motivated to actively participate. We believe this committee shares these two vital features.\nLead\nLarry Sukernik 42 (Sheepshead Bay, LLC)\nReviewer\nGetty Hill 13 (Grapefruit Trading)\nReviewer\nAparna Krishnan 11 (Opyn)\nReviewer\nMonetsupply 14 (Independent)\nReviewer\nKen Ng 11 (Ethereum Foundation)\nReviewer\nSam Simons 18 (Independent)\nReviewer\nJesse Walden 9 (Variant)\nReviewer\nRyan Yi 24 (Independent)\nThe lead will be tasked with managing and operating the program while reviewers will have a duty of holding the lead accountable. To guarantee accountability, the committee will operate with a 4 of 7 multi-sig managed by the reviewers (note that the lead is not part of the multi-sig). In other words, four out of seven reviewers will need to sign the transaction for a grant to be approved and disbursed.\nCommittee Compensation\nWe propose the lead be paid $5k upfront and $100 per hour thereafter with a cap of 30 hours per week for a maximum compensation of $83k (~184 COMP). Since the majority of the work will be performed by the lead, they will be the only committee member to receive payment as part of the program pilot. Payments to the lead will be approved by the reviewers and made according to the following schedule: $5k upfront, with the balance paid at the end of each quarter (i.e., if the program begins on 3/22/21, the lead will be paid $5k on 3/22/21, and the again on 6/22/21 and 9/22/21 based on hours worked).\nThe funds for both the program and the lead’s pay will be allocated to the CGP multi-sig from the Compound treasury. Running the CGP will come with setup and operational costs; we will set aside approximately 10% of the CGP budget to cover these costs. All unspent funds will be returned to the community treasury at the conclusion of the CGP.\nWhat Does Success Look Like?\nWe expect success to come in two forms: one that’s measurable and the other that’s of the “you know it when you see it” type.\nMeasurable success metrics:\n\nNumber of projects, ideas, and events funded\nCommunity engagement (e.g., increased activity on forums, Discord, and so forth)\nIncrease in number of applications\n\n“Know it when you see it” success metrics:\n\nImproved sentiment and goodwill within the community\nImprovement to Compound’s brand and positioning in the market\n\nConclusion\nIf approved, the CGP will begin accepting applications for grants on a rolling basis shortly after its approval. To assist with the evaluation of potential grants, each grant will be classified into three buckets: high, medium, and low. High priority grants will be funded first; medium priority grants will be funded second; low priority grants will be funded last.\nThis program is a pilot. As such, we intend to keep the budget lean for a maximum disbursement of ~5,000 COMP over six months across grants, setup and operational costs, and lead compensation. Make no mistake: this is an experiment. If the CGP works, Compound may want to graduate the program from a pilot to a full-time endeavor. If it doesn’t work, we will learn why it didn’t work and what should be done differently. Most of us are COMP holders here. That gives the privilege to try something that’s never been tried before at a meaningful scale: to let the test subjects run the experiments.\nWe hope to hear your thoughts, comments, and suggestions in the thread below.\\nKudos for taking the initiative and moving forward with this proposal! I’d love to know how y’all ended up reaching the conclusion that a committee is the right solution here. I am actually inclined to think that’s the wrong direction for governance - I would much prefer to see us move in the direction of more radically transparent in all things - and would love to see COMP holders large and small incentivized to ‘stake’ votes of confidence rather than paying a small group of people to make decisions for everyone else.\\nThank you @sukernik and everyone involved in drafting this program. Having a community-organized grants program will grow the ecosystem in dozens of ways that are currently limited by the proposal process (which was originally imagined as autonomous, with the ability to add programs like this down the road).\nPresently, there’s not a clear path for the community to propose improvements, have their journey supported, and deliver improvements. I’m extremely excited for this pilot/experiment.\nThat being said, a few basic questions / areas to clarify:\nMembership\n\nWhy should the community, and COMP token-holders trust you? What are your qualifications, and background? What is your experience in the Compound community to date?\nWhat about the other members, that disburse funds?\nShould the members be active Compound community participants? Some names I don’t recognize from Discord or these forums.\nNobody on the list (to my knowledge) has a deep familiarity with the protocol codebase, which might be useful in quantifying grants for technical development.\n\nGrants Process\n\nIn the case of a protocol development, do you envision a grant before work is completed, after it is completed, after it is merged into the protocol? At what stage of development, would somebody apply for a grant? In the workflow, this isn’t made clear.\nIt seems like somebody would (or should) apply before completing development work (or another type of contribution). Would you disburse funds, partially disburse funds (with the remainder at a later stage), or communicate a “provisional” grant/acceptance?\nAny steps in mind to eliminate potential fraud?\n\nOverall, I see this as providing huge upside to the community, and the ability to participate.\\nI think a program like this is just the thing COMP needs to incentivize and scale broader community participation.  I believe that a critical component to the success of any protocol’s governance is that it cast as wide a net as possible to incentivize anyone, regardless of their resources, to be able to contribute to a protocol and be compensated on the basis of that contribution’s merit.\nTo jared’s comment, I believe a program like this can be effective - I believe these are smaller initiatives that do not require full weigh-in from all COMP holders, and can be delegated to a committee.  This will ensure things get funded and action is taken, instead of bogged down with potentially onerous procedure.\nTo rleshner’s comment, I do believe it is important to ensure that the community has faith that this committee is knowledgeable, transparent, and fair.\\n\nI believe these are smaller initiatives that do not require full weigh-in from all COMP holders\n\nI agree that not everything necessarily needs to happen on-chain or as a governance proposal, but I think there’s a huge spectrum of design space in between full governance proposals for everything and delegated to a committee.\nAlso I agree with the goals here, so I’m glad that we are talking about this.\\nI support this, but I did not see repairing oracle solutions and compensating damaged users (or at least a statement) among the listed priorities.\nI think we should first pay attention to things that don’t work on protocol.\nThere are several threads that stagnate here in the forum where users of the Compound community mostly share the same opinion and seek change, however there is a lack of technical knowledge to implement the proposals.\nI would prefer that the interests of your team be aligned with the interests of the users and that these be priorities. That will be win-win situation.\n\n\n\n sukernik:\n\nwhich will provide funding to projects, ideas, and events that benefit Compound and its stakeholders\n\n\nAnd i dont like expressions like this because it sounds to me like you’re addressing a board of directors of a joint stock company. The very purpose of the DeFi and DAO concept is for users to manage the protocol, so you can start with that.\nhttps://www.comp.xyz/t/should-compound-retroactively-airdrop-tokens-to-early-users\\nI love the thinking around this initiative and props off to everyone coming together for pulling this through. If I was at an earlier stage of my crypto career and looking into this I’d probably not contribute. Why? Well it’s super cumbersome to even get the idea approved before I even work on it.\nI can’t think of a structure off the top of my head but one which would be much nicer is if it prioritised autonomy of individuals to contribute and realise that a few thousands might be misallocated but it makes it easy for grass roots contribution to occur.\nIf I have a $5000 request there’s literally 5 steps and plenty of stakeholders to get through (lead, multisig, community lobbying etc). With programs like this it’s usually well connected folks who can request the most and lobby for the most.\nI could probably be entirely wrong with all of this but just my 2c \\nIt’s an important question, and one we considered seriously as we were developing the proposal. And I should add: even though we’re one committee, our views on this topic are multiple. I’ll reply with my thoughts, and the other committee members can chime in with theirs if they’d like.\nIdeally, 100% of the tokenholders would be active members of the governance process. In that ideal world, people who contribute to Compound would ask tokenholders for a grant, and tokenholders —100% of whom are active members of governance process — would vote to approve or deny the grant request in a reasonable amount of time. While that would indeed be ideal, it’s unfortunately not practical today. Tokenholders are unlikely to have the time to review and approve dozens of small grants, and even if they did find the time, the process would certainly operate far more slowly than the speed of a nimble committee. And speed, I believe, is a critical ingredient for a successful grants program. The more quickly you move, the better the process is for grants applicants. A better process leads to more applicants, which leads to a more successful grants program.\nIn short, it’s not so much that we wanted to form a committee as a committee happened to be the least-bad option for piloting a small-scale grants program!\\nI am glad we could work together on this!\\nWe’re excited about running this experiment too!\nThe questions you pose are all very good and the community should get an answer to them. Let’s see if I can do a good job of answering them.\n\nWhy should the community, and COMP token-holders trust you? What are your qualifications, and background? What is your experience in the Compound community to date?\n\nI spent the last three years investing at Digital Currency Group 1 (DCG). During that time, I invested in 100+ companies and projects (some went on to be very successful, and some, umm…did not go on to be very successful). Early this year, I left DCG to work on something that I think is critically important and all at once underinvested in: network governance. That’s how I find myself here today.\nUnfortunately, I didn’t have the time to actively participate in Compound’s community while I was at DCG. Since early this year, however, I have been lurking on the forum and Discord on a daily basis. Going forward, I plan to be a far more active member of the community. (Less lurking and more posting).\n\nWhat about the other members, that disburse funds?\n\nWe tried very hard to make sure members of the committee represent the interests of various stakeholders in the community and/or have experience in providing funding to projects in the space. Jesse is an investor at Variant (and a16z before that). Getty, Monetsupply, and Sam are independent members of the Compound community. Ken has deep experience with the grants process thanks to his work for the Ethereum Foundation. Finally, Ryan works at Coinbase, and before that, was an investor at CoinFund. I’m biased, of course, but I think we have some very high quality committee members!\n\nShould the members be active Compound community participants? Some names I don’t recognize from Discord or these forums.\n\nActive community members are absolutely critical to the success of the program. Few know what areas/people Compound should provide grants to as well as they do. We tried our best to form a committee that’s well-rounded: it has both active community participants as well as folks who know how to provide funding to projects and ideas. Having said that, we’re always open to hear from other community members who’d like to get involved.\n\nNobody on the list (to my knowledge) has a deep familiarity with the protocol codebase, which might be useful in quantifying grants for technical development.\n\nThat’s certainly a weakness of the committee today. We’d absolutely love to hear from people who are familiar with the codebase.\n\nIn the case of a protocol development, do you envision a grant before work is completed, after it is completed, after it is merged into the protocol? At what stage of development, would somebody apply for a grant? In the workflow, this isn’t made clear.\n\nThis is a question we hope to have an informed answer on after we get data points from running the grants program for several months. Certain applications, for example, would benefit from receiving a grant before the work is completed. Other applications may benefit from receiving a grant after the work is finished. Applications that have several intermediate stages may benefit from a hybrid approach: part of the grant is given upfront, part is given once intermediate stages are completed, and part is given once the work is merged into the protocol. We hope to have a better answer to the question after running the pilot and seeing what works and what doesn’t work. After several months of experimentation, best practices should start to emerge.\nSince we don’t have the best practices yet, we’ll want to start with the largest funnel we can to get as many data points as possible. To that end, when we start, we would accept applicants who are at any stage of the development process: those who haven’t started the work yet, those who have completed some of the work but have several more steps to go, and finally, those who have completed the work and merged it into the protocol but haven’t yet received payment for a job well done.\n\nIt seems like somebody would (or should) apply before completing development work (or another type of contribution). Would you disburse funds, partially disburse funds (with the remainder at a later stage), or communicate a “provisional” grant/acceptance?\n\nI think the above answer covers this question too.\n\nAny steps in mind to eliminate potential fraud?\n\nBeing a victim to fraud is the cost of doing business, particularly in this space. While we will take utmost care in screening out bad actors, mistakes may occur and losses may be incurred. That said, a small potential loss shouldn’t prevent us from doing something that has a chance of delivering a large potential gain to the community.\nTo keep the probability of loss as low as possible, we will likely need to verify the identity of each applicant prior to disbursing a grant. That is how we can ensure no baddies make it through the grants process. If this is indeed the final approach we take, we will be extremely careful with protecting the identity of applicants (the only people who would see their identity are members of the grants committee). But we’re still thinking this part through, so input from the community would be very welcome!\\nThanks for the comments @dabar90!\n\n…but I did not see repairing oracle solutions and compensating damaged users (or at least a statement) among the listed priorities.\n\nRepairing oracle solutions would certainly fall into the scope of the program (my guess is it would fall into the protocol and parameter development category). As far as compensating damaged users goes — do you think a grants committee should be handling that, or would a separate governance proposal be the better option?\n\nI would prefer that the interests of your team be aligned with the interests of the users and that these be priorities. That will be win-win situation.\n\nWe’re in agreement! But at the same time, we want to keep the scope of the program manageable so as not to boil the ocean. We will be listening to user requests closely: if there is something they want to improve on Compound but lack the technical knowledge to do so, we will encourage them to find someone with the technical knowledge, apply for a grant, and make the improvement.\n\nAnd i dont like expressions like this because it sounds to me like you’re addressing a board of directors of a joint stock company. The very purpose of the DeFi and DAO concept is for users to manage the protocol, so you can start with that.\n\nI think that’s a completely fair comment. At the same time, I do think the term stakeholder is apt because it captures users who use Compound but aren’t actively managing it. How else do we call these people? Do their interests matter?\\nThis an extremely important point. We are well aware that most grants programs are annoying to apply to, require some degree of lobbying, and in general, are a dreadful process for the applicant. We want to make the process as quick and easy for applicants as possible.\nWe’re still finalizing the process, but perhaps I can share some of the details that will hopefully encourage you to apply if the proposal passes.\nApplying will be fast. We will ask applicants for only the most important information. The application won’t take more than 30 minutes.\nApprovals will be fast. We won’t require applicants to jump through hoops to receive a grant. They won’t need to lobby all of the reviewers. That would stink, and it would prevent applicants from applying ever again. Our default position is applicants are here because they care about improving Compound. With that in mind, we will be optimistic with approvals. (We’re not gatekeepers; rather, our goal is to open as many doors as possible).\nYou don’t need to be well-connected. But you do need to care about Compound and have the ability to make it better.\nHope that dispels some of your concerns!\\nThanks @sukernik and everyone else involved in bringing this forward. A well-structured COMP grants program should help encourage a broader set of developers and community members to come forward and contribute to the ecosystem. As you note, the need to front audit and other development costs is often a prohibitive barrier to all but the most well-funded development teams. A grants program could help address this issue and bring more diversity and breadth to the ecosystem. It could also fund smaller, one-off “growth” projects that are too small or otherwise not practical to bring through the full governance process.\nWhile delegating authority to a committee does have certain downsides (as @jared notes), I think in this case it’s worth the tradeoff, for a few reasons. First, any updates to the core protocol would still need to be passed through the formal governance process before being merged. The committee itself would only be empowered to disburse funds to help defray some of the upfront development and audit costs. Second, for grants not involving core protocol development, the relatively small size of the budget - together with the 4-of-7 multisig design - further limits the downside risk of bad actors trying to abuse the system. And finally, as discussed above, the program could meaningfully broaden the set of community members who are able to contribute to the protocol, which should in turn enhance overall protocol health. Collectively, these factors weigh in favor of the committee structure for this specific use case, at least on an experimental basis.\nOperationally, the program should aim for efficiency while also not becoming overly bureaucratic (per @kermankohli’s comment above). One way to help strike this balance is to scale the amount of reporting, disclosures, etc. that a grantee has to provide alongside the amount of funding requested. For example, it seems reasonable that a project requesting $250,000 should have to provide more robust disclosures and reporting than a project requesting $5,000. A basic tiering system along these lines could help to reduce the risk of fraud while also not over-burdening smaller projects and discouraging them from participating.\nFinally, the proposal could ideally aim to quantify the success metrics a bit further. This exercise could be informed by lessons from the Uniswap grants program or from other more long-standing grant programs in the industry (e.g., Ethereum Foundation). Perhaps the program could even include incentives for the committee members that are tied to achieving certain metrics (though perhaps that’s better suited for a v2 of the program).\nOverall, this looks like a great first step towards creating a COMP grants program. We look forward to hearing other feedback from the community.\nJeff at a16z.\n\nFor disclosures, please see Disclosures - Andreessen Horowitz\\n\nfurther limits the downside risk of bad actors trying to abuse the system\n\nFWIW I would be more concerned about the filtering bias than malicious behavior, but I’ve found a lot of the points on this thread pretty compelling. And actually, if a record of all the applications for grants are publicly visible, as well as all the decisions (i.e. if its totally transparent), I think it might work pretty well.\\nWhile I believe that a Grants committee would be a new positive to the Compound ecosystem, I think there are a lot of issue which should be addressed with the current proposed program prior to initiating it.\nMembership\nFirstly, to echo Robert, I find the committee membership questionable. Some of members have never posted on these forums or in the Discord as far as I can tell, and I don’t believe that any member is familiar with the Compound protocol codebase.\nWhile in a more broad grants committee this may not be problematic, the scope of the committee is defined as the Compound ecosystem. For such a scope, an intimate involvement within the community is a must for each member in my eyes. Also, having at least one person with a strong understanding of the Compound codebase is a must (I would recommend @adam for this role).\nGrant Process\nThere should be a more concrete process defined for the lead and reviewers to commit to follow. Under the current post, there doesn’t seem to be any actual rules by which the grants committee must follow—I don’t see how Governance can entrust $2M to a committee without any predefined process and rules to follow.\nBuilding on the idea of processes, there should be an off-chain method for the community to vote on CGP related issues such as amendment to defined processes and changes to the committee.\nPracticality\nSomething which should be considered is the practicality of distributing $1M per quarter on the Compound Ecosystem. Compared to the broad $750k of the Uniswap grants program, this is a very large influx of spending, which I’m not sure could be done effectively as of now. As one of the main active developers within the community, I think that $1M of spending per quarter within the ecosystem could cause chaos. I would rather see grants spending scale up to a sustainable amount.\\nThese are really good comments. Thanks @arr00! Responses below.\n\nFirstly, to echo Robert, I find the committee membership questionable. Some of members have never posted on these forums or in the Discord as far as I can tell, and I don’t believe that any member is familiar with the Compound protocol codebase.\n\nSince we posted the proposal, we’ve had a community member reach out and volunteer to join the committee. The company they work for does understand the Compound codebase, which should improve the committee’s grasp of the technical side of things. If you’re reading this, know Compound’s codebase, and want to join the committee too, please don’t hesitate to reach out! (I’m larry#7198 on Discord).\n@adam: if you’re interested in joining, let me know!\nAs far as some of the members not posting on forums/Discord goes — this is something we really went back and forth on. One question I have is should being an active community member be a prerequisite for being on this committee and potentially future grants committees? You can sort of argue both sides. On the one hand, active community members know what the treasury needs to provide funding for better than anyone. On the other hand, someone who is an active community member may not actually be good at running a grants program!\nThen there’s this question: what does it mean to be an active community member? Is an investor who owns and thinks deeply about COMP but doesn’t have time to be part of the Discord / forums not an active community member? We certainly don’t have all the answers to these questions, but I think as a community, it’s worth thinking through them to set norms that define how future committees of this nature form.\n\nThere should be a more concrete process defined for the lead and reviewers to commit to follow\n\nAbsolutely. We didn’t post the process as part of the original proposal to keep it digestible (my sense is the longer it is, the fewer people read it in full). The original plan was to post the proposal, get community feedback, post it for an on-chain vote, and — if the proposal passes — to share a process grants applicants should expect to go through. But since you asked, here is the tentative process! It’s tentative because it hasn’t been tested in practice yet. I’m 100% sure parts of the process will change as we go about administering the program.\nGrants Process\n\nWe want to make this a speedy process for applicants. Apart from the initial ramp-up period, we want to get the time from application submission to grant disbursement to 10 days or less (8 days for lead to review and 2 days for reviewers to vote and disburse the funds)\nThe lead will review every application and send an “approve” or “deny” decision along with a brief explanation to the reviewers. Reviewers are expected to review the lead’s email and reply with an “approve” or “deny” vote as per their independent judgement\nDepending on the outcome of the reviewers’ vote, the grant will either be denied (no disbursement) or approved (grant is disbursed). If the grant is approved, reviewers should send it to the recipient no longer than 2 days after they first received the lead’s email\nMulti-sig will require majority vote to pass (4/7 or more votes)\n\n\nBuilding on the idea of processes, there should be an off-chain method for the community to vote on CGP related issues such as amendment to defined processes and changes to the committee.\n\nThat’s a great idea (I took note of it when you first mentioned it on the Clubhouse community call). We’ll work on setting up an off-chain vote for making small changes to the committee.\n\nSomething which should be considered is the practicality of distributing $1M per quarter on the Compound Ecosystem.\n\nTo your point, it’s hard to know how many applications we will receive before the fact. We may get $250k or $5mm worth of grant applications — the only way to find out is to launch the program and gather data! Our thinking was it’s better to play it safe than sorry: if $2mm is too much, we will simply return the funds to the treasury at the conclusion of the program. The outcome we wanted to avoid is we get something like $1.5mm in fantastic grant applications per quarter, which would require us to go through the governance process once more to ask the community for more funding. That would seriously slow the process down and make it less appealing to grants applicants.\\nFantastic update @sukernik, excited to see this move to a proposal \\nFirst of all, I would like to express my respect to you for making this proposal.\nI also have a lot of respect for the members involved in the proposal.\nI would like to share my thoughts on this proposal.\nI believe that Compound will collaborate with existing lenders in the future, but only if the governance of Compound remains well-diversified.\nA report on Defi from the Bank of St. Louis\nhttps://research.stlouisfed.org/publications/review/2021/02/05/decentralized-finance-on-blockchain-and-smart-contract-based-financial-markets 1\nThere is a concern that this proposal may run counter to this.\nEven if you get the right people to make the right decisions, it is still “centralized”.\nWhat I think is the best thing about Defi is that it is a system where no one can intentionally manipulate the Compound.\nThe following is an alternative idea to the current proposal.\n・A mechanism to conduct small-scale proposals through an off-chain.\n・Include in the proposal the cost of the audit and an estimate of the cost in advance.\nIf the community agrees to the “Grants Program”, I will follow it.\\nUpdate - 3/8/11\nThank you to everyone who weighed-in on the proposal! The community gave a lot of really good suggestions, a good portion of which we will incorporate into the final proposal. We’ve made the following updates after listening to the community:\nProposal updates\n\n\nCommittee membership: to improve the technical sophistication of the committee, we’ve replaced Ken with Nick Cannon 3 from Gauntlet. Gauntlet has successfully passed several technical updates to the protocol. The committee will benefit from having them onboard. We’ve also swapped in Min Teo 2 from ConsenSys and Leighton Cusack 1 from PoolTogether, who will be taking the place of Ryan and Sam (shout-out to @massnomis who posted the original idea for a grants committee). Playing committee Tetris wasn’t fun, but we think it was well-worth the effort — thanks to feedback from the community, the committee is way more well-balanced than before. The final committee will include:\n\nLarry Sukernik (Sheepshead Bay, LLC)\nJesse Walden (Variant)\nGetty Hill (Grapefruit Trading)\nMonetsupply (Independent)\nAparna Krishnan (Opyn)\nLeighton Cusack (PoolTogether)\nMin Teo (ConsenSys)\nNick Cannon (Gauntlet)\n\n\n\nGrants program amendments: during the community’s first Clubhouse call, @arr00 made a wonderful suggestion. If we need to make changes to the grants program after the proposal passes, it wouldn’t be practical to set up a formal governance proposal just for making small changes (e.g., swapping committee members around). Instead, @arr00 proposed the grants program set up a Snapshot page for the community to vote on amendments to the program. Setting up the Snapshot will require writing a bit of code (querying the amount of COMP delegated to addresses that vote). If the proposal passes, we will provide a grant to any community member who can help write this code and set up the Snapshot! While we’re on the topic of the grants program structure, we’d like to remind everyone that none of the grants program committee members are being elected into permanent roles. Once the pilot program wraps up in six months, the committee will be disbanded, allowing any community member(s) to set up a new committee with a fresh slate of members.\n\n\nWe will be posting the proposal on-chain tomorrow from Gauntlet’s address 6, 0x6626593c237f530d15ae9980a95ef938ac15c35c. Why are we using Gauntlet’s address? Gauntlet has ~126k COMP delegated to them, allowing us to more quickly post the proposal (as a reminder, Compound governance requires addresses to have 100k COMP to make a proposal). We’re excited about providing grants to the Compound community, so the quicker we can start, the better!\nIf you are in favor of the proposal, please vote once it’s posted on-chain! (You will see the proposal tomorrow at compound.finance/governance/proposals 20).\\n@sukernik this is great; can you post the address of the multi-sig, and have the members announce their addresses that they will be using (using some social channel, even Discord)? It is a good diligence step to ensure that COMP gets sent to the right place, and that it’s usable / without issues.\\n\n  \n      twitter.com\n  \n  \n    \n\nnick cannon (inkymaze) 3\n\n I will be on the Compound Grants multi-sig using this address:  0xb01474b50382fAe1A847E3A916ECDf07Ba57BcC7\n\n\n  5:42 PM - 8 Mar 2021\n    \n      \n        \n       1\n    \n\n\n  \n  \n    \n    \n  \n  \n\n\\n@sukernik Thanks for the detailed responses and addressing the concerns of the community. I’m excited to see this as a proposal.\\nSpeaking from experience of having been an undergrad who applied to several EF + Maker grants any money makes a difference. What makes a bigger difference is the community and support. In fact, a big reason of why I got into DeFi is the Maker grant program and the support from the other people who were building things through the grant program at the same time! If we can create a community for the people who are building things in the grant program, I think that can go a long way! \\nThe goal should be to make it as accessible as possible to people who are in need of it!\nThat said, I think this is significantly quicker than other grants or incubators that I’ve applied to in the past and if we can actually have decisions within 10 days or less that’s brilliant!\nFor more context, as a scrappy first time founder I applied to several grants and incubators. Any money when you are small makes a difference and goes a long way. 2 * $10k = $20k enough to cover cost of a simple audit.\nGrants:\nEF grant application took months. The Maker grant process took 3-4 weeks. Non crypto grant programs required connections. Some university based grant programs took months. The Dfinity grant program took months. I still applied to every single one of those. $10k across several grant programs starts to add up!\nIncubators:\nYC takes a couple months of work for $125k and 7%? (not sure if this has changed since).\nI still applied to all those grants and incubators because the application process (especially for YC) very clearly helped me think through parts of the business that I hadn’t previously. They forced me to crystallize my ideas.\nThe most important thing in the process is to make sure the grant program is tailored towards helping builders succeed. I think people will invest time into the process if they are able to see how the application itself helps them succeed.\\nGreat suggestion.\nMultisig address: 0xF1D8c2eED95D5fC2EaDe4E6Bb15a5969453E89a9\nMultisig members announcing their address\n\nGetty: https://twitter.com/getty_hill/status/1369110008839860225?s=20 12\n\nAparna: https://twitter.com/aparnalocked/status/1369330208445108225?s=20 7\n\nMonetsupply: https://twitter.com/MonetSupply/status/1369112944139673602?s=20 5\n\nJesse: https://twitter.com/jessewldn/status/1369289647164100609?s=20 4\n\nMin: https://twitter.com/_MinTeo/status/1369298973488513034?s=20 7\n\nNick: https://twitter.com/inkymaze/status/1369101198922555392?s=20 7\n\nLeighton: https://twitter.com/lay2000lbs/status/1369291729715798016?s=20 6\n\n\\nAs well as thinking this is a good idea, I’ll just throw in the newbie question here:\nWhat is the quorum threshold, and/or other criteria for success currently? (Mildly surprised I don’t see them on the vote page. I see the green bar, but I’m not sure that counts for anything)\\nTnx for answer, with your statements and little research I am totally support selected team and proposal.I hope this will accelerate the development of the Compound protocol and improve communication and align the interests of long-term users and early investors.\\nThanks for being in favor of the program!\nThe quorum threshold is 400k COMP (you can find more in-depth documentation here 10).\nJust to walk you through the process in a bit more detail, the proposal currently has ~1.37mm votes. If that doesn’t change, the proposal should pass and be queued for a two-day timelock. After the timelock passes, the proposal can be implemented."
  },
  {
    "number_of_comments": 10,
    "postid": "fcdd9153-bb2b-48c3-bbf3-9ff664ba3265",
    "posturl": "https://www.comp.xyz/t/investigate-market-manipulation-risk-in-zrx-and-other-tokens/3555",
    "combinedcontent": "In our recent security review of Compound v2, the Volt Protocol team identified a class of market manipulation risks. The full report is available here 34, but I have quoted the most relevant section below.\n\nIn reviewing the security of Compound in preparation for PCV deposit, the Volt Protocol team explored the risk of market manipulation attacks. The attack is possible when the amount of a token borrowable on markets like Aave and Compound is large compared to the liquid market. The most notable example is ZRX, which has borrowable liquidity on each of these markets comparable to or greater than the usual daily volume across all centralized and decentralized exchanges.\nCollateral Withdraw Attack\nIn the most basic form of the attack, a user borrows a large amount of the available supply of a token, such as ZRX, and sells it across multiple centralized and decentralized exchanges, depressing the open market price. The combined borrowable liquidity for ZRX on Aave and Compound v2s is more than twice the average daily volume currently. Once the oracles which inform Aave and Compound update, the user withdraws most of their original collateral. With numbers: supply $30 million collateral in stablecoins. Borrow “$20 million” of illiquid token and sell it, depressing the token’s market price by 95% and realizing $7.5m. New market value of the user’s debt is $1 million, allowing withdrawal of $28m collateral. Attacker profits $5.5m, leaving underlying market(s) with bad debt.\nFor this to work, it is necessary that the amount of the token borrowable be sufficiently large to depress the open market price. When the same token is borrowable on both Aave and Compound, as is the case with ZRX, this appears more likely. It is uncertain whether the current amount of borrowable liquidity would be sufficient to carry out the attack, though we believe that it is possible based on our analysis. If carried out in the form described above, bad debt would be confined to the ZRX market and not concern Volt Protocol PCV.\nTwo-Oracles Attack\nThere is a more sophisticated form of the attack that relies on differential oracle updates between Aave and Compound, and is in theory possible whenever two markets use different oracle systems. Whenever a Chainlink update gets sent into Compound, its price is compared against the cached 30 minute TWAP of that asset on Uniswap, which can be re-updated every 30 minutes. If the price on Uniswap and the price from chainlink are more than 15% apart, the Compound Oracle system does not accept that update as a valid price input, and will refuse to accept this new market price into the system.\nAn attacker can take advantage of this difference by, as in the Collateral Withdraw Attack, borrowing a large amount of ZRX from Aave and selling it rapidly across all exchanges to depress the price. Chainlink will “correctly” update the Aave price down, allowing the user to borrow even more against the same collateral, with the end result that all of the ZRX on Aave is borrowed. Once there is little to be gained from selling into the market, the attacker can instead deposit ZRX on Compound as collateral and use it to borrow. Since Compound rejected the price update from Chainlink and is still using a higher old TWAP price, the attacker can cash out by borrowing stablecoins or ETH on Compound. This would result in bad debt in the ZRX market on Aave, and bad debt in a stablecoin or ETH market on Compound.\n\nGiven the low returns of borrows against the least liquid tokens, and the large total value locked in Aave and Compound, we feel taking active precautionary action is warranted even if the possibility of the attack is theoretical rather than certain, so long as it cannot be proved infeasible.\nMore extensive simulation might shed light on the precise amount of borrowed ZRX or other tokens needed to manipulate the open market price, and as such what total borrowable supply across DeFi is safe. We believe this risk vector is worthy of ongoing monitoring and concern, even if short term precautionary action is not taken.\nVolt Protocol will soon integrate into Compound v2 as a lender, and will conduct a similar review of all future governance proposals, including direct testing, so long as the integration remains.\\nGiven the recent GMX oracle attack 7 which involved manipulation of exchange prices, perhaps there is interest in revisiting this topic.\\nThe Mango Markets manipulation from yesterday also seems to indicate that liquidity attacks represent a real risk to the protocol.\n\n  \n\n      twitter.com\n  \n\n  \n    \nJoshua Lim 4\n@joshua_j_lim\n\n\n  1/ this is how I think the mango attack played out, please let me know if I got anything wrong:\n\nat 6:19 PM ET, attacker funded acct A (CQvKS...) with 5mm USDC collateral\n\ntrade.mango.markets/account?pubkey… twitter.com/osec_io/status… https://t.co/rkdtJ8KU7h\n    \n      \n        OtterSec @osec_io\n      \n\n      @mangomarkets was just drained for over $100M. \n\nhttps://t.co/SI4hccCIQx\n\n\uD83E\uDDF5 https://t.co/IAKyXgN8gM\n    \n\n\n\n  12:09 AM - 12 Oct 2022\n\n    \n      \n        \n      \n      1.2K\n    \n\n    \n      \n        \n      \n      380\n    \n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nIn this case the attack worked somewhat differently from attack described above in OP. General process (slightly different due to use of perps, but this would be the equivalent for spot liquidity market like Compound):\n\nDeposit stablecoins or other hard collateral with high collateral factor in account A\nBorrow/short low liquidity asset XYZ from account A\nTransfer XYZ from account A to account B and deposit as collateral\nPurchase a lot of XYZ across DEX/CEX to push price up significantly\n(optional) Deposit additional purchased XYZ to account B\nAccount A may be pushed into liquidation, causing further upwards price pressure on XYZ\n\nBorrow hard assets (stablecoins, ETH, etc) at max LTV from account B using inflated value of XYZ collateral\n\nIf amount borrowed in step 7 (XYZ collateral * price * collateral factor) is greater than sum of collateral provided in account A (step 1) and cost of purchased XYZ (step 4), then the attack will generate a profit. The largest risk factor for an attack like this being profitable is illiquidity of a prospective target XYZ asset - this would allow the an attacker to move the price up significantly with relatively smaller amount of capital. Secondary risk factors include collateral factors for XYZ asset and of other hard collateral assets that could be used to borrow XYZ.\nI’m working on a proposal that I think could reduce risk of these sort of liquidity attacks, while having minimal impact on capital efficiency and UX of regular / non malicious users. Hope to share more soon!\\nI will be excited to hear the details of this proposal. You beat me to the punch coming in here to bring up the Mango incident.\nWe’ve brainstormed a few possible solutions to this category of risk:\n\nglobal rate limits on borrowing. Market manipulation is much more difficult as duration increases, something like an hourly rate limit equal to the max organic borrow in the last month.\nper-collateral borrow limits such that no matter how high the price is, risk per collateral type is capped\nmaximum and minimum oracle prices. Compound has already used the idea of a circuit breaker or fallback oracle. The system rejects a Chainlink update that deviates too far from the Uniswap TWAP. Instead of simply rejecting the update, it could also pause all new borrows until a valid update comes in or a governance action unpauses the market.\n\nSBF says it well below.\n\n  \n\n      twitter.com\n  \n\n  \n    \nSBF 7\n@SBF_FTX\n\n\n  1) When it comes to oracles,\n\nyou just have to make up your own damn mind https://t.co/7kZATSLpQM\n\n\n\n  12:15 PM - 12 Oct 2022\n\n    \n      \n        \n      \n      1.4K\n    \n\n    \n      \n        \n      \n      270\n    \n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\\n\n\n\n OneTrueKirk:\n\nper-collateral borrow limits such that no matter how high the price is, risk per collateral type is capped\n\n\nI think this can be achieved immediately with borrow limits feature.\n\n\n\n OneTrueKirk:\n\nglobal rate limits on borrowing. Market manipulation is much more difficult as duration increases, something like an hourly rate limit equal to the max organic borrow in the last month.\n\n\nThis could be achieved using borrow limit automation features, probably would look similar to Maker’s debt ceiling instant access modules (target available borrowable amount that can update every 6-12 hours). Would require some new engineering work I think, and replacing the borrow cap guardian from multisig to a contract.\n\n\n\n OneTrueKirk:\n\nmaximum and minimum oracle prices. Compound has already used the idea of a circuit breaker or fallback oracle. The system rejects a Chainlink update that deviates too far from the Uniswap TWAP. Instead of simply rejecting the update, it could also pause all new borrows until a valid update comes in or a governance action unpauses the market.\n\n\nOther possibility (although maybe more complicated) is to limit the rate of change of oracle updates. Eg for any prices that have changed more than 30% over the past hour, limit the change to 30% instead. Would need to be careful of possible edge cases for doing this.\\nThanks, @monet-supply @OneTrueKirk. Gauntlet is running analyses to estimate the cost of these attacks and will return to the forums ASAP to provide recommendations. It would be great to collaborate together on actions the protocol can take to mitigate these risks.\\nFor those who might not be aware, OpenZeppelin has built monitoring to watch for low liquidity attack vectors in V2 markets. I don’t think this bot covers exactly the same attack vectors as a Mango-style incident but further bots could also be developed to tackle this once a methodology is figured out.\nBot Description: This bot monitors Compound Finance cToken contracts that have low liquidity for potential market attacks where a malicious actor mints cTokens and then transfers additional tokens in\norder to unbalance the contract such that subsequent mints will not yield cTokens.\nBot Link: https://explorer.forta.network/bot/0xe49ab07879658c258d5007ac6b88428a2b88cc5cfef206222ad94690840be87a 9\\nMoving forward, it’s clear that these attack vectors do exist on the current implementations of Compound V2.\nInstead of band aid solutions that don’t fully fix the problem, I would propose that the oracle implementation be fixed with the same solution that SBF uses on FTX. Collateral factors on long tail assets get adjusted very far down to account for lack of liquidity on both centralized and decentralized exchanges, and instead of ignoring price updates if they deviate more than 15% from the TWAP value, always update the price in the oracle, just allow up to a maximum of 15% price deviation on any update. This means if an update comes in that says the price changed 30%, the protocol instead calculates the price of the asset at a 15% higher rate and stores that. Additionally, there should be a minimum period of time enforced between oracle updates so these 15% changes cannot be applied in too short a period of time that would nullify the effect of capping percentage changes.\nRemoving the concept of the uniswap anchor would be helpful as well because it would simplify the oracle codebase and remove the dependency on a TWAP value that in some cases has less than $200k in liquidity on the uniswap V2 pairs.\nThe clock is ticking, and the longer the protocol takes to implement a solution, the more likely someone exploiting this economic threat vector becomes.\\nWe have published recommendations here 44.\nAn oracle manipulation-based attack analogous to the one that cost Mango Markets $117m is much less likely to occur on Compound due to collateral assets having much deeper liquidity than MNGO and Compound requiring loans to be over-collateralized. However, out of an abundance of caution, we propose pausing supply for the above assets, given their relative liquidity profiles.\nImportantly, this will be a step towards incentivizing migration from Compound V2 to Compound V3, which the community has voiced support for. Compound V3 has meaningfully more risk controls, including supply caps.\\nI’m happy to see this risk mitigation step being taken (and also looking forward to when we can integrate Compound v3 in the future). New borrows against these assets are not likely to be large in size in the near term compared to the tail risk they bring to the market. Will be looking forward to hearing about the results of modeling attack cost.\\nI will support this proposal.\nand this is the proposal summary written by korean.\n\n  \n      \n\n      mirror.xyz\n  \n\n  \n    \n\ncompound 거버넌스 소식 #131 8\n\n  ZRX, BAT, MKR, YFI 마켓 신규 예금 중단\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"
  },
  {
    "number_of_comments": 11,
    "postid": "939573ce-93ee-4587-b671-68d2656bd4a4",
    "posturl": "https://www.comp.xyz/t/i-donated-78-000-usdc-to-the-compound-usdc-pool/1475",
    "combinedcontent": "Fellow Investors,\nI attempted to resend a transaction of 78K USDC for which I had a far too low gas fees to compound. In the process I dropped the “mint” function and effectively donated my entire Compound investment to the Compound USDC pool.\n\n  \n      \n      Ethereum (ETH) Blockchain Explorer\n  \n  \n    \n\nEthereum Transaction Hash (Txhash) Details | Etherscan 52\n\nEthereum (ETH) detailed transaction info for txhash 0x784d73c8d4faa0cee425e2badc0c8c552a2cc93f48daff54b720bd758b94dfe4. The transaction status, block confirmation, gas fee, Ether (ETH), and token transfer are shown.\n\n\n  \n  \n    \n    \n  \n  \n\n\nMy heart sunk, was glad it wasn’t stolen and have resolved to think of it as a good deed for the Compound community.\nI understand there’s no way to recover this money, but it was pointed out to me that I could appeal to Compound’s Governance, some of which may have benefited from my donation, however minutely, for sympathy.\nIf anyone would be willing to help me bring this forth to a proposal that could utilize Compounds reserves to assist me in recovering even a small fraction of this unintentional donation I would be eternally grateful.\nThank you,\nYoshi\\nSorry to hear this  Sadly cUSDC is not upgradeable and doesn’t have the new helper to make this type of thing easy to handle. Also, I believe that new helper function doesn’t work if the token is that same as the underlying, but there might still be hope to recover this one day…\\n@jared  thank you for the boost in hope as I want to believe there could be a way to recover this in the future.\nFrom what I understood so far, and correct me if I’m mistaken if you send USDC to the USDC pool without the mint function, it becomes extra interest to all the Compound USDC pool users so all USDC suppliers get a proportional part of it. So the only way to recover it, I thought would be to have to ask each USDC supplier if they’d be willing to return the unintended funds if there was even a way to know who they were and what they’d received.\nIf the funds just sat in the pool and weren’t distributed then I hoped there could be a way in the future to assign the funds back to myself, or if there was a way in the future to re-direct the same amount of interest I sent out to everyone back to me at a random time, or if there was a way for me to borrow Compound’s money interest-free and put it in a Compound pool until it generates the same amount in interest… I have a lot of hope, just no idea how realistic any of it is.\nAt this point, I’m just educating myself on how exactly this all works so any insights or suggestions are quite appreciated. Thanks!\\nusdc reserves can be swept to his address, it would need to go to governance\\nYes the protocol could vote to reimburse with USDC (or any other currency it has) - just to be clear @Yoshi is correct that his USDC was automatically distributed to suppliers (and not part of reserves)\\nHow about proposing USDC reserves be swept with a 10% penalty, that way I don’t get off scot-free and it could be said that they’ll be a 7800 USDC residual benefit after all is said and done, which may help garner support\\nOkay, so you basically have to get a proposal passed by governance in order to do this - you can read up on how governance works - its not the easiest task, but over time there may be others in your shoes and maybe you can band together to get something done, I don’t know \\nThis has been done before at proposal 37. Check the description there, it lays out the framework for how it will be conducted. Here it is:\nThis proposal acts to establish a policy for returning accidentally sent funds accessible by Governance under the following terms.\n\nSender must verify their identity through a signature or on-chain calldata and request return of funds on the compound forums within 6 months of accidental transfer\nFunds will only be returned to the originating address\nA 10% penalty will be imposed\nThe sent funds must have a minimum value of $1,000 at the time of the post\nAt most, once every six months, a proposal will be made that returns all funds eligible under these terms\n\nCurrently, there is one eligible transaction, and this proposal acts to return the locked funds under the terms stated above. In order to do so, we must set a new implementation for cUNI which adds the sweepToken functionality. It additionally removes unused verify callbacks resulting in minor gas savings on most interactions. This code change was developed in the open and reviewed by community members. Full testing has been done including unit tests and forking simulations. Due to the limited scope of the changes, they have not been professionally audited.\nYou hit all the requirements except for that last one. So don’t lose hope.\\nThis transaction is definitely not eligible under the terms setup in proposal 37. This transfer is of a completely different type than what was setup there. The USDC has been dissolved into the cUSDC pool and distributed to all the holders.\nGovernance should do what is possible to return accidentally lost funds when it is recoverable. In this case the funds are completely unrecoverable, and asking Governance to refund would be asking Governance to take the hit instead of the user.\nI would consider these funds completely lost with no chance of Governance refunding them. If Governance could, I think it would, but it is not possible.\\nThe actual funds themselves are distributed and unrecoverable. I don’t want to be overly optimistic, but I want to propose looking at this from a different angle to create a framework that may make returning these funds possible.\nI want us to focus on the APY for a moment. I would argue that dissolving the funds and distributing them to the holders caused about a ~0.0027% increase in the APY for USDC. If it’s possible to tap into the 7% reserve kept in the USDC pool and extracted the funds from there, we would see a similar drop in APY while the reserve replenishes back to 7%, then the APY returns to the state it was at before the surprise distribution. The distribution caused a mild benefit to all the holders and a disproportionate loss to one, and since most holders are in it long term, and if they also agree that lost funds should be returned when possible, there may be a chance. Add a 10% penalty or more and a residual benefit to the APY will remain. A 20%, 30%, or 50% penalty may put more people at ease given the nature of this proposal.\\nHey folks, I want to feel good about putting my money into Compound again in the future and would really like to see a way to get back a portion of my accidental distribution to the community. Would anyone support tapping the USDC reserves for half the amount, say 39,000 USDC, that way I can get something in return, and the overall community still benefits from the other half of the distribution?\\nYoshi, as arr00 mentioned, your tokens were effectively distributed to the users supplying USDC.\nUnfortunately, Governance does not have the resources or ability to reimburse these mistakes. I’m very sorry for your loss."
  },
  {
    "number_of_comments": 9,
    "postid": "07c1ff18-d60c-40d7-a3bf-80a3214e8a43",
    "posturl": "https://www.comp.xyz/t/a-single-supply-borrow-transaction-cost-300-in-fees/2207",
    "combinedcontent": "A single supply / borrow transaction, the core function of compound, currently cost $200-400 in fees. This effectively makes this platform unusable.\nwhat is being done to resolve this ridiculous situation with the ether gas fees, otherwise i dont see how this platform can survive. Why would anyone pay those fees especially considering the very average yield\\nThis is very true compound is just here to use our cash to fill  development  very High fees,  very poor customer and product service infact even their Ceth is not supported by same coinbase  they claim  to work with, whoever has extra cash take cardano and stake solana forget compound\nI advice once the cycle is over all  move your funds from here to yield farming  Amm. And binance or cardano and solana\nFor now am disappointed by compound\\nAs a noob, last year I thought it was a good idea to put money in compound…(ceth, cbat  just to name a few) now idk what to do! I am making a little gains but to withdraw or convert costs soooo much money!\\nYou are not alone, everyone is in the same boat.  I wanted to withdraw my usdc supply and pay off the usdt borrow since I am being charged 13% interest and losing money everyday. But just to do those 2 transaction will cost me $900\nThis defi suppose to save you fees and fix issues with traditional banking, but instead has turned into a complete joke, add on to the very real risk of bugs that can make you lose your whole investment. I don’t know why anyone even bothers, it will be dead if those fees continue\\nIf you just want to invest in Compound without high gas fees, we built OUSD 15 which lets you swap into and out of 100% stables backed by a current mix of mostly Compound and a little AAVE for the gas price of a single uniswap transaction (or less). It also automatically harvest’s COMP, and reinvests it without you having to pay for the gas.\\nI agree with the point stated here (and already in other discussions in the forum) that the extreme gas fees represent a very unpleasant property of this environment.\nWouldn’t it be an option to run the Coumpound transactions with support of http://loopring.io 3 (or sth comparable, if there are alternatives) going forward?\\nThat’s great!  How do you connect to OUSD from a Coinbase Ethereum wallet?\\n@Casgan You should be able to go to OUSD 6 and connect on that page.\\nSorry for the basic questions, but which wallet type would be correct for connecting to the site?\\nIt should be on the Ethereum network."
  },
  {
    "number_of_comments": 10,
    "postid": "1d0b9352-521f-40fd-b8c6-d1104267e510",
    "posturl": "https://www.comp.xyz/t/supplied-dai-to-compound-but-anything-happened-and-fees-got-deducted/326",
    "combinedcontent": "Hi, I have supplied DAI to Compund but anything happened. I did first the approval transaction and then the supply transaction.\nThe supply transaction is terminated, fees deducted but still in Compund I cannot see the supplied DAI. Is there a way to see what happened?\nThanks\\nلا يمكن رأيت DAlونرجو التعاون معي واكثر توضيح وزيادة ارباح والتشفير والتوضيح والحفاض على مجهودنا كلنا\\nI only speak english, sorry\\nIt’s a welcome idea, I believe it can happen\\nTried to supply DAI again to Compound, again got 5$ fee deducted but DAI are not supplied. There is no support from Compund platform?\\nYou should provide or analyze the ethereum transaction, which is accessible if you know the transaction hash.\\n\n  \n      \n      Ethereum (ETH) Blockchain Explorer\n  \n  \n    \n\nEthereum Transaction Hash (Txhash) Details | Etherscan 6\n\nEthereum (ETH) detailed transaction info for txhash 0x639fb3b7136b19fb05f991933c37eb973404070563d2c0dd64d5e895df10a06e. The transaction status, block confirmation, gas fee, Ether (ETH), and token transfer are shown.\n\n\n  \n  \n    \n    \n  \n  \n\n\nThis is the last transaction that I have done to supply DAI to compund. I’m do not see any action in it\\nThere was an insufficient allowance error emitted. This means that you need to approve cDAI to withdraw DAI from your account. This in generally done when you enable an asset on the Compound frontend.\\nThank arr00. I can now send DAI to Compound!\\nHi , I experienced a similar problem cashing out my BAT , a cBAT wallet appeared & records show I sent to my other wallet but I never received anything, I’m a confused novice & don’t understand what happened looks like my funds disappeared\\nCan you please help me ?"
  },
  {
    "number_of_comments": 41,
    "postid": "76aba93f-3595-4959-9858-5066f3049409",
    "posturl": "https://forum.makerdao.com/t/qgov-ad-recognition-submission/20494",
    "combinedcontent": "AD Recognition Submission\n0xB0524D8707F76c681901b782372EbeD2d4bA28a6\n0x0a907fe3adb890db7db27a7f21e188a4127b2e7c: Followed Resiliency AVC\n0xca0c8bedc85c2ec9b0dfb42b3f2763486ddea1b6: Followed Growth AVC (Now neutral GSL)\n0x1dd6c65e6e22f196d5c2209f439d1f07d02ba7a4: Followed Regenerative Finance AVC\n\n[Cryptograpically signed AD Recognition Submission Message from the Ethereum address controlling Delegation Contract 1]: https://etherscan.io/verifySig/16765\n[Cryptograpically signed AD Recognition Submission Message from the Ethereum address controlling Delegation Contract 2]: https://etherscan.io/verifySig/16764\n[Cryptograpically signed AD Recognition Submission Message from the Ethereum address controlling Delegation Contract 3]: https://etherscan.io/verifySig/26528\n[Cryptograpically signed AD Recognition Submission Message from the Ecosystem Actor Ethereum address, where this address is not one of the addresses controlling a Delegation Contract]: https://etherscan.io/verifySig/16763\n\\nReserving for future edits; @prose11 @Patrick_J could you please confirm this is all correct?\\nConfirming for the Arbitration Scope, you can view the details compiled here:\n  \n      \n\n      MIPs Portal\n  \n\n  \n    \n\nMIPs Portal 5\n\n  Maker Improvement Proposals are the preferred mechanism for improving both Maker Governance and the Maker Protocol.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\\nThank you!\\nRatification Poll for MIP Amendment Subproposals (MIP102c2-SP2) - April 10, 2023 4\nResiliency CVC: YES: Summed up, this clarifies a lot of ambiguities that possibly arose from the Constitution vote. Emphasis on upfront costs to lead to future savings down the road in terms of efficiency and more defined roles. Sets up foundation for more privacy focused contributors and users’ privacy.\nGrowth CVC: YES: Initial investments for future growth is the main attractor here currently. No finalized and clear CVC “north stars” yet; but many instruments put in place for clarity in the future and ability to shore/concentrate budgets for future growth.\\nRaising GSM Pause Delay, Recognized Delegate Compensation, DAI and MKR Streams, ESM Interaction Changes - April 5, 2023\nResiliency & Growth CVC: YES: Voting to approve the prior voted in executive votes. Thanks to the team for their hard work as always.\\nOnboard Coinbase Custody (RWA014) as a new RWA Vault Type - April 17, 2023\nResiliency & Growth CVC: YES: This is a great new yield opportunity for Maker and directly in line with the Growth CVC. With regards to the Resiliency CVC, this new yield will directly increase protocol surplus with no extra expenses for the protocol.\nHow Should Coinbase Custody Hold Deposited USDC - April 17, 2023\nResiliency & Growth CVC: Cold Wallet: Voting together on this as this is the most logical choice with regards to risk liquidity tradeoff. In this scenario, the extra insurance for this type of custody will allow for more little to no risks for the Coinbase vault.\nDecentralized Collateral Parameter Changes - April 17, 2023\nResiliency & Growth CVC: YES: Voting to implement these collateral parameter changes. The higher stability fees mainly will result in substantially increased profits (~$5m) and fit in line with both CVCs goals. Thanks to the team for their smooth transition over to ecosystem actors.\\nArbitration Scope Clarification Edit - Determining the Minimum Verified MKR Holding for CVC Members - April 24, 2023\nResiliency & Growth CVC: Increase to 1 MKR: Voting to increase the minimum CVC verified weight to 1 MKR from both contracts as Growth thinks it’s appropriate from the perspective of Spam prevention and to some extent aligning incentives with Maker protocol long term. Similar reasoning for Resiliency CVC can be applied.\nOnboard PullUp as an Incubating Ecosystem Actor - April 24, 2023\nResiliency & Growth CVC: YES: PullUp is a logical transition from the doxxed PE core unit to anonymous ecosystem actors who will help continue to build the Maker protocol and make it more resilient. The goals of PullUp are also very aligned with the goals of the Growth CVC and all in all will make sure that we can respond quickly and efficiently to important projects in the future.\nEcosystem Scope Clarification Edit - Amend PullUp Ecosystem Actor MKR Allocation - April 24, 2023\nResiliency & Growth CVC: YES: Addressing bugs and fixing issues that arise in documentation is clearly within both CVCs interests.\\nStability Fee Changes, Collateral Offboarding Preparation, Coinbase Custody Legal Documentation, DAO Resolutions, PE MKR Stream Cleanup - April 26, 2023\nResiliency & Growth CVC: YES: Voting in support of this executive vote. Namely, we want to point out the stability fee changes prior approved as well as the Coinbase custody legal documentation. Thanks to all involved parties for their hard work in this transitionary time!\\nProtocol Engineering Scope Clarification Edits - May 1, 2023\nResiliency & Growth AVC: YES: Voting in support of this operational mistake. We understand mistakes and errors happen and thank the team for catching it early. Needless to say, fixing errors and correcting things operationally fall in line with both Resiliency and Growth AVCs\\nSpark Lend D3M Onboarding - May 02, 2023\nResiliency & Growth AVC: YES: Voting in support of this executive vote. We are looking forward to the new vault parameters and instructions for the Spark Lend D3M and look forward to how it will optimize growth for Maker protocol.\\nAmend Keeper Networks - May 8, 2023\nResiliency & Growth AVC: YES: Updating the streams for these Keeper Networks is reasonable and nothing out of the ordinary. Both AVCs in support as it upkeeps the protocol and keeps things running smoothly\\nRisk Parameter Changes, Increase Starknet Bridge Limit, DAI Transfer, Vesting Stream Management, DAO Resolution - May 10, 2023 1\nResiliency & Growth AVC: YES: Voting in support of this executive vote. The risk parameter and stability fee changes are logical and make sense. Also voting to transfer the prior agreed MKR and DAI streams to the EA listed. Also increasing the Starknet bridge limit from 1 to 5 million DAI is logical and we’re excited to see this grow!\\nRatification Poll for MIP Amendment Subproposals (MIP102c2-SP7) - May 8, 2023 1\nResiliency & Growth AVC: YES: Voting in support of these very substantial changes in the MIPs. With respect to ReFi AVC, these MIP amendments will allow for a future with more public goods and possibilities of investing in regenerative finance. Under a new brand name and marketing effort, the “new Maker” will be a super interesting project and we’re excited to see where everything goes! With respect to Resiliency AVC, we expect to be in support and have voted accordingly. The new rebrand to bolster up expenses and to allow for a more clear path forward and opportunities to generate yield back to tokenholers and the protocol. A point we’d like to point out is we’re interested in seeing how the clause on anonymous ADs and facilitators will play out and think there is still some useful discussion and plans to be implemented with respect to that topic in the immediate term.\\nOnboard GNO to Spark Protocol - May 15, 2023\nResiliency & Growth AVC: YES: Voting yes for Growth AVC as Spark is a crucial component of Maker’s growth in the early stages of Endgame and onboarding this strategic asset onto Spark is a very logical step. Similar reasoning applies for Resiliency AVC as this logical addition to spark will increase revenues and directly benefit the Maker protocol as well.\nAdjust BlockTower Credit Debt Ceilings - May 15, 2023\nResiliency & Growth AVC: YES: It makes sense to combine the RWA vaults into one and consolidate. This is logistical from a reporting and operation standard and will hopefully reduce logistical costs and make things smoother in the future, something both AVCs support.\nApprove Credix Finance Assessment Work - May 15, 2023\nResiliency & Growth AVC: YES: As specified in the forums, this is for just determining if SF should start assessing and determining if Credix is appropriate to work on. In this case, both AVCs are in support of seeking and analyzing new strategic growth opportunities and we look forward to their findings!\nCAIS Bootstrap Funding - May 15, 2023\nResiliency & Growth AVC: YES: Voting for both AVCs as this is a good first step in working towards the Protocol Scopes of the Maker protocol in Endgame. We are hesitant to have too much work done before a little more clarity regarding use of funds is used but are confident the team will be diligent in their funding and spending, and look forward to what will come out of such experimentation.\\nOffboarding Multiple Vault Types, Constitutional Delegate Compensation, Multiple DAI Budget Streams, MKR Vesting - May 17, 2023\nResiliency & Growth CVC: YES: Voting to approve the prior voted in executive votes. Thanks to the team for their hard work as always.\\nCoinbase Custody (RWA014) Onboarding, Keeper Network Amendments, Core Artificial Intelligence System (CAIS) Bootstrap Funding, Spark Lend GNO Onboarding and Associated Changes - May 25, 2023 1\nResiliency & Growth CVC: YES: Voting to approve the executive vote. Nothing seems out of the ordinary here and everything is within reason. Thanks to the team for their hard work as always.\\nAdd BlockTower Andromeda (RWA015) as a new RWA Vault Type - May 29, 2023\nResiliency & Growth CVC: YES: Voting to implement these new 1.2B debt ceiling. One concern we’d like to point out is just some of the wordplay; at the 1.2B debt ceiling, the max Blocktower fee is equal to the minimum fee they are charging. Therefore, the fee will be the same with these parameters no matter the vault balance. Overall, the 50+ million of yield this will generate is simply too good of an opportunity to be passing up right now.\nNon-Scope Defined Parameter Changes - May 29, 2023\nResiliency & Growth CVC: YES: These parameters make sense. As we turn to an environment when interest rates are substantially higher than before, stability fees in defi should also naturally raise.\nAdjust Spark Protocol D3M Parameters - May 29, 2023\nResiliency & Growth CVC: YES: We’re happy to see the Spark Protocol debt ceiling be reached and are in favor of expanding it.\nDeploy and Use Spark Protocol Executive Proxy - May 29, 2023\nResiliency & Growth CVC: YES: This is safer from a security standpoint.\nOnboard rETH to Spark Protocol - May 29, 2023\nResiliency & Growth CVC: YES: rETh has proven itself to be a valuable collateral type on other money markets and should also be onboarded here.\nAdjust Spark Protocol DAI Interest Rate Strategy to Match the DSR - May 29, 2023\nResiliency & Growth CVC: YES: This change falls in line with the new DSR changes and makes sense.\nReduce PSM-USDP-A Debt Ceiling to 0 - May 29, 2023\nResiliency & Growth CVC: YES: USDP is no longer as needed and valuable in today’s Maker given its no yield generation.\\nGUSD PSM Parameter Adjustments - June 12, 2023\nResiliency & Growth CVC: YES: This was a relatively hard tradeoff here. We think that in the long run, as more and more high yielding opportunities come up, there will be higher and higher opportunity cost for the lower 2% yields. Currently, the USDC baseline rate is what we think should be the minimum. On the contrary, we applaud and thank Gemini for their timely monthly payments where other RWA vaults have not done the same. Changing the PSM params should passively decrease GUSD over time.\\nBlockTower Andromeda (RWA015-A) Onboarding, Risk Parameter Changes, DSR Increase, Spark Protocol Proxy Activation, MKR Vesting Transfers - June 14, 2023\nResiliency & Growth CVC: YES: Voting to approve this current executive vote with the respective params as well as the DSR increase, Spark proxy activation, and MKR vesting transfers. Nothing looks out of the ordinary here.\\nOut-of-Schedule Executive Vote - RWA014-A (Coinbase Custody) DAO Resolution - June 16, 2023\nResiliency & Growth CVC: YES: Voting to support this out of cycle exec vote to increase immediate liquidity of the PSM and prevent any death spiral or bank run illiquidity scenarios. Thanks to the teams for getting this up and running so quickly\\nRatification Poll for MIP Amendment Subproposal (MIP102c2-SP8) - June 12, 2023\nResiliency & Growth AVC: YES: Not too many issues here. The most contentious part of this we presume is the AD spots decreasing from 7 PD and RD to 5. Generally speaking, this would decrease the total number of delegates being paid but increases delegate pay for 1-5 and 8-10. Our ultimate yes is such that we believe this creates a slightly more competitive space for delegates to “do better” and outperform the next in order to achieve as much as possible.\nRatification Poll for MIP Amendment Subproposal (MIP102c2-SP9) - June 12, 2023\nResiliency & Growth AVC: YES: Voting yes as we believe the Bug Bounty program has shown its worth the last many months and believe this reiteration of it shall happen again, this time under the Scopes Framework. Overall, we think the team will optimize even more this time around and be more efficient.\\nSmart Burn Engine Launch Parameters - June 26, 2023\nResiliency & Growth AVC: YES: Excited to see the new Smart Burn Engine being kickstarted! By buying back MKR and LPing on the main Uniswap v2 pool, we’ll have a much deeper on-chain liquidity depth and increase protocol owned emissions tremendously.\\nBlockTower Andromeda Updates, GUSD PSM Parameter Changes, and Other Actions - June 28, 2023\nResiliency & Growth AVC: YES: Voting to approve the respective executive votes. Implementing the GUSD parameters, MKR payments to ADs and EAs, as well as the Andromeda updates.\\nDAO Resolution to Approve Legal Representation - July 3, 2023\nResiliency & Growth AVC: YES: It’s unfortunate to see this case that started over a year ago reach its way to where it is now and have the judge issue the current order. We are confident that the proposed Samar Law will do a great job at representing MakerDAO at court. From a budget standpoint, we think this should fall under the resiliency fund that was recently approved.\\nNotification to Community: Joining KISS AVC\nAD Recognition Submission\n0x1dd6c65e6e22f196d5c2209f439d1f07d02ba7a4: Followed KISS AVC\n[Cryptograpically signed AD Recognition Submission Message from the Ethereum address controlling Delegation Contract 3]: https://etherscan.io/verifySig/21305\n\nCc: @Patrick_J @LongForWisdom @iammeeoh Thanks everyone.\\nBlockTower Andromeda Upgrade, Smart Burn Engine Deployment, Keeper Job Updates, Scope Defined Parameter Changes, Delegate Compensation, Ecosystem Actor and Core Unit Funding Updates, Spark Protocol Proxy Spell Execution - July 14, 2023 3\nResiliency & Growth CVC: YES: Voting to approve the prior voted in polls. Thanks to the team for their hard work as always.\\nDecrease the Harbor Trade Credit (RWA004-A) Debt Ceiling - July 17, 2023\nResiliency, Neutral, & KISS AVC: YES: We believe decreasing the Harbor Trade Credit Debt Ceiling to 0 is the safest and smartest thing to do currently. There is a possibility that by keeping it high, they can draw down more DAI and cause further defaults.\\nStability Scope Bootstrapping Edits - July 24, 2023\nResiliency, Neutral, & KISS AVC: YES: The proposed enhanced Dai Savings Rate is a way to increase Maker adoption over time and increase utilization of the DSR. This proposal lets the protocol get ready for Endgame initial phase 1 launch and implements other safety nets and solutions for governance.\nNew Silver (RWA002-A) Restructuring and Parameter Changes - July 24, 2023\nResiliency, Neutral, & KISS AVC: YES: We see these changes as beneficial to the current RWA vault structure. This lets more flexibility for yield generation and also opens the door to more and more yield buckets getting involved in the future, for potentially higher yields.\nAdjust Spark Protocol D3M Debt Ceiling - July 24, 2023\nResiliency, Neutral, & KISS AVC: YES: The Phoenix Labs team’s proposed changes to the D3M made sense. By increasing to 200M, we also don’t need to update the ceiling every few weeks and there’s more time for growth before governance intervention again.\nSpark Protocol WETH and DAI Market Parameter Changes - July 24, 2023 1\nResiliency, Neutral, & KISS AVC: YES: The Phoenix Labs team’s proposed changes to the WETH and DAI market made sense. By setting the DAI Market LTV and LT to 0.01%, this reduced the likelihood of full utilization and subsequent liquidation risks. For the WETH market, the decrease in RF will boost yield for lenders.\\nEnhanced Dai Savings Rate Activation, Spark Protocol Debt Ceiling Increase, RWA Vault Updates, AVC Member Compensation for Q2 2023, DAO Resolution for Monetalis Clydesdale, Launch Project Funding, Spark Proxy Spell Execution - August 2, 2023\nResiliency, Neutral, & KISS AVC: YES: Voting to approve the prior voted in polls. Thanks to the team for their hard work as always.\\nNon-Scope Defined Parameter Changes - wstETH-B DC-IAM Changes - August 7, 2023\nResiliency, Neutral, & KISS AVC: YES: The two proposed changes of “Increase WSTETH-B DC-IAM gap for 15M, from 30M to 45M,” and “Reduce WSTETH-B DC-IAM ttl from 57,600 seconds to 43,200 seconds” seem logical and the explanations given by Rema make sense. We are in support.\nSmart Burn Engine Parameter Update - August 7, 2023\nResiliency, Neutral, & KISS AVC: YES: The proposed changes to increase each Smart Burn Engine purchase and occur a proportionally amount of time less often makes sense. There is no reason to pay so much gas now that there is enough liquidity.\\nStability Scope Bootstrapping Edits (EDSR Changes) - August 14, 2023\nResiliency, Neutral, & KISS AVC: YES: The proposed changes to edit the prior voted in EDSR parameters make sense as we’ve seen the massive inflow of mercenary funds to the DSR the last few days. We think these proposed changes will help stabilize the DSR with new sticky DAI demand.\\nEDSR Adjustment, Vault and Smart Burn Engine Parameter Updates, CRVV1ETHSTETH-A Offboarding, Delegate Compensation, and Other Changes - August 17, 2023\nResiliency, Neutral, & KISS AVC: YES: Voting to approve the prior voted in polls. Thanks to the team for their hard work as always.\\nRatification Poll for MIP Amendment Subproposal (MIP102c2-SP11) - August 14, 2023\nResiliency, Neutral, & KISS AVC: NO: As the bounty program has evolved over the last many months, we believe this program should adapt to new Endgame standards as well.\nRatification Poll for MIP Amendment Subproposals - Article 1 Edits (MIP102c2-SP12) - August 14, 2023\nResiliency, Neutral, & KISS AVC: NO: True Name has done a great job in raising some concerns to the forums and we believe that a lot of these have yet to go answered. WE are in favor of delaying this until more questions are ironed out in the future.\nRatification Poll for MIP Amendment Subproposal (MIP102c2-SP13) - August 14, 2023\nResiliency, Neutral, & KISS AVC: YES: Mostly clerical changes, we are interested in how the 1:12000 split will psychologically hopefully encourage more Maker adoption and demand.\nReserve Governance Facilitator Appointment - August 21, 2023\nResiliency, Neutral, & KISS AVC: Bateleur, VoteWizard, JanSky: As the transition happens, we think having more manpower to take over everything is the best move forward. These candidates are the best options currently we think under the soon to be enforced anonymity clause.\nIncrease Spark Protocol wstETH Supply Cap - August 21, 2023\nResiliency, Neutral, & KISS AVC: YES: The Phoenix Labs team’s proposed changes to the wstETH supply cap makes sense. Glad to see caps being raised once again.\nSpark Protocol ETH Market Parameter Changes - August 21, 2023\nResiliency, Neutral, & KISS AVC: YES: The Phoenix Labs team’s proposed changes to the ETH market parameters make sense.\\nApprove DAO Resolution Pertaining to HV Bank (RWA009-A) - August 28, 2023\nResiliency, Neutral, & KISS AVC: YES: As we move forward to NewChain and future endgame progressions, it’s best to follow MIP104 and offboard legacy legal recourse assets.\nDecrease Fortunafi (RWA005-A) Debt Ceiling - August 28, 2023 1\nResiliency, Neutral, & KISS AVC: YES: As we move forward to NewChain and future endgame progressions, it’s best to follow MIP104 and offboard legacy legal recourse assets.\\nManagement of ConsolFreight (RWA003-A) Default, ESM Authorization, Chainlog Updates, Budget Management, Spark Proxy Spell - August 30, 2023\nResiliency, Neutral, & KISS AVC: YES: Voting to approve the prior voted in polls. Thanks to the team for their hard work as always.\\nAdjust Spark Protocol D3M Parameters - September 4, 2023\nResiliency, Neutral, & KISS AVC: YES: The Phoenix Labs team’s proposed changes to the D3M parameters makes sense.\nAdjust Spark Protocol DAI Interest Rate Strategy Borrow Spread - September 4, 2023\nResiliency, Neutral, & KISS AVC: YES: The Phoenix Labs team’s proposed changes to the DAI interest rate strategy borrow spread makes sense.\nAdjust Spark Protocol Flash Loan Fee - September 4, 2023\nResiliency, Neutral, & KISS AVC: YES: The Phoenix Labs team’s proposed changes to the Spark Protocol Flash Loan Fee makes sense.\nJAT1 to JAT2 Asset Reallocation - September 4, 2023\nResiliency, Neutral, & KISS AVC: YES: This asset reallocation makes sense and we are in favor. Overall, there are a few things we are concerned about such as trust specifics that we would like to be broadcast more clearly to the forums but at face value right now, this T-Bill ladder specified is appropriate.\\nStability Scope Parameter Changes, Spark Protocol D3M Parameter Changes, Set Fortunafi Debt Ceiling to Zero DAI, DAO Resolution for HV Bank, Delegate Compensation and Other Actions - September 13, 2023\nResiliency, Neutral, & KISS AVC: YES: Voting to approve the prior voted in polls. Thanks to the team for their hard work as always.\\nActivate Gnosis Chain Instance of Spark Lend - September 18, 2023\nResiliency, Neutral, & KISS AVC: YES: Voting to approve the activation of this instance of Spark Lend. We are in favor of this deployment and hope it generates significant income.\nRatification Poll for MIP Amendment Subproposals - Article 1 Edits (MIP102c2-SP15) - September 11, 2023\nResiliency, Neutral, & KISS AVC: YES: Voting to approve these changes and edits for advisors to Maker in endgame. Nothing seems out of place.\nRatification Poll for MIP Amendment Subproposal (MIP102c2-SP16) - September 11, 2023\nResiliency, Neutral, & KISS AVC: YES: Voting to support these MIP changes. Nothing stands out as out of place.\nRatification Poll for MIP Amendment Subproposals (MIP102c2-SP4) - September 11, 2023\nResiliency, Neutral, & KISS AVC: YES: These requirements for arrangers make sense and nothing seems out of place or in error.\\nReconfiguring RWA Allocator Vaults - September 25, 2023\nResiliency, Neutral, & KISS AVC: YES: These increased RWA allocator vaults should allow for more yields in a safe manner.\\nDAO Resolution for Monetalis Clydesdale, HV Bank On-chain RWA Agreement Update, Fortunafi Vault Change, Trigger Spark Protocol Proxy Spell - September 27, 2023\nResiliency, Neutral, & KISS AVC: YES: Voting to approve the prior voted in polls. Thanks to the team for their hard work as always.\\nNon-Scope Defined Parameter Changes - WBTC DC-IAM Changes - October 2, 2023\nResiliency, Neutral, & KISS AVC: YES: The proposed changes to the WBTC DC-IAM makes sense and reducing the available debt across the board are in line with the stability scope.\nIncrease rETH Supply Cap to 60k - October 2, 2023\nResiliency, Neutral, & KISS AVC: YES: The Phoenix Labs team’s proposed changes to the sETH supply cap makes sense.\nOnboard USDC to SparkLend Ethereum and Activate USD eMode for USDC and sDAI Markets - October 2, 2023\nResiliency, Neutral, & KISS AVC: YES: The Phoenix Labs team’s proposed changes to the SparkLend ETh market makes sense.\nOnboard USDT to SparkLend Ethereum and Activate USD eMode - October 2, 2023\nResiliency, Neutral, & KISS AVC: YES: The Phoenix Labs team’s proposed changes to the SparkLend ETh market makes sense."
  },
  {
    "number_of_comments": 9,
    "postid": "aba06f23-ae81-40a6-9c94-ad3285541158",
    "posturl": "https://www.comp.xyz/t/gauntlet-weekly-market-updates-base-weth/4676",
    "combinedcontent": "[Gauntlet] BASE v3 WETH Update (09/08/2023 - 09/14/2023)\nGauntlet would like to provide the community with an update on metrics from the BASE v3 WETH comet over the past week.\nSimple Summary\n\nWETH Borrows are up 62.87%, from 0.87k ($1.42M) to 1.42k ($2.3M).\nWETH Supply is down 2.60%, from 3.84k ($6.42M) to 3.75k ($6.25M).\nUSDC utilization increased 67.23%, from 22.0% to 36.9%.\nThe minimum USDC reserve growth was -68.3%, and the maximum was -20.3%. The average USDC reserve growth was -45.3%.\nThe comet accumulated $-0.29k USDC reserves while distributing $5.32k COMP rewards for a weekly Net Protocol Profit of $-5.61k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-09-15 at 2.54.59 PM1792×486 65.4 KB\nTotal Collateral (USD) is up 57.21%, from $1.72M to $2.71M.\nScreen Shot 2023-09-15 at 2.55.19 PM1804×466 67.6 KB\nWETH Supply is down 2.60%, from 3.84k ($6.42M) to 3.75k ($6.25M).\nScreen Shot 2023-09-15 at 2.55.42 PM1798×474 64.1 KB\nWETH Borrows are up 62.87%, from 0.87k ($1.42M) to 1.42k ($2.3M).\nScreen Shot 2023-09-15 at 2.56.00 PM1644×460 31.2 KB\nUSDC utilization increased 67.23%, from 22.0% to 36.9%.\nSupply Caps\nScreen Shot 2023-09-15 at 2.57.39 PM3804×618 81 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-09-15 at 2.58.06 PM3802×840 125 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-09-15 at 2.56.17 PM1662×424 27.4 KB\nThe minimum USDC utilization was 18.7%, and the maximum was 36.9%.\nThe minimum USDC reserve growth was -68.3%, and the maximum was -20.3%. The average USDC reserve growth was -45.3%.\nScreen Shot 2023-09-15 at 2.56.38 PM1674×470 32.9 KB\nThe comet accumulated $-0.29k USDC reserves while distributing $5.32k COMP rewards for a weekly Net Protocol Profit of $-5.61k.\\n[Gauntlet] BASE v3 WETH Update (09/15/2023 - 09/21/2023)\nGauntlet would like to provide the community with an update on metrics from the BASE v3 WETH comet over the past week.\nSimple Summary\n\nWETH Borrows are up 1.12%, from 1.42k ($2.25M) to 1.43k ($2.27M).\nWETH Supply is up 2.68%, from 3.75k ($6.09M) to 3.84k ($6.26M).\nUSDC utilization decreased 1.52%, from 36.9% to 36.3%.\nThe minimum USDC reserve growth was -22.3%, and the maximum was -15.6%. The average USDC reserve growth was -20.1%.\nThe comet accumulated $-0.21k USDC reserves while distributing $5.51k COMP rewards for a weekly Net Protocol Profit of $-5.73k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-09-22 at 10.41.47 AM1794×474 64.1 KB\nTotal Collateral (USD) is up 1.04%, from $2.74M to $2.76M.\nScreen Shot 2023-09-22 at 10.42.06 AM1800×458 65.8 KB\nWETH Supply is up 2.68%, from 3.75k ($6.09M) to 3.84k ($6.26M).\nScreen Shot 2023-09-22 at 10.42.26 AM1798×472 63.7 KB\nWETH Borrows are up 1.12%, from 1.42k ($2.25M) to 1.43k ($2.27M).\nScreen Shot 2023-09-22 at 10.42.40 AM1638×456 33 KB\nUSDC utilization decreased 1.52%, from 36.9% to 36.3%.\nSupply Caps\nScreen Shot 2023-09-22 at 10.44.48 AM3792×612 80.5 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-09-22 at 10.45.06 AM3792×842 127 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-09-22 at 10.43.00 AM1656×416 31.3 KB\nThe minimum USDC utilization was 35.8%, and the maximum was 39.5%.\nThe minimum USDC reserve growth was -22.3%, and the maximum was -15.6%. The average USDC reserve growth was -20.1%.\nScreen Shot 2023-09-22 at 10.43.19 AM1666×456 32.6 KB\nThe comet accumulated $-0.21k USDC reserves while distributing $5.51k COMP rewards for a weekly Net Protocol Profit of $-5.73k.\\n[Gauntlet] BASE v3 WETH Update (09/22/2023 - 09/28/2023)\nGauntlet would like to provide the community with an update on metrics from the BASE v3 WETH comet over the past week.\nSimple Summary\n\nWETH Borrows are up 8.86%, from 1.43k ($2.37M) to 1.56k ($2.58M).\nWETH Supply is up 14.39%, from 3.85k ($6.52M) to 4.39k ($7.45M).\nUSDC utilization decreased 4.69%, from 36.3% to 34.6%.\nThe minimum USDC reserve growth was -27.4%, and the maximum was -20.5%. The average USDC reserve growth was -24.0%.\nThe comet accumulated $-0.25k USDC reserves while distributing $5.74k COMP rewards for a weekly Net Protocol Profit of $-5.99k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-09-29 at 12.33.12 PM1790×476 64.1 KB\nTotal Collateral (USD) is up 8.39%, from $2.72M to $2.95M.\nScreen Shot 2023-09-29 at 12.33.31 PM1800×468 65.7 KB\nWETH Supply is up 14.39%, from 3.85k ($6.52M) to 4.39k ($7.45M).\nScreen Shot 2023-09-29 at 12.33.50 PM1798×488 64.1 KB\nWETH Borrows are up 8.86%, from 1.43k ($2.37M) to 1.56k ($2.58M).\nScreen Shot 2023-09-29 at 12.34.09 PM1644×446 35.2 KB\nUSDC utilization decreased 4.69%, from 36.3% to 34.6%.\nSupply Caps\nScreen Shot 2023-09-29 at 12.36.28 PM3542×622 78.2 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-09-29 at 12.36.56 PM3542×850 120 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-09-29 at 12.34.31 PM1666×422 34.3 KB\nThe minimum USDC utilization was 33.3%, and the maximum was 36.8%.\nThe minimum USDC reserve growth was -27.4%, and the maximum was -20.5%. The average USDC reserve growth was -24.0%.\nScreen Shot 2023-09-29 at 12.34.55 PM1658×446 32.4 KB\nThe comet accumulated $-0.25k USDC reserves while distributing $5.74k COMP rewards for a weekly Net Protocol Profit of $-5.99k.\\n[Gauntlet] BASE v3 WETH Update (09/29/2023 - 10/05/2023)\nGauntlet would like to provide the community with an update on metrics from the BASE v3 WETH comet over the past week.\nSimple Summary\n\nWETH Borrows are down 5.47%, from 1.56k ($2.52M) to 1.47k ($2.38M).\nWETH Supply is up 6.46%, from 4.39k ($7.28M) to 4.7k ($7.75M).\nWETH utilization decreased 11.21%, from 34.6% to 30.7%.\nThe minimum WETH reserve growth was -33.6%, and the maximum was -24.0%. The average WETH reserve growth was -29.7%.\nThe comet accumulated $-0.31k WETH reserves while distributing $6.38k COMP rewards for a weekly Net Protocol Profit of $-6.69k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-10-06 at 5.58.27 PM1786×480 64.9 KB\nTotal Collateral (USD) is down 3.89%, from $3.05M to $2.93M.\nScreen Shot 2023-10-06 at 5.59.08 PM1798×480 64.3 KB\nWETH Supply is up 6.46%, from 4.39k ($7.28M) to 4.7k ($7.75M).\nScreen Shot 2023-10-06 at 5.59.28 PM1796×478 64.7 KB\nWETH Borrows are down 5.47%, from 1.56k ($2.52M) to 1.47k ($2.38M).\nScreen Shot 2023-10-06 at 5.59.47 PM1640×452 33.3 KB\nWETH utilization decreased 11.21%, from 34.6% to 30.7%.\nSupply Caps\nScreen Shot 2023-10-06 at 6.02.09 PM3530×606 77.2 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-10-06 at 6.02.31 PM3538×836 123 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-10-06 at 6.00.06 PM1656×430 30.1 KB\nThe minimum WETH utilization was 30.5%, and the maximum was 35.0%.\nThe minimum WETH reserve growth was -33.6%, and the maximum was -24.0%. The average WETH reserve growth was -29.7%.\nScreen Shot 2023-10-06 at 6.00.27 PM1656×458 33.7 KB\nThe comet accumulated $-0.31k WETH reserves while distributing $6.38k COMP rewards for a weekly Net Protocol Profit of $-6.69k.\\n[Gauntlet] BASE v3 WETH Update (10/06/2023 - 10/12/2023)\nGauntlet would like to provide the community with an update on metrics from the BASE v3 WETH comet over the past week.\nSimple Summary\n\nWETH Borrows are down 0.74%, from 1.47k ($2.27M) to 1.46k ($2.25M).\nWETH Supply is up 2.73%, from 4.7k ($7.39M) to 4.83k ($7.59M).\nWETH utilization decreased 3.37%, from 30.7% to 29.6%.\nThe minimum WETH reserve growth was -41.4%, and the maximum was -33.3%. The average WETH reserve growth was -34.8%.\nThe comet accumulated $-0.33k WETH reserves while distributing $5.9k COMP rewards for a weekly Net Protocol Profit of $-6.22k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-10-13 at 2.44.45 PM1792×474 61.6 KB\nTotal Collateral (USD) is up 1.51%, from $2.78M to $2.83M.\nScreen Shot 2023-10-13 at 2.45.12 PM1802×476 61.9 KB\nWETH Supply is up 2.73%, from 4.7k ($7.39M) to 4.83k ($7.59M).\nScreen Shot 2023-10-13 at 2.45.34 PM1796×492 62.1 KB\nWETH Borrows are down 0.74%, from 1.47k ($2.27M) to 1.46k ($2.25M).\nScreen Shot 2023-10-13 at 2.45.52 PM1644×454 30.7 KB\nWETH utilization decreased 3.37%, from 30.7% to 29.6%.\nSupply Caps\nScreen Shot 2023-10-13 at 2.50.02 PM3790×608 80.3 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-10-13 at 2.50.25 PM3786×792 117 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-10-13 at 2.46.10 PM1666×426 29.7 KB\nThe minimum WETH utilization was 27.4%, and the maximum was 30.7%.\nThe minimum WETH reserve growth was -41.4%, and the maximum was -33.3%. The average WETH reserve growth was -34.8%.\nScreen Shot 2023-10-13 at 2.46.30 PM1676×458 33.6 KB\nThe comet accumulated $-0.33k WETH reserves while distributing $5.9k COMP rewards for a weekly Net Protocol Profit of $-6.22k.\\n[Gauntlet] BASE v3 WETH Update (10/13/2023 - 10/19/2023)\nGauntlet would like to provide the community with an update on metrics from the BASE v3 WETH comet over the past week.\nSimple Summary\n\nWETH Borrows are down 6.13%, from 1.46k ($2.29M) to 1.37k ($2.15M).\nWETH Supply is up 3.99%, from 4.83k ($7.73M) to 5.01k ($8.04M).\nWETH utilization decreased 9.83%, from 29.6% to 26.7%.\nThe minimum WETH reserve growth was -43.4%, and the maximum was -35.1%. The average WETH reserve growth was -37.9%.\nThe comet accumulated $-0.34k WETH reserves while distributing $5.65k COMP rewards for a weekly Net Protocol Profit of $-5.99k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-10-20 at 1.45.42 PM1790×480 64.7 KB\nTotal Collateral (USD) is down 7.76%, from $2.82M to $2.61M.\nScreen Shot 2023-10-20 at 1.46.03 PM1802×486 65.3 KB\nWETH Supply is up 3.99%, from 4.83k ($7.73M) to 5.01k ($8.04M).\nScreen Shot 2023-10-20 at 1.46.23 PM1802×478 64 KB\nWETH Borrows are down 6.13%, from 1.46k ($2.29M) to 1.37k ($2.15M).\nScreen Shot 2023-10-20 at 1.46.44 PM1656×468 37.3 KB\nWETH utilization decreased 9.83%, from 29.6% to 26.7%.\nSupply Caps\nScreen Shot 2023-10-20 at 1.51.24 PM3790×614 80.4 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-10-20 at 1.51.43 PM3798×836 127 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-10-20 at 1.47.01 PM1680×432 36.3 KB\nThe minimum WETH utilization was 26.6%, and the maximum was 29.9%.\nThe minimum WETH reserve growth was -43.4%, and the maximum was -35.1%. The average WETH reserve growth was -37.9%.\nScreen Shot 2023-10-20 at 1.47.22 PM1682×462 32.5 KB\nThe comet accumulated $-0.34k WETH reserves while distributing $5.65k COMP rewards for a weekly Net Protocol Profit of $-5.99k.\\n[Gauntlet] BASE v3 WETH Update (10/20/2023 - 10/26/2023)\nGauntlet would like to provide the community with an update on metrics from the BASE v3 WETH comet over the past week.\nSimple Summary\n\nWETH Borrows are down 8.16%, from 1.37k ($2.48M) to 1.26k ($2.27M).\nWETH Supply is down 1.78%, from 5.01k ($9.25M) to 4.92k ($9.09M).\nWETH utilization decreased 6.47%, from 26.8% to 25.0%.\nThe minimum WETH reserve growth was -52.6%, and the maximum was -42.9%. The average WETH reserve growth was -47.3%.\nThe comet accumulated $-0.38k WETH reserves while distributing $6.16k COMP rewards for a weekly Net Protocol Profit of $-6.55k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-10-27 at 5.54.52 PM1788×470 63.9 KB\nTotal Collateral (USD) is down 7.65%, from $2.98M to $2.75M.\nScreen Shot 2023-10-27 at 5.55.09 PM1798×466 63.7 KB\nWETH Supply is down 1.78%, from 5.01k ($9.25M) to 4.92k ($9.09M).\nScreen Shot 2023-10-27 at 5.55.30 PM1798×462 64.2 KB\nWETH Borrows are down 8.16%, from 1.37k ($2.48M) to 1.26k ($2.27M).\nScreen Shot 2023-10-27 at 5.55.47 PM1650×438 32 KB\nWETH utilization decreased 6.47%, from 26.8% to 25.0%.\nSupply Caps\nScreen Shot 2023-10-27 at 6.06.47 PM3782×608 80.2 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-10-27 at 6.07.05 PM3794×840 125 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-10-27 at 5.56.06 PM1662×426 27.7 KB\nThe minimum WETH utilization was 23.4%, and the maximum was 26.8%.\nThe minimum WETH reserve growth was -52.6%, and the maximum was -42.9%. The average WETH reserve growth was -47.3%.\nScreen Shot 2023-10-27 at 5.56.22 PM1678×458 32.8 KB\nThe comet accumulated $-0.38k WETH reserves while distributing $6.16k COMP rewards for a weekly Net Protocol Profit of $-6.55k.\\n[Gauntlet] BASE v3 WETH Update (10/27/2023 - 11/02/2023)\nGauntlet would like to provide the community with an update on metrics from the BASE v3 WETH comet over the past week.\nSimple Summary\n\nWETH Borrows are up 7.57%, from 1.26k ($2.27M) to 1.36k ($2.44M).\nWETH Supply is up 11.83%, from 4.92k ($9.07M) to 5.52k ($10.15M).\nWETH utilization decreased 3.81%, from 25.0% to 24.1%.\nThe minimum WETH reserve growth was -50.9%, and the maximum was -34.5%. The average WETH reserve growth was -43.3%.\nThe comet accumulated $-0.39k WETH reserves while distributing $6.48k COMP rewards for a weekly Net Protocol Profit of $-6.86k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-11-03 at 10.59.02 AM1784×502 63 KB\nTotal Collateral (USD) is up 7.82%, from $2.84M to $3.06M.\nScreen Shot 2023-11-03 at 10.59.22 AM1802×498 63.3 KB\nWETH Supply is up 11.83%, from 4.92k ($9.07M) to 5.52k ($10.15M).\nScreen Shot 2023-11-03 at 10.59.50 AM1798×490 62 KB\nWETH Borrows are up 7.57%, from 1.26k ($2.27M) to 1.36k ($2.44M).\nScreen Shot 2023-11-03 at 11.00.09 AM1648×464 33.9 KB\nWETH utilization decreased 3.81%, from 25.0% to 24.1%.\nSupply Caps\nScreen Shot 2023-11-03 at 11.02.49 AM3538×614 77.6 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-11-03 at 11.03.13 AM3534×834 126 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-11-03 at 11.00.27 AM1666×424 30.6 KB\nThe minimum WETH utilization was 23.9%, and the maximum was 30.1%.\nThe minimum WETH reserve growth was -50.9%, and the maximum was -34.5%. The average WETH reserve growth was -43.3%.\nScreen Shot 2023-11-03 at 11.00.47 AM1676×460 34.1 KB\nThe comet accumulated $-0.39k WETH reserves while distributing $6.48k COMP rewards for a weekly Net Protocol Profit of $-6.86k.\\n[Gauntlet] BASE v3 WETH Update (11/03/2023 - 11/09/2023)\nGauntlet would like to provide the community with an update on metrics from the BASE v3 WETH comet over the past week.\nSimple Summary\n\nWETH Borrows are up 1.89%, from 1.36k ($2.88M) to 1.38k ($2.93M).\nWETH Supply is down 5.68%, from 5.55k ($11.95M) to 5.21k ($11.28M).\nWETH utilization increased 8.02%, from 24.1% to 26.0%.\nThe minimum WETH reserve growth was -50.6%, and the maximum was -43.4%. The average WETH reserve growth was -46.5%.\nThe comet accumulated $-0.45k WETH reserves while distributing $7.06k COMP rewards for a weekly Net Protocol Profit of $-7.5k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-11-10 at 1.41.56 PM1794×482 61.8 KB\nTotal Collateral (USD) is up 2.85%, from $3.14M to $3.23M.\nScreen Shot 2023-11-10 at 1.42.18 PM1796×478 61.3 KB\nWETH Supply is down 5.68%, from 5.55k ($11.95M) to 5.21k ($11.28M).\nScreen Shot 2023-11-10 at 1.42.41 PM1800×470 60.8 KB\nWETH Borrows are up 1.89%, from 1.36k ($2.88M) to 1.38k ($2.93M).\nScreen Shot 2023-11-10 at 1.43.54 PM1664×460 35.8 KB\nWETH utilization increased 8.02%, from 24.1% to 26.0%.\nSupply Caps\nScreen Shot 2023-11-10 at 1.46.00 PM3794×616 80.7 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-11-10 at 1.46.20 PM3794×832 129 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-11-10 at 1.44.13 PM1690×434 33.6 KB\nThe minimum WETH utilization was 24.1%, and the maximum was 26.6%.\nThe minimum WETH reserve growth was -50.6%, and the maximum was -43.4%. The average WETH reserve growth was -46.5%.\nScreen Shot 2023-11-10 at 1.44.30 PM1688×466 32.3 KB\nThe comet accumulated $-0.45k WETH reserves while distributing $7.06k COMP rewards for a weekly Net Protocol Profit of $-7.5k.\\n[Gauntlet] Base v3 WETH Update: (11/10/23 - 11/16/23)\nGauntlet would like to provide the community with an update on the Base v3 WETH comet over the past week.\nSimple Summary\n\nWETH Borrows increased 52.75%, from $2.88M to $4.39M.\nWETH Supply decreased 10.62%, from $11.06M to $9.89M.\nWETH utilization increased 70.9%, from 26.0% to 44.43%.\nThe minimum WETH reserve growth was -45.63%, and the maximum was -5.4%. The average WETH reserve growth was -27.84%.\nThe comet accumulated $-0.34K WETH reserves while distributing $7.58K COMP rewards for a weekly Net Protocol Profit of $-7.92K.\n\nCollateral Asset Supply\nThis graph shows the time series of total supply of all collateral assets.\nSupply1920×1080 76.4 KB\nTo see updated statistics, please see the live version of this graph here.\nWETH Borrows\nThis graph shows the time series of WETH borrows.\nBorrows1920×1080 61.3 KB\nTo see updated statistics, please see the live version of this graph here.\nUtilization\nThis graph shows the utilization (borrow / supply) of WETH over the past week.\nUtilization1920×1200 171 KB\nSupply Cap Usage\nThis graph shows the supply cap usage (supply / supply cap) of all collateral assets over the past week.\nSupply Cap Usage1920×1200 136 KB"
  },
  {
    "number_of_comments": 57,
    "postid": "a845644e-2148-4d2b-b22e-1605c2a5b223",
    "posturl": "https://www.comp.xyz/t/dai-liquidation-compensation/684",
    "combinedcontent": "This is a dedicated thread for the community to discuss compensation for users borrowing DAI that were liquidated on 11/25. See the main thread 42 for information and analysis of the liquidation event.\nData\n\nList of addresses liquidated during event 40\n\nBackground\n\nThe protocol and documentation (Prices 4, Docs 2) clearly describe that the protocol uses the price of DAI on Coinbase Pro, and the protocol contracts all performed as written.\nThe market price of DAI on Coinbase Pro reached $1.30, triggering liquidations in the protocol for users borrowing DAI.\nImpacted users have requested compensation.\n\nInitial Questions\n\nWas the market price of DAI on Coinbase Pro fair? Was it intentional manipulation, or a market dislocation?\nWere impacted users aware of the risk?\nDo impacted users deserve compensation? Why?\nIf so, how should compensation be calculated?\nIf so, should compensation come from reserves, or by distributing COMP?\nDoes compensating users create a social contract that doesn’t already exist? Does a social contract already exist? How would compensation change the future activity of users?\n\nOther Considerations\n\nAll COMP (which there is a fixed supply of) were previously distributed; the protocol has a sizable inventory controlled by Governance, but doesn’t yet have a function to send COMP to an address outside of the use-based distribution; this would require a protocol upgrade. Gauntlet has an in-development branch 12 which includes this feature.\n\\nI will add another question for everybody to consider:\nHere’s an example:\nUser had 1 Million USDC in his wallet initially.\nHe Supplied 1 Million USDC to Compound and borrowed 700 000 DAI, using Supplied USDC as Collateral.\nNow user has 700 000 Dai in his wallet.\nHe was liquidated in the event.\nShould he be compensated considering amount of liquidation, or considering amount of actual funds he had in Compound, since only 300 000$ of his funds were actually in protocol when liquidation occured?\\nI think we need analysis on this question: “Was the market price of DAI on Coinbase Pro fair? Was it intentional manipulation, or a market dislocation?”\nIf the market price was fair, I see no reason for compensation. If it was not fair, then we should start a secondary discussion around if / how compensation could work.\nMy assumption at this point is that it was not fair but also not completely wrong. One way of saying it may be that the price was accurate to the letter of the law but not the spirit of the law.\nIf this is the case, I think some limited pro-rata compensation based on asset reserves (not COMP) could be in order. I don’t think treating COMP as cash makes sense I think it makes more sense to use protocol reserves for this (starting with deprecated assets like Sai).\nAlso could we get a number for what the total cost would be if we wanted to compensate all borrowers 100% of their liquidation penalty?\\nI think that you should back to the begininng - purpose of compound.finance?\nObviously, I did not expect high interest income from my deposit because the interest in Ethereum and other cryptocurrencies is embarrassingly small.\nSo I decided to choose compound.finance because of the relatively decent LVT ratio of 75% I have on Ethereum. I was borrowed “stablecoin” DAI for the reason that I at least reduced the risk to the volatility of the borrowed funds and I was still at risk of the volatility of Ethereum.\nI used Compound.finance to keep my crypto-business liquid and not to create leverage with two stablecoins - which makes no sense to me.\nI did not lose USDC in this obvious manipulation but Ethereum.\nBy your question you have actually admitted that compound.finance has no purpose other than yield farming. Here we lose all the advantages of blockchain and return to the traditional FinTech where you using only dollars - “stablecoins”.\\nI don’t think it’s hard to conclude that this is a manipulation. Stablecoins are meant to protect you from volatility. DAI jumped 30% on one exchange (accidentally Compounds oracle) in just one hour and returned to normal.\nWe are now at the mercy of these 3 VC funds that hold control of the Compound. So much for how useful $ COMP is…\\nI would like to add a question for you @rleshner:\nWere you aware of the risk? If so, why did you think it was a good idea to take the risk?\\nThis was an exploit plain and simple- no different than a synthetic short squeeze. For example if you were short Facebook at $250 and your banker was Morgan Stanley - they tell you if it goes to 350 you have to cover your short. Then one day in Hong Kong there’s a price spike bc of an error or whatever and the stock trades at 350 in HK but never in the USA but ms stops you out and makes you cover and take the loss.  This is even worse bc the person/people responsible knew exactly what they were doing and it was nefarious. People will run from this exchange if there’s no compensation for losses\\nI am user 0x0be0ecc301a1c0175f07a66243cff628c24db852 , I was liquidated twice, in the first liquidation 76,547,382,213.00 cETH was liquidated from me, and in the second, 48,501,406,899.00.\nThe liquidator paid back somewhere around 10,000 DAI to seize my $15,000-$20,000~ worth of Ethereum.\n1: * Was the market price of DAI on Coinbase Pro fair? Was it intentional manipulation, or a market dislocation? - This, in my opinion, was intentional manipulation. There was no other reason DAI would spike to $1.30 and someone would be instantaneously ready to liquidate.\n2: * Were impacted users aware of the risk? - Of course, impacted users were aware that they could be liquidated, but the liquidation rules specify far lower liquidation penalties than we paid, and I was personally absolutely unaware we were using a singular price feed or I would not have been in this market as a singular price feed is well known to be exploitable in this way.\n3: * Do impacted users deserve compensation? Why? - YES, but it is not a question of “deserve” compensation, and the fact that anyone has to frame it that way is beyond insulting. Impacted users were severely affected by an exploit in the protocol - and the protocol SHOULD have funds set aside for dealing with these kind of emergencies, outside of just this case or just these users.\nIn my opinion, a COMP distribution should be offered to these users, acceptance of which is predicated on them giving up all claim to damage. Otherwise, it seems very likely many of them will sue.\n4: If so, how should compensation be calculated? - This is an interesting question. In my opinion, it seems pertinent to offer less than the damage. We should look at the percentage of assets that were unfairly seized [IE: 38%], and offer that much as COMP to the affected users. As much as I or the rest of the liquidated users would probably like more, that seems more doable and agreeable to the users who were not liquidated. Therefore we still take a hit in damage, but we are able to walk away with something instead of nothing.\n5: * If so, should compensation come from reserves, or by distributing COMP? - Distributing COMP seems more sensible in my opinion than compensating from the reserves. I would honestly prefer to get back ETH because I was saving for a staking node, but that seems less possible than COMP.\nCan we get a report that specifies what sort of reserves are available for a liquidation event compensation?\n6: * Does compensating users create a social contract that doesn’t already exist? Does a social contract already exist? How would compensation change the future activity of users?\nNo, it doesn’t create a social contract, but it is demonstrative that we are capable of solving our own problems through governance, and empowers the user base to see the value of governance as not only a mechanism to specify risk parameters, but as a mechanism to truly govern and deal with emergency situations such as this that may threaten the solvency of Compound as an American company.\n7: * All COMP (which there is a fixed supply of) were previously distributed; the protocol has a sizable inventory controlled by Governance, but doesn’t yet have a function to send COMP to an address outside of the use-based distribution; this would require a protocol upgrade. Gauntlet has an in-development branch  1 which includes this feature.\nThis is an interesting remark, I will read more in to this to have a more thorough understanding of the protocol upgrade.\nSigned,\nHiturunk.eth\\nI don’t think anyone should be compensated. I will be voting against this proposal if it ever goes to a vote.\\nWas the market price of DAI on Coinbase Pro manipulation, or a market dislocation – are we able to ask Coinbase to investigate this?  We need to know if one or a few accounts were solely responsible for the price action.\nWere impacted users aware of the risk – If we decide to upgrade/improve the oracle system does that mean we acknowledge the current system is inadequate?  It seems our own engineers at compound were not fully aware of the oracle vulnerability, which means many users were similarly unaware.\nDo impacted users deserve compensation – there is no explicit requirement for compensation, but the social contract question is pertinent here.  Why do so many other crypto projects reimburse users after an exploit?  Do the affected users have a legitimate shot at class action success if a lawsuit follows?  The cost of reimbursement is a low single digit % of compound market cap.  The cost sounds low in exchange for putting this liability behind the community.  Cost/benefit of reimbursing affected accounts seems attractive in this case.\\nI would like to add to this discussion, that it is quite possible to say definitively that DAI was not globally trading at 20-30% premiums globally (see attached chart) at this time of the liquidations under discussion.\nI cannot speak to whether it was manipulation, or simply a sell-side liquidity crisis on a CEX with very thin books, but the chart speaks for itself.\nDAI_price_cross_exchange-24-27NOV1533×900 73.4 KB\nWhether or not the community decides in favor of compensation, I believe the oracle solution must be changed. Yes, CBPRO:DAI-USD was at ~1.30 momentarily, but this was not the price of DAI globally and should not have been treated as such in any price feed.\nI was not liquidated, but I know people who were; they should not have been, in my view. I am a long-time Compound user and huge fan of the protocol and community, but if the orcale solution is not changed in a timely manner, I personally will seriously consider moving the majority of my funds to Maker or Aave until it is.\\n\n\n\n rleshner:\n\n\nWas the market price of DAI on Coinbase Pro fair? Was it intentional manipulation, or a market dislocation?\nWere impacted users aware of the risk?\nDo impacted users deserve compensation? Why?\n\n\n\nI can speak to these questions but lack the knowledge to answer the others.\nHere is my address, I was liquidated in the event.\n0x196d7a2f4f78a15fc52c35d9d4456db1e0e628d8 26\nFirst Question: Was the market price of DAI on Coinbase Pro fair? Was it intentional manipulation, or a market dislocation?\nAs mentioned by @mrhen It seems to me that there was an undeniable and massive market dislocation between the coinbase price (or synonymously the Compound oracle price) and all other exchanges/ oracle services. From this, we can conclude that the price supplied to the compound protocol was erroneous.\nSecond Question: Were impacted users aware of the risk?\nAs an impacted user, I would like to describe my experience with compound:\nI learned of the protocol about a week ago and did not just dive right in without educating myself. I sought out any and all available educational resources, including the ones provided by coinbase that explain the lending and borrowing protocol and some third party ones as well. Once I felt comfortable with the protocol and knew it inside out, I supplied my eth as collateral and borrowed dai. The compound app provided a suggested “safe limit,” which I made the decision to trust. Had this suggested borrow ratio not been there, I would likely have chosen to borrow less.\nHere is the nuance of this question: I was completely aware of the risk associated with borrowing. I was not, however, aware that the oracle used by compound was vulnerable to exploitation. Nor was I aware that the “safe limit” was evidently not safe in the slightest.\nThird Question: Do impacted users deserve compensation? Why?\nImo the impacted users deserve compensation. We deserve compensation because we did nothing wrong. I don’t know what ratio the other users chose to use, but I know that many of the liquidated users were being smart with their money. This liquidation affected routine users who were operating within the “safe limit” of the protocol. I did my due diligence and educated myself on the risks involved with using the compound protocol. I am a beginner to the DeFi space and the protocol and app made me feel safe, as it should! It is not a punishable offense to use this protocol within safe operating limits. It is not a punishable offense for new users like me to acknowledge our limits and seek out a protocol like compound that makes things (appear) straight forward.\nUsers that were using the safe limit feature on compound should undeniably be compensated in full. My (uneducated) opinion is that users that were using more risky borrow-supply ratios should be fully compensated as well in a gesture of good faith and goodwill.\\nStablecoins are not guaranteed to stay on peg. That’s a risk you take when you use a stablecoin like Dai which historically almost never trades 1:1 with a dollar.\nSo just because a stablecoin is trading above a dollar does not mean there was market manipulation. There may have been market manipulation but it is worth noting stable coins are not always stable and users should not expect them to be.\\nIn your first week? Ow! That can’t feel good! I’d like to say I’ve used this protocol for well over a year, and nothing like this has ever happened before. This was an unusual event and I hope you don’t let it affect your perceptions of Compound too heavily.\\nI didn’t present direct ratios anywhere - I don’t know where you found it. I said that stablecoins serve to reduce volatility.\nIf that is not their purpose please let me know what is?\nIn this situation, DAI was not in the function of stablecoin only on the - Compound.finance platform?\\n\n\n\n rleshner:\n\n\nWas the market price of DAI on Coinbase Pro fair? Was it intentional manipulation, or a market dislocation?\nWere impacted users aware of the risk?\nDo impacted users deserve compensation? Why?\nIf so, how should compensation be calculated?\nIf so, should compensation come from reserves, or by distributing COMP?\nDoes compensating users create a social contract that doesn’t already exist? Does a social contract already exist? How would compensation change the future activity of users?\n\n\n\nI was one of the affected users ,i will try to answer the question you mentioned above .\n*Obviously the market price of Dai on coinbase was not fair ,you can look at the price globally and you can easily come to that conclusion , it will be hard to prove if it was an intentional manipulation but i believe it was , and it was a part of a wide attack on the protocol , this vulnerability was highlighted couple of times by the community but actions were not taken to mitigate it until the protocol got exploited . If the protocol need an upgrade to deal with this issue , that means that protocol didn’t behave as it should .\n*I was aware of the liquidation risk and i took several steps to avoid been liquidated i never thought the price of DAI will be over 1.2 $ and obviously it wasn’t the case globally , relying on a single price oracle is what lead to my liquidation (aave and maker didn’t suffer any issues)\n*Ideally for the compensation we should make the users whole , most of the user missed out on the price appreciation of their collateral .\n*I think the compensation should come from the reserves that way COMP holders won’t feel the impact as much, if we were to distribute COMP most comp holders will vote against it since it will dilute their assets\n*Compensating users for Protocol malfunctioning will cement Compound as a market leader in the space and it will lead to more trust in the protocol which will most likely increase the number of user and TVL .\nRegards\\nPosting for Discord user “Dmitry” as in conversation I have no reason to believe they were not legitimately affected, so I offered to get their voice heard if their account was not verified in a timely manner. I have my own thoughts, which I will post at a later date.\n\" I cannot write in the forum as immediately after the registration my account has been placed on hold (automated message from Compound Community Forum to let you know that your account has been temporarily placed on hold as a precautionary measure.). That’s why I will leave this message in Discord, and then, once my account is reactivated, will post it in the forum thread.\n[ 3:46 PM ]\nI, being one of the impacted users, would like to answer Initial Questions, despite the fact that my answers will be biased. I hope that everybody understands that Compound is used by people with different education and knowledge and not everybody thoroughly studies the technical details of the protocol. That’s the way things are, you cannot be an expert in every field and people just trust, otherwise there would not be such a thing as a reputation. Most customers use the functionality of the protocol based on its reputation and many years of history. Since the protocol was working for many years without any issues and was entrusted with more than 1 billion USD, I have also entrusted it with my money. Of course, you can say that if you do not understand all details then you should not be here. But I would like to know who we build the DEFI infrastructure for. For software developers and technical specialists or for ordinary people after all? When you buy a car you usually look at the ease of operation and you choose a brand you Trust. You do not dive into technical details. And if a manufacturer finds a critical defect, the manufacturer recalls cars and pays compensation to owners. That’s why compensation is very important for the Trust. In the world of cryptocurrencies, where everything is anonymous, Trust is most probably the most important success indicator of any project.(edited)\n[ 3:47 PM ]\n1. Was the market price of DAI on Coinbase Pro fair? Was it intentional manipulation, or a market dislocation? I think the market price of DAI was not fair.\nBeing a user and not a technical specialist I would like to refer to the original source. Main page of https://makerdao.com/ The world’s first unbiased currency. Dai is a stable, decentralized currency that does not discriminate. Any individual or business can realize the advantages of digital money. Financial freedom with no volatility A price-stable currency that you control. Generate Dai on your terms, instantly. 1 Dai=1$ When during a short period of time a price of the token that should be equal to 1$, changes by 30% on one source (while at the same time other sources do not confirm such price change) and this leads to positions liquidation, which under normal circumstances should not have been liquidated, then there should be an issue with the Compound protocol. How can I consider the liquidation of positions at this price to be fair?(edited)\n[ 3:47 PM ]\nWas it intentional manipulation, or a market dislocation? I think that this question is not essential. It could be a hacker attack, Coinsbase failure or any other reason. The fact is that this happened and led to an incident. Other sources did not confirm such a sharp price deviation. If we accept that DAI price was not fair, then anything else is not that important and just helps us to understand what happened but should not be taken into consideration when deciding whether the compensation should be paid or not. Compound team shall focus on fixing the issue and quickly find the solution that will protect people from similar situations. Solving this problem will increase the trust. If nothing will be changed, a competitor, who will fix this problem, will appear and customers will go there since it is safer.\nWere impacted users aware of the risk? I think impacted users were aware of the risk according to the knowledge about the token and Compound protocol, but nobody could foresee the risk of 30% DAI price change.(edited)\n[ 3:48 PM ]\nDo impacted users deserve compensation? I think that impacted users do deserve the compensation. Why? Impacted users were severely affected by an exploit in the protocol, then the protocol MUST… However, I do not agree with this. No one can guarantee anything and we know it. Compound worked as it was programmed. Condition for liquidation appeared and liquidation took place. Some users lost money, liquidators made money, everything is according to the market rules. But Compound is not just an immutable set of code on a blockchain, it is a community: people who use and trust the protocol. The most important point is that people, who trusted Compound and were part of its community, suffered. They suffered by entrusting their money to Compound. Compound gave the opportunity to liquidate their positions at a non-market price. This is result of force majeure caused by an error at the quotes provider, hacker attack, or just a glitch. This happened and part of the community suffered. Users, who suffered, hope that the community will admit that the liquidation was a mistake. Positions should not have been liquidated.\n[ 3:49 PM ]\nAnd the community can evaluate this situation and take a decision based not only on commercial interests, but also on the ideas and spirit of DEFI. The global idea of DEFI is to create financial products for people, and Compound is one of the biggest players on this market that builds this infrastructure for the community and is a benchmark for many. I believe that the community should take a positive decision about the compensation because this happened to the part of Compound community. There is no need to look for somebody to blame or the reasons why Compound OWES to somebody, but simply help out. If Compound is not willing to help users, who it builds infrastructure for, then that is wrong. There will be less trust and confidence in such a project. Next time other users might suffer, but they will know that nobody will help them. If there is no trust and confidence in the project, users will start looking for an alternative and leave DEFI, since even market leaders cannot provide safety to its users. If Compound does not support its users, then other projects will adopt the same practice, thinking that why should they if top players do not. Eventually DEFI will become a pyramid where you cannot trust anybody, and everybody wants to grab somebody’s else. I believe that here we have to discuss whether the Compound community is ready to help users in difficult and force majeure situations. I will not give names, but one of the impacted users wrote that it was his last money and I see no reason not to believe him.\n[ 3:49 PM ]\nIf so, how should compensation be calculated? That’s a technical question and calculation of losses for each user is a difficult task that requires a lot of time for analysis. Maybe some basic conditions for estimating losses, for example based on several major liquidations. And then approximate this estimate to other users, for example by linking to the column “DAI Repaid”. Maybe it will not be fair towards someone, but I think that many users do not even hope to get anything and will be happy to accept any positive decision. This will also take less time than to analyze every user out of 142 impacted users. Time in this case is very crucial. How fast the community will be able to mobilize and solve this issue? If so, should compensation come from reserves, or by distributing COMP? In my opinion by distributing COMP, but same has to be discussed with the Board.\"\\nUsers should be compensated. This doesn’t seem like it is working as intended, especially since Dai price was trading normally elsewhere. The users that are effected in this incident did not do anything wrong, but suffered. COMP as a leading DEFI platform should try hard to maintain the trust, or else people would just move their funds elsewhere. No one would want to risk their money at a place that is vulnerable to said attack.\\nI’m sorry but I feel obligated to take the unpopular position against compensation.  I have great sympathy as a fellow human for those who lost money to the incident but I think compensation is a mistake for reasons that I hope I articulate clearly below.\n\n\n\n rleshner:\n\n\nWas the market price of DAI on Coinbase Pro fair? Was it intentional manipulation, or a market dislocation?\n\n\n\nAlmost certainly manipulation by a liquidator who planned and scripted out the attack in advance.\n\n\n\n rleshner:\n\n\nWere impacted users aware of the risk?\n\n\n\nNo, but the cold truth is that in finance, this is immaterial.  The impacted users should have and could have been aware of the risk, and Compound is a protocol that can be used in risky ways. Users have a responsibility to understand the protocol before they put large sums of money in it.  Compound could have done a better job of educating users about these risks but ultimately, any complex financial product, whether in the crypto space or the traditional finance space, has ways to use it dangerously and it is generally impossible to protect people from their own ignorance when using complex financial products.\nI certainly think that the idea of a “safe max” to borrow is ridiculous and I do blame the Compound UI for using that term as being part of the problem.  There is no 100% safe amount to borrow of any product.  Some amounts might be 99.99% safe, or 99.9% safe, or 99% safe, or 95% safe, and users need to evaluate this on their own if they are going to short sell a product (I know that not all protocol users realize this, but when they supply USDC and borrow DAI, then keep selling the DAI and supplying more USDC to build up the “recursive” leverage for yield farming, they are short DAI).  DAI is a soft peg and short selling it has counterparty risk against MKR holders because if they mismanage DAI it could actually (as opposed to via an oracle exploit) globally reach prices well above 1 (e.g. if they fail to allow more USDC deposits into vaults once the USDC max supply is reached).  Similarly, being long USDC has tail risks associated with it, e.g. if Circle somehow turned out to be engaging in fraud or had US regulatory action against them (I think this is highly unlikely but I think people should consider and understand these tail risks when they use these products).\nIn the other thread people have proposed switching to Chainlink oracles, which is clearly an improvement that could have prevented this specific attack, but it still doesn’t prevent all tail risks to using Compound and the idea of there being a “safe” max borrow is still a dangerous and incorrect idea in my opinion.\n\n\n\n rleshner:\n\n\nDo impacted users deserve compensation? Why?\n\n\n\nSadly, I believe not. I already touched on this in my response to the previous question, but users should understand how a protocol works before using it, including the oracle and the liquidation process.  I have used Compound with DAI but I am very careful about my ratios because even before this incident, if you look at the price history of DAI-USDC on Coinbase (for a long time Coinbase only had DAI-USDC but no DAI-USD, I guess because they wanted to promote USDC and force users to use it) you can see other times in the last year when the illiquid market has resulted in temporary ridiculous prices.  For example the 2019-07-14 candle shows a max price of 1.35 and the 2020-03-11 candle shows a max price of 1.126, so even just pulling up the Coinbase price history would show anybody that they ought to be planning their positions around the possibility of the Coinbase-based DAI oracle printing 1.3.\nIn fact, I would go so far as to posit that many people people who understand how the oracle works have been cautious with how they use the protocol out of fear of the exact thing that happened occurring, and that part of the reason for the apparently outsized returns offered by yield farming is that maybe you can get a 20% return with aggressive recursive yield farming, just like you can by buying junk bonds, but they also pose non-trivial chances of significant loss of your initial capital.  In reality, if you see something easy that offers 20% return and it seems safe, you are almost certainly wrong.  Either the return is not as high as you think or the risks are higher than you think.  If Compound was safer, more people would use it until the return was pushed down to a reasonable risk-adjusted return. The only reason the return is so high on using recursive leverage for yield farming is that it is actually risky.  For awhile the yield farmers were taking this high risk (and I admit many of them did not realize it) and they got what appeared to be a really high ROI, and then they got burned.  It makes no sense to reimburse them because these sorts of risks in a new protocol are the exact reason why more people aren’t using it yet (and once DeFi gets to the point of protecting against these risks better, more people will use it and they will push the returns down).\n\n\n\n rleshner:\n\n\nDoes compensating users create a social contract that doesn’t already exist? Does a social contract already exist? How would compensation change the future activity of users?\n\n\n\nI think compensating the affected users sets a bad precedent because people need to understand the financial products they use and they need to take responsibility for evaluating the risks, and accept responsibility when the bad side of their risks manifests.  This was not a bug in the code, it was a combination of a bad design choice (Coinbase oracle vs Chainlink) and users who didn’t DYOR by understanding the existing oracle or pulling up the Coinbase DAI price history (or inspecting the liquidity of the Coinbase DAI markets, which could have revealed that the DAI liquidity on Coinbase was small enough that this attack could be profitable for the attacker).  If the affected users are compensated it will just encourage more risky behavior in the future and encourage people to use the protocol without understanding it.  What if Compound compensates the users, and switches to Chainlink, but somebody figures out an attack on the Chainlink oracle?  People would definitely expect compensation again but it starts to get impractical and create a slipper slope.  Sure it’s unlikely, but it is ultimately the responsibility of the users to consider and evaluate these risks.\nI mentioned this above too, but I strongly believe Compound should remove any terminology around a “safe max” borrow from the UI.  “Safe Max” is a ridiculous and inherently flawed idea in this context because there are always multiple risks in the protocol code, the oracle code, counterparty risk from the managers of the stable coins, and many more.  Users need to be responsible for understanding and evaluating these risks on their own.  If you think that there is such a thing as a “safe max” borrow or a “safe” way to use Compound (Disclaimer: I am Compound user) I highly recommend that you do not use it.  There are relatively safe ways to use it, and I think many people have found ways to use it that are low risk and that most users understand these risks, but for the UI to bait people into thinking their position is “safe” because it displays as such is dangerous and will just lead to more tragedies in the future like this one.\\nOkay, let me get this straight. You correctly believe the protocol was attacked through oracle manipulation.\n\n\n\n TWS49:\n\nAlmost certainly manipulation by a liquidator who planned and scripted out the attack in advance.\n\n\nYou correctly point out that the UI misled users into believing using 80% of borrowing power was “safe.”\n\n\n\n TWS49:\n\nI certainly think that the idea of a “safe max” to borrow is ridiculous and I do blame the Compound UI for using that term as being part of the problem.\n\n\n\n\n\n TWS49:\n\nI mentioned this above too, but I strongly believe Compound should remove any terminology around a “safe max” borrow from the UI.\n\n\nI’ll point out that the protocol has intentionally set aside funds as insurance 7:\n\nSecond, a portion of the interest paid by borrowers is set aside as  Reserves  1, which acts as insurance and is controlled by COMP token-holders.\n\nAnd yet, you do not feel the facts and circumstances warrant a discussion of compensation for the victims. What exactly are those insurance funds for? Recapitalizing the protocol in the event of fund loss? Why not just write down everyone’s balance if users are not to be looked after and only protocol solvency is a priority.\nI get the feeling that people are fine with screwing borrowers, because they “should have known better.” Well, without borrowers, there is no protocol and there is no reason for depositors too. I knew the market was volatile and paid down my debt to 80% per the protocol recommendation before going to sleep. Just hours later I was shocked to learn that I was liquidated improperly, and to add insult to injury my collateral was seized at an insulting and incredibly incorrect price thanks to the oracle attack.\nMost if not all businesses as well as DeFi protocols have went out of their way to make it right for users who fell victim to protocol exploitation, which this clearly was. Yet you think Compound has no business doing so because users should have known better. If that’s the case, it will be hard to recommend Compound to anyone who doesn’t fancy themselves a smart contract researcher, and a hardcore one at that.\nI have moved my funds and won’t be back unless there is at least an admission of fault on behalf of the the protocol, and partial if not full compensation.\\nKeep in mind users lost more than the 8% liquidation penalty. Collateral was seized at an incorrect price.\\nYes this is 100%. While some dilly dally about compensation - the truth is investors got screwed. If you don’t compensate them to some degree they will leave en masse - all investment banks compensate investors to some degree-willingly or not- when they fk them over. If there isn’t some restitution then compound as an exchange will shrivel up\\n\n\n\n rleshner:\n\n\nWas the market price of DAI on Coinbase Pro fair? Was it intentional manipulation, or a market dislocation?\n\n\n\nNo it was not fair. It was not the market price. It was an outlier that should have never been published. Further, it should have never been used to trigger liquidations or price the collateral that was being seized. In fact, Coinbase Oracle promised three layers of defense 2 to guard against publishing prices that did not reflect market value, including off-chain filtering. All failed.\n\nData quality\nFor an oracle to provide a reliable price feed it is important to address various scenarios in which a data point to be signed does not reflect an actual market price of an asset. There are three layers in the Coinbase price oracle architecture designed to solve this:\n\n\nPrice source . We use the Coinbase Pro API as the source of the price data. Coinbase Pro is one of the most liquid crypto-exchanges in the world. There is already an ecosystem of oracles, market makers and traders that rely on an accurate data feed provided by the Pro API. As such, Coinbase is making continuous investments in the quality of the API itself, as well as the market, as measured by liquidity.\n\nOff-chain filtering . The Coinbase price oracle implements a filtering mechanism that rejects data points that significantly deviate from the expected volatility of each asset.\n\nOn-chain filtering. Compound open oracle’s contract implements concepts of an ‘anchor’ source. Data points that significantly deviate from the last price reported by the anchor source are rejected.\n\n\n\n\n\n rleshner:\n\nWere impacted users aware of the risk?\n\n\nNo, users were specifically told by the Compound UI that 80% borrowing power was safe. Furthermore, users had every indication that the contracts had undergone audit and had been battle tested for months, and would not be subject to faulty price feeds. I don’t believe the Compound engineers were aware of this risk themselves. If the expectation is that users should understand the risks of a protocol better than the people that built it, well that is frankly preposterous. I believe the fact that there are discussions of hardening the protocol indicate that there is an admission of improper risk currently in the protocol.\n\n\n\n rleshner:\n\nDo impacted users deserve compensation? Why?\n\n\nI’ll admit we don’t deserve compensation, but at the same time, Compound does not deserve trust if they do not make some effort to admit fault and mitigate impact. Most if not all businesses and DeFi communities go out of their way to mitigate impact when there is a hack/exploit resulting in loss of user funds. Why would Compound buck this trend?\n\n\n\n rleshner:\n\n\nIf so, how should compensation be calculated?\n\n\n\nTake the market value for the collateral seized minus the true market value for the DAI repayment in all liquidation events (with DAI being worth 1.03). You have your answer for the  loss. Whether it’s shared partially or fully repaid is up to the community.\n\n\n\n rleshner:\n\n\nIf so, should compensation come from reserves, or by distributing COMP?\n\n\n\nReserves first makes sense since that was explicitly allocated for “insurance 1.” COMP to fill in any any gaps makes sense as well.\n\n\n\n rleshner:\n\n\nDoes compensating users create a social contract that doesn’t already exist? Does a social contract already exist? How would compensation change the future activity of users?\n\n\n\nNo, I don’t believe it does create a new social contract that doesn’t already exist. It seems to be already implied through the setting aside of reserves as insurance, that the protocol would use its resources to mitigate the impact of an incident. I anticipate compensation would allow Compound to retain trust and users.\\n\n\n\n cryptoguy123:\n\nMost if not all businesses as well as DeFi protocols have went out of their way to make it right for users who fell victim to protocol exploitation, which this clearly was. Yet you think Compound has no business doing so because users should have known better. If that’s the case, it will be hard to recommend Compound to anyone who doesn’t fancy themselves a smart contract researcher, and a hardcore one at that.\n\n\nI would like to highlight this point. The reserves are there for a reason and this is undeniably it. We the voters are here for a reason and this is undeniably it.\nThe Compound protocol exists in a competitive and ever-changing DeFi space. For compound voters to choose to abstain from compensating the losses suffered by the users of the protocol, they are choosing to separate Compound from the tradition of decentralized goodwill found in the interactions between protocols and their users (see uniswap’s airdrop). Decentralized governance systems (democracies) like the one on compound, through their financial transparency, have the freedom to exercise this tradition when they are presented the opportunity to make wrongs right. Looking at other protocols that have been exploited in some way: After their users lost money due to an exploit in their protocol, Pickle created a token that tracks the losses suffered in this attack and can be burned for reimbursement dai (correct me if I’m wrong).\nThe oracle exploitation needs rectifying on a technical level (changing the oracle, and misleading “safe limit”) and on a financial level as well (compensation).\n__\nLet me congratulate @TWS49 for not losing money in this exploit! Speaking honestly, I appreciate your sympathy. In reading your post, I couldn’t help but notice that you devoted a plurality of your response to the description, in technical detail, of your level of care and the exhibition of your high level of knowledge and understanding of the compound protocol. I don’t believe that these particular sections of your post are helpful, and your particular trading habits and your higher level of education have no relevance to this discussion.\nBoiling down @TWS49’s statement to reveal the argumentative aspects exclusively…\n\n\n\n TWS49:\n\nThe impacted users should have and could have been aware of the risk, and Compound is a protocol that can be used in risky ways\n\n\nFirst: ‘the users impacted could have been aware of the risk in the first place.’ This argument has been thoroughly debunked in @cryptoguy123’s second response.\nSecond: ‘compound is inherently risky, therefor compensation is wrong.’ My problem with this view is that it neglects the nuance in this situation. I would love to read an answer to the following question: in what circumstance would compensation would be correct? I believe any attempt to respond to that question will reveal the shakiness of the other stance in this debate. The protocol itself has been audited 9 separate times, it’s unlikely that there exist holes in the protocol that can be exploited in a way more severe than this, therefore, as the DAI liquidation is the severest exploitation currently possible in relativity to the security level of the protocol, the exploitation should be recognized as such and should not be dismissed on the basis of inherent risk in the compound protocol. An example of an inherent risk in Compound is the risk for potential liquidation if the collateralization ratio of ones supplied ethereum drops to 75%. What is not an inherent risk is the potential for anyone who understands how flash loans work and has sufficiently large and liquid capital to exploit the users and buy their supplied ethereum for absurdly cheap and liquidate a portion of their borrowed dai.\n@TWS49 also mentions that he believes that compensating the exploited users sets what he sees as “a bad president.” He goes on to say that compensation will encourage riskier behavior in the future and cause people to “use the protocol without understanding it.” But let us not leave anything implicit and revise the latter quote to be explicit to the opposition’s argument. What they are saying in actuality is that everyone who uses the Compound protocol must understand not just the protocol itself, but also educate themselves beyond a reasonable extent (as I believe I did). In addition, they must also possess an absolute galaxy brain in order to look past the user experience comforts of not only compound (see: safe limit) but also Coinbase’s branding/corporate propaganda that describes their “three layer” price oracle and exalts it as a technically robust and reliable price feed.\nIn my opinion, @TWS49 asks an unreasonable amount of Compound users. In addition, there exist other borrowing/lending protocols that do not require their users to go to these lengths. For example, aave uses the chainlink oracle service so that users don’t need to bother with all the trouble that @TWS49 went through.\\n\n\n\n lay2000lbs:\n\nAlso could we get a number for what the total cost would be if we wanted to compensate all borrowers 100% of their liquidation penalty?\n\n\nI think conservatively of the ~$100M liquidated, 15-20% would be the financial impact, with individual users losses varying from 10%-40% depending on their collateral and LTV. I also don’t think 100% compensation would be fair to the 90% of funds (using funds % rather than user % intentionally) that weren’t affected by the Dai pricing issues.\nRealistically, if we ask the community for any $dollar amount, we’re going to get the cold-hard reality of “protocol worked as expected, rules/logic were followed exactly, you assume the risk when you use the protocol.” This is all objectively true.\nAs for realistic compensation, this might be a bit of pipe dream, but is there any possibility for some type of B share version of Comp, where the token is worth nothing but has more voting rights. I see this as win-win. The opposition for financial compensation would never vote to pass “bail-out” of sorts, so this proposal would meet that criteria. For those affected, they aren’t in a better place financially, but encourages them to be more involved with the Compound Community and protocol upgrades/updates conversations.\nFull Disclosure: I am one of the impacted address, but have accepted situation for what is is.\\nI respect the opinion but your arguments are completely contradictory. You claim that this oracle attack is planned but that there should be no compensation to users? In which direction do you want this project to go?\nTomorrow, a new weakness of the project may be discovered and your address will be compromised, so how will you react?\nBy ignoring such planned attacks, the project will lose the real users who will have to find a safer alternative.\\nThe compromise is if the affected addresses are compensated in the COMP token. My collateral was liquidated in a ratio of 1 ETH: 377 DAI and given the obvious market manipulation, I expect to buy back Ethereum in that ratio.COMP token is worth nothing if such situations are ignored by community.\nPeople who are against compensation for damaged addresses do not have a single valid argument for their suggestions.\nThe situation is very clear -\n\nStablecoin volatility of 30% is unacceptable\nStabilecoin volatility occurred at only one price discovery source\nThe protocol whose users were affected was exposed to information only from that manipulated price discovery source\n\\nWhat you’ve laid out is all true, and I completely agree with you. Where I think we start to diverge is asking for X dollars for compensation and what actually will happen if the compensation proposal goes up to vote.\nScenario Testing:\nCase1 - Ask for financial compensation:\nAssume impacted users control 20% of the voting power, and request 30% of losses to be compensated to impacted addresses (for sake of example we’ll say $3m). Also assume proposal goes up for vote. We potentially have 80% vote power against us, taking a look at the top 10 delegated votes some have already voiced their opinion on not compensating users. I’m very doubtful that a16z or Polychain Capital will vote in our favor. It’s an unbalanced situation.\nCase2 - Ask for something that material but financially not going to impact the community or protocol directly\nIn my response, I was asking for vote-only shares but the objective I’m optimizing for is getting a proposal up for voting and passing. I want to be compensated somehow that is meaningful (this is up for interpretation, to you it could be financially and for me it could be bigger stake at the table). We laid out great reasons on why this shouldn’t happens and how we should improve protocol, and both sides are in agreement here. Using that momentum, we have a greater chance of convincing the 80% to vote in favor of our proposal.\\nI’m going to write up a longer post soon, but wanted to reply to @tacocat just to let you know you should not get too disheartened. Monetary compensation is an investment in community and the social layer of the protocol, which a firm like Andreessen Horowitz is well aware of. They think long-term and hope to get good ROI. No need to metagame yourself into despair just yet.\\nSomeone tricked us and liquidated Ethereum at a ridiculous price. The purpose of such protocols is to automate processes while PROTECTING users. And those who are against compensation admit that the attack was planned.\nIf the community votes against compensation to liquidated users, it is a sign that the users’ funds on the platform are not safe and that the community doesn’t have long-term goals with the project.\\nWhat would be the next steps to move this forward though? Would it be to bring a proposal up to vote? It seems like Compound’s message is that this is all ‘up to the community’, which sounds pretty hopeless as it seems like it is out of the platform’s interest to do anything as long as the number of affected users / $ size is too small to have an impact, or if it does not impact the big stake holders.\nIf this is ever brought up to vote, I guess it will need to be a more general proposal of setting up an insurance fund from the reserves for incidents like this for everyone, so that it is more aligned with most people’s interests. Even with that, by nature people don’t really care until bad things happen to them; so I don’t know how attractive it is going to be.\nIn the end of the day, the true meaning of ‘decentralized’ and ‘community governance’ is that no one will be held accountable when things go south, but the ‘community’ doesn’t really have control as advertised either since the majority of tokens are held by a few people. Others can only rely on their “good will”, which I hope works out, but we shall see.\\n\n\n\n michjun:\n\nWhat would be the next steps to move this forward though? Would it be to bring a proposal up to vote? It seems like Compound’s message is that this is all ‘up to the community’, which sounds pretty hopeless as it seems like it is out of the platform’s interest to do anything as long as the number of affected users / $ size is too small to have an impact, or if it does not impact the big stake holders.\n\n\nExcactly. It’s obvious that the original Compound team is playing the “community card”. I guess just to make sure, that they are not held accountable for the obvious wrong decision to use the open oracle with only one source. It’s interesting to read the following posting in that context:\n\n\n\nMigrating to the Open Price Feed\n\n\nThis proposed implementation is designed to minimize the risk of relying on Coinbase (either due to a technical, or malicious error on their part), which is already “low” risk in my opinion, and present a path for the Compound community to add more price inputs over time, without ever relying on our team.\n\n\nI agree, the implementation is designed to minimize the risk of relying on Coinbase. But this doesn’t help, if Coinbase is used as the only source. Coinbase is already low risk? That’s an interesting assumption.\\nCompound shoud cover liquidation compensation in similar way as Aave v2 in safety module (Aave token are used in this way and staked to 10% per year)\\nI agree, the solution is actually very simple. COMP will end up in the hands of users who keep their funds in the protocol.\\n\n\n\n dabar90:\n\nlution is actually very simp\n\n\nWe should raise a vote to the DAO\\nI’m not sure why you all are making the assumption that “everyone who got liquidated deserved it because they were yield farming and dumping the COMP”\nI was not, but unfortunately I got liquidated and I don’t have enough COMP to raise a vote.\nCompound is not only for “whales”, everyone should feel safe using the platform and be able to know that in the event that it doesn’t work as designed and is manipulated into liquidating users at a ridiculous premium,  that they have users’ backs.\nCompound has been successful because of how simple it is to use, you deposit one asset, borrow Compound’s “safe max” of some other asset, and that’s it. The UI is very clean, and it appears safe. On the surface it seems to accurately be using the price of assets in the general market.\nCompound totally dropped the ball here (it’s not a “safe max” if it’s set at 80% for every asset, and it’s definitely not safe if you can get liquidated borrowing a safe max amount of one stablecoin against another).\nIn the long term interest of the platform’s success, compensating users who were unfairly liquidated due to a price oracle attack seems a small price to pay. For those of you hoarding your COMP and refusing to support a vote to help any affected users, not supporting the community is going to dramatically reduce the value of your COMP in the long run. Once people realize Compound is selfish and doesn’t support their community, people are going to leave. And that’s not going to do well for anyone with a vested interest in the long term success of the platform.\nPaying affected users from the insurance fund or from the treasury reserves can only have a positive long term impact. And it needs to happen if Compound wants to maintain the trust and market leader reputation that it had before this event took place.\\nI think you missed something or just fail to reply to the right person.\nI am liquidated in “DAI liquidation event” and  of course I think that is not my fault in risk management. It is a clear manipulation of the centralized price discovery source with frontrunning elements.\nWhere you find this: “because they were yield farming and dumping the COMP”?\nMaybe you got it in the wrong context.\nI personally use Compound to save cryptocurrencies while remaining liquid in business.\nIt’s just not clear to me what kind of benefits Compound protocol has from yield farming when someone doing leverage with stablecoin pairs? For me its just COMP exploitation without long-term benefits for protocol.\\nUnfortunately, in this situation we depend on VC funds which are early investors and hold the majority of voting rights. We can’t even do it proposal because treshold is too high. So much about decentralization…\nI did not come across any quality argument of those who are against compensation.\\nSomeone should reach out to a16z (Andreessen Horowitz), Polychain Capital, Gauntlet, and Paradigm with a good case for why they should vote to compensate affected users\\n\n\n\n dabar90:\n\ny, in this situation we depend on VC funds which are early investors and hold the majority of voting rights. We can’t even do it proposal because treshold is too high. So much about decentralization…\n\n\nI think VC would be happy to earn revenus from investment rather than only capital gains\\nIt would be nice to hear their opinion about this situation. Coinbase Venture is also an investor with voting power?\\nCompound should definitely compensate affected users and prevent putting the compound protocol reputation in line.\\nThere should be some form of compensation (disclaimer: i was among the ones getting liquidated for supplying eth and borrowing DAI)\nI was literally sitting in front of compound to check the risk and getting more collateral in to reduce risk, checked the global average price of ETH and dai to see if the risk was getting higher, while my borrow limit suddenly spiked from ~60-65% to 100%… the night before it was sitting at a presumably safe ~50%… and I was actually planning in the second to add more collateral which was just hard because gas was at 500-1000 and tx’s failing across the board.\nI’d be fine with the liquidation if the price of ETH and dai would have globally spiked, but not only was the DAI price much higher than average, but also the ETH price at the same time, creating this squeeze.\nFairest would probably to put it to a vote, and let governance decide on the amount of compensation. Even a small % would go a long way for the status of the project.\nBefore the hack I considered compound the most trustworthy and solid dapp in the whole space (and i’m a heavy dex trader and eth user since 2016)… this attack wasn’t a huge loss for me but completely changed my view of the project unfortunately, and I bet it was similar for many others in the space.\nThats why I think we need a\n\nsustainable oracle solution first\nand partial or full compensation (20-100%) decided by gov (compound is liquid/big enough to easily absorb a full compensation, but gov might obv be biased in their fiduciary interests)\n\\nmaking a proposal for a vote require a lot of comp tokens , i think most of the victims don’t have enough comp to make a proposal for a vote and the big holders of comp don’t care enough about compensating the users since there’s no much talk about this issue in Discord most of the discussion is moved here , and it feels it’s like a close conversation between the victims rather than the community at large , i feel the community will continue to ignore the compensation until it’s a forgotten topic\\nI see a few misunderstandings of the governance process in this thread, so I thought I’d chime in with some updates.\n\nIt only takes 100 COMP to create a proposal and gather support; yesterday, @kybx86 created created an Autonomous Proposal 11 to increase the DAI Reserve Factor to 15%, which he indicated is the first step in a process to compensate users 3. You can delegate 5 votes to 0x5576a4db81a44cb7158fc8d5ae752cb44f57be76 to support this change; if the proposal receives 100k votes, it will enter the formal voting process to upgrade the protocol. This is a tangible example of how changes to the protocol are made by the community.\nGauntlet has proposed upgrading the protocol 2 to be able to distribute COMP; this is a pre-requisite (as discussed in the original post above) to compensating users.\n\nThe community is responsible (and capable) of self-organizing; every single user of the protocol is a COMP holder, and has the ability to drive changes to the protocol. Everyone in this thread has received some amount of COMP to participate with. Wheels are in motion – vote, delegate your votes, and find ways to help if you can.\\nbrave_joF024raym804×779 70.4 KB\nThey dont ignore us\\nAm aware of this proposal and this was already i development before the incident and the main purpose of it is pay for grant not compensation , that same functionality can be used to pay for compensation if there’s a positive outcome of a potential vote for compensation …this proposal was going regardless if the liquidation incident\\nThank you for sharing my post! Finally i got full access to my account after staff review.\\nok, but in some way refers to “liquidation incident”?\nI see that you are active in the development and you obviously have more knowledge than me, so I have to ask you:\n\nWhat is the purpose of reserves if not insurance of protocol users in “DAI manipulation event” (from my perspective it is pure manipulation)?\nDo you think if affected users will be compensated in COMP token, makes a COMP token cash? Because affected users are active users of the protocol and the majority of them hold a significant share of their assets in the protocol.\nWhat are the negative implications in case the affected users get compensation?\nIs a protocol gambling with the reputation if ignore this “liquidation incident”?\n\nThanks for the mentorship \\n\n  \n      \n      compound.finance\n  \n  \n    \n\nCompound 11\n\nCompound is an algorithmic, autonomous interest rate protocol built for developers, to unlock a universe of open financial applications.\n\n\n  \n  \n    \n    \n  \n  \n\n\n29,000 votes left. Who has a comp, vote please\\nI don’t understand from where you come up with the assumption that am active in the development and that am more knowledgeable than you are (unless it’s sarcasm) , just that you we are in the same page i believe users affected in the incident should be compensated i don’t think there’s any negative impact on the protocol i think it’s a positive thing and it will increase trust in protocol in the long run , in terms of how the compensation should be either comp tokens or ctokens or a combination of both seems fair to me .\\n\n\n\n s3v3n:\n\nand this was already i development before the incident\n\n\nperhaps a misunderstanding,\nthere was no bad intention.\nThank you for answers\\nEsteemed kybx86 has created a thread “Compensation Proposal: Distribute COMP to Affected Users in the DAI Liquidations” - I suggest everybody to move there, to that thread for the discussion of the specifics.\n\n  \n    \n    \n    Compensation Proposal: Distribute COMP to Affected Users in the DAI Liquidations Proposals\n  \n  \n    Note: I’ve deployed the CAP for this proposal . If you support this, you can delegate to this address: 0x2f04664b18fb9b6d49124fcc876b52a4ba797718 \n\nObjectives: \nFollowing the first proposal to increase the DAI reserve factor with the goal of de-risking the DAI market and mitigating against future improper liquidations, this next step outlines the mechanics to compensate users for funds lost in the liquidation events of 11/26 by distributing 55,255 COMP (0.55% of total COMP supply) to affected u…\n  \n\n\\nThank you @Dmitry for cross-posting.\nI’d like to point out is not just a thread, but that there’s an actual proposal deployed live in governance to implement this compensation.\nYou can view the CAP here: https://compound.finance/governance/address/0x2f04664b18fb9b6d49124fcc876b52a4ba797718 10\nAnd if you support it, you can delegate COMP to its address: 0x2f04664b18fb9b6d49124fcc876b52a4ba797718\\nThank you! Can you explain how to “delegate” for people who haven’t done it before?\nEdit: Thanks! Figured it out\\n\n  \n    \n    \n    Governance Guide — How To Delegate Governance Process\n  \n  \n    The Compound Governance voting system works via delegation of votes, which is a built-in-feature of the COMP token. As a COMP token holder, you can delegate voting rights while keeping your COMP safe in your wallet, cold storage, wherever. This post is a step-by-step guide on how to delegate your votes and the options you have in doing so. This should act as a visual, intuitive guide to accessing Compound’s Voting Interface (if you have a COMP balance), and delegate your voting rights. If you’d …\n  \n\n\\n@tob, thank you. Just to follow through:\n\nGo to https://app.compound.finance/vote 1\n\nClick vote --> delegate\ndelegate to the autonomous proposal 3 address: 0x2f04664b18fb9b6d49124fcc876b52a4ba797718\n\n\nReminder: when you delegate votes, you are still in full control of your COMP. You are just passing the voting power of your COMP to the proposal.\\nIdeation for a new proposal that addresses objections with the initial has been occurring in the thread for the previous proposal Compensation Proposal: Distribute COMP to Affected Users in the DAI Liquidations 31"
  },
  {
    "number_of_comments": 16,
    "postid": "77b3ca98-59f1-4844-b09f-1d7192baa9ff",
    "posturl": "https://www.comp.xyz/t/cp039-avoiding-liquidations/1323",
    "combinedcontent": "One thing that all participants in the recent vote seem to agree on is that changes should try to avoid increasing the quantity of liquidations users face. As users of Compound only provide one piece of identifying information - their address - it’s not clear how to contact them to make sure they are aware of these changes. I’d like to suggest a few things that we could to minimize liquidations and wanted to get feedback on these ideas.\n\n\nTry to contact the users using ETH transactions. The idea here is to:\n\nSend a small, but strange amount of eth, e.g. .0001234 or .000420420\nEncode a message into the Hex data field that says something like “Check your WBTC borrow on COMP”\n\n\n\nThis is not a great option for obvious reasons - the user has to see the transaction and also try to parse the Hex Data. However, we’re prepared to “message” as many addresses that are at risk as time and money allows.\n\n\nUpdate the Compound UI to notify users of the pending parameter change\n\nOne thing that we can’t do as a community is to prevent people from opening new WBTC positions that are high risk. Maybe the Compound labs team could push a quick update to the site to warn new users of this change, you could imagine a small toast that users would see until Sunday\n\n\n\n@getty and other community members have repeatedly espoused the virtues of WBTC borrowers - that they are sophisticated and that they manage their positions actively. I hope this is true. But we are prepared to do everything we can to try to prevent liquidations in the case this hope is even partially misplaced. No one has been liquidated yet, and I’m optimistic that the community can rally together to minimize the impact of these important changes to the protocol.\\nShould we try sending small ETH transactions to notify users? Yes, it might help! No, this is a total waste of time30voters\n                \n                Results will be shown on vote.\n               \n                  \n                  Votes are public.\n                 \\nGauntlet is also keeping an active list of those who will be impacted by the new collateral factors. We will keep the community informed as changes occur.\\nThanks to @rleshner for sending the newsletter: Alert: WBTC Collateral Factor Decrease in 48 Hours - The Compound Digest 14\\n\n  \n      docs.google.com\n  \n  \n    \n\nCompound WBTC Risky Positions 21\n\nFriday 2pm PT\n\nAddress,Health,Total Borrow Value (ETH),Total Collateral Value (ETH),Supplied WBTC,Supplied Tokens,Borrowed Tokens,WBTC Supplied Value,Maximum Liquidation Penalty...\n\n  \n  \n    \n    \n  \n  \n\n\nreally one guy who is gonna take most of the impact here:\n\n  \n      \n      DeBank\n  \n  \n    \n\nDeBank ｜ DeFi Wallet for Ethereum Users 18\n\nA DeFi wallet for managaging and tracking your DeFi portfolio, with data and analytics for decentralized lending, stablecoins, margin trading and DEX projects.\n\n\n  \n  \n    \n    \n  \n  \n\n\\ntypo on my vote. to clarify I think it’s worth doing for this particular case, but not sure it’s the best solution for future collateral factor changes.\\nIt’s so few addresses right now, we might as well give it a shot\\nI guess sending eth with message could be done, nothing bad can happen from that idea. But frankly it doesn’t look very effective. Not like it’s going to produce sort of popup for users, and might very well pass completely unnoticed. In my opinion it’s mostly going to be just a waste of gas.\nIt would be great if Compound UI could display a message on front page. And not just in that particular case, but for any future event governance consider important. There unlikely to be many such events, but it likely will be more effective in communication to users.\\n\n  \n      \n      Ethereum (ETH) Blockchain Explorer\n  \n  \n    \n\nEthereum Transaction Hash (Txhash) Details | Etherscan 12\n\nEthereum (ETH) detailed transaction info for txhash 0x360b25c3a1fa0255e3650c0ea37d34f7e3753db5dc3be06e9994c3f4c26272ff. The transaction status, block confirmation, gas fee, Ether (ETH), and token transfer are shown.\n\n\n  \n  \n    \n    \n  \n  \n\n\n  \n      \n      Ethereum (ETH) Blockchain Explorer\n  \n  \n    \n\nEthereum Transaction Hash (Txhash) Details | Etherscan 5\n\nEthereum (ETH) detailed transaction info for txhash 0xccf0c853a6c540e97f20a1fa6ff930fd72ba153bc19b01741ddd409e0f02f1f1. The transaction status, block confirmation, gas fee, Ether (ETH), and token transfer are shown.\n\n\n  \n  \n    \n    \n  \n  \n\n\n  \n      \n      Ethereum (ETH) Blockchain Explorer\n  \n  \n    \n\nEthereum Transaction Hash (Txhash) Details | Etherscan 8\n\nEthereum (ETH) detailed transaction info for txhash 0x48475cddc7fd216a3de07bfae1cfb3cabdae6fcaf7236385976278e6b1727103. The transaction status, block confirmation, gas fee, Ether (ETH), and token transfer are shown.\n\n\n  \n  \n    \n    \n  \n  \n\n\nSent a couple, will do so again tomorrow\\nI support the idea of the front page message brought up by @Sirokko\nFurther, I’d like to utter my oppinion that we are dealing with mature and responsible users here that invest serious amounts in a volatile and fast-chaning environment. This has to managed actively and changes in the system followed. This is not a set-it-and-forget-it environment as I see it. Changes of the CF have been discussed here actively for a while now and the change attempt only went through with the 2nd proposal; so there was even more time to be aware. I don’t see the need to “run after” folks who operate their borrowing/lending business on the edge to liquidation more than necessary, i.e. front page / news message should be fine.\\nAn ideal solution is not to liquidate due to change in CF but leave liquidation to market force.\\nWhat was the aftermath of the change?\\nPreliminary analysis shows only one account impacted immediately (1) and one liquidated a bit after (2). Large borrowers are clearly sophisticated users of the protocol and most concerns about the large CF changes appear to have been overstated.\n(1) Note this account has also been liquidated 9 times in the past 2 months. Ethereum Transaction Hash (Txhash) Details | Etherscan 13\n(2) Ethereum Transaction Hash (Txhash) Details | Etherscan 9\\nWas thinking of moving my some btc into comp to better make use of some of my assets, but recent changes seems to have me rethinking this. I guess users mulling over their investment decisions a bit more is ideal for the platform in general, but the risk I’m weighing now is whether a protocol/governance change that liquidates a part/all of my entire positions might happen  while I’m out on vacation.  I don’t know if that’s a message we want to convey.\n\n\n\n Sirokko:\n\nIt would be great if Compound UI could display a message on front page. And not just in that particular case, but for any future event governance consider important. There unlikely to be many such events, but it likely will be more effective in communication to users.\n\n\nThis would be very useful, a popup/modal announcing protocol changes that impact users liquidity/positions would be very helpful.\nAlso perhaps apart from notifications, for protocol changes that impact users or rather shown liquidate users or increase users chances of liquidation, a longer effectivity time would be ideal.\\nHi @jmo , is there a specific update to the protocol that is motivating your concern to minimise liquidation OR is your motivation a general one to avoid liquidations on the platform?\nIf more general, perhaps there could be an email input opportunity when you make a trade on compound?\\nMaybe adding a “Notifications Settings” button on the UI that directs the user to enter their email address and name (optional).\nCould say “notify me when…” My Borrow/Supply ratio reaches ##%, Upcoming protocol changes are scheduled, My COMP rewards reaches ## COMP, News regarding the Compound Protocol, etc.\nShould be a way to get the user’s ETH address (if they connected their wallet).  Then users are more than just ETH addresses interacting with the protocol because they can be contacted/notified via email.\nMaybe even have 5,000 COMP (or anything really) set aside to split evenly between users who link their ETH address and email address.\\nTyler (TRiLeZ) here.\nI could see these types of notifications going unnoticed most of the time.\nWhat I think would really help when it comes to liquidations are three things:\n\nPartial liquidations. When the liquidation threshold is hit, liquidate just enough of the collateral assets to hit an X% borrowing rate.\nTime delays. Wait X amount of time after passing the liquidation threshold before liquidating. This gives users more time to manage their loans in the case of a sudden/unexpected market drop.\nLiquidation preferences. Where I can select the order of my collateral assets to be sold off in the event of a (partial) liquidation.\n"
  },
  {
    "number_of_comments": 11,
    "postid": "921df98f-fabf-42d4-9500-bfa5fbf00c55",
    "posturl": "https://www.comp.xyz/t/reserve-factor-consensus-for-the-compound-protocol/3487",
    "combinedcontent": "\nSimple Summary\nTo facilitate community discussion and establish consensus regarding the next steps on Reserve Factors for Compound II, Gauntlet provides the following 3 options. Below, we outline the tradeoffs for each option in more detail. Gauntlet recommends that options 1 and 3 are the most prudent options. Since adjusting Reserve Factors is as much a matter of business strategy as it is of market risk, it is important for the community to find consensus.\n\nDecrease Reserve Factors by 0.025 for USDT and DAI\nIncrease Reserve Factors by 0.025 for USDT and DAI\nWait until Compound III is live\n\n\nMotivation\nThe community made excellent points on the purpose of reserve factors 40 and their utility beyond mitigating insolvencies. To continue the discussion with the community, we want to provide insights around important questions raised. As a point of clarification: we are aware that mechanics around reserves will change with Compound III. Nonetheless, we find the discussion around user elasticity relevant to optimizing protocol risk and returns. This analysis and recommendations focus specifically on Compound V2.\nBefore we dive into the discussion, let’s recenter on why the reserve factor is a valuable parameter in the first place. The reserve factor is an attractive parameter for navigating volatile markets since it can be set at the asset level (as opposed to other parameters like liquidation bonus which is global and applies at the protocol level). The reserve factor represents the portion of protocol revenues (paid by borrowers as interest) retained by the protocol as a liquidity backstop or insurance fund and potentially to supplement the treasury in the future based on community feedback. Reserve factors are generally viewed as supply side incentives because they are expressed as a portion of revenues not paid to suppliers. Conceptually though, the reserve factor is the delta between what the protocol takes in from borrowers and what it pays out to suppliers. A change in reserve factor could be passed along to borrowers as an incentive if an equivalent change were made to borrower interest curves. The reserve factor could therefore be used to incentivize either borrow or supply behaviors. Having robust models for this behavior is important, especially in times of high volatility, to set this parameter at optimal levels that maximize revenue for the protocol.\n\nSpecification\n\nWhat drives reserve growth on Compound?\nReserves are a function of interest rates and total amount borrowed. As you can see in the graphs below, borrow usage on Compound has declined significantly in recent months. Since the reserve growth rate is driven by borrow balance, the annualized reserve growth has also declined.\n\n1464×1174 22.1 KB\n\n\n1436×1186 25 KB\n\nLooking at the borrow balance by token, we observe that the USDC/USDT/DAI stablecoins represent nearly all of the borrowed value and the lion’s share of reserve growth. Since USDC will be part of the initial Compound III launch, any changes to reserve factors on Compound 2 for USDC would result in noisy data. We, therefore, chose to focus our analysis and recommendations on just USDT and DAI.\n\n1502×1130 17.1 KB\n\n\n1592×1130 20.8 KB\n\n\nHow might reserve factor changes impact borrower interest rates?\nReserve factors have not changed on Compound in a long time (since December 2020), and markets have changed significantly since then. Consequently, the protocol does not have accurate estimates of borrower and supplier elasticity with respect to reserve factors. In lieu of that, we have modeled the scenario of perfect elasticity to uncover what would happen if users reacted proportionately to the changes. We acknowledge this likely overestimates the true elasticity of users, who should be stickier for various factors like switching cost, relative risk level vs. alternative protocols, etc. This does illustrate the risk and potential upside of adjusting RF up or down. Borrower interest rates are a function of supplied liquidity, so we must first estimate the impact of the reserve factor on supply.\nAs explained in our previous post, to estimate the effect on supply, we use the following formula:\n\nIf the protocol increases reserve factors, we expect the amount supplied to decrease, driving up borrow usage and therefore borrow interest rates.\nThe borrower interest rate for the USDT/DAI stablecoins is calculated as the utilization ratio (total borrows / total supply) times a constant factor (a much larger constant factor is used once utilization surpasses 80%, but for simplicity, we will ignore this tail case for now) plus a constant base rate which the rate cannot fall below (currently set equivalent to 0).\n\n826×146 8.97 KB\n\nFor the purposes of this analysis, we assumed borrowers were unit elastic to the borrower interest rate. This enables us to compute the expected borrower interest rate that incorporates this change in supply, shown in the figure and table below:\n\n1620×1144 56.4 KB\n\nThe current reserve factors are indicated with black dots above:  USDT is currently set to 0.075 while DAI reserve factor is set to 0.15. Some point estimates for borrower interest are outlined in the table below.\n\n608×886 33.5 KB\n\nWhen benchmarked against rates in the market, borrower interest rates are already higher on Compound:\n\n1308×406 25.8 KB\n\nThe current reserve factor USDT is lower than the market (.075 vs. .1). It is already higher for DAI (at .15 vs. .1). Raising reserve factors would increase the spread in the market between Compound and the market rate for both assets. Supposing any level of elasticity on the part of the users would project a decrease in borrows as a result (due to rising interest rates) at a time where our models show risk is well contained (Value at Risk, currently equal to essentially $0 at the 95% tail scenario). Given the decreased volume of total borrows as compared to previous quarters (reserves are projected to grow $2.5 million over the next year at its current growth rate), the opportunity cost of decreasing reserve factors is currently low, with the potential upside of driving borrows (calculation with unit elasticity below).\n\n1554×1146 52.8 KB\n\nWith risk well managed, this becomes predominantly a question of business strategy. We presented numbers in both directions (increasing and decreasing RF for the top 3 assets) to help inform the discussion, as we defer to the broader community regarding business strategy.\nSeveral available options for Compound are:\n\n\nDecrease Reserve Factors by 0.025 for USDT and DAI - since reserves are currently high enough to cover the 95% tail scenarios showing $0 VaR for all collateral assets, and borrows have sharply declined on the protocol, reducing Reserve Factors is an opportunity to bring interest rates close to the market rate with the goal of driving borrow activity while also gathering data around borrower and supplier elasticity. These assets are also not part of the initial Compound III roll-out and, therefore, relatively isolated from that market.\nLimitations and Tradeoffs: Since we do not have data on borrower elasticity, we cannot model borrower behavior. If users are inelastic to interest rate changes, the reduced reserve factor might result in lost revenues for the protocol. This risk may be acceptable given the low borrow volume at the moment.\n\n\nIncrease Reserve Factors by 0.025 for USDT and DAI - proponents of this option may assume user behavior is inelastic to interest rates, therefore allowing the protocol to capture a larger share of the revenue. Any change in reserve factor would enable the community to gather data on user elasticity to build better models in the future.\nWhile we support either option to gather data and improve models, Gauntlet does not recommend this option since any user elasticity with respect to this change would push interest rates farther away from the market rate as liquidity leaves the protocol.\nLimitations and Tradeoffs: same as above - since we do not have data on elasticity, we cannot model behavior. If users are elastic to this change, an increase in reserve factor could result in decreased revenues for the protocol, the extent to which is unknown given the limited data.\n\n\nWait until Compound III is live - with the changing dynamics to be brought on by Compound III (reserve factor will no longer be set as a global parameter), the community may decide to wait to change reserve factors.\nThe tradeoff here is that we will collect no information on user elasticity.\n\n\nWe see options 1 and 3 as the most viable options. Our concerns are outlined under Option 2. We will initiate a Snapshot poll and look forward to further input and feedback from the community.\n\nNext Steps\n\nTargeting a Snapshot vote on 8/15/2022 with the above 3 options.\n\nBy approving this proposal, you agree that any services provided by Gauntlet shall be governed by the terms of service available at gauntlet.network/tos.\\nHey Paul! Thanks for this proposal!\nKirill here from Penn Blockchain. We support the decrease in reserve factors by 0.025 for both USDT and DAI because, as much as cash flow generation is an important aspect of the protocol, we believe that moving towards the market rate is a better idea than away from it in an attempt to analyze the elasticity of users with regards to the interest rate.\nI’d argue, however, that reducing the reserve factors by 0.025, especially for DAI, would be insufficient to bring Compound significantly closer to the Market Rate or to significantly impact its market share. Have you considered lowering them further, making Compound more competitive in those markets?\\nThank you, Kirill and @pennblockchain. We appreciate your feedback. Reducing reserve factors by 0.025 would be an initial step, and we can then reevaluate the next steps after gathering data. The reason why we are limiting the change of reserve factor per symbol is to avoid the polarized and drastic solutions which may shock the Compound ecosystem. By making relatively smaller changes, we can gather more fine-tuned data which can better inform the models and future parameter change decisions.\\nAn so it is right back at pretty much same point with Gauntlet again. I don’t know what made Gauntlet so convinced that lowering RF would somehow benefit protocol, as not much indicate that.\nBut let’s start with common ground we have here. Yes, RF is important parameter and maybe it is time to discuss it’s values.\nWhile most arguments below will address points made by Gauntlet, i suggest for reader not to treat it as a standalone post, and carefully read Gauntlet post prior to it to be somewhat in context, especially if not being an actual user of protocol.\nFirst of all, we should clearly understand that RF by itself does not have direct impact on borrow rates, and the impact on supply rate is rather minor. That rates are determined by relation of amount borrowed to amount supplied, as indicated by formula, kindly presented by Gauntlet. Simply speaking, RF is a sort of a tax, imposed on revenue, collected from borrowers before it is distributed to suppliers. So, if borrower is paying 2% APR, then with 0 RF all of that revenue will go to Suppliers, and with RF 0.1 about 10% from that 2% APR collected will be substracted before distributing remaining to suppliers. However in both cases borrower will still pay same 2% APR regardless of RF being 0.1 or 0. Changing RF does not mean reducing Borrowing APR by itself, but it does have immediate impact on amount of revenue of protocol coming to reserves. (It have a bit more complex relations, as amount of total reserves accumulated in market somewhat decrease rate for borrowers, but as reserves are relatively small comparing to total supply, impact is negligeble, so we not going to discuss that further)\nGauntlet logic implies, that by decreasing RF, amount, distributed to Suppliers will increase, thus will produce iniative to Supply more, which in turn will decrease borrow rate, as being a function from Supply/Demand. That, in turn would incentivise to Borrow more, which will then increase revenue, as lower RF from higher total Borrow can potentially produce more revenue, given Total borrow will increase big enough to make up and even beat losses from RF decrease. Simply speaking, it is suggesting directly decrease revenue in assumtion that IF Supply will increase and IF Borrow will follow increase of supply AND IF that increase will be big enough to cover the losses from decreasing revenue at the first place, than WE MIGHT see some benefit.\nWhile such relations do somewhat exist, there is just so many IF, that it REALLY raises a question.\nTo begin with, logic is completely backwards. Supply is pretty much irrelevant. WE don’t need to incentivise Supply.\nThe core problem we have in macro now is actually financial Supply. It’s just too much of it and it is not productive by itself. No matter how much money you print and disperse, it ain’t going to produce more products or services. It’s just inflation. You will get more money chasing same amount of goods and services and that is how inflation is created. Compound is good showcase to demonstrate it.\nWhat we DO need, is actually more Borrowing, Not more of Supply. Supply is overbloated, there is just no place to actually put it to productive use. But before i’m going to go a bit deeper into topic, let’s look at the Compound as a service.\nAs we all know, it’s a borrow protocol. It allows you to borrow assets given you are able to provide enough Collateral. You can also Lend, obviously, but that as long as there is demand for your assets. Rates are generated by a function, based primarily on total amount supplied vs borrowed for a given market.\nFor simplicity, we can observe 3 big groups of assets, supported by protocol: Collateral assets, Stable coins, S…t coins. S…t coins group doesn’t imply those assets have no value at all, but rather reflects the fact that their monetary value is very speculative and could easily become zero or close to such. They do have a value assigned by market, but can easily loose it, sometimes overnight.\nFor collateral assets available on Compound we can see established assets, which unfortunately gives just 2 assets: WBTC and ETH. While WBTC by itself is just a derivative, it inherits it’s value from being backed 1 to 1 by BTC, which is established\nasset. However it’s still not pure asset. It’s just a reciept for actual btc which you might or might be not able to redeem at a time of need, so it actually bears quite a bit of more risks than btc itself.\nThen we have ETH, the native token of Ethereum, chain, on which pretty much everything what matter resides.\nNow let’s look at our prospective userbase. We can assume that vast majority of Compound users are basically traders and speculators. Primarily because using Compound as a funding base for some real world enterprises is kind of difficult because\nof lack of Quality Collateral assets. Yes, borrow rates might be amazing to fund any sort of business, but collateral is just crap. Literally best collateral you might use are ETH and WBTC and both of those can easily plummet up to 80%. Not type of collateral you want to have in borrowing money for year or longer.\nThat’s not Compound fault, but it’s just how it is. So we can’t really expect a lot of capital using Compound as a source of funding for some actual productive business in real economy. Sure, maybe someone could be able to manage it, but most likely we can’t expect big numbers. Thus Compound users are going be primarily for some financial operations inside crypto itself.\nAnd now let me explain why i went to such long description of userbase. (though i tried to make it short)\nLet’s look at that Total borrow graph, presented by Gauntlet, where we can observe Total Borrow to plummet from 7b to 1b.\nWhat we can observe in that time perioud?\nWell there were 2 big events, which matter for us:\nFirst, of course, the crypto downtrend. ETH went from 3k to below 1k, and BTC went from 45k to below 20k. That important for us, because those 2 are collateral assets. Those people, who borrowed before just can’t afford it anymore. Their collateral at\nleast halved, they had to repay. And now even with eth and btc going up from their lows it is still impossible to borrow same amount of as before because collateral just not allow it anymore (even if we assume everybody preserved exact amount of tokens through going down)\nThen you could argue that stable coins also decreased and rightfully so. Well, the group that supply stable coins to borrow eth, btc, or any small token always was very small. If you look at total borrowing of those assets retroactively, not much demand was there. And for a good reason. It’s just too risky to short crypto. That group of people who actually consistently do it, and for prolonged time and end with a profit is so small.\nAnd so we come to another considerable event. Cutting down COMP distributions in half. Kudos for Gauntlet not actually voting for that.\nThat was another big initiative for bigger Supply and Borrow of stables at that point. It was profitable to supply stables and borrow stables in recursive way, farming rewards. While it was not designed activity, it also kept Supply and Borrow of stables at higher rate as it would be otherwise. As profitabilty of such activity vaporised, TVL followed,\nhowever not instantly as people were prematurely celebrating at that time about stickiness of liquidity.\nSo we arrived here, where we are now, at a much different macro reality, much of uncertanty, higher inflation and higher fed rates as well. Do we really expect 0.6% APR at Compound to be attractive when you can buy treasury at above 2% APR? I don’t think so.\nNow let’s roll back a bit to a Borrow rate. Should i remind everybody that besides of flat Borrow rate there is also Effective Borrow rate? As well as Effective Supply rate? And Effective Borrow Rate = Borrow Rate - Initiatives?\nNo, Compound Borrow rates are not above Competitors, if you mean that by a market rate.\nEffective DAI borrow rate is actually below 0.6% APR\nEffective USDT Borrow rate is below 3% APR also.\nWhile COMP distribution isn’t really primarily intended as subsidy, it however do bear market value regardless, and thus can not be just excluded from equation like nothing is there.\nActually if a person is up for borrowing USDC or DAI it’s hard to find lower rates to borrow than at Compound. WE don’t really need to make it lower to begin with. BUt if we would like to, it’s easy to make it even cheaper, even negative and without actual expense for protocol reserves. We can just ramp up COMP distribution rate for Borrow side of those markets. And that, unlike RF has direct impact on borrow rates, without many “IF”.\nBut of course, if we look at broader perspective, what do you really expect? Compound isn’t here in vacuum, it have competition. After total valuation of crypto markets shrinked, remaining money concentrated at most profitable positions.\nCompound never expressed interest to deploy V2 on perspective L2 (Optimism, Arbitrum) or sidechains, like Polygon. Anybody seriously expect to win competition for TVL, when there is aave on optimism distributing optimism tokens for next 3 month to\ncome?\nDo you really believe that change from 0.6% to let’s say 0.75% APR for stable coins going to attract liquidity, when you can get from AAVE above 3% APR for Dai AND another about 0.8% in OPT tokens on top of that? Or 2.5% APR for USDC with 0.8% in OPT on top Or 2.7% APR for USDT with another 2% on top? Just NOT GOING TO HAPPEN. Of course, as the rates are variable that might and probably will change overtime, but the point here is if you going to compete with other protocols for shrinked liquidity, you can’t expect to win when your rates are several times lower.\nCompound can compete for farmers, question is do we need to? We have more Supply side than we need actually and there is just no organic demand on borrow side. Sure, we could stimulate it with some distributions, but truth is there is just not much demand for stables for now.\nFor example, sure i could, for example borrow more stables from Compound. Great rates, yea. And what should i do with them? Farm on Optimism? Well, that could be done to extent, risk profile doesn’t allow to pour everything into one place. Buying\nETH doesn’t look great idea given merge upcoming with unpredictable results price wise. Buying wbtc? Macro not exactly rainbows and butterflies again.\nI’m sorry for wall of text. It actually starts to feel like a job compiling arguments here. To keep it clean i’ll move actual suggestions to separate post.\\nOk, now when i hope we all are on same page and within context, here is alternative.\nI suggest instead of trying to change things in area when we not going to win anyway, we concentrate our tuning efforts in area where Compound probably could beat anyone else and take a look mostly on collateral assets for now. I suggest we change RF:\nETH from 0.2 to 0.75\nWBTC from 0.2 to 0.75\nTo balance such a change i suggest at the same time increase COMP distribution speed for Supply side only:\nfor ETH x3 (eth is a bigger market and with upcoming merge i expect even more interest in it, thus i want to promote Effective Supply rate to be higher)\nfor WBTC x2\nWe can clearly observe increased market interest in borrowing WBTC and ETH from Compound, as effective borrow rates are close to zero. Why such a big change in RF? I believe suppliers of those assets couldn’t care less about Supply APR at all, as it is less than quarter of percent APR (less than 0.1% for ETH). On the other hand, if most of these go to reserves, we could have meaningful reserve growth in those assets overtime. At the same time, to make it effective improvement for users as well, by increasing distribution we will provide effective supply rate bigger than it is now for a relatively small price of COMP tokens. I expect it to stimulate usage of those markets even more, which in longterm would benefit protocol and can potentially lead to more usage of stable coin markets as well. Those markets deserve much more love in governance distribution in general, as while being pretty much as important as stable coin ones, they always were getting scraps from distributions.\nFor stablecoins on the other side\nDAI leave intact\nUSDC leave intact for now\nFor USDT market i’m very tempted to support Gauntlet idea of decreasing reserves just for a sake of observing results. I do personally believe nothing good would come of it, and if anything, RF for other stables should be increased instead to about 0.1, which is very reasonable tax rate, neither too stimulative, nor too prohibitive. But well, if we need to make an experiment USDT might be a good enough candidate for it.\nWe can come back in 2-3 month to evaluate and make adjustments for RF if needed. BY that time we will be after merge and possibly with v3 deployed, and can take a look at RF of stablecoin and collateral markets again.\\nThank you Gauntlet for this proposal and @Sirokko and @pennblockchain for your input. This is @remusofmars (twitter) from LionDAO (ColumbiaU).\nRather than a one-off vote, this proposal seems like the beginning of a longer term discussion around how the Compound protocol ought to treat its reserve factors. Put simply – how do we balance constant cash flow generation vs market rate optimization, and what are the longer term effects of focusing especially on either one? I think that maximizing protocol revenue ought to be the goal, but that that is also a long term consideration, and we fortunately have an opportunity to do some testing before the release of Compound III.\nBefore I digress, I think it is crucial that we get data on elasticity and would support an increase in the DAI RF and a decrease in the USDT RF. The reason for increasing DAI over USDT is simply that it has a higher TVL and is thus likely to produce more revenue during this experiment, but I do not think it matters so much which increases/decreases – just want to collect data on each possibility. (Ideally I would actually increase USDC and decrease USDT, as those are the closest comparables and least likely to have supply/demand fluctuations against each other due to exogenous events.)\nIf only given the options in the poll, we vote for option 2 . Option 3 is the least preferred since we collect no data, and option 2 is likely to increase protocol revenue in the short term (direct effect of raising RF, while we observe the second/third order effects).\nCollecting this data before the launch of Compound III would be valuable to help us make an informed decision/policy then.\nOk, now let me dive into some of the logic of rate adjustments and their effects. Here is a simple step-by-step visualization of Gauntlet’s reasoning for the two active scenarios (* denotes an effect that is not a real change):\n\nPlease correct me if I am wrong on how Supply Interest acts, but all of the others are correct (and more important to our debate).\n@Sirokko makes the point that suppliers in DeFi are probably quite inelastic, as other protocols offer better APRs on the same assets. Additionally, he says that Compound hasn’t made efforts to build/incentivize via L2s like AAVE has with OP (and is distributing OP rewards). I don’t entirely agree here – I think that Compound benefits from its trustworthiness and that these (OP) types of incentives are mostly short term beneficial and long term inconsequential (except insofar as attracting new users). Either way, I do not really know and am sure Sirroko has more direct knowledge here than myself, but would be interested to see how this change would play out if we agree to treat this mostly as a trial period.\nNow, assuming that suppliers are at least a bit elastic, here are the possibilities around revenue given different borrower actions. If our goal is maximizing protocol revenue, which happens by maximizing R = Total Borrows * (RF * Borrow Interest), the actual effect on R depends on borrower elasticity. If we increase the Reserve Factor (as is option 2 in this vote):\n\n\nIf borrowers are very reactive (elastic) to the increase in borrow interest, then the decrease in borrowers will overcompensate for the increase in borrow interest rate, and thus will decrease protocol revenue.\n\n\nIf borrowers are not so reactive (inelastic) to the increase in borrow interest, then the decrease in borrowers will not be enough to offset the increase in borrow interest rate, and thus will increase protocol revenue.\n\n\nI am not sure how elastic borrowers are, but would guess that they are probably more elastic in the past few months than during periods with higher growth - this is a prime opportunity to collect near worst case data on preferences.\nPlease do reach out to me if there are any questions/comments/concerns on this post. Its important that there is mutual understanding.\\nHi everyone,\nRoss here from a16z. This is a thoughtful proposal from Paul and the Gauntlet team that considers the nuances of both risk mitigation and protocol revenue generation. Given current borrow volumes and low protocol risk (defined by Gauntlet’s modeling of value at risk), this is an appropriate time to assess the impact of reserve factor modifications on protocol usage. A reserve factor reduction of 0.025 for both USDT and DAI will (1) slightly nudge borrow rates toward the market rate and (2) allow the collection of user elasticity data. Both of these are worthwhile experiments to run in our view. Some quick follow-on questions from this:\n\nWhat drove the selection of a 0.025 magnitude change? i.e. is there data supporting 0.025 as the optimal increment size for safe yet meaningful changes to the reserve factor or was 0.025 arbitrarily selected as a safe magnitude?\nWhat specific criteria would validate further decreases in these reserve factors? What do you think the timeline looks like for both observing that validation and the subsequent proposal of further decreases?\nAs @blockchaincolumbia has pointed out, an increase in one reserve factor and a decrease in another may provide more data on user elasticity than simply decreasing both. If this hybrid approach is employed, what specific metrics should be tracked and at what point should subsequent modifications to the reserve factors be made?\n\nThanks so much!\\nThanks, @Ross @blockchaincolumbia @Sirokko for your thoughts. Given the open questions and community engagement here, we will wait to publish the Snapshot vote and will give a heads up before we do so. We will come back to the forums shortly to respond to the open questions.\\n\n\nChanging RF will not let us collect data on borrower elasticity and would not move Borrowing rate towards “market rate”. RF factor have no direct impact on Borrower rates. Raising or lowering that parameter will still result in same Borrower rate.\n\n\nCompound Effective Borrowing rate after taking in account COMP distributions are already among the best if not THE best in industry. There is simply no need to make it lower, because if we don’t have more borrowing demand while asking half rate than competition, we ain’t going to increase borrow demand if we lower it a bit more. Thus, no encreased earning to reserves.\n\n\n                   USDC Borrow          DAI Borrow            USDT Borrow             \n  \n  AAVE (eth)          1.58%              2.29%                  2.3%\n  \n Compound             0.65%              0.67%                  2.6%\n(effective rate)\n\n*USDT stands aside here, as distributions mostly were concentrated in USDC and DAI historically, and USDT is not usable as collateral and thus bearing less cumulative effect on protocol.\nIf you in market for borrowing stables you just not likely to find better deal. Already. Compound rates are NOT higher, but are WAY lower than “market” for 2 of 3 stables coins.\nIt is true, though, that it would allow to collect data on Supplier elasticity, as that is where the change would be observable, while still relatively minor. Since it would be small, i fully expect it wouldn’t bring Supplier rate to high enough levels to be interesting for Suppliers. So speaking in elasticity terms, while i do expect Suppliers to display elasticity in response to change of RF in general, we unlikely be able to observe it, as Supply rate will still remain quite low as we already have too much Supplied for current market Borrow demand.\nI want to give special note on increasing DAI rate. RF for that market is already high, at 15% taxation rate we should be very careful to push it higher. The reason i don’t suggest lowering it, because it works fine as is. Effective Borrow rate is exceptional, effective Supply rate is 1.3%, which while not something amazing, it can grow fast if at some point Borrow demand will catch up. And it might, because when it will, it will come to Compound first, because our Borrow rates are just better than elswere.\nIncreasing RF for DAI short term will increase earnings to reserves, but in long run might put unnesessary down pressure on Supply rate. If anything i’d suggest to experiment on relatively isolated USDT market FIRST if it’s just for data, before potentially breaking something which works well with adjustments not called for.\nPlease, listen me up, collegues. Here is a napkin math for you. Let’s take USDT market and what we are discussing here. With current Total Borrow of 233M and RF 0.07 we are getting about 1295$ in reserves daily. With RF 0.1 we would be getting 1850$\nin reserves daily. 500$ increase. Nothing groundbreaking actually.\nNow lets look at WBTC. With current Borrow of 24M and RF 0.2 we getting 493$ in reserves daily, with RF 0.75 we going to be getting 1850$ in reserves daily. AND we don’t really take anything from suppliers, we take out their flat dust interest they\nrecieving, and replacing it with increased COMP distributions. Effectively, they are going to recieve more yield on their position than before. And that is for mere doubling COMP distributions to Supply side. Currently we disperse about 70 COMP totally for both sides of that market, doubling supply side will make it 100.\nEffectively we trading our “treasury” COMP for increased reserves in WBTC. And doing so without selling single COMP into market as protocol. And users benefit from that change. WE DO want more reserves in ETH and BTC than more COMP in stash, because that COMP is very volatile\n.\nAND if we want actually to increase our earnings, rather than slightly optimise what we currently having, we can double not only Supply, but also Borrow side. That action i expect to be VERY elastic at Borrow side and will actually bring more borrowers. That can also bump stable coin Supply markets, because, yes, those guys, who short wbtc and/or eth are likely using stables as collateral, not anything else.\nSuggested increase in RF for ETH and WBTC actually benefits both users and protocol, allows us to capitalise on our market strength (amazing deal on borrowing eth and btc, for those users who want to bet on that) and in the long run reserves accumulated from such adjustments are likely way overshoot anything we going collect from stable coin markets in usd value in current market conditions. Though, let’s not call that profits, as currently protocol spends more than it earns, so it’s kind of non-profit system really.\nYes, i am aware it’s probably hard to push something like this through governance, and for those people, that going to complain about how we could take away mighty 0.15% APR away from suppliers. Well, if you supply 1 million of usd in WBTC, that going earn about 1600$ per year, or about 133$ per month from 1M supplied. That is dust. It only makes accounting more complex, rather than bear any financial meaning. On the other hand, that supplier is going to recieve more than 1600$, but distributed in COMP instead. He can sell and convert it to wbtc if he choose so and come ahead. Or keep it in COMP.\nInterest for Supply side of ETH and WBTC markets is dust for pretty much any individual user. But all interest combined do have meaningful value for a protocol as reserves. By replacing that interest with COMP for users, we do good service for both users and protocol. It’s a beautiful and meaningful improvement, while moving RF by 25 basis point for stable coin market is likely not. Especially when motivation is like: we are not sure what going to happen, so let’s try that.\nYes, that RF increase for WBTC and ETH isn’t going to be forever, of course. COMP are not unlimited, and while we can continue distributions for years to come at the current rates eventually they will run out. It’s a temporary measure indeed, maybe for several quarters, maybe for a year. That is going to be adjusted down when we gather more data on effects. But in the mean time we going to get bigger reserves and likely higher total borrow for those markets.\nTo Summarize it i suggest:\nINCREASE RF for WBTC and ETH markets to 0.75 with simultaniously doubling suply side distribution of COMP for those markets to compensate Suppliers.\nLEAVE DAI RF intact for now until we have results from USDT adjustments.\nINCREASE USDT RF to 0.1\nIncrease of USDT RF will increase reserve growth for protocol with possible decrease of Supply side of USDT market, which short term might increase reserve growth even more. Long term if borrow side turn to be elastic to Supply side decrease it might decrease reserve growth. I’d expect it will still net higher reserve growth than before increase of RF anyway.\nAlternatively we can decrease USDT RF to 0.05 instead. That action will immediately decrease reserve growth for that market and slightly increase Supply rate for that market. It might increse total Supply for USDT, (especially as long as borrow dai/usdc, converting it to usdt and resupplying it as USDT is profitable activity in Compound), which in turn will decrease borrow rate, which might increase borrow side. I do not expect though, that it will be enough to compensate for loss imposed by decrease of RF at first place, so we highly likely are going to end with net loss of reserve growth for that market.\nI strongly believe that RF adjustments for ETH and WBTC is an improvement for both protocol and users, while adjusting RF for stables is kind of less impactful at the current conditions. I still would expect that bringing RF for USDC and USDT to 0.1 in the long run likely going to work well for protocol without asking to much from a users.\nGoing above 0.1 RF for stables will likely work (as it works for dai), but that is trying to extract more value for protocol at expense of users. Question here is protocol about profit, OR protocol is about common good? Not for profit, but only for covering own expenses. (which means protocol isn’t tuned to generate profits for itself, but rather as balanced system, on top of which users can build for profit enterprises)\nI think we need more options for snapshot, as increasing RF for DAI isn’t something i am willing to advocate for. Let’s concentrate only on USDT for now.\\nWe received some great questions from the community (some of which we paraphrased for brevity). Our feedback is below. Our upcoming Snapshot will present the same options outlined initially above.\nWe plan on publishing a Snapshot vote on Wednesday, August 31.\n\nWhat specific criteria would validate further decreases in these reserve factors? What do you think the timeline looks like for both observing that validation and the subsequent proposal of further decreases?\n\nTVL (supply) of DAI and USDT is where we’d expect to observe the primary impact. Two factors that create noise in the system are market volatility and the behavior of large whales, who can have an outsized impact on TVL. For this reason, we will analyze not just total numbers but also individual accounts to observe behavior after the change.\nNext, it is valuable to consider secondary effects on total borrows against the collateral. If the reserve factor decrease does result in increased supply, we expect borrowers to adjust their positions to maintain similar borrow usage as before. We previously observed this behavior in response to collateral factor changes.\nFinally, to estimate the impact to the protocol, we will compare the net interest generated for the protocol after the change (reserve factor * borrower interest paid) and compare that to the interest generated the day before the reserve factor change went into effect. If total supplies, borrows, and reserves increase due to the change, we would consider impact as a positive outcome validating that users are elastic to reserve factors. As mentioned before, this will need to be considered in the context of external factors such as market volatility and whale behavior.\nRegarding timing, we’ve observed with collateral factor changes that some users react very quickly while others can take weeks to adjust their positions. We recommend waiting for ~one month between subsequent reserve factor changes to observe the effect.\n\nWhy did you choose .025 as the change increment?\n\nThe increment was selected based on the scale in statistical analysis. This increment is expected to be large enough to show impact if users are elastic to the change since the relevant stables here have reserve factors that are already low. This represents a 33% decrease for USDT and 17% for DAI. It is prudent to avoid making drastic changes since yield farming is largely an all-or-nothing practice. Once yield farming becomes unprofitable, we’ve observed that users pull their liquidity from a protocol and that liquidity might not return.\n\nHow about increasing one reserve factor and decreasing another?\n\nChanging reserve factors in two different directions, as suggested by @Blockchaincolumbia, creates noise in the data. While the suppliers of each collateral are generally separate profiles, making opposing changes introduces an additional variable where suppliers might flow from one asset to the other. It would be hard to distinguish if the change was due to a change in strategy/risk appetite or to elasticity to the yield impacted by the reserve factors. For this reason, we do not recommend this approach.\n\nEffective rates are already higher on Compound for LPs than the market due to COMP rewards. Should we raise RFs and incentivize supply with additional COMP instead?\n\nThe COMP rewards are indeed what make supplying on Compound more profitable due to their impact on the effective interest rates, but these returns fluctuate with the price of COMP. More importantly, a strategy that relies on incentives in this manner presupposes that those incentives will last forever, a strategy which @sirokko already pointed out is not sustainable 1, particularly in a bear market. If the goal is to uncover elasticity of users to the real interest rates and reserve factors, then we should not add additional inflation to the interest rates by increasing COMP reward spend.\nThe behavior currently observed related to USDT borrowing is only profitable due to COMP rewards (and is therefore not sustainable long term). See the following example.\nRates without COMP incentives\n\n\n\n\nToken\nSupply Rate\nBorrow Rate\n\n\n\n\nUSDC\n0.73\n2.14\n\n\nDAI\n0.72\n2.22\n\n\nUSDT\n1.39\n2.96\n\n\n\nRates with COMP incentives\n\n\n\n\nToken\nSupply Rate\nBorrow Rate\n\n\n\n\nUSDC\n1.15\n1.01\n\n\nDAI\n1.23\n0.90\n\n\nUSDT\n1.52\n2.71\n\n\n\nIf one were to borrow USDC, convert it to USDT, and resupply it as USDT, they would make around -0.75% interest without COMP incentives and 0.51% interest with COMP incentives, plus however much interest they are making from the original collateral they put down. For instance, if someone were to supply DAI, borrow 83.5% of it (the maximum allowed) as USDC, convert it to USDT, then resupply the USDT, they would make around 0.09% interest without incentives and 1.66% interest with incentives. If we were to decrease reserve factors for USDT, the first order effect would be an increase in supplier interest rate for USDT, which may make this more profitable without additional COMP rewards.\n\nShould we change the ETH reserve factor?\n\nThe challenge with this suggestion is that the ETH merge is upcoming. It would be difficult to attribute observed supply/borrow behavior changes to a reserve factor parameter change with that backdrop.\\nSnapshot is live for voting below. Voting ends September 9th, at 2 PM PT.\nhttps://snapshot.comp.vote/#/proposal/bafkreibb7hbrrgz5a2xdg55uzhynfotw7xadn45wefzr3uecazhcnu2oeq 15\\nNote: it would be great if the Snapshot could include cCOMP somehow.\nCompound III is still in a preliminary testing phase with limited supply caps; I would like to contribute the following thought:\nIf Compound III is demonstrated to be more efficient, it may make sense to migrate reserves (from a version of the protocol with excess reserves) to a version with too few reserves–or deployments on multiple blockchains. This would change the calculus for the v2 Reserve Factors–potentially to one in which the community chooses that the better direction is to leave them unchanged, or even raise them.\nMy individual preference is to “wait” productively–by designing a framework for Target Reserves for any given deployment based on total borrowing volume, or some other metric, that the community could then use to determine which deployments are most over or under reserved given the level of risk to users."
  },
  {
    "number_of_comments": 11,
    "postid": "0878d483-8616-4b12-9cb7-e22061335bf6",
    "posturl": "https://www.comp.xyz/t/upgrade-compound-ii-s-oracle-to-uav3/3826",
    "combinedcontent": "\nAbstract\nThis proposal is to upgrade the Compound II UniswapAnchoredView (UAV) Oracle. The upgrade will update the UAV’s reference prices from Uniswap V2 pools to Uniswap V3 pools.\nNote that this proposal references the CIP-style format.\n\nMotivation\nA significant amount of liquidity on Uniswap V2 has shifted towards Uniswap V3 as liquidity providers capitalize on concentrated liquidity provisioning strategies. Compound’s UAV Oracle, which serves as a reference anchor mechanism for pricing data ingestion, must follow this shift of market liquidity to provide higher reliability and security assurances by reducing the risk of price manipulation or other issues related to low liquidity. This proposal aims to address this by updating the UAV to check the prices on Uniswap V3 instead of Uniswap V2.\n\nProposal Details\nBelow are details that have stayed the same from the original proposal 18.\n\nThe new UniswapAnchoredView (UAV) no longer records and keeps observations but instead queries the respective Uniswap V3 pool’s observe function to get a TWAP with the set anchor period when it’s needed (i.e., when a reporter calls validate on the UAV).\nThe price of an asset in Uniswap V3 is a function of the “tick” of the pool. The formula is price = 1.0001^tick. The math required for conversion between posted prices and the Uniswap V3 TWAP and the UAV representation has been modified to support this. Relevant libraries (TickMath and FullMath) have been included from the Uniswap V3 codebase.\nThe tests from the original UAV have been adapted to the new Uniswap V3 architecture - i.e., observations/TWAP tests were removed. Hardhat (+typechain) has been implemented so real Uniswap V3 pools can be used via forked mainnet in tests and eliminate the need for mocking Uniswap V3 pools.\nIncreased the Uniswap observation cardinality to 150 on pools that were below 150. “cardinality” in Uniswap is a term for the TWAP lookback history. It represents the state changes per block, so ten trades in a block get condensed and recorded as one update in the lookback. To save on gas costs, the cardinality starts at 2, and anyone can pay the gas cost for a deeper lookback. Therefore, increasing the cardinality to 150 ensures a safe minimum for the lookback history.\n\nWe note that a reduction in liquidity within Uniswap v3 pools can make the TWAP price in those pools easier to manipulate and may cause the Uniswap anchor to be less accurate than expected. However, this will not affect the accuracy of the primary Price Feeds from Chainlink.\nNotably, the main change from the last implementation is to update the getUnderlyingPrice function to fetch the token configuration using the cToken address instead of calling the underlying function from the cToken. The reasoning is that getUnderlyingPrice fails when passing in the address of the cETH token as cETH does not implement the underlying function like other cTokens. The community can review this change in PR14 6 in Github.\nThe new UAV3 contract is deployed here 10, which matches the changes from PR14 outlined above. We encourage the community to inspect this instance and provide any feedback.\n\nRationale\nThe rationale behind the proposed design goes back to when the UAV was initially created. After the DAI liquidation event 2, in which the Coinbase oracle deviated from the market-wide price for DAI/USD, the Compound community came to a consensus to upgrade Compound II’s oracle system 3 to use Chainlink Price Feeds. The community had also determined to develop a custom implementation using the Uniswap Anchored View (UAV) to have a “sanity check” to ensure data from Chainlink Price Feeds were within acceptable bounds of the time-weighted average price of the token/ETH pair on Uniswap v2. To learn more about the UAV’s architecture, please see the Compound Docs 3.\nHowever, the custom implementation of the UAV is very different from how other protocols integrate Chainlink Price Feeds. These protocols, such as Compound III 6, use Chainlink Price Feeds directly to retrieve price data. For alternative considerations for this and future proposals, the direct integration method is the preferred route for a streamlined implementation while minimizing the potential for false positives from a reference anchor.\n\nExpected Impact\nThe new UAV3 is deployed 3 in the proposed state with Price Feed aggregator contracts pushing pricing data to it. Currently, the contract is functioning as expected and without issue. If an on-chain vote passes this proposal, the upgrade will immediately take effect to utilize Uniswap V3 prices in the reference anchor.\n\nSecurity Considerations\nAs with any protocol enhancement, important security implications have been taken into serious consideration to avoid any unintended impact on the Compound II protocol, as the UAV influences its core markets. The original code was audited, and we have worked closely with the OpenZeppelin team to review the small code change and performed tests to ensure a smooth transition to the new UAV. This includes:\n\nQuerying the price directly with the new function to retrieve the correct value without reversion\nComparing the configuration for cETH with the params file and old versions of the same contract\n\nLinked here 6, the community can review the result of a successful test that:\n\nCompares the prices for cTokens with their addresses’ underlying prices\nCompares the proposed UAV and production UAV\n\nYou can also run these tests by following the instructions here 1.\nLastly, any updates to Compound’s oracles will follow the same rigorous testing processes. As mentioned in @cylon’s post here 2, this includes:\n\nProposal Simulations - comprehensive simulation testing before submission for a governance vote.\nChange management - ensuring that code deployed on-chain always matches the versions of the Compound protocol repository to maintain high quality and readability.\n\n\nNext Steps\nAs discussed on the developer community call, we want to allow the community to review the updates before going to an on-chain vote this week. We will post an update once the proposal has been submitted on-chain so that the community can prepare to perform post-deployment checks to ensure everything is working correctly.\nWe look forward to completing the UAV upgrade and invite the Compound community to share their feedback.\n\nCopyright\nCopyright and related rights waived via CC0.\nThis contribution is offered “as is” without warranty of any kind.\\nThanks @CL_Michael for the detailed post and overview of these changes. I can confirm that the OpenZeppelin team worked with ChainLink to ensure that the fix and simulations were in order.\nI also appreciate this example of how the CIP format 2 can be utilized for protocol changes without needing to be submitted as a CIP.\nIn this case, we had discussed on the last community call that this change did not require a CIP due to the fact that these changes have already been accepted by long-standing community consensus in the forum and that the security issues have been addressed through a thorough review. That being said, we would always appreciate additional eyes on this from other community members to be sure.\\nThanks – this proposal makes sense to me, and I support it.\\nThe post on the governance dashboard says there were no integration tests. Compound 7.\n\nScreen Shot 2023-01-11 at 11.08.33 AM1258×202 24.8 KB\n\nI reviewed the PR and I could not find any integration tests. Fetch underlying price using ctoken by cds95 · Pull Request #14 · smartcontractkit/open-oracle · GitHub 7\nI also reviewed both the open and closed pull requests on the compound v2 main repo where I did not find any integration tests. Pull requests · compound-finance/compound-protocol · GitHub 4\nThis is a protocol that manages billions of other people’s money. Please do the responsible thing and prove the protocol works after the upgrade with an integration test.\\nHi @elliot , thank you for reviewing.\nWhat you’ve highlighted in the submission states that GFX has not simulated it, but you can find the successful tests in a separate PR here 2 in the open-oracle repo 3. Further explanation from the original post:\n\n\n\n CL_Michael:\n\nLinked here , the community can review the result of a successful test that:\n\nCompares the prices for cTokens with their addresses’ underlying prices\nCompares the proposed UAV and production UAV\n\n\n\nThe test suite contained in the PR was run on a mainnet fork by our team to ensure they work correctly with the live contracts deployed. The deployed implementation 2 can also have its functions called directly to confirm that price reporting works successfully on all existing markets.\nAdditionally, you can review the previous tests in the open-oracle repo and this integration test 2 in another PR. This test also called the current live UAV, and our live proposed UAV then compared the results for all the markets. All prices matched without reverting.\nOpenZeppelin also confirmed the simulations here:\n\n\n\n cylon:\n\nI can confirm that the OpenZeppelin team worked with ChainLink to ensure that the fix and simulations were in order.\n\n\nTo be more comprehensive, we have merged another PR 1 with further test coverage. It includes a forked mainnet test 1 and a script 1 to hit mainnet directly and compare the output.\nWe encourage community members to run the test suite or do their simulations to validate our findings.\\nTo follow up on what @CL_Michael has shared, OpenZeppelin performed our own separate simulations using Foundry to confirm that the fix was successful and the price reporting works as expected.\nWhile we don’t normally publish the testing scripts used in our security reviews, I’ve had our auditors make an exception in this case given the past incident. This should serve as additional validation of the upgrade’s safety on top of the more comprehensive tests that ChainLink has already run and published. Community members are welcome to review and run the scripts themselves in the repo below.\n\n  \n      \n\n      GitHub\n  \n\n  \n    \n\nGitHub - OpenZeppelin/proposal143 8\n\n  Contribute to OpenZeppelin/proposal143 development by creating an account on GitHub.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\\nThanks for the details @CL_Michael @cylon, this is reassuring. By the way, why aren’t the tests that are written for each proposal published ? I think that this is very valuable for the community.\nIn the meantime, we (Morpho Labs) also did some quick integration testing of the new oracle contract, that you can find here: GitHub - morpho-labs/compound-UAV3-integration-test 5\\nI too have concerns about proposals without corresponding simulations, so I’ve started to create some (example) scenarios that can be used for testing v2 proposals:\nSome basic v2 scenarios by jflatow · Pull Request #660 · compound-finance/comet · GitHub 4.\nThis only covers half the actions in the top 4 markets, it would be nice to have these be a bit more sophisticated, but the cool thing is that using the Comet scenario framework, we can trivially run any scenarios written like this against any future or pending proposals. My hope is we can add to this optional v2 scenario suite, and also make v2 proposals using this framework (Compound Labs is already doing this for all proposals), in order to increase confidence.\\nIt’s great to see the tests published for the community to see!\nOne final item that seems like it needs to be completed before this proposal executes is the ownership of the new oracle needs to be transferred from the EOA that deployed it to the Compound Community Multisig.\nUniswapAnchoredView | Address 0x50ce56a3239671ab62f185704caedf626352741e | Etherscan 2 is owned by the deployer Address 0x61E5E1ea8fF9Dc840e0A549c752FA7BDe9224e99 | Etherscan 1\\nThank you to everyone who reviewed and supported the proposal. The vote has passed, and we have initiated the transfer of ownership of the UAV for the Compound Community Multisig 3 to accept.\nTagging the current multisig signers for visibility: @TennisBowling @arr00 @blck @jared @brendan_dharma & Gauntlet (@pauljlei)\\nI’ll try to get this through the multisig asap\\nThank you to everyone who supported the UAV3 upgrade and helped ensure a smooth transition.\nNow that it has been successfully running for over a week on Compound V2, we plan to deprecate the old UAV. Please be advised that we are targeting the shutdown for Friday, Jan 27. If any other protocols or users still reference the old UAV contract, please migrate to the current UAV contract to avoid any issues."
  },
  {
    "number_of_comments": 12,
    "postid": "fe261c9b-6c84-4e5f-a0d6-13911b8921ae",
    "posturl": "https://www.comp.xyz/t/add-market-lusd/2623",
    "combinedcontent": "\nWhat is Liquity?\nLiquity is a decentralized borrowing protocol that allows users to draw interest-free loans against ETH as collateral (akin to MakerDAO). Loans are paid out in LUSD (USD-pegged stablecoin) and need to maintain a minimum collateral ratio of 110%. Loans are secured by the Stability Pool 3, where users can deposit LUSD that may be used to instantly repay uncollateralized debt. In return, they receive ETH collateral when liquidations occur and continuous LQTY rewards (Liquity’s secondary token capturing the protocol’s fee revenue).\nLiquity recently launched on April 5th, 2021 and has attracted ~$2.5B TVL since then, ranking among the top 10 projects listed on DeFi Pulse. In addition, LUSD is also ranked among the top 10 stablecoins on DeFi Pulse and is the 2nd largest collateral-backed stablecoin behind DAI. Source 4.\n\nDecentralization\n\nThe Liquity contracts have no admin keys and are accessible via multiple interfaces hosted by third-party frontend operators, making it censorship resistant.\nThe protocol is immutable and governance-free, as all operations are algorithmic and fully automated, and protocol parameters to maintain system health were set at time of contract deployment.\nLUSD is only backed by ETH and no other collateral types may be added.\n\nAs concerns and regulations continue to arise around the stablecoin landscape, it’s clear that the DeFi ecosystem needs decentralized alternatives. With LUSD being fully decentralized, censorship-resistant, and already detached from governance by design, it seems like an obvious choice as a hedge against regulatory risk for Compound and its users.\n\nLUSD Peg Maintenance\n\nRedemptions: An arbitrage mechanism that allows users to redeem LUSD for ETH at face value (i.e. 1 LUSD for $1 of ETH minus fees) against the riskiest Troves (loans). In other words, when LUSD is below $1 on the open market, users can redeem (exchange) their LUSD for the underlying ETH collateral 1:1 within the protocol. The purpose of this mechanism is to pull excess LUSD out of circulation (i.e. deleveraging users), resulting in LUSD’s price going back up to $1.\nMinimum Collateral Ratio (MCR): Thanks to Liquity’s instant liquidation mechanism, Troves are allowed to maintain a CR as low as 110%. This means that if LUSD ever exceeds $1.1, users can open Trove at the minimum CR and sell it for an instant arbitrage profit.\n\nBesides these two hard-peg mechanisms, LUSD also maintains its peg through less direct mechanisms covered here 5.\n\nLUSD Stability\nWhile the protocol is still relatively young, LUSD has been trading within a narrow range of 0.991 - 1.002 against DAI on Curve (14d stats) — where LUSD is most liquid.\n\nLUSD Price1884×243 37.9 KB\n\nSource (Snapshot taken on 11/17)\n\nProtocol Stability\nAs a borrowing protocol, minted debt (LUSD) and system health is heavily dependent on liquidation efficiency. In contrast to collateral auctions, Liquity liquidates under-collateralized Troves instantaneously — substantially reducing the possibility of protocol loss and the risk of LUSD becoming partially unbacked.\nAround one month after launch, the protocol faced its first stress test (flash crash on 5/19) and liquidated $93.5M against the Stability Pool with 0 protocol loss. Liquity has completely innovated this necessary function of borrowing protocols by creating a liquidation mechanism that’s fast, effective, and works under extreme conditions. More info here.\nIt’s also worth highlighting that almost half of all positions got liquidated, going from ~1130 Troves to ~496 Troves within a 9 day span. Since then, Liquity has seen consistent growth and is now at a new all-time-high of >1200 Troves.\n\n# of Troves937×398 18.3 KB\n\nSource\n\nLUSD Utilization\nAlthough there is currently ~$550M LUSD sitting idly in the Stability Pool, it doesn’t need to be as large as it is considering it could have managed liquidations just fine if it were 5x smaller based on the liquidation volumes of 5/19. We also expect it to decrease in size as meaningful integrations appear such as an LUSD market on Compound. At time of writing, the Stability Pool dominance is hovering around the all-time-low of ~64% and is on a downtrend as new integrations are becoming available.\n\nScreenshot (113)932×392 41.5 KB\n Source (Snapshot taken on 11/17)\n\nLiquidity and Volume on DEXes\n\nSource 2\n\nSource\n\nSource\n\nMotivation\nAlthough Liquity has favorable borrowing parameters such as the MCR at 110% and being interest-free, some users would prefer borrowing LUSD on Compound for three main reasons:\n\nBorrowing LUSD against other collateral assets besides ETH\nProtection from redemptions\nProtection from Recovery Mode in Liquity, which allows any Trove below 150% collateral ratio to be liquidated.\n\n\nAsset Parameters\nWe’d like the Compound community to decide on what the following parameters should be for LUSD:\n\nCollateral Factor:\nReserve Factor:\nBorrow Cap:\nInterest Rate Curve:\n\n\nAudits\nTrail of Bits Security Assessment - January 2021\nCoinspect - March 2021\nTrail of Bits Liquity Protocol and Stability Pool Final Report 1 - March 2021\nTrail of Bits Liquity Proxy Contracts Report 1 - March 2021\n\nContracts:\nLUSD Token Address: 0x5f98805a4e8be255a32880fdec7f6728c6568ba0\nChainlink LUSD Price Feed: 0x3D7aE7E594f2f2091Ad8798313450130d0Aba3a0\n\nResources\nWebsite 2\nDocumentation\nDune Analytics \nTwitter\nDiscord \nBlog\\nBy far one of the best stablecoins out there as it is one of the few that make borrowing against eth a one time fee.\nI support listing LUSD on compound 100% asap. These are the kind of stablecoins you can use as collateral in compound.\nPls include LUSD\\nSound proposal, hence it‘s a yes - please add LUSD!\\nThank you for setting it up @Derrick , @rleshner  could you give us your opinion, probably LUSD is one of the very few coins in the entire defi space that has absolutely no parameters to tweak and provides stability-as-service model. Few notable mentions:\n\nhttps://twitter.com/CurveFinance/status/1462876062036238341 15\nSCCP-150: Wrappr Parameter Changes (L2) 7\n\\nSupport the LUSD addition to the platform. This is that algorithmic decentralized stablecoin that can become standard in DEFI. Strong technical team, several contracts audits, strong backers, significant TVL.\\nStill, for support, you need to be known about you. The best marketing is to list LQTY on one of the well-known exchanges. Otherwise, one and a half diggers will monitor your progress.\\nI’d like to further vouch for the addition of LUSD. DeFi needs a trust-minimized dollar-pegged coin with trustless collateral (ETH only), and we haven’t had that since sai (single-collateral dai) was wound down. The Liquity team took trustlessness a mile further by excluding all forms of governance from the design and deploying all contracts as non-upgradable. This feat is basically singular across the whole stablecoin space.\nAnyone on Compound who values trust minimization must currently settle for dai, which, although less centralized than other supported stablecoins, is still a weak substitute for this role. This is due to dai’s reliance on fiat-backed stablecoins for stability (USDC, USDP), the inclusion of other custodied assets in its collateral portfolio (WBTC, home improvement loans 1, etc.), a monumental governance overhead, the governance capture implicit in that, increasing intertwining with the legal system 1, and a degrading level of trust 1 in the decentralized nature of its governance (predicted 5 years ago by its co-founder).\nI’ll also add a longer-term view of the stability chart Derrick posted. This one shows LUSD’s price in dai, USDC and USDT on Curve since the genesis of Liquity. (Up-to-date chart here 3.)\n\nl1541×371 54.5 KB\n\nApart from the early fluctuations and the lasting distortion of the reward program in the first few months, you can see a few off-peg spikes. The “largest” ones (in quotes, since both were under 4%) happened at the end of May and the end of October. In both cases, the peg successfully returned to normal values in a few days, which was a battle-test validation that Liquity’s fully automated peg mechanisms work.\\ncToken for LUSD: CErc20Delegator | 0xc0da79a6a0f255ed6d31a8ffd719c19a52aa5a36 17\\nA proposal for this is going to be submitted very soon, first with a 0% collateral factor.\nThen a subsequent proposal will significantly increase the collateral factor (I think we’ve been forgetting this second part for some of the new assets added)\\nThanks @JacobPPhillips . With regards to increasing the collateral factor, Gauntlet can set CFs (as well as the other risk parameters) for all assets, so we are happy to coordinate together on that. Our platform ingests on-chain data in order to determine risk parameter settings that best optimize capital efficiency and manage risk according to market conditions. A summary of our December parameter updates can be found in this risk review here 1 and we look forward to setting parameters for new assets like USDP and LUSD.\\nExcited to see this moving forward!\n@JacobPPhillips @Derrick would either of you like to speak about the upcoming proposal on next week’s developer community call?\nThe call will take place at 9:30am PT on Wednesday (1/12) in the community-dev-calls channel of the Compound Discord. Please let me know if so and I’ll be happy to add you to the agenda.\\nHappy to join the call \\nFantastic – looking forward to having you on the call tomorrow at 9:30am PT!"
  },
  {
    "number_of_comments": 24,
    "postid": "d8464f0e-8499-4f23-8c10-05967bf0e474",
    "posturl": "https://www.comp.xyz/t/rfp12-implementation-ctoken-cleanup/2694",
    "combinedcontent": "\nTldr;\nThe Equilibria Team 19 has implemented various changes to the core cToken contracts as part of RFP12: cToken Cleanup 38.\n\nBackground\nThe existing cToken contracts are written in Solidity 0.5.x and 0.6.x. While not extremely old, these versions lack some of the newer features present in later versions of Solidity like custom errors, checked math, and gas optimizations. By upgrading these contracts, we can unlock a number of quality of life improvements for developers, decrease gas costs, and position the codebase to be upgraded to newer versions at a more regular cadence.\n\nProposal\nThese changes implement the following:\n\n\nUpgrade the Solidity version of the cToken and related contracts to 0.8.6 - all contracts in the repo were changed to 0.8.6 but only the cTokens will be upgraded as part of upcoming governance proposals. This is due to the complexity of having multiple Solidity versions in the same repo.\n\nRemove the usage of SafeMath and CarefulMath in favor of Solidity 0.8’s checked math - Solidity will now automatically revert when math errors occur (overflows, division by zero, etc)\n\nRemove the custom errorCode return values in favor of reverts and custom errors - this allows for a more structured way to deal with errors rather than enum or string comparisons.\n\nFor more detailed code changes, please take a look at the PR which can be viewed here: [RFP12] CToken Cleanup by arjun-io · Pull Request #152 · compound-finance/compound-protocol · GitHub 30\nIt is important to note that the goal is to have no behavior changes in the happy path case, and to only move away from errorCodes and to revert in the failure case (both math errors and checks). All existing unit and scenario tests should pass with only changes to the error code cases.\n\nStatus\n\n[x] Code written and tested\n[x] PR made for review - PR #152\n\n[x] Changes audited - link will be posted once audit is published\n[x] Audit remediations implemented - the PR will be updated with audit remediations once the audit is published\n[ ] Create upgrade proposal(s)\n\nWe plan on creating proposals to upgrade low TVL cTokens first\n\n\n\\nThe audit report has been published and can be viewed here: Compound – cToken – Chainsecurity 30. We have also pushed all audit remediations to the GitHub Pull Request\\nI am very excited to get this into production.\nAfter reading the (very well written) Chain Security audit, I wanted to highlight point 7.1 in particular since the community has been talking more about asset listings recently.\n\nIn the future, new tokens might be added. When markets for those are created, issues can appear. In this non-exhaustive list, we highlight some of those issues:\n• On-demand Balance Modification + Callback:\nDifferent token types (inflationary, deflationary, or rebasing) can have balances which change without a Transfer occurring. For some of these tokens there is a permissionless trigger to update everyone’s balances. Tokens with such a permissionless trigger and a callback on transfer should not be added for the following reason. While receiving the callback of mint() the depositor could trigger the balance adjustment and thereby increase the ERC20 balance of the market without making a deposit.\n• Blacklist, Freezable, Seizable:\nTokens where some addresses can be blacklisted, certain funds can be frozen or some funds can be seized/burnt, need to be added with great consideration. A blacklisted market would stop working properly. A (partially) frozen market would not function correctly (as the underlying fungibility assumption is violated). Finally, seizing could lead to sudden drops in the exchange rate.\n• Transfer Fees:\nIn principle the protocol supports tokens with transfer fees. However, if a user borrows a certain amount of tokens with transfer fees, it will be almost impossible to completely repay that borrow. This is because the existing feature of providing -1 as the amount wouldn’t work due to the transfer fees. Hence, a small borrow residue will most likely remain.\nWhen borrowing tokens with transfer fees, the requested amount will not be received. Similarly, when reducing the reserve of a token with transfer fees, there will be unexpected losses.\n• Tokens with potential for sudden increase in value:\nIf a token whose value can suddenly increase by a significant amount, can be borrowed, then attacks due to extremely bad positions are possible. Such tokens include UniswapV2 and Curve pool tokens, but also DPI tokens. Extreme care has to be taken, when adding such tokens to the protocol as they will most likely lead to an attack.\n\nInflationary, deflationary, and rebasing tokens pose the most risk to Compound (even with a 0 cf). The protocol should continue to consider adding new assets carefully. One of the first things I check when reviewing new assets is the mint functionality. Either minting needs to be impossible or if possible, it needs to be clearly documented what is possible and two what extent.\\nProposal 80 6 to upgrade cSUSHI has been created!\\nJust so everyone is aware, Proposal 80 has not been audited by the OpenZeppelin team. I discussed this with @qlo. Since the scope of this proposal is limited to cSUSHI and Chainsecurity has already performed an audit, the risks appear low.\nCommunity voters should proceed at their own discretion knowing OpenZeppelin cannot yet vouch for this proposal’s security although we can say that Chainsecurity auditors have done excellent work in the past. We are planning to perform our own audit of this refactor in the near future before it gets deployed more widely.\\nThrew in a No vote on this for Polychain. We may as well delay slightly this to let OZ take a look.\nNonetheless, great improvement, and thanks to Equilibria for taking lead on this. Hopefully you guys are getting paid for this work.\\nShoot, just saw @cylon  from OZs comments in the gov chat.\n\"Yeah, it would take us some time to get it full audited. It’s already received an audit from Chainsecurity and since the rollout is limited to cSUSHI the impact isn’t massive. According to qlo , this is meant to be a limited rollout anyway.\nI wanted to make sure people knew OZ hasn’t looked at it yet just in case that was assumed. We do plan to give it a solid look before it rolls out further.\"\nWhat’s the benefit of rolling it out and then having OZ take a look? The extent to which this improves comp as a product feels so minor that the very slim chance a bug that hurts COMPs rep of some sort probably overweighs the benefit of getting it into prod sooner rather than later?\n(Worst case, PC can always void our vote)\\nHi all - we have re-proposed the cToken upgrade following the completion of the OpenZeppelin audit: Compound 4\\nThanks for all of the effort! This is something I’ve been looking forward to for a while.\nI’m thinking, maybe it’s best to only upgrade one or two really small markets - USDP and SUSHI?\nCould you please link to OZ’s audit report?\nAlso, have you written simulations for the governance proposal to ensure the proposal executes correctly and that the upgraded markets all function correctly?\nThanks so much!\\n\n\nWe know that this is a blocking change for a lot of other work so we wanted to get this out a bit quicker - the additional audit by OpenZeppelin gave us more confidence to tackle a few more markets. If the community would rather do fewer markets in the initial proposal we’ll be happy to re-propose\n\n\nWe’re requested OZ post the updated audit pointing to our commits, but for now here is the initial report: OpenZeppelin Security Updates for February 2022 - #5 by cylon 12\n\n\nWe do have an internal repo for integration testing our proposals via mainnet forks. It’s not currently public but we can look into making it public Here’s a link to the COMP096 test: governance/CompoundcTokenUpgrades.ts at master · equilibria-xyz/governance · GitHub 3\n\n\\nI can confirm this new CToken implementation has been audited and all issues have been resolved and/or noted in this report: Compound-TUSD Integration Issue Retrospective - OpenZeppelin blog 15\nExcited to see this move forward after a long wait.\\nProposal 101 to upgrade mid-TVL cTokens is up and will start voting tomorrow: Compound 4\\n@qlo exciting to see the modern implementation rolling out across additional markets \\nHi, what is a sufficient time frame after changing to this new implementation to deem it safe to use in new asset listings? Also, is there an official version of this implementation on test net?\\nWould love for OpenZeppelin to confirm, but the new implementation likely should be the default for all new asset listings; this will reduce discrepancies and future migration work; the protocol is currently on a path to replace all legacy implementations with the new one.\nThe new implementation has now been audited twice, and is currently live for 5 markets.\\n+1 to what Robert said - from the Equilibria side we encourage using the new implementation going forward for new markets. For testnet development, we haven’t deployed anything official to testnets yet - is there a specific testnet you’d like a deployment on? We can get that up soon.\\nGreat to see this work and to see RFP’s 6 getting knocked out!\\nWe have deployed implementations to some testnets:\n\nKovan 2\nRinkeby\nGoerli 3\nRopsten\n\\nthank you for deploying on testnets!\\nAgreed. The new implementation should be the default for all new asset listings going forward.\\nHi all! The final upgrade proposal is up 6 and will start voting soon \nOur integration test to verify the proposal can be viewed here 5\\n\n\n\n qlo:\n\nThe Equilibria Team  has implemented various changes to the core cToken contracts as part of RFP12: cToken Cleanup .\n\n\nFrom what I tell by clicking on the link for RFP12, RFP12 was $250,000, but the current proposal is requesting $350,000.  “In addition to updating the implementation, it grants $350,000[1] worth of COMP to Equilibria for the upgrade work and 79,764.36 USDC to Compound Labs for the ChainSecrity audit of the protocol[2].”\nDid the work completed also cover RFP13 or 14?  Or is there perhaps another explanation (or perhaps, other discussion/thread) that explains the difference in the amounts? Thanks for any clarification.\\nYes! Sorry for the confusion - we should have mentioned in earlier posts that the upgrade also has some gas savings, about 8k total across the main mint/redeem/borrow flows. So after discussing with the Compound Labs team we thought it was fair to also claim RFP14. You can see the gas savings from the gas profiler here 6\\nHi all - we’re super happy to announce that this upgrade has been executed and live for the past two days! This concludes Equilibria’s work for RFP12 and 14 - we look forward to more productive community building \\nThank you @qlo and team! "
  },
  {
    "number_of_comments": 49,
    "postid": "65772f26-5691-4bb5-8d07-f2bd1f2fadbf",
    "posturl": "https://www.comp.xyz/t/auditing-compound-protocol/2543",
    "combinedcontent": "Summary\nA few weeks ago, Compound suffered an exploit that led to the loss of ~$50 million 40 worth of COMP. The exploit happened due to a process failure in the governance process: a governance proposal that tweaked COMP distribution rates (Proposal 62 49) had a flaw that allowed certain lenders on Compound to drain the treasury for hundreds of millions of dollars worth of COMP.\nAlthough Proposal 62 was reviewed by several community members, none spotted the exploit in the proposal code in time. In short, the process failure came down to a leaky proposal review process.\nI believe an additional complication made something like Proposal 62’s exploit more likely: historically, community members have had to shoulder the burden of arranging an audit for their proposal. This has resulted in extremely long integration times or improvements never getting implemented at all. That’s no bueno.\nProcess failures like the one we recently had should not be occurring for a protocol of Compound’s size. I’d like to propose a simple but effective solution to fix the process failure: hiring a top-tier smart contract audit firm to review every governance proposal prior to its on-chain execution. In my opinion, having a top-quality auditor review every proposal is a no-brainer that the protocol should be willing to pay for (since the magnitude of potential losses far exceed the cost of the service).\nBackground\nShortly after the exploit, I reached out to OpenZeppelin 30, which is one of the top smart contract security firms in the industry. I told them about the Compound exploit and encouraged them to consider submitting a proposal with a comprehensive set of security solutions to protect the protocol. After a few weeks of brainstorming, OpenZeppelin is now ready to post the proposal here on the forums to gather feedback from the wider community.\nTo be as transparent as possible, I’ll share just a few more details. I reached out to OpenZeppelin first because I had a personal relationship with the team: we worked on a few projects together in the past and I have great respect for the work they do and the team they put together. In addition, Reverie (the company I co-founded with Derek Hsue 9) is an advisor to OpenZeppelin and Forta (a new protocol launched by the OpenZeppelin team). In a nutshell, we stand to benefit if they do well.\nI should also note that I spoke to Trail of Bits to encourage them to propose something for Compound too. Finally, I made an attempt to provide the same background to ConsenSys Diligence, although we haven’t had a chance to speak about Compound’s needs yet.\nWhat’s Next\nAs a next step, I’d love to invite OpenZeppelin as well as the other audit firms to post their proposals in this thread. The community could then discuss the proposals and offer feedback on the scope of service, price, and other factors that are relevant to the purchasing decision.\\n\nSecurity Solutions For Compound Governance\n\nSummary\nA proposal to implement Security Solutions to prevent and mitigate loss of funds resulting from security risks introduced by community-proposed upgrades to the Compound protocol.\n\nBackground\nFor the past two years, OpenZeppelin has worked formally and informally with Compound to:\n\nPerform 10+ security audits 16 of the Compound protocol\nDevelop OpenZeppelin Governor 4, a standardized version of GovernorAlpha and GovernorBravo governance contracts, part of OpenZeppelin’s widely used open-source Contracts Library 3\n\nIntroduce smart contract security best practices guidelines, including strategies for safer governance systems 14\n\nIncubated the Forta Protocol 13, the first decentralized runtime security protocol to prevent or mitigate smart contract exploits as they occur, and has developed bespoke agent scripts monitoring Compound 11 on Ethereum mainnet.\n\nDuring that time, OpenZeppelin developed deep expertise and set industry standards on secure smart contract development, including best practices on protocol functionally, incentive structures, oracle integrations, and governance processes for protocols like Compound.\nAs events from the past weeks demonstrate, governance upgrades can introduce new security risk vectors that could result in reputational damage to the protocol and possible loss of user funds. A proven way to mitigate risk is performing ongoing security audits. OpenZeppelin prides itself on auditing each line of code manually; automated code analysis tools are no substitute for seasoned, dedicated professionals with deep knowledge of smart contract security.\nHowever, as helpful as security audits and code libraries are in identifying or preventing vulnerabilities in code, effective security requires a continuous and holistic approach.\n\nProposal\nIn the following sections, we outline the case and scope for OpenZeppelin to develop a comprehensive set of best-in-class Security Solutions for Compound throughout all stages of the governance proposal lifecycle:\n\nc11093×576 11.2 KB\n\n\nSecurity Advisory\nOpenZeppelin will provide a dedicated Protocol Security Officer (PSO) to act as Compound’s security advisor. The PSO will be the main point of contact for the Community and provide advice and guidance to help improve and maintain Compound’s governance process using industry best practices.\nThe PSO’s advice, guidance, and feedback will be focused on:\n\nGovernance process improvements\n\nIncorporating security audits and input from security experts throughout the proposal lifecycle\nSetting up procedures for threat detection, monitoring, and alerting\nProcess recommendations on use of timelock; and specific recommendations on how to extend Pause Guardian functionality into incident response procedures\n\n\nCommunity support by answering smart contract development security related questions during the governance proposal creation process, including:\n\nDevelopment best practices based on known and potential attack vectors, and types of vulnerabilities to avoid\nRecommendations on available market solutions (security tools, bug bounties, services) for improving overall technical risk\n\n\n\n\nSecurity Training\nIn order to reduce the probability of introducing new security risks in new proposals, the PSO, together with OpenZeppelin’s Education team, will provide tailored training and resources to the Community on secure smart contract development best practices. OpenZeppelin will host bi-annual workshops led by senior smart contract security researchers including:\n\nAn additional set of four workshops to outline and educate the community on unique aspects of Compound’s security landscape\nA customized, six-session training course to take place over the span of three months\nUnique course materials and certifications for all attendees, which could become a potential requirement for community members to introduce new governance proposals\n\nFor examples of educational content developed by OpenZeppelin, please refer to our Secure Smart Contract Development Series on Strategies for Safer Governance systems 14, The Dangers of Price Oracles 6, Strategies for Secure Access Controls 7, and The Dangers of Token Integration 4.\n\nSecurity Audits\nOpenZeppelin will continuously audit and provide actionable security recommendations on new governance proposals. OpenZeppelin will assign a dedicated team of security auditors to review proposed changes on an ongoing basis, and will verify that the transaction to be executed matches the code submitted. The Compound protocol will be audited as these changes occur within the governance process, and the related deliverables will be communicated upon an agreed-upon schedule.\nFor new governance proposals that meet an agreed-upon threshold (e.g. minimum # of votes), OpenZeppelin will commit to provide within an agreed-upon time frame:\n\nA detailed report with an actionable description of any issue found\nAn outline of potential exploitable vulnerabilities in the code and any unexpected behavior caused by errors in the architecture and/or logic, and additional recommendations to increase security, and a general analysis of the contract dynamics reflecting state-of-the-art security patterns\nAll governance proposals including parameter changes will be reviewed and audited\nA classification of issues according to severity with proposed, concrete fixes\nA review of any fixes to issues identified in the audit process\n\nAs the first step in implementing this Solution, OpenZeppelin will perform an initial, comprehensive audit of Compound’s deployed smart contracts over a fixed period of time. For examples of OpenZeppelin’s security audit reports, please see our blog 5.\n\nThreat Detection Monitoring and Alerting\nIn order to enable faster issue prevention and mitigate the risk associated with newly passed proposals, OpenZeppelin will provide operational and security monitoring to detect issues and threats during and after the deployment of passed proposals. OpenZeppelin will implement or enhance existing monitoring and alerting systems to provide complete coverage. This will include (as appropriate):\n\nSetup of comprehensive operational and security monitoring allowing near real-time threat detection of attacks or exploits related to Compound smart contracts and specific governance proposals\nConfiguration of alert notifications and dashboards for Compound’s designated parties and for OpenZeppelins PSO as appropriate\nValidation that all governance proposals are deployed accurately\nSupply chain monitoring (libraries, oracles or protocols used by Compound Protocol in association with new governance proposals)\n\nExamples on how the threat detection monitoring and alerting Solution works:\n\n\nVideo 15 demonstrating threat detection developed by OpenZeppelin on the Forta Protocol of the issues encountered after the deployment of Compound’s proposal 062\nExample of an agent script monitoring the Compound protocol 11 on Ethereum mainnet\nScreenshot of alert notifications configuration\n\n\nc31624×1068 64.9 KB\n\n\nScreenshot of the Forta Explorer, showing OpenZeppelin-developed alerts from Ethereum mainnet associated with the start of the recent Indexed Finance exploit 3 on October 15th, 2021, with losses of $16m\n\n\nc21600×910 136 KB\n\n\nExpectations\n\n\nDuration: the nature of Security Solutions requires certain predictability to ensure the availability of teams and resources. Therefore we propose a 1-year engagement given the high and growing demand for smart contract security services.\n\nCommunications:\n\nPSO Community responses and recommendations (2 business day/48 hour response time)\nAudit Reports published as required by community-proposed upgrades to the Compound protocol\nInitial education event series and Bi-Annual education workshops and training sessions thereafter\nMonthly forum posts and participation of PSO on community calls with explanation relating to audit and threat reporting\nDiscord Developer & Twitter Spaces Community Calls as necessary\n\n\n\nTerms of Service 11: applicable to OpenZeppelin’s provision of Security Solutions\n\nOut of scope of this proposal: Setup and administration of bug bounty services, incident and emergency response services not explicitly laid out in this proposal, crisis communications and public relations support, compliance risk management, fees associated with external monitoring services (e.g. future Forta Protocol fees), as well as any other product or service not explicitly described here.\n\n\nFee Structure\nOpenZeppelin charges a service fee for its best-in-class Security Solutions that seeks to be commensurate with the value that our solutions add to protocols. OpenZeppelin also wants to provide a strong signal of our alignment with the protocol. At the start of every quarter for one year OpenZeppelin will create a formal Compound Governance Proposal to update the service fee payment in accordance with the simple formula below:\n\nScreen Shot 2021-11-02 at 10.47.29 AM1328×124 5.94 KB\n\n\nRetainer Fee [COMP]\n\n$1,000,000/quarter, payable in COMP at the Volume Weighted Average Price of COMP of the previous month (source: Messari)\nPayable at the beginning of each quarter\n\n\nMajor Security Event Occurrence (\uD835\uDFAA) [1,0]\n\n\uD835\uDFAA=1 no Major Security Event occurred during the previous quarter\n\uD835\uDFAA=0 major security event has occurred during the previous quarter\n“Major Security Event’’ means any material smart contract vulnerability that is exploited arising from code audited by OpenZeppelin which causes a major loss of funds to users generally (i.e. a loss is not particular to any specific individual user), or OpenZeppelin audited vulnerabilities that are exploited and cause significant loss of protocol function or service to the community.  OpenZeppelin will not be responsible for events that occur when OpenZeppelin’s audit or governance process recommendations are ignored and not implemented into the Compound Protocol, which are beyond OpenZeppelin’s control.\n\n\nPerformance Fee [COMP]\n\n$1,000,000/quarter, payable in COMP at the Volume Weighted Average Price of COMP of the previous month (source: Messari)\nPayable at the end of each quarter\n\n\n\n\nAbout OpenZeppelin\nOpenZeppelin is the premier smart contract security technology and services company, trusted by the most used DeFi, NFT, and DAO projects. Founded in 2015 with the mission to protect the open economy, OpenZeppelin safeguards tens of billions of dollars in funds for leading crypto organizations including Coinbase, Ethereum Foundation, Compound, Aave, TheGraph, and many others.\\nI really like the idea of a top-tier audit firm entering into an ongoing agreement to audit the protocol. As @sukernik stated, historically, the burden of getting an audit has typically been on the contributor rather than the protocol. Establishing a structured process for making protocol changes and an additional review step by OZ will be valuable.\nIn my mind, one of the critical roles of governance is to ensure the protocol grows and, in doing so, balances security with innovation.\nOZ’s proposal at a high level seems to be robust yet flexible to the protocol’s needs. Everyone is on the same page as to the goal of attaining an auditor. So lets skip to the elephant in the room: 1-year engagement, $4m base + another $4m assuming the protocol doesn’t experience a “Major security event.”\nFor $8m, Compound gets OZ, a team who is considered the best in the industry, who has audited the protocol extensively, a dedicated OZ advisor (they call them a Protocol Security Officer), tailored training resources, four workshops to educate the community on Compound’s security, a six-session training course (a course that could be used for onboarding full-time community members), all parameter changes and protocol changes thoroughly audited, and a threat detection system.\nThe services and resources provided here seem quite valuable, but when asked to put a number on it, $8m looks relatively high.\nTo date, there have been 67 governance proposals. Of which 39 occurred in the last year. If I am generous in counting, 15 of the 39 had protocol changes. That would put each of these at just over $500k each. Even if we were to say the protocol will make 25 code changes over the next year, that would average $320k. Lets also keep in mind many of these changes are pretty small in scope. I only expect 2-5 significant changes in a year.\n30: Contributor Grants\n32: Dai liquidations pt1\n33: Remove automatic COMP claims and COMP speed refresh\n34: DAI DSR\n37: Sweep function\n42: Migration to Gov Bravo\n47: Oracle Improvement\n49: Upgrade ctokens\n50: Comptroller upgrade\n59: Dai liquidations pt2\n60: Address Whitelist for Governance\n62-65: Split COMP rewards\nI know it is a relatively new concept that DAOs hire services providers and that OZ is the best, but $8m seems very high. Not to mention a performance fee based on their ability to secure the protocol is laughable. If Compound experiences a “Major Security Event” and OZ (or any auditor) checked the code,  they should not expect to get paid further.\n@OZSecure Lets get some clarity on the pricing. A couple of weeks back, I did some back-of-the-envelope math and came up with a $3-$5m range (which I thought was already really generous).\\nThe performance fee is a strong mismatch for aligning incentives here. Let’s be pessimistic, and consider that the odds of Compound suffering a giant loss in the next year is 5%. That means that if there was zero effort to improve the security of Compound, the odds of OZ getting the performance bonus would still be 95%.\nOn the other hand, an excellent audit, and perfect checking of all proposals through the year, might reduce the odds of a major loss to 2%. This only increases the odds of a performance bonus from 95% to 98%. That’s only a 3% change in expected payoff amount between no effort vs insane effort. It just doesn’t make sense here. Compound would be paying 4 million dollars to create $120,000 worth of incentives.\n\nI agree that the price is surprisingly high and would love another look at it from them.\n\nWe at OUSD 19 have had audits from Trail Of Bits, Solidified, Certora, and Open Zeppelin in the last year. Open Zeppelin would be my first pick for this audit. They did excellent, comprehensive, through work, and were a pleasure to work with.\nI also do think that a re-audit of the current state of the code and systems would be a wonderful thing.\\n\n@Getty and @dvf thanks for your thoughtful and direct response to our proposal and your kind words about our past work and position in the crypto security space.\n\n\nBefore addressing your questions, it’s important to note that our proposal goes well beyond ad hoc auditing or tool-based approaches to protocol security, which is what you assume in your price analysis. We priced our fees in this proposal across two dimensions 1) comprehensive security products and services that extend beyond ad hoc or tool-based auditing services and 2) reputational risk for both parties -- Compound and OpenZeppelin.  See below for a more detailed breakdown of the various components of our proposed security solution:  \n\n\nSecurity of the Compound protocol: \n\n\nWhile state-of-the-art tools and automation are evolving, audits of smart contract systems remain costly because — to be truly effective — they must be manual in nature. In order to conduct timely continuous audits, as needed and related to governance code changes, OpenZeppelin will reserve the bandwidth of dedicated, knowledgeable blockchain security professionals and auditors. Our proposed fee structure accounts for the opportunity cost to us when we dedicate our top teams’ availability for the Compound DAO.  We are also including a full audit of the Compound protocol in this fee and it’s relevant to note that in many cases our current audit pipeline is booked 12 months in advance. \n\n\nThat said, your breakdown of our fee structure only focuses on ad hoc audits. Our proposal offers a more comprehensive security solution, which includes -- \n\n\n\nThreat Monitoring and Threat Dashboarding and related customization services and integration: not only will we provide Defender and Forta integration (OpenZeppelin-developed software) but also customized reporting, dashboarding and Forta agent integration and development. This will require a significant investment in customized dashboard development, third party integration, and interaction with the community.  Threat Monitoring and Threat Dashboarding is a critical cost component of our fees, which includes our software.  The value of this solution to Compound will be:   \n\n Improved visibility and Intelligence of threats \n\nIntegration of relevant risk and open-source data \n\nReporting and analytics to help prioritize and act on the most relevant information \n\n\nWith the goal of faster threat detection and response thereby reducing overall protocol risk\n \n\n \n\n\n\nSecurity Education:  The Compound community will be provided with dedicated OpenZeppelin content and education events that will allow them to learn about security issues and best practices to apply in their Compound related activities.  By utilizing this training and educational content, the Community will become more aware of coding practices that lead to vulnerabilities, being more proactive in avoiding these issues, thereby reducing overall risk in code submissions and governance decisions.  (For examples of content and education and see our original proposal please)\n\n\nPSO Advisory Service: We are including senior project management and advisory services to recommend improvements to the incident response and emergency response procedures and response times. Improving the ability to respond in a more timely manner is critically important to a secure and resilient protocol.\n\n\n\nReputational risk:\n\n\nWhen we conduct a one-time regular audit we caveat our reports based on specific code, timing, and scope; through this proposal we become the Compound DAOs trusted security solution provider. As such, our reputation and brand is tied directly to the protocol's security.\n\n\n\nThe value of reduced risk: The goal of continuous security solutions is to reduce risk of downtime due to a significant misconfiguration event or security incident, which can not only disrupt protocol continuity and lead to loss of funds but also materially damage the long term reputation of the DAO. Attaching a value to prevention of reputational damage always presents a challenge, however we suggest that the value of the security solutions we can provide will protect the overall reputation as well as  the value of the project. \n\n\nAligning our reputation to Compound’s reputation: The proposal aligns OpenZeppelin’s reputation to yours. If an incident occurs under our watch, it impacts our brand as well as Compound’s.\n\n\nRisk fee aligns our interests: Keeping the above in mind, we proposed a fee structure such that  Compound DAO pays the performance fee only after a successful quarter with no incidents. As you suggest, we get nothing if an incident occurs, which aligns our mutual interests.\n\n\n\nIn sum, our proposal is valued such that security is a continuous and dedicated process, that goes beyond ad-hoc audits, and takes into account our role as a Trusted Security Solution Provider, directly aligned to Compound’s needs and reputation. \n\\nHi, we at Blockchain @ Berkeley support this proposal and appreciate how clear the duties of OZ will be once they are hired to audit each proposal. We share @getty’s concerns, and look at the pricing from another perspective:\nExisting bug bounty programs in the DeFi space offer comparable rewards (i.e., 100-500k) per bug found in existing core contract code, not for auditing individual proposals before they are passed and in timelock.\nThe time of OZ professionals is very valuable, but would it make more sense to reserve such high per-code-change payments as a bonus for extreme bugs avoided rather than as a baseline, no protocol incidents rate?\nWe support adding, in addition to the OZ auditing proposal, a bug bounty program like this: www.comp.xyz/t/bug-bounty-program-for-compound-proposals/2590 15\nCurious to hear more folks’ thoughts on pricing for auditing versus finding a bug, especially at different stages in protocol development (proposed vs. timelock vs. deployed code).\\nSince the launch of governance last summer, the Compound community has shepherded the protocol through a period of remarkable growth. Under the community’s watch the protocol has seen significant expansion across every relevant metric, from loans outstanding to TVL to net revenue. This period also witnessed a number of community-led upgrades to the core protocol itself, including the Governor Bravo 5 migration and the Oracle Improvement 1, among others. These not only represent meaningful upgrades to the system, they also speak to the quality of the Compound community and its ability to shepherd the protocol into the future.\nLike any community-led project though, growing pains are to be expected. To leverage the power of open-source development is to accept certain new risk tradeoffs, and Compound is no different. The protocol has benefited greatly from the open-source model, drawing on a great deal of third-party developer talent and resources to upgrade the protocol and extend its core capabilities. At the same time, the very properties that make this model so compelling (openness, permissionless, etc) can also introduce new risk vectors when not properly managed. We saw this recently with Proposal 62.\nBut the takeaway from 62 should not be a retreat from the open-source model that has otherwise served Compound so well. It should instead be an opportunity to give the community the resources and support it needs to contribute to the protocol in a safe and responsible manner. Doing so will allow Compound to continue to harness the benefits of community development while mitigating the accompanying risks.\nThis brings us to the OpenZeppelin proposal, which we think is positioned to do exactly that. OpenZeppelin not only has an industry-leading reputation among blockchain security firms, it also has extensive experience working on the Compound protocol itself, including 10+ audits as well as its work on the OpenZeppelin Governor contracts. Enlisting a firm of this caliber will give community developers the support they need to do their job effectively, while also ensuring the protocol remains safe.\nWhile the proposal does come with a high nominal price tag, it ultimately seems reasonable given the quality of OpenZeppelin’s offering, the demand for similar services across the industry, and the need to safeguard Compound following Proposal 62. And furthermore the short contract term (and quarterly repricing options) allow the community to assess OpenZeppelin’s performance in relatively short-order and determine whether it’s worthwhile to renew.\nWe appreciate OpenZeppelin’s hard work in crafting a thoughtful and detailed proposal and also incorporating feedback from the community. In terms of next steps, we defer to the community’s view on timing/process, though at this point we are supportive of this proposal moving towards a formal vote. We welcome any additional thoughts or feedback that the community might have.\nJeff Amico\na16z\n\nDisclosure: a16z is an investor in Forta, a blockchain security platform that was recently spun out of OpenZeppelin. a16z is not an investor in OpenZeppelin.\\nIt’s great to see this proposal from OpenZeppelin, who has generally been a great partner to Compound Labs over the last few years. The scope of the proposal is quite large however, and includes services which Compound Labs has never used before, and therefore cannot attest to the value of. Audits have never been a silver bullet, and never will be, so personally I think it’s important to think about and balance all of the quality assurance resources we have available. This should probably include things like formal verification 8 and investment into the tools developers actually use to build and test the protocol themselves.\nWhile the price tag seems extremely high, I would like to clarify what exactly would be covered by the continuous security audits? Would this include all new contracts developed by the community and adopted by governance?\nLogistically speaking, when can a proposal be submitted for audit? If we are truly just monitoring proposals that have been made on-chain, that seems not nearly enough time for an audit and a response to the audit. In that case, what would the actual process of scheduling an audit look like? What sort of lead times should the community expect?\nI think it’s important we understand the answers to these questions in order to understand the offering. Thanks to everyone on this thread!\\n\n@jared thanks for joining the discussion!\n\n\nI have tried to break out each of your questions with answers below:\n\n\nWhat about Formal Verification (FV)?\n\n\nAs you may be aware we are trying to use formal verification on critical components of the OpenZeppelin Contracts Library. We see FV as helping to lower the overall risk footprint but FV also has its clear limitations. Therefore, we see FV as a component of an overall holistic security process that must include coding best practices, manual auditing, education, threat monitoring and bug bounties (our internal process encompasses all of these). Formal verification is only as good as the initial description of the desired properties of critical elements of the protocol, which is why manual auditing is still a critical step in the overall program. We have relationship with Certora and are fans of the progress their technology is making but would be happy to work with whichever formal verification vendor Compound chooses and implements. Security as you indicated is not about looking for a “silver bullet” but layering in services, processes, and technologies to lower overall risk exposure. We would be happy to include additional work and cooperation with Compound’s desired FV vendor as part of this existing proposal. Our PSO and auditors can aid in ensuring that the desired properties of the protocol are described properly from a security perspective, so the FV vendor can write the rules effectively for implementation.\n\n\nWould this include all new contracts developed by the community and adopted by governance?\n\n\nYes, that's the expectation. Any substantial changes to the Compound protocol would be covered by an OpenZeppelin audit. Audit times would be set aside in advance for upcoming proposals and protocol changes. We would commit to auditing all community-supported proposals on a timely basis. That said, not all proposals are created equal; some will be high priority for the protocol and some will be low priority. We expect to work with the community on helping us separate the signal from the noise\n\n\nLogistically speaking, when can a proposal be submitted for audit? If we are truly just monitoring proposals that have been made on-chain, that seems not nearly enough time for an audit and a response to the audit. In that case, what would the actual process of scheduling an audit look like? What sort of lead times should the community expect?\n\n\nAs part of the Advisory Solution, the first task of the OpenZeppelin Protocol Security Officer (PSO) is to propose to the community a clear approach and process as to how security audits and security related issues would be incorporated into the proposal lifecycle. From our perspective, security audits should be a prerequisite for submitting a governance proposal that makes changes to the protocol. \n\n\nOnce a potential community proposal has initial support and begins development, the submitter will work with the PSO and dedicated OpenZeppelin auditors to ensure the agreed defined process is followed, and everything is ready for code completion in the most efficient way possible. \n\n\nThe initial proposal is considered code complete with a frozen commit, it would then be submitted to the OpenZeppelin Audit team for audit. \n\n\nBased on similar audits (e.g. Celo governance upgrades 2) our initial estimates suggest 2 weeks for auditing minor protocol proposals although exact time required depends on the lines of code and complexity. This includes 1 week for the audit and another week for reviewing fixes although this timing also depends on how quickly the proposal submitter responds with fixes. Once the final report is finalized and shared with the Community, the proposal will be able to move forward with the governance process starting with the two day review. Good communication between the OpenZeppelin team, the submitter, and the community will be critical and will be managed by the PSO. Please note that we have been conservative in these estimates, it's our assumption that we can improve the timeliness based on improved processes, developer education and good community communication.\n\n\nBefore auditing new proposals it's our expectation that we will do a full protocol audit, starting out with existing critical components of the Compound protocol which should include the governance contract, Comptroller, and other components typically involved in governance proposals. This initial audit will ensure that OpenZeppelin can take full ownership for the security for the entire protocol and give our auditors the necessary context to review new protocol changes.This would take approximately 7-9 weeks based on our knowledge of the existing Compound code base, we would begin this audit as soon as possible upon acceptance of this proposal.\n\n\nI have attached an example to illustrate the process:\n\n\nProposal 62 12 would have consisted of the following steps:\n\n\nLate July - Forum post responding to RFP16 is made and development begins on the proposal changes. The PSO is made aware of the proposal in progress and works with the submitter to prepare for an audit in accordance with community guidelines.\n\nSeptember 13th - Proposal code is completed as a Pull Request 2 with a frozen commit one week prior to audit start\n\nSeptember 20th - Audit begins the week of Sep 20th and ends that week on Sep 24th. An audit report is delivered to the submitter no later than Sep 27th.\n\nSeptember 27th - Submitter addresses bugs found and submits fixes to the audit team to review. The audit team reviews the fixes and updates the report.\n\nOctober 4th - The report is finalized and shared with the Community. The proposal is able to move forward with the governance process starting with the two day review.\n\n\n\nI hope this clarifies our willingness to support FV as a component of an overall holistic security process, and the way we are thinking about integrating our offering into the Compound Community governance process.\n\n\nSteve Gant\n\n\nOpenZeppelin\n\\n\nOn behalf of the OpenZeppelin team, I want to thank everyone for bringing their thoughts and expertise to a constructive and open exchange about the Compound Security Solutions Proposal over the past two weeks.  \n\n\nOverall, Compound stakeholders and community members agree that a set of comprehensive security solutions are essential to the Compound DAO’s overall security and continued strong reputation. To this end, this week we will submit only the retainer fee portion of the OpenZeppelin Proposal for a vote.  This will allow us to move forward on this important project and work with the Compound DAO on immediate governance and security matters.   \n\n\nSeparately, in March 2022 we will submit the performance fee for a vote, which will allow time for further discussion while moving forward with this important matter. We believe the foundation of our initial work will demonstrate the merit of our proposal’s overall approach and look forward to the prospect of working with Compound!\n\\nThis sounds workable then, @OZSecure.\nI am in favor of doing it.\\nWhy is does OpenZeppelin believe that using the built in contributor comp speed is a good idea? It is not the way to go about this proposal at all. The community should reject any proposal that creates indefinite payments to anyone.\\nHey all,\nTrail of Bits has a proposed solution in the works for the issue that Larry Sukernik described. We are aware that there’s an active vote on Open Zeppelin’s proposal 41. Please vote “no” on this governance proposal if you’d like to consider how Trail of Bits would provide these services before deciding on a vendor.\nIn advance of a finished proposal from us, I’d like to lay out why I think Trail of Bits could be the right choice for this job, a few key ways that our proposal will differ from Open Zeppelin, and why I think the performance fee may not provide the intended incentives.\n\nIntroducing Trail of Bits\nTrail of Bits has worked extensively with Compound over nearly 1,000 hours of security review through four focused engagements covering their governance, core protocol, and internal security procedures. Many firms in DeFi, including MakerDAO 7, Balancer, Uniswap, and Rocketpool trust our expertise to help secure their code, and you can find many more in our Publications 2 repository. We’ve succeeded in finding vulnerabilities in highly verified systems 7 and providing the best solutions regardless of whether we invented them. We’re relentless about raising the baseline in the communities we work, and have developed and made freely available some of the most-used security 3 tools, reference guides 2, and security research in the industry.\nTrail of Bits is differentiated from other firms by our diversity of experience and expertise. We work on securing software wherever the risks are high in the technology, defense, and finance industries and employ a wide-ranging team of experts in programming language theory, cryptography, cloud-native software, and low-level exploitation to do so. Roughly one-third of our firm works on fundamental research in their field through long-term contracts with DARPA on automated program analysis 4, zero-knowledge proofs, and software verification 3. Access to these competency areas improves our ability to consult with all of our clients, and particularly on blockchain systems.\n\nTrail of Bits proposal preview\nAt our core, we’ll be proposing many of the same services as Open Zeppelin. Therefore, I’ll keep these sections brief by focusing on the key differences in our approach.\n\nSecurity review of governance proposals\nTrail of Bits believes this is the most important service provided by the proposal. We plan to review all governance proposals, including parameter changes, new token integrations, and more extensive code changes. For each proposal, we will fully describe any identified security issues, including scenarios for abusing the issue and specific recommendations to address it.\n\n\nReviewing the security of a proposal after voting has already begun is too late. We will begin reviewing new proposals after it becomes clear they are seriously considered by users on the Compound Discourse.\n\nProposal authors will receive a one-on-one counseling session. We will host a video call with the author to understand their goals and provide immediate feedback. These video conversations will ensure information is effectively shared.\n\nWe will review and report any identified security issues. These issues will take the standard form of a description of the issue, a scenario for abusing it, and a recommendation for addressing it. We will work with proposal authors to validate any fixes that result from these reports.\n\nWe will contribute a “Security Considerations” section to every proposal. The absence of specific security issues does not ensure the safety of a proposal. We will contribute a standardized section to reviewed proposals that informs developers and users of limitations, risks, monitoring guidance, or other considerations.\n\nWe will help define security properties for the proposal. Human review is necessary but insufficient to provide for the security of DeFi systems. We will work with the authors to provide reasonable security invariants alongside the proposal.\n\nWe will provide our analysis directly to the community. Prior to a vote on any governance issue, we will host a public community call and walk attendees through any specific issues we discovered and the documented Security Considerations.\n\n\nCommunity training and continuous improvement\nTrail of Bits will develop scaffolding for proposal authors to write safer proposals and provide continuous training and guidance to improve the quality of proposals over time. In particular, our approach takes after the Security TAG 7 process for improving the CNCF security baseline by engaging directly with developers.\n\n\nWe will iterate on public guidance for developing secure proposals. This will consist of a template repository with testing and verifications tools pre-configured for different proposal types, prompts to elicit a self-assessment of the proposal’s security by its authors, and a public walkthrough for navigating the security review process with Trail of Bits.\n\nWe will host bi-weekly workshop sessions or “office hours” with developers. These sessions will cover testing and verification tools, review previous DeFi incidents, and deeply investigate common areas of risk. As appropriate, these will be recorded and made available for community use.\n\nWe will publish a minimum-security checklist for new proposals. Specific to Compound, we will describe the minimum viable due diligence steps we believe are required to make a reasonable governance proposal. Like our Token Integration Checklist 4, this checklist will include specific, actionable steps that authors can complete on their own, before circulating a proposal.\n\n\nSecurity intelligence\nTrail of Bits takes an expansive, intelligence-driven view of security monitoring. On-chain monitoring is important, however, if you are finding issues at the time that transactions are being processed then you are typically too late. Furthermore, on-chain monitoring tools have substantial limitations regarding the types of issues they can detect. Like human-only code review, on-chain monitoring is necessary but not sufficient to ensure safety in DeFi.\nWe will evaluate and recommend an on-chain monitoring vendor. We will evaluate on-chain monitoring vendors (including Forta), recommend the selection of one, and assist in configuring it appropriately. Furthermore, the code invariants developed through security review will be regularly provided to the owner of this system. The quality of your on-chain monitoring is directly dependent on the quality of your system’s specifications.\nWe will provide security intelligence and facilitate community engagement. We will facilitate community development of new attacks against Compound. Rather than wait to report a fully-developed attack to Compound, Trail of Bits will be available to co-investigate unproven potential vulnerabilities with reporters and prepare tailored remediations and fixes to Compound alongside their reports. With our earlier involvement, we will shorten the window of exposure for new attacks reported against Compound.\n\nPricing structure\nFees to Trail of Bits will be split into three discrete categories:\n\nQuarterly retainer. Trail of Bits will be paid the equivalent of $1 million USD in COMP every quarter for one year to provide the baseline services.\nTrail of Bits will be eligible for bounty awards in cases where it co-investigates and co-reports issues to Compound, outside of the retainer fees.\nPerformance fee. The motivations behind this fee are positive, however, the design as a binary all or nothing with factors outside the auditor’s control are not.\n\nTo be clear, we don’t have a perfectly designed performance fee incentive to propose at this time and desire additional time to work on this aspect of the proposal specifically. We’re eager to work with the community on a model that benefits everyone.\nFinally, we are happy to cut the bounty co-investigation piece from this proposal and incorporate it into the Bug Bounty for Compound Proposals 5 discussion. I included a discussion of it solely due to the mention of security monitoring by Open Zeppelin and because I see security intelligence as an inseparable part of security monitoring.\nWe’re very excited to hear your thoughts!\\n\nWhile we welcome competition, ToB’s last minute proposal  -- that by their own admission is nearly identical to ours -- is undifferentiated  and lacks a solution for threat monitoring and security intelligence. ToB had several weeks to respond but chose not to, making it clear that Compound’s security is  not a priority for them. Delays in moving forward with security solutions, regardless of with whom, compromise the security of high value projects (DAOs or others), which must make a decision to act, not to delay. \n\n\nIn the 6-8 weeks since the Proposal 62 event, we have actively and publicly engaged the community and stakeholders to determine the most effective method of providing security given OpenZeppelin’s current and future capabilities. (Note, the proposal in question was in development within OpenZeppelin for our DAO strategy and we believed that our long standing Compound relationship and the  incident merited a custom, early version of it given our long history of working with Compound). \n\n\nA counterproposal from another security firm, particularly a proposal with significant alternatives, rather than a copy with small tactical differences, would have been valuable had it been brought forward in a timely fashion. Then the community could have determined the best option -- comparing all proposals with the appropriate time and attention it deserves. However, given security has urgency, we strongly advise the vote continue as planned and invite other vendors with valuable, original approaches to offer a counter proposal next year when the solution comes up for a vote again.  \n\n\nGiven we are all working together to shape the future of B2DAO relationships and how engagements like this take off, we have no doubt that we and the community will have the opportunity to improve and iterate our solutions in the open and in competition with other solution providers in the space. OpenZeppelin is confident in our ability to deliver best-in-class security solutions, as our clients can attest (https://www.comp.xyz/t/auditing-compound-protocol/2543/4).\n\n\nWe ask that those who are genuinely interested in Compounds Protocol Security vote for our current proposal. (https://compound.finance/governance/proposals/70 38)\n\\nI certainly understand the desire to swiftly plan a path forward but urgency on this issue is driven by new governance proposals, of which I only see a parameter change right now. For the record, the first we heard there was any activity on this topic was October 26th. It was a lot to wrap our heads around and deserved some time thinking about an approach rather than diving in with recommendations. I was surprised to see a vote so quickly given the magnitude of this effort!\nAt the end of the day, we’re all offering software assurance services so of course a lot of this is going to look the same. If I had to characterize the differences, the Trail of Bits proposal focuses most of its effort on a) finding and reducing risk in governance proposals, and b) increasing security by reducing the cost of control (e.g., with templates, checklists, assessment guides, specification development, and other systems that continuously reinforce themselves). I think we were much more specific on that front. On the other hand, the OZ proposal contemplates a broader set of activities, like security training and threat detection, that apply only indirectly to the core issue of merging insecure or unsafe proposals. E.g., I’m less sure how to take a Q&A channel staffed with OZ employees to the bank.\nReading between the lines, this seems like a similar take that Jared had earlier in the thread:\n\nThe scope of the proposal is quite large … and includes services which Compound Labs has never used before, and therefore cannot attest to the value of. Audits have never been a silver bullet, and never will be, so personally I think it’s important to think about and balance all of the quality assurance resources we have available. This should probably include things like formal verification  3 and investment into the tools developers actually use to build and test the protocol themselves.\n\nIn my opinion, on-chain monitoring in particular, while possibly valuable, probably should not get bundled as a must-purchase alongside the auditing service for governance proposals. These types of systems are an active field of research (see also, flashbots), may apply to less attacks than expected, or may have undesirable performance impacts or other costs. I’d want to study them further to understand their value and limitations.\\n\nIf you want Compound to be protected by OpenZeppelin as of Dec 1st, then vote Yes.\n\n\nIf you believe the question of Compound’s security can wait for further deliberation at this time and be addressed at an unknown future date, then vote No.\n\n\nThe facts are that we had a timely and immediate response to the needs of Compound and the Compound community. We did not show up opportunistically, or late, focused solely on missing out on a business opportunity. Our priority at OpenZeppelin is and has been to protect the open economy, including DAOs and communities like Compound. \n\n\nWe are past the stage of deciding whether Compound needs a security partner. There is no formal RFP process for DAO security solutions today, based on feedback and responses, it's clear that we not only conducted a fair and open process, but have set the standard for what a Security Solution should look like.\n\n\nDelaying a decision to an unknown future date and vendor could come at the cost of Compound’s long-term security preparedness.\n\n\nThank you for your consideration,\n\\nWe really appreciate the hard work of both OpenZeppelin and Trail of Bits in bringing forward their respective proposals. To receive competing bids of this quality speaks to the importance of the task at hand. It’s now incumbent on the community to do its part and evaluate these proposals fairly and in a timely manner.\nAs we wrote earlier in this thread, we supported OZ’s original proposal, and delegated to their CAP to help kick off a live vote. However, we chose to abstain from voting on Proposal 70 12 once we saw Trail of Bits submit a competing bid. We did so to ensure that ToB had a fair chance to pitch its proposal and be heard by the community. While this will extend the timeline, we think a competitive process is in the best interest of the protocol, and will ultimately serve Compound well in the end.\nTo that end, we would support extending the process for another 1-2 weeks. This would allow the community to hear more about ToB’s proposal, to evaluate the two proposals side-by-side, and to answer any remaining questions before making a decision. Following this period, we would suggest that the two proposals be introduced as simultaneous governance votes (or something similar), with the token holders ultimately deciding which one they prefer.\nWe think a simple approach along these lines would give ToB an opportunity to pitch its proposal, while also setting a firm deadline to ensure the process does not drag out indefinitely. This latter point is important both for the security of the Compound protocol and to ensure we remain respectful of both vendors’ time and effort.\nWe welcome any thoughts or ideas that the community might have, and look forward to identifying a process that works for all parties.\nJeff Amico\na16z\\n\nThanks Jeff! \n\n\nWe strongly believe that OpenZeppelin has the best vision, team, and Security Solutions for Compound.  We also know that we are trailblazing a new process here of how the DAO will ensure security. \n\n\nOn November 3rd, OpenZeppelin was invited to submit a proposal and has led a well paced, open, and transparent process. But we accept that this process will now be extended.\n\n\nGiven the significant time, effort and leadership OpenZeppelin has shown here, we simply ask that you and other community members come up with a fair process with a clear timeline, and decision-making criteria.\n\n\nWe would request that a vote start NLT 1200 PST, Dec 6, 2021. Secondly, whatever vote date is selected, we would kindly ask that the proposals be locked one week before the vote, so we can revise our proposal one last time, given that our proposal has been open for scrutiny and copying since we proposed it on Nov 4, 2021.\n\n\nWe appreciate that this is a new process for Compound - and DAOs in general - and look forward to continuing to work with you all in breaking new ground!\n\\nI appreciate the write-ups from @OZSecure (OpenZeppelin) and @dguido (Trail of Bits). These teams’ work with Compound Labs on the protocol’s core contracts, especially pre-governance, was invaluable for the bootstrapping of the protocol. As a community member and user, I deeply appreciate your past work for Compound and your broader efforts as good actors in the smart contract security space.\nTo me, the tenor and character of this current thread feel out of step with the bona fides of OZ’s and ToB’s prior work with Compound Labs. Let me try to lay out the key discrepancies I’m seeing. I am hoping it could help us think through the origins of the current tension; increase community buy-in for @jamico’s suggestion on a path forward; and hopefully identify some ways to circumvent such tensions in future efforts as stewards of the protocol.\nConflicts of interest: I appreciate @sukernik’s transparency at the top of this thread in explaining how the proposal from OZ came to us. That said, declaration of a conflict of interest doesn’t absolve us of the need to agree on how to work around it. In this case, it seems likely that there was some degree of information or timing asymmetry between the vendor with whom @sukernik has a business relationship (OZ) and the vendor with whom he presumably does not (ToB).\nNow, we have one vendor (OZ) that has been, at times in this thread, projecting a sense of entitlement to an up-down vote as a sole-source provider, and another vendor (ToB) stuck playing catch-up through no fault of their own while trying to manage the optics of the awkward situation that we have placed them in.\nI have to say, I am a bit baffled by the number and variety of high-pressure sales tactics employed by @OZSecure throughout this thread, from aggressive up-selling relative to @sukernik’s initial outreach, to intentionally promoting an atmosphere of urgency, to rather bizarrely accusing ToB of copying aspects of OZ’s proposal that any reasonable vendor would include. In my eyes, these tactics are incongruous with, and damaging to, OZ’s good reputation.\nI see two take-aways here, one for community members taking on new initiatives, and one for vendors proposing fee-based services to the protocol:\n\n\nCommunity members: Let’s please at least start a thread here on the forums before engaging in direct outreach to potential vendors for any new initiatives that could possibly involve a distribution of COMP, especially if there are known conflicts of interest involved. Otherwise, vendors like ToB (and others who might be well-qualified but whom we might not personally know) have every reason to question the integrity of the process.\n\n\nService providers: Especially in cases where a conflict of interest may exist, prospective service providers should be careful to keep proposals clearly in-scope of the request. Here, the request was audit review of new governance proposals prior to on-chain execution; what OZ proposed in response is a much more elaborate engagement, some of which is necessary to deliver the core service, some of which is almost certainly not.\nFor example, I love the idea of an experienced auditing firm providing educational materials for community members seeking to improve the protocol, and I would likely be a user of this resource if it existed. Furthermore, OZ’s suite of public contract templates offer ample evidence that they would likely do a fantastic job. However, it is related, but quite far removed from, the core aims of this security audit request and, at least in my view, should be considered as a separate proposal.\n\n\nCost model: I’m not qualified to objectively assess how reasonable the costs are, but I will state as an outsider to the security audit industry that they seem absurdly, even comically high to me (like, by about one order of magnitude). While I acknowledge that the community is likely to support paying a premium relative to reasonable market rates, any additional supporting evidence OZ can provide to justify the cost structure relative to industry standards would be helpful here.\nI also agree with community members who question the cost structure of OZ’s proposal and its performance fee: I do not consider a performance fee model to be appropriate for this work. If the service rendered is to secure the protocol, why should the service provider get extra cookies every quarter the protocol is not exploited? Compound does not need to offer an extra incentive here. The service provider’s motivation to do their work well should be to score the next security agreement with the protocol, not to earn extra compensation for services already rendered.\nFinally, I agree with @arr00 that compensation for this work should not be streamed to the vendor. It makes more sense to me that it be paid out periodically (perhaps quarterly for one year, then annually) with a new proposal to authorize the payment for each following period.\nThe question of payment schedules leads to an important final follow-up question: how can the community secure auditing services beyond the runway of the current COMP distribution schedule, which is only a few years iirc? While this question goes beyond the core focus of the audit proposals, it is worth discussing in this context as we continue what I consider to be the most important discussion we’re having 6: how to adjust COMP distribution for the health, security, and sustainability of the protocol.\\n@allthecolors – as always, your commentary is extremely thoughtful. I’m sure @sukernik will explain, but OZ and TOB were contacted around the same time — OZ just happened to move a lot quicker on this. And the process run here seemed to give ample time for other auditors to throw in a competing proposal. OZ’s request for the other auditors & the COMP community to move quickly on this seems reasonable given the timeline they’ve worked with so far (though I think the Dec 6th deadline is too tight, given the holidays).\nThis idea had been discussed by a number of folks in the community long before Larry finally took the initiative to have the conversations and make this happen. There’s an intricate web of potential conflicts of interests (some obvious, some less obvious) for just about any leader in the crypto community, so it’s not surprising to see this here. I think the best we can do is just be transparent about potential COI’s. In some ways, Larry’s close relationship with OZ may have been valuable in helping mobilize OZ to make the prop it did (and perhaps he should even be compensated for this!). COIs can be dangerous if they lead to proposals that are misaligned with the long-term health of COMP, but in this situation, COMP would be very lucky to have OZ or TOB on our side, both are excellent options.\nThe [scope of the proposal] and [cost] concerns are also very valid. I think the service providers should be free to propose whatever they want… if their proposals are off the mark from what the COMP community would like, we should negotiate the services/terms. This highlights a key problem — it’s really hard to negotiate the right deals with so many voices on an open forum, especially when considering multiple proposals. (h/t @sukernik who planned to bring up this exact issue on the community call).\nPerhaps forming a pod (committee) of people with expertise in structuring and negotiating these types of deals and giving them the autonomy to negotiate off-forum with OZ, TOB, and others to get a deal done would be a better way to accomplish the objective here.\n(p.s. Happy Thanksgiving to anyone celebrating!)\\nI’m the security guy for OUSD 20. Currently we are the #8 holder of cUSDT and the #10 holder of cUSDC, and growing rapidly - yesterday 29 million stables were deposited into OUSD. I don’t want something happening to that money. My goal here is that Compound not lose a pile of money suddenly.\n\nThe Compound codebase needs a first tier audit. It is madness to continue the current ad-hoc deployment of new code changes. This is one of the biggest protocols in Defi. It is vital that each code change pass a serious code review.\nAs far as I can tell, everyone here is in agreement on the need for an audit, and the need for code change reviews.\nBeyond that, Compound’s single collateral pool design means that a security failure of any listed coin could empty all the liquidity of all Compound lending pools. Both coins that destroyed CREAM, and the coin that could have destroyed AAVE, would have wrecked Compound had they been listed here. Compound governance’s strong bias for not listing coins has probably saved the protocol.\nA listed coin is a much a part of the security of Compound as the Compound codebase itself. Listing a new coin requires a high bar of security and economic analysis. It’s an integral part of the security of Compound.\n\nIndividuals making code contributions can’t be expected to negotiate, schedule, and pre-fund an audit of their own code that then may or may not even be approved for payment. Even for good teams with existing auditor relationships, it is difficult to quickly get an auditor lined up to review code changes. If we want Compound to have any development velocity, there needs to be a friction free way of getting excellent code reviews on all changes that is not up to the individual contributor. Both of these proposals provide that, and this is good.\nThis capability is not cheap no matter how it is done, and that is okay.\n\nOne long term alternative is for Compound to build out their own internal security team to handle the majority of the security work.\nThe proposals from OZ, Trail of Bits, and Certora all assume that Compound has an internal development/security team. This isn’t the case as far as I can tell from forum lurking. Compound development currently seems to be individuals making ad-hoc proposals, and no one really checking them, outside sometimes getting a random external audit.\nFour million dollars, even in this market, does buy a substantial amount of developer/security time. The ideal mix of internal and and external from both a cost and security perspective probably isn’t being 100% external.\nBut while the long term solution may be different, it is urgent to put Compound on a better security footing. I’d run a version of one of these proposals for a year and use that time to plan and build out the longer term solution.\n\nIt would be good for external security proposals to reflect the current Compound security staffing, rather than that of a typical project.\n\nI am strongly against any form of “performance fee”. That seems like it would reward gaming the system, more than just doing the job and getting paid / earning the reputation. I think performance fees have has strong downsides. It’s a distraction for everyone involved. The auditors don’t want to be the ones that signed off on the code change that got one of the largest defi projects hacked. That’s enough.\nUnless someone has an epic, genius idea, let’s just nix anything to do with performance bonuses.\nIf an auditor is willing to do so, denominating the payment amount in COMP would be an alignment that would be good, and easy to do. If not, I understand.\n\nI don’t have any friendships with anyone at either OZ or Trail of Bits. I’ve had audits by both. Both were professional, competent, and reasonably through.\nI preferred the OZ experience, but I know other people who have preferred working with Trail of Bits. I’m assuming your experience and strength of audit is based on which individuals are assigned to your audit team.\nIn either case, either company is first tier and capable of providing a good team.\nThe two proposals are currently at the same price. Of the two current proposals, the OZ proposal offers more, and I lean that way.\nBoth proposals would make a marked security improvement over the current situation. In my opinion, given the funds in Compound, both bring more value than they cost.\nBoth the OZ proposal and the Trail of Bits are obviously overpriced, and also contain low value filler to pad the dollar value of the proposal.\nBecause these proposals are currently overpriced there’s a lot of weird dynamics going on in this thread from both parties, as they try to get the cash cow, while not getting into a bidding war and driving the price down.\nHow to properly negotiate in this open forum is beyond the scope of what I’m willing to think about on my Thanksgiving day off. \nCompound needs an audit, needs a system for auditing future changes, and needs it now. We should have a deadline, and go for it.\nHappy Thanksgiving!\\nPlenty of things to respond to here; it may be a good idea to do so before the post-Thanksgiving dinner food coma.\nFirst of all, I’d like to thank @allthecolors for his pragmatic reply. It’s always a bit difficult to communicate all of the information one has on the forums: private conversations happen of course, and relaying every bit of detail that happens in private can be cumbersome. As a result, many forum posts are (rightfully) questioned by the outside observer.\nNow to address @allthecolors’ comment directly. A sizable chunk of my net worth is in COMP, so when the Proposal 62 exploit happened, I was quite frustrated that a simple and avoidable process failure wasn’t being rectified. In my mind, the fastest fix was to get a top-tier auditor to review every single governance proposal for Compound. I ran the idea by a few folks in the community, and after they responded favorably to the idea, I decided to reach out to the audit firms. When you need help with something, you naturally turn to the folks you’re closest with first. I had pre-existing relationships with the OZ folks, so I reached out to them first. A few days after speaking with OZ, I reached out to Trail of Bits, ConsenSys Diligence, and Sigma Prime (it took some time for me to get connected with them). In each case, I told each of them the same information: Compound protocol needs an auditor, you should bid for the work, and I personally have a personal and economic relationship with OZ. Conflicts of interest are unavoidable; as long we’re transparent about them, we should be able to achieve fair and favorable outcomes.\nI’d also like to share some thoughts on the process by which Compound protocol chooses an auditor. Clearly, this whole shebang is a first for Compound: we’ve never had to deal with multiple vendors bidding for the same work, which means we’ve never had to establish a process for choosing the winner. Is this an annoyance to vendors looking to work with Compound? Absolutely. Is Compound protocol the one spending millions of dollars for the work? Yeah. In my book, the guy who is opening his wallet is the one who dictates the terms of engagement. To that end, it’s my opinion that Compound should be laying out the process by which it buys things rather than the other way around.\nPractically speaking, Compound doesn’t make any decisions on its own (wouldn’t that be nice); it’s the tokenholders/community that make the decisions. How then should tokenholders/community members proceed on making the decision of which vendor wins the audit work? I’d like to propose several options:\n\n\nTokenholders delegate purchasing authority to a group of individuals. The idea here is it’s impractical to have a tokenholders/community members evaluate vendors. Most large tokenholders aren’t experienced in evaluating audit vendors; instead, they could delegate authority to a group of experienced folks to make the purchasing decision on their behalf. This group of folks could evaluate the vendors, publish an analysis, and make a recommendation to tokenholders. If the tokenholders agree, they would vote for the “recommended” vendor.\n\n\nTokenholders vote on the vendor. A faster but slightly more chaotic process would be for tokenholders/community members to informally assign a deadline by which interested audit firms must submit proposals. After that date, no new proposals will be accepted. The “accepted” proposals can then be put up for an on-chain vote. Let the best vendor win!\n\n\nAs far as timing goes (if we go with the second option), I think it’s fair to honor OZ’s request of having the vote start on December 6. As part of that, we can set an informal rule saying that all proposals must be submitted on the forums a week before that — November 29. Would that mean that vendors posting their proposals after that are automatically disqualified from the vote? No. But it would mean that they would be tardy to the informal deadline set by the community, which reflects poorly on them. And if they can’t get their act together by then, then do we really want them as our vendor of choice? Probably not.\nAnyway, that’s how I’d go about it. Now for some turkey…\\n\n\n\n OZSecure:\n\nWe would request that a vote start NLT 1200 PST, Dec 6, 2021. Secondly, whatever vote date is selected, we would kindly ask that the proposals be locked one week before the vote, so we can revise our proposal one last time, given that our proposal has been open for scrutiny and copying since we proposed it on Nov 4, 2021.\n\n\nFor what it’s worth, I agree with the dates suggested by Open Zeppelin here. We’ll post a full proposal within the next day and think it’s appropriate to have a vote shortly after.\\n\n@jacobpphillips, @sukernik, @allthecolors, @dvf, @dguido  thank you, great recent comments and feedback.\n\n\nIt seems like now is a good opportunity to “clear the air” and address several items as well as specificities related to this Proposal and discussions.\n\n\nFirst and foremost, we are new to working and partnering with DAOs and made some assumptions about the speed at which we should approach this process.  We were also surprised by the lack of competing bids during the 2-3 week period that we presented our proposal.  We assumed, incorrectly, that given the lack of bids we should go ahead and press forward with a vote.  We did consult with various folks privately and they also felt it was strange that no other firms jumped into the fray.  As such we went forward, even though we understand that people like @Getty were publicly pushing for competition.   We understand this could have looked like we were pushing through a vote without proper competition or time for the community to reflect. However if you put yourself in OpenZeppelin's position, it made sense to continue moving forward with the process.   Even in retrospect we still are not sure how we would have known when the right time to pull the trigger would have been, but understand how this could have been perceived as high pressure. @allthecolors and others hope that give some context\n\n\nSecondly in regards to the streaming payment method we chose, it had been used before, and given the parameters available made more sense to use than asking for a lump sum payment for a year or half year etc.  We felt that it would have been wrong for us to ask for a bunch of money upfront as opposed to having it stream out and let governance end the stream when appropriate. Lesson learned here as well…@arr00 and others hoping this gives our perspective.  Please let us know what you would have suggested here...\n\n\nThirdly, in regards to the performance fee and its binary nature. We listened to the initial feedback and given the complexities of changing it, specifically suggested that the community continue discussing the fee and delay it until the end of March next year.  We did not include this fee in our formal proposal for the vote  Also, at the last community call, we discussed that we were postponing the decision on this fee and also discussed that it would make sense for us to roll in concepts that blockchain @ berkeley were proposing vis a vis bug bounties.  \n\n\nWe now understand that even though competition was demanded, even necessary for some, Compound Governance doesn’t have a tried and true mechanism for evaluating competing vendors and that we are treading on new ground here. Therefore we are happy to be patient as the Community deliberates on what the best way forward is here.\n\n\nIn the meantime, we will be revising our proposal to reflect community feedback and our new thinking on the performance fees,  as well as various Q&A that has come from this process.  \n\n\nPlease keep the feedback coming and we look forward to all your thoughts and comments!\n\\n\nSecurity Solutions for DAOs\n\nOverview\nWe live in a new world where more and more DAOs are in control of a project. Ensuring that safe choices are made is a major challenge and DAOs have been exploring different approaches. According to current proposals, Compound would effectively outsource their security to a vendor who would take care of various security-related needs.\nWe believe that this approach is not the route to take. The chosen audit company would become a critical dependency for the protocol. The suggested solutions with their intransparent, quarterly, lump-sum payment for a mix of services do not focus on the most effective tools. Hence, we would like to suggest a process that can leverage the strength of the security community.\nThe main security challenge for a protocol like Compound is the growing complexity that needs to be taken into account with every change:\n\n1280×720 85.2 KB\n\n\nDue to upgradability, Compound needs to ensure compatibility with previous versions. Failure to do so can have significant consequences 9.\nAs cTokens are used in various other DeFi protocols, these integrations must not be affected by the proposed changes.\nWhile certain governance votes are being voted on, others are in development or under review. This creates complicated dependencies.\nAs the Compound protocol grows and starts to consist of more contracts it must be ensured that all contracts still interact correctly. A current compound liquidation involves roughly ten contracts and roughly fifty calls between these contracts.\n\nOverall, we conclude that such systems exhibit quadratic complexity growth. To tackle this, we aim for scalability. Furthermore, we believe that the security can best be ensured by a broad base of security providers. This prevents any form of vendor lock-in and conflicts of interest, while allowing to combine the strengths of different providers.\n\nSuggested Process in a Nutshell\nOur suggested process is as follows and acts on different stages of the DAO:\n\n1280×720 75.5 KB\n\nIn detail our process has the following steps:\n\n\nEarly feedback  during RFP stage with code reviewers adding security consideration paragraphs\n\nThis often reduces review cycles and hence development costs, review costs and time-to-market as code reviewers can foresee likely issues\nExample: Security considerations for RFP 8 4: Balancer V2 liquidity as collateral | $25k\n\nThe RFP aims to allow Balancer V2 liquidity as collateral. Recent attacks against cream.finance have shown that repeated borrowing can create an extremely leveraged position. When reviewing the security, special care must be taken to ensure that the value of the liquidity tokens cannot be artificially inflated. This would cause the protocol to lose funds as the collateralization ratio is insufficient to compensate for the price change. An attacker could extract these protocol losses and hence retroactively finance the artificial inflation of the liquidity token values.\n\n\n\n\n\nPerform thorough code review of newly developed code\n\nThorough expert code review of to-be-deployed code is still the most effective tool available today.\nCode reviewers inform the developers about all shortcomings related to security, efficiency or design.\nDuring code review, code reviewers jointly contribute to an audit suite by encoding all successful attacks as well as attacks that are currently not possible but might work in the future.\n\n\n\nDevelop an audit suite\n\nComposed primarily of extensive tests executable against forked mainnet at any point in time, allowing to continuously evaluate the security of the protocol and new proposal\nThe audit suite is open source under the same license as Compound Smart Contracts (BSD-3). It must be executable publicly without restriction by having suitable licenses in place for potentially proprietary components.\n\n\n\nLeverage tool-based analysis where efficiently possible\n\nProgram Verification Tools currently cannot show correctness for a protocol as complex as Compound.\nUse tools selectively with more complicated techniques to validate correct behavior of subsystems, e.g., formal verification, static analysis or fuzzing.\n\n\n\nPerform the Proposal Verification\n\nOnce the development has completed, the proposal is prepared on-chain.\nThe extended test against forked mainnet of the audit suite allow anyone to verify the compatibility of the proposal. In particular they can check that the proposal does not break any integrations and is compatible with previous versions.\nAs the proposal includes the concrete parameters chosen, these parameters can be verified as part of this step.\n\n\n\nIn contrasts to other suggested processes, we focus on prevention. While monitoring is certainly nice to have, Compound already has some monitoring 3 and attacks often take place in one (large) transaction. Hence monitoring and alerting have limited effectiveness, especially considering the present timelocks.\n\nFinding Suitable Code Reviewers\nThe biggest hurdle for code reviewers getting into a new complex system is the required initial investment to understand it. Fluctuation and internal availability of employees also require any review companies to constantly invest into training, which can reduce either quality in times of high demand.\nTo tackle these problems, we propose the following general solution:\n\n\nCompound Governance votes in several code reviewers each year to onboard into auditing through a paid training phase.\n\n\nAuditors apply to become wardens of Compound by providing:\n\nAn example of how a code review for the Comptroller and the cToken would be performed, detailing methods and time required.\nThe cost and amount of hours reserved for Compound per quarter, which are if accepted guaranteed to be paid by Compound.\nThe expected compensation for the training phase.\n\n\n\nOn-boarding code reviewers can also greatly benefit from the existing audit suite which contains many relevant attack scenarios.\n\nAny time reserved for Compound not spent on reviewing code or RFPs is used to improve this audit suite.\n\n\n\nAuditors who are selected by Governance and complete the training take shared ownership of the audit suite and collectively improve the coverage.\n\nDelivering good work in improving the Compound Audit Suite will likely be used to evaluate the quality of the auditor which aligns interests long-term by increasing the likelihood of being voted in again next year.\n\n\n\n\nBenefits of the Audit Suite\n\nAuditors think of many cases, most of which are not exploitable currently. Putting them in the suite helps to test for this case going forward. It aggregates knowledge from multiple code reviews and companies.\nExpert know-how of the system and the possible attacks against it are captured for future auditors.\nWith testing against forked mainnet it is possible to simulate the effects of all the currently open proposals independently as well as combined.\nThe Audit Suite enforces the definition of explicit value ranges, making code reviews more effective and simplifying certain governance proposals which just update parameters within those ranges.\nIt provides extremely good feedback for developers reducing review cycles and thereby financial and temporal overhead.\nAuditors will gain confidence in having audited the system holistically even when reviewing seemingly small changes.\nTasks that needs to be performed repeatedly, such as the compatibility check with past versions, can be automated once and efficiently repeated.\nThe suite ensures the correctness of on-chain proposals including correctly chosen parameters which are generally not part of the code review.\nGiven its design the audit suite provides a decentralized and permissionless mechanism for community members to inspect the state of the protocol and to evaluate ongoing proposals.\n\nConsidering all these benefits, we believe that the audit suite can provide the necessary scalability by capturing a lot of the growing complexity. It requires an initial investment to build it up, but we consider it worthwhile.\n\nSummary\nOur suggestion incentivizes broad participation from small to large security companies. Thereby, it broadens the base of experts in Compound and leverages proven techniques. Using the audit suite, we can aggregate the knowledge and insights from multiple audits (instead of only relying on the last audit) and thereby tackle the growing complexity. We believe that this provides a viable long-term solution.\nWhile even a single audit company chosen to shepherd the protocol could write such an audit suite, only by having multiple companies who rely on each other to have covered critical parts of the system enforces that a public, high-quality audit suite gets created. It also allows every security provider to play to their strengths when contributing. Lastly, it allows a decentralized verification of proposals by any community member.\\n@sukernik @dguido @OZSecure\nHi everyone, I’m Rishabh from Blockchain at Berkeley.\nThank you to both the OpenZeppelin and TrailOfBits teams for starting a discussion of the need for more Compound security. While the entire community seems to agree on the need for security, there is the question of which company to procure those services from. I also agree with Larry that we should allow other security companies to forward their bid for the contract as well.\nOverall, I believe, of the two options that Larry presents, the second is overall more beneficial to the protocol.\n\nDirect voting by tokenholders is most in-line with the spirit of decentralized governance. Voting should not be filtered through the lens of some pre-selected committee; it adds additional layers of unnecessary bureaucracy.\nIn OZ’s original proposal, they highlighted the need for quick iteration on this issue. Other commenters have tended to agree. Direct voting lets one company start work on this issue as fast as possible, rather than having to wait for some committee to deliberate, then possibly multiple governance proposal cycles (if the tokenholders reject the committee’s recommendation).\n\nHowever, the immediate problem with this idea is that Compound proposals have traditionally only supported only Yes, No, or Abstain votes. However, in the Governor Bravo contract, there is an option for voters to attach text to their on-chain votes. Thus, if we assign each candidate in our vote a two-letter code, we can measure the support for each option through the comments.\nSupporting multiple options in proposals is also a necessary step for Compound governance, so that tokenholders can make more complex decisions in the future (i.e. elected positions, setting variable parameters, etc.).\\nHi Guys,\nThis is Rex from DeFiSafety. As I indicated in an article 3 I wrote about the Compound Proposal 62, I believe this latest proposal is trying to solve the wrong problem. What Compound (and other protocols) need is not more 3rd party review but rather more staff. The Proposal 62 needed a PM and an internal software and process quality staff more than a third party auditor.\nProtocols need more on the internal team watching and reviewing tasks before they present the results to an external auditor or a DAO. Based on my analysis of 62, the dev was mostly on his own. He wrote the code, wrote the tests, ran the tests, put the code on the testnet and asked the community to review and test. I never saw any evidence of actual community review. Finally, an external auditor looked it over and the result went to the DAO. I did write to the dev to get his comments on the article, but I did not get a response.\nThere did not seem to be an internal quality person or a PM. There did not seem to be internal dev process that might have said dev can’t write his own tests.\nI believe a large protocol like Compound should have at least one internal QA person, a PM and a written process.\nIs there a reason I have missed why protocols do not build internal staff? Could the staff be a target of regulatory attacks? But if that was true, it would be true of the auditors too, especially if they take over tasks that would be internal in the corporate world.\nAnother weakness I see is a lack of internal process documents. An example of this is that latest bZx EOA failure. The dev should not have used his main computer to execute trades on a major protocol account (remember Hughs’s exploit?) and the account should have had a multi-sig. There should be a public process document where bZx says how things should be done. In the case of COMP 62, the process should not have allowed one dev to do the entire requirements/code/test process on his own.\nActually, that inspires me. DeFiSafety can write a process doc, get community content and allow individual DAO’s to vote the document into their process. Perhaps Compound community could contribute?\nI am not against more auditor participation and have nothing but praise for all the auditors in the chain of messages. DeFiSafety is not bidding for any work. We do not do consulting to protocols. Our goal is revenue from DeFi users so when we rate protocols like yours we are not in any conflict of interest.\nHowever, our team watches DeFi protocols every day and thinks about problems from a process quality perspective. Please take this as our comments from that perspective. We are always available for discussions.\\nPlease find our anticipated proposal below. We iterated on several items and terms that were present in our first draft, so there are a few refinements versus what I posted earlier!\nI also spoke on Discord at the Compound Community Dev Call earlier today. I carved out my part of the presentation 9 so you can listen to it briefly since it may add color to the proposal.\n(For convenience, I also extracted the audio from ChainSecurity’s portion of the call 4.)\n\nExecutive summary\nCompound seeks to prevent insecure proposals from being merged via the decentralized governance process. Trail of Bits will provide comprehensive software assurance services to mitigate this risk across three specific activities: consulting services, security engineering, and process creation.\nWe believe it is essential to create an easy-to-follow process with highly robust tools that makes security transparent, with or without a code review from Trail of Bits. We consider the primary goal as building capacity in the Compound ecosystem to secure itself. To that end, we will provide engineering efforts to develop critical security infrastructure and processes.\nTrail of Bits will be paid the equivalent of $1 million USD in COMP every quarter for one year to provide the baseline services. This payment is all-inclusive of all services defined in the proposal.\n\nGoals and Non-Goals\nGoals of this effort include:\n\nPrevent insecure code from being merged into Compound through the governance process, and ensure that any remaining risks of proposals under consideration are well known before a vote. Compound desires services that eliminate and illuminate these risks.\nProvide first-class security tools and analytical capabilities to Compound developers. Compound developers must have every opportunity to analyze their code and understand its security ramifications. Compound desires a variety of tools that enable thorough inspection of code by developers and users.\nCreate repeatable processes that build capacity for security and avoid dependence on external audits or third-party services for security. Security efforts should result in higher quality code being developed by the Compound community over time through the adoption of consistent processes.\n\nNon-goals of this effort include:\n\nDeploy, configure, or maintain on-chain monitoring software. This goal does not address the security of governance proposals. These systems are an active field of research, may apply to fewer attacks than expected, or may have undesirable performance costs. Instead, we will commit to evaluating their opportunities and limitations during the project.\nFurther development of the bug bounty program. We support the formation of a bug bounty proposal to specifically address this issue. We will provide consultative advice to this proposal based on our years of experience seeing both ends of the bounty ecosystem.\n\n\nDescription of the Services\n\n1. Consulting services\nTrail of Bits will provide at least one full-time security engineer, with additional engineers supporting the project as needed, to perform the following consulting services:\n\n\nMaintain a presence on Discord and the Compound forums. We will actively engage in conversation, reach out to developers as needed, and identify any new proposals.\n\nProvide proposal authors with 1:1 counseling sessions. We will host a video call with the author to understand their goals and provide immediate feedback, including on the architectural design of their proposal and suggestions to reduce complexity.\n\nReview and report any identified security issues in the code for proposals. We will describe the issue, a scenario to abuse it, and a recommendation to address it. We will work with proposal authors to validate fixes that result from these reports.\n\nDefine security properties for proposals. We will work with the authors to provide reasonable security invariants alongside the proposal and tests for them in Certora, Echidna, or other tools, as appropriate for the specific invariants under test.\n\nDocument “Security Considerations” for every proposal. We will contribute a standardized section to reviewed proposals to inform developers and users of limitations, risks, or other considerations to form an opinion about their vote on it.\n\nProvide our analysis directly to the community. Before a vote on any governance issue, we will host a public community call, walk attendees through the documented Security Considerations, and run test suites, fully informing their decisions to vote.\n\nHost bi-weekly Office Hours with developers. We will cover testing and verification tools, demonstrate new security processes and tools for Compound, and solicit feedback for new areas of development and guidance needed by the community.\n\nEvaluate new security techniques for adoption by Compound. We will perform detailed evaluations of the applicability of these techniques directly on the Compound codebase, sharing our empirical results of efficacy and utility.\n\nAd-hoc services sourced from across Trail of Bits, as needed. This includes expertise from our separate teams for cryptography, application security, cloud-native security, threat modeling, machine learning, and other research teams. Our firm employs more than 80 researchers servicing clients in tech, defense, and finance on high-risk security challenges.\n\n\n2. Security engineering\nTrail of Bits will engineer solutions in software to critical security risks faced by Compound. We will ensure that first-class security tools are available, easy to use, and customized for use on Compound, with knowledge of Compound-specific risks built in.\n\n\nEnsure that Slither, our static analysis framework, and Echidna 2, our rapid security property tester, always work on Compound code. These security tools are delicate machines that must ingest all code possible to write in Solidity and EVM. We will ensure that no breaking changes affect Compound for an extended period of time and these tools always “just work.”\n\nCustomize Slither and Echidna to the Compound codebase. We will extend each tool to understand Compound’s architecture, expected security properties, and third-party protocols, vastly enhancing their depth of results. For example, we can build static analyses that understand and aggregate data from Certora properties or that understand specific Compound interfaces.\n\nCustomize Slither to evaluate the security of upgrades. Upgradeability exposes low-level complexity with possibly disastrous results. Slither already evaluates for 17 such flaws, and we will enhance this analysis with Compound-specific conditions.\n\nDevelop scaffolding for new proposals with pre-integrated security analyses from Slither, Echidna, Certora, or others, as appropriate. These templates will provide secure beginnings for parameter changes, new tokens, protocol features, and governance changes.\n\nContinuously define and evaluate security properties across the Compound codebase with analyses from Slither, Echidna, Certora, or other techniques, as appropriate. New proposals may expose under-specified areas of Compound that require greater formalization. We will work to fill in these gaps with new properties.\n\n\n3. Repeatable processes\nTrail of Bits will build the capacity of the Compound community to secure itself, minimizing dependency on external security audits for security and most efficiently using their time when engaged. We will define and socialize repeatable processes that encapsulate common tasks with security integrated within them. Adoption of these processes will set a floor on proposal quality, continuously secure proposals regardless of security auditor review, and improve the quality of proposals over time.\n\n\nDesign a repeatable process for starting a new proposal. We will develop onboarding guidance for developers that describes the end-to-end process for securely creating new proposals. Touchpoints will include security training, using pre-created templates, example security properties, guidance on tools, self-assessment, engaging with Trail of Bits for code review.\n\nDesign a repeatable process for proposal self-assessment. We will provide guidance to assess the security of proposals before sharing them publicly. This will facilitate more effective conversations about security with the community and Trail of Bits, knowing that an initial baseline has been met.\n\nDesign a repeatable process for risk assessment by the community. We will share the risk factors that security experts focus on when reviewing proposals and document steps to run testing and verification tools, thus providing a map for those voting on proposals to become better informed and obtain empirical evidence.\n\nDesign a repeatable process for evaluating third-party protocol integration risks. We will design a repeatable process to investigate the security considerations of new token implementations and other linkages to DeFi building blocks from Compound. We will document existing pitfalls and concerns in third-party code already used by Compound, and facilitate the discovery and documentation of others.\n\nDesign a repeatable process for other protocols to securely integrate with Compound. In the reverse of the above, we will describe security considerations for DeFi users of Compound. In our mind, any compromise involving Compound, even if it does not originate from our own code, will compromise its reputation.\n\nRegularly update a “treasure map” for bug hunting in Compound. We will guide other security researchers to less specified, more risky areas of the code. We will regularly report statistics on these identified hotspots, such as % coverage for specifications.\n\n\nRisks\nCompound may have unknown, latent security vulnerabilities already present in the code. Our proposal focuses on new code added to Compound and, therefore, these issues may continue undiscovered. To mitigate this risk, we have included a) a task focused on specifying new security properties, as needed, and b) a task to build a treasure map to aid other bug hunters.\nCOMP holders may vote for proposals that contain documented security risk or that are otherwise highly risky, considering the reward to be greater in that circumstance. To mitigate this risk, we will a) add documented Security Considerations to every proposal and b) host a community call to walk through those considerations, demonstrate how to run included test suites, and evaluate the coverage of them.\nCompound is highly complex and new proposals may use new features of Solidity or combine features of Solidity in ways that break the security tools upon which it depends. To mitigate this risk, Trail of Bits has specifically prioritized the development and testing of new features for Slither and Echidna against the Compound codebase.\nProposals may require swift approval without waiting for input from Trail of Bits, or input from Trail of Bits may be otherwise unavailable within the timeframe required due to unknown circumstances. To mitigate this risk, Trail of Bits has proposed a robust sequence of processes and tools for the community to enhance trust in themselves and better understand the risks of their actions.\n\nFinancial Terms\nTrail of Bits will be paid the equivalent of $1 million USD in COMP every quarter for one year to provide the services. This payment is all-inclusive of all services defined in the proposal.\\nI have to say that for the long-term, the model proposed by ChainSecurity seems to me to be the most comprehensive and beneficial to the protocol. I agree that cultivating an RFP process, and in general focusing on the codified process, regardless of auditor, is a direction I believe we should move in to put the protocol in the strongest position going forward. That said, I can see how signing an agreement with a single auditor to guarantee their time is something that could be more useful in the shorter term. I think it would be a great if we could find some sort of balance that moves us towards a robust process, without lock-in, but acknowledges some of the difficulties implementing the full RFP process immediately.\\n\n@jared as we said on the community call this week, OpenZeppelin would be open and happy to work with other auditors where appropriate or desired by the DAO. We agree that the community will best benefit from a codified robust process that does not involve vendor lock-in, and we have tried to make that clear in our proposal. We are also open to working closely with firms like Certora if and when Formal Verification is required by the DAO. Overall we view partnerships and codified processes as critical to a layered security approach to ensure the security of the DAO and complementary to our continuous audit proposal.\n\n\nPlease find our revised proposal summary below:\n\nOpenZeppelin’s Updated Proposal Summary\n\nThe Compound DAO’s long-term security requires a comprehensive and continuous set of audit and security solutions to prevent loss of funds and protect its reputation resulting from risks to the Compound protocol, specifically those introduced by community-proposed upgrades\n\n\nOpenZeppelin will provide dedicated continuous audit services for all Compound governance proposals and will work with the Compound community to develop comprehensive security requirements and to implement best practice security monitoring. \n\n\nOpenZeppelin's services will be coordinated by a dedicated Security Advisor who along with the OpenZeppelin team, the Compound DAO and the community will work to:\n\n\nImprove the overall process to ensure the security of community proposed upgrades to the Compound Protocol  \n\nProvide continuous audits and dedicated resources to respond rapidly to all community proposed upgrades and changes \n\nCoordinate the creation of documented security checklists and requirements that can be shared with all proposal authors\n\nImplement an open security monitoring and security dashboard solution that will allow the community to validate security \n\nIntegrate, support, and analyze other possible future important security program components such as formal verification, bug bounties, and white hat monitoring approved by the DAO.  \n\n\n\nThe combined effort of the OpenZeppelin team, the Security Advisor, and the Compound community will thereby reduce potential security risks and further assure the DAOs trusted reputation.\n\n\nOpenZeppelin has revised its original proposal to focus on community feedback and excludes performance fees.  OpenZeppelin’s fee will be the equivalent of $1 million USD in COMP every quarter for one year, to provide these services.  This fee covers all services defined in the proposal.  Please see our full revised proposal here :\n\n\nOZ Final Proposal 36\n\n\nWe believe that no other firm in the market can bring the same breadth and depth of  offerings to the DAO.  We provide best-in-class continuous auditing and security advisory services; established leadership in secure development and secure operations; and external relationships and partnerships at a cost to value no other firm can match. \n\nNext Steps\n\nAssuming a vote begins on Dec 6th and OpenZeppelin’s Governance Proposal is selected by Dec 13th, the Compound community can expect the following timeline:\n\n\n\nProposal Security Process (starting on Dec 13th) - the Security Advisor (SA) will immediately engage with the Compound community to create a well-defined process for auditing protocol changes prior to being proposed. This will include a regular and agreed upon set of KPIs and communication plan to update the community on a regular basis, including but not limited to community calls.\n\n\nComprehensive Compound Audit (starting in January): A team of dedicated OpenZeppelin security auditors will perform a comprehensive review of the currently deployed smart contracts. Many of whom have worked with Compound smart contracts in prior audits.\n\n\nContinuous Audits (starting in February): With the Proposal Security Process defined and a Comprehensive Audit complete, OpenZeppelin will be fully prepared to provide audits on all protocol change proposals. Note: We can begin auditing proposal changes earlier but given the lack of protocol changes currently pending, we expect February to be the ideal time to start.\n\n\nSecurity Requirements (starting in January): the SA and assigned members of the OpenZeppelin team will engage with Compound Labs and the Compound community to gather current security requirements and begin building comprehensive security requirements documentation and checklists.  \n\n\nSecurity Monitoring: (starting in January): the SA and assigned members of the OpenZeppelin team will begin bi-weekly community workshops to gather requirements and share progressive design and implementation plans on comprehensive security monitoring and building a security dashboard. \n\n\n\nWe would be honored to partner with the Compound DAO to not only deliver continuous auditing but to also work together to be leaders and innovators in how to securely and efficiently run an effective DAO security program!\n\\nDear Compound Community,\nChainSecurity thanks you for the feedback on our proposal.\nWe’ve received positive feedback regarding increased competitiveness, redundancy of audits, collaboration between auditors. However, the following questions were raised:\n\nHow can we ensure continuity with multiple auditors? How can we avoid things falling through the cracks? How can we avoid the risks and costs of switching auditors?\nWhat is the cost of this proposal?\nWhat exactly is the audit suite and what role does it play in the governance process?\nWhat role would ChainSecurity play?\n\nPlease find our answers below.\n1. The shared audit suite provides a common work basis and alleviates the risks of switching between auditors\nRisks of discontinuity can be best alleviated with an extended test suite, called the “audit suite”. The audit suite is a collection of test cases designed by each auditor as they review or study Compound code.\nWhen an auditor reviews the code manually, (s)he imagines numerous potential attacks and their impact on the system. If the system is vulnerable to a few of these attacks, only those end up in the audit report, while the countless other attacks imagined for the audit remain undocumented, only stored in the mind of the auditor.\nWith the audit suite, the auditor would create tests for each of these attacks, even if they are not successful with this specific code. By storing this thought process in the shared testing suite, we can build up on the thinking of that auditor in future updates of the system, even if (s)he is not present for the next audit.\nRecording all test cases also helps to monitor the quality of an auditor’s methodology, and offers more transparency to the community.\nMultiple auditors would be working on Compound, each of them contributing to the test suite. Each auditor would have an incentive to create the best tests possible, as it would be a great reputation builder if one of their tests identifies a vulnerability in the future. The common test suite would expand the Compound auditing knowledge as it is shared, instead of keeping it segregated. As time goes by, the audit suite will comprise more and more attack scenarios, and allow an increasingly performant review of the protocol.\nWe believe this extended test suite could recreate the continuity of knowledge you’d find in one auditing firm, or even exceed it: even in a single auditing firm, employees may get sick, go on holidays, or leave the company, and their unrecorded knowledge will go with them.\n(See more details on the audit suite in our initial proposal.)\n2. Multiple auditors are trained on Compound codebase and ready to jump in the audit process at any time\nTo clarify this point, let us describe the next steps of the voting process with our proposal:\n → STEP 1: Compound governance votes for our proposal\n\nThis doesn’t select a specific auditor, it just approves the system.\n\n → STEP 2: Auditors apply in a standardized way, which allows easy comparison between service providers. As stated in our original proposal:\n\n“Auditors apply to become wardens of Compound by providing:\n\nAn example of how a code review for the Comptroller and the cToken would be performed, detailing methods and time required.\nThe cost and amount of hours reserved for Compound per quarter, which are if accepted guaranteed to be paid by Compound.\nThe expected compensation for the training phase.”\n\n\n\n → STEP 3: Compound votes for the auditors\n\nCompound governance can vote for several auditors and/or proposals\n\n → STEP 4: Auditors are onboarded into the system\n\nSelected auditors start the training phase (which is a full audit of the system). This training phase brings the auditor up to speed, secures the system even more and improves the shared testing suite: the auditor will add tests coming from a new perspective\nDuring the training phase, previously onboarded auditors can spend time helping new auditors (if time allows)\n\n → STEP 5: Auditors work in partnership\n\nAuditors secure the protocol and contribute to the test suite as per their proposal\nEvery year, Compound governance votes to keep the auditors in the system or not\n\nAs an example, please find here 19 the proposal we intend to submit.\n\nPS: Thanks @dguido for recording the audio of our intervention expalining the proposal in the Compound dev call: chainsecurity.m4a - Google Drive 4\\nIt’s been over a month since I posted the original request for auditors. Since then, we’ve received several comprehensive proposals from top-tier smart contract auditors. Candidly, I thought there would be some interest to audit Compound protocol, but I didn’t expect we’d get as many proposals as we did. It’s spectacular to see so many vendors vying for Compound’s business. If you’re looking for what a B2DAO process looks like, this is it.\nAs mentioned in an earlier post from this thread, Compound protocol does not have a clean and efficient process for evaluating several vendors. As a result, tokenholders and vendors aren’t sure what happens next. This is problematic for both parties: Compound needs an auditor as soon as possible, and the audit firms have a business to run and need to assign resources to their customers ahead of time. In short, we need a process for picking the vendor of choice.\nI spoke to several community members about this, and folks suggested a quick and easy process. In a nutshell, we’d be using the community multisig to run a simple process. The community multisig would first whitelist each vendor to make a proposal. After that, tokenholders would vote on their favorite proposal. The proposal with the most “For” votes wins. To prevent more than one proposal from winning, the community multisig would then cancel the losing proposals after the vote is completed.\nI’m including a more detailed version of the process below.\nAudit Selection Process\n\nReverie to create a form for vendors to submit their Ethereum address - 12/7\nReverie to confirm the address belongs to the vendor through a video call - 12/8\nReverie to share addresses with the community multisig - 12/8\nCommunity multisig to whitelist the address for each vendor - 12/9\nAfter being whitelisted, each vendor will submit their proposal for an on-chain vote - 12/13\nTokenholders vote for their favorite proposal after a two-day review period - 12/15-12/17\nWinning proposal is queued for execution in the timelock; losing proposals are cancelled by the community multisig - 12/17\n\nSince Reverie (the company I started with Derek Hsue) initiated the audit search process as part of this thread, we propose that we also complete the vendor selection process on behalf of Compound protocol. It took considerable time for us to reach out to the audit firms, explain Compound’s needs to them, encourage them to submit proposals, and to walk them through the process. We would appreciate a $75k COMP grant for the work we’ve done here.\\nThis structure makes a lot of sense and is a good use of the whitelisting proposers feature.\nWhile I don’t think we need to worry about the firms making other proposals, setting a relatively short expiry on the ability to propose is a good idea. Maybe two weeks? It could be even shorter, like 10-days.\nThis whole process has been a great learning experience for DAO, and this setup will be good for future B2DAO situations.\\nThanks @sukernik. This structure makes sense and provides a clear path to finalizing the process.   A couple thoughts:\nTiming. The proposal deadline Larry suggested above (12/13) is reasonable. The community has given ample time to receive new bids and needs to be respectful of the auditors who have committed time, effort and resources towards the process to date. Also, it makes sense to require all proposals to be submitted at the same time so they run simultaneously.  To that end, it might make sense to specify an exact time on 12/13 for the whitelisted auditors to introduce their proposals. That way we can ensure the votes run as close to simultaneous as possible.\nVoting Mechanics. Under this model, we should make clear that tokenholders should only vote for one proposal.  In other words, tokenholders should vote “yes” on the proposal they prefer, and abstain from the others (since the multi-sig will cancel all proposals except the one with the most “yes” votes). This will ensure that we avoid any weird overlapping vote dynamics between the various proposals.\nOtherwise, this approach looks great. Thanks again to @sukernik and the Reverie team in taking the initiative and creating a framework that works for all sides.\nJeff Amico\na16z\\n\n\n\n sukernik:\n\nSince Reverie (the company I started with Derek Hsue) initiated the audit search process as part of this thread, we propose that we also complete the vendor selection process on behalf of Compound protocol. It took considerable time for us to reach out to the audit firms, explain Compound’s needs to them, encourage them to submit proposals, and to walk them through the process. We would appreciate a $75k COMP grant for the work we’ve done here.\n\n\nNo complaints yet for the $75k fee? good to see. We’re making progress people.\\n“Community Multisig”\\n\nNo complaints yet for the $75k fee? good to see. We’re making progress people.\n\nIt’s really clear from our end that Larry has been a tremendous help in getting this done, and the multisig-based voting process is just the latest effort that he’s invested. He’s definitely an asset for all you folks!\\nHey folks, apologize for the tone of this post. I feel quite strongly that anyone who does work to add value to a protocol (especially one as big as this) should be able to capture meaningful value/ownership from that work. COMP has not done this well in the past, but I see things like this (meaningful fee for arranging an auditing firm to work with the community) as a great step in the right direction. This should be the standard going forward (and we should even consider going back to retroactively reprice past contributions).\\n\nUnder this model, we should make clear that tokenholders should only vote for one proposal. In other words, tokenholders should vote “yes” on the proposal they prefer, and abstain from the others (since the multi-sig will cancel all proposals except the one with the most “yes” votes). This will ensure that we avoid any weird overlapping vote dynamics between the various proposals.\n\n\nIn terms of voting mechanics it seems like this adds additional and unnecessary friction. If voters & the community multisig are required to do this every time we pick a vendor for a specific function on Compound, it sets a bad standard for future votes (though this model should work in the short-term for this vote). We should look into doing one of the following:\nDirect Voting for Vendor Selection. Adding additional functionality to the GovernorBravoDelegate 2 contract to enable multiple & customizable options for voting on governance proposals. In this model, a “Audit Vendor Selection” Proposal would be created with ToB & OZ & Abstain listed as potential options by the community multisig or an unbiased third party, with the ToB & OZ correlated to a specific action (i.e. sending COMP tokens to the auditor as the first payment for auditing). There should be an arbitrary limit on the number of options allowed on a governance proposals (i.e. 2-4 to start), and voters would only be allowed to pick one option.\nCorrelating Proposals. Similar to the role that the community multisig is playing in @jamico’s model, we would create a function that would enable proposals to be batched together, with only the proposal in the batch with the highest votes FOR (where the proposal is passing) being approved. In this model, a third party proposer would create the batched proposals, and launch them together. Should any proposal pass by the end of the voting period, all proposals but the one with the most FOR votes would be cancelled by the contract. Additionally, voters would only be able to vote for one proposal within a batch.\nIn either of these two options, we solve the problems of 1) relying on the community multisig & 2) requiring voters to be cognizant of not voting on more than one proposal. Still trying to figure out the specific details for implementation, but would love to hear thoughts on implementing such functionality into the governance module.\ns/o to @Rk2357 & @annamira for the inspiration here\\nAgree that this method is not ideal, but the best option currently available. Modifying governance is not a quick task but could be done with time. I find the first option (multiple choice voting) to be the best solution.\\nWe have three great proposals from three reputable firms - thank you all for posting proposals! Thank you @sukernik for organizing this - the $75K COMP grant seems fair.\nRight now I’m leaning towards OpenZeppelin as my top choice. The revised proposal seems like a great value not only for Compound but for the whole ecosystem! OpenZeppelin’s Ethernaut 6 was a tool I used to learn about smart contract exploits and how to avoid them. I can’t wait to see how the new Audit Suite will further improve smart contract security.\\nWhile the ChainSecurity proposal is interesting, I don’t think it meets the short term needs, so I’ve ruled it out for now.\nI’ve evaluated the two proposals from Trail of Bits and from Open Zeppelin. Both are overpriced considering market rates, but both provide more value to Compound than they cost. I would be happy with either.\nFocusing on the differences, rather than the similarities.\n\nTrail of Bits\nPros:\n\nEchidna customization/rules. Echidna is a good tool to have in the security toolbox. As a fuzzer it is worse at finding tiny errors than formal verification, but is much easier to use and reason about, making it overall better at finding big stupid hidden things you have done that have snuck past your unit test because you didn’t think of them…\nExternal integration guide for safely integrating Compound. While Compound is not downright evil to safely integrate with (unlike Curve), it would be good to have a clear guidance for others here. It would be good for both the safety of the protocol, and for potential increase in future usage.\n\nCons:\n\nToB’s proposal is focused on building Compound’s ability to self audit and self manage security. While this is a good goal for most teams, the Compound protocol currently has neither a core developer group, nor it’s own in-house security group. Instead outside individuals code and make proposals, and may or may not be around to work with the later proposals of others. I think that in the current state of Compound, this focus is misplaced. (If I though Compound would be realistically building out a team in the next few months, this con would become a pro)\n\n\nOpen Zeppelin\nPros:\n\nNot just auditing changes, but monitoring changes through the rest of their lifecycle, including deploys, on-chain configuration, and beyond.\nUpstream monitoring. Compound’s security depends on the oracles behind it and the and security of the tokens it lists, some of which can be upgraded. It would be vital to know when changes are happening to upstream tokens.\nIn general, the OZ proposal focus more on OZ doing the work, rather than bringing a non-existent Compound team up to speed. I think this is more of what is needed at the current time.\n\nI lean towards OZ.\\nQuick update.\nOpenZeppelin, Trail of Bits, and ChainSecurity have posted their proposals:\n\nOpenZeppelin 70\nTrail of Bits 48\nChainSecurity 44\n\nI encourage everyone to review the proposals in depth in order to make an educated vote.\nBest of luck to all of the vendors!\\nAt Gauntlet, we are excited to see these proposals and believe that auditing is a valuable and necessary service for the Compound protocol. We are abstaining from this vote as it is difficult to weigh in from a quantitative perspective, which is what Gauntlet always strives for in its decision-making process. We look forward to working together with whichever provider the Compound community ends up deciding on, and are excited to collaborate on value-add proposals such as risk parameter recommendations and listing new assets.\\nCongratulations on a successful governance proposal @OZSecure!\nAs a first order of business, who will be our Security Advisor, and how might we contact them regarding scheduling upcoming audits?\\nHi @jared\nMy name is Michael Lewellen. I’ll be the Security Advisor for you all on behalf of @OZSecure.\nBased on the Next Steps outlined in our proposal, our first goal will be to define a security process that proposal authors can follow leading up to an audit and submission to the DAO for a vote. This will start with engagement in the next community call and one-on-ones with key community members. After collecting feedback from everyone, we’ll then propose a process that will go through refinements with the community before being finalized.\nWhile we work on defining this process, I’ll share a draft version that I’ve been working on based on our usual audit process in a separate forum thread as I believe it deserves its own dedicated discussion.\nI’d love to start talking to key members of the community including yourself although it might be slow going over the next week due to holiday plans. I expect that we’ll really ramp things up by Jan 3rd to start preparing for a comprehensive audit of the Compound Protocol.\nAnyone that has protocol changes planned in the next future or would like to share feedback can contact me at the following:\n\nEmail: michael@openzeppelin.com\n\nTelegram: @cyloncat\n\nDiscord (in the Compound server): Michael L#3462\n\nI’m really looking forward to working with you and the rest of the Compound community!\\nQuick and final update on the process:\n\nOpenZeppelin’s proposal won, receiving 1.4M votes 16. Going forward, OpenZeppelin will provide audit services to Compound protocol and its community. If you’re a community member who needs audit work done, I strongly encourage you to reach out to @cylon.\nOpenZeppelin’s proposal is currently in a queue 4 to be executed on-chain. Once that occurs, OpenZeppelin will receive COMP on a block-by-block basis.\nIn return for starting and facilitating the audit vendor selection process, Reverie will receive $75k COMP from the grants multisig.\n\nFinally, I’d like to thank @dguido (Trail of Bits) and @Emilie_ChainSecurity (ChainSecurity) for participating in the process. I thought all the proposals were top-notch; Compound is lucky to have such great firms bid for the work!\\n@OZSecure what was the number used for blocks per year on your calculations?\nI want to ensure that we keep the payment as expected in case we migrate over to Sablier as discussed below.\n  \n    \n    \n    Migrate GFX Labs and Gauntlet COMP streams over to Sablier Proposals\n  \n  \n    TL;DR\nReplace the current COMP streams which GFX Labs and Gauntlet are recipients of with Sablier streams. \nHere is an introduction to Sablier, and the reasons its adoption within Compound matters. \n\nProposal\nCompound’s current token-vesting solution essentially relies on an endless COMP stream which can only be ended by a Compound governance proposal. The streaming solution itself is largely undocumented (Sablier docs). Additionally, the current COMP streams are based on block number rather th…\n  \n\n\\nHi @arr00, 6350 blocks / day\nSource:\nhttps://www.etherchain.org/charts/blocksPerDay 8\nPrice: 265 COMP/USD\nPlease let me know if you need anything else!\\nNice Information, thanks for sharing such valuable information"
  },
  {
    "number_of_comments": 9,
    "postid": "0cc666af-7b74-4e7f-9954-69868756ebcb",
    "posturl": "https://www.comp.xyz/t/gauntlet-weekly-market-updates-base-usdbc/4675",
    "combinedcontent": "[Gauntlet] Base v3 USDbC Update (09/08/2023 - 09/14/2023)\nGauntlet would like to provide the community with an update on metrics from the Base v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are down 9.19%, from $6.07M to $5.51M.\nUSDC Supply is up 21.65%, from $9.25M to $11.25M.\nUSDC utilization decreased 25.34%, from 66.0% to 49.3%.\nThe minimum USDC reserve growth was -1.0%, and the maximum was 15.4%. The average USDC reserve growth was 7.9%.\nThe comet accumulated $0.34k USDC reserves while distributing $11.96k COMP rewards for a weekly Net Protocol Profit of $-11.62k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-09-15 at 1.45.23 PM1796×476 67.8 KB\nTotal Collateral (USD) is down 4.28%, from $10.73M to $10.27M.\nScreen Shot 2023-09-15 at 1.45.48 PM1800×474 68.8 KB\nUSDC Supply is up 21.65%, from $9.25M to $11.25M.\nScreen Shot 2023-09-15 at 1.46.06 PM1794×470 67.5 KB\nUSDC Borrows are down 9.19%, from $6.07M to $5.51M.\nScreen Shot 2023-09-15 at 1.46.27 PM1656×464 40.5 KB\nUSDC utilization decreased 25.34%, from 66.0% to 49.3%.\nSupply Caps\nScreen Shot 2023-09-15 at 2.13.30 PM3786×620 87.2 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-09-15 at 2.13.53 PM3786×842 140 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-09-15 at 1.46.44 PM1666×426 35.9 KB\nThe minimum USDC utilization was 49.1%, and the maximum was 66.9%.\nThe minimum USDC reserve growth was -1.0%, and the maximum was 15.4%. The average USDC reserve growth was 7.9%.\nScreen Shot 2023-09-15 at 1.47.01 PM1670×452 31.7 KB\nThe comet accumulated $0.34k USDC reserves while distributing $11.96k COMP rewards for a weekly Net Protocol Profit of $-11.62k.\\n[Gauntlet] Base v3 USDbC Update (09/15/2023 - 09/21/2023)\nGauntlet would like to provide the community with an update on metrics from the Base v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are up 8.68%, from $5.51M to $5.99M.\nUSDC Supply is down 9.32%, from $11.25M to $10.2M.\nUSDC utilization increased 11.30%, from 49.3% to 54.9%.\nThe minimum USDC reserve growth was -0.8%, and the maximum was 8.5%. The average USDC reserve growth was 5.6%.\nThe comet accumulated $0.21k USDC reserves while distributing $12.41k COMP rewards for a weekly Net Protocol Profit of $-12.19k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-09-22 at 10.35.17 AM1800×488 67.4 KB\nTotal Collateral (USD) is up 7.54%, from $10.37M to $11.15M.\nScreen Shot 2023-09-22 at 10.35.34 AM1798×482 68.2 KB\nUSDC Supply is down 9.32%, from $11.25M to $10.2M.\nScreen Shot 2023-09-22 at 10.35.53 AM1792×486 66.8 KB\nUSDC Borrows are up 8.68%, from $5.51M to $5.99M.\nScreen Shot 2023-09-22 at 10.36.08 AM1644×462 35.5 KB\nUSDC utilization increased 11.30%, from 49.3% to 54.9%.\nSupply Caps\nScreen Shot 2023-09-22 at 10.37.20 AM3770×612 87.3 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-09-22 at 10.37.48 AM3784×834 147 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-09-22 at 10.36.24 AM1648×418 30.8 KB\nThe minimum USDC utilization was 49.3%, and the maximum was 58.7%.\nThe minimum USDC reserve growth was -0.8%, and the maximum was 8.5%. The average USDC reserve growth was 5.6%.\nScreen Shot 2023-09-22 at 10.36.43 AM1658×452 31.9 KB\nThe comet accumulated $0.21k USDC reserves while distributing $12.41k COMP rewards for a weekly Net Protocol Profit of $-12.19k.\\n[Gauntlet] Base v3 USDbC Update (09/22/2023 - 09/28/2023)\nGauntlet would like to provide the community with an update on metrics from the Base v3 USDbC comet over the past week.\nSimple Summary\n\nUSDC Borrows are down 3.64%, from $5.99M to $5.78M.\nUSDC Supply is down 3.67%, from $10.2M to $9.83M.\nUSDC utilization increased 0.10%, from 58.8% to 58.9%.\nThe minimum USDC reserve growth was 6.6%, and the maximum was 10.1%. The average USDC reserve growth was 8.5%.\nThe comet accumulated $0.35k USDC reserves while distributing $12.92k COMP rewards for a weekly Net Protocol Profit of $-12.57k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-09-29 at 12.24.32 PM1796×496 67.3 KB\nTotal Collateral (USD) is down 4.38%, from $10.98M to $10.5M.\nScreen Shot 2023-09-29 at 12.24.53 PM1790×472 67 KB\nUSDC Supply is down 3.67%, from $10.2M to $9.83M.\nScreen Shot 2023-09-29 at 12.25.13 PM1794×484 66.2 KB\nUSDC Borrows are down 3.64%, from $5.99M to $5.78M.\nScreen Shot 2023-09-29 at 12.25.32 PM1638×448 39.5 KB\nUSDC utilization increased 0.10%, from 58.8% to 58.9%.\nSupply Caps\nScreen Shot 2023-09-29 at 12.27.04 PM3544×620 84.4 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-09-29 at 12.27.28 PM3542×844 138 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-09-29 at 12.25.52 PM1650×420 37.8 KB\nThe minimum USDC utilization was 56.6%, and the maximum was 60.5%.\nThe minimum USDC reserve growth was 6.6%, and the maximum was 10.1%. The average USDC reserve growth was 8.5%.\nScreen Shot 2023-09-29 at 12.26.13 PM1646×452 31.4 KB\nThe comet accumulated $0.35k USDC reserves while distributing $12.92k COMP rewards for a weekly Net Protocol Profit of $-12.57k.\\n[Gauntlet] Base v3 USDbC Update (09/29/2023 - 10/05/2023)\nGauntlet would like to provide the community with an update on metrics from the Base v3 USDbC comet over the past week.\nSimple Summary\n\nUSDC Borrows are down 5.06%, from $5.91M to $5.61M.\nUSDC Supply is up 9.16%, from $10.03M to $10.95M.\nUSDC utilization decreased 11.64%, from 58.9% to 52.0%.\nThe minimum USDC reserve growth was 2.1%, and the maximum was 12.5%. The average USDC reserve growth was 7.7%.\nThe comet accumulated $0.31k USDC reserves while distributing $14.35k COMP rewards for a weekly Net Protocol Profit of $-14.05k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-10-06 at 5.51.47 PM1798×470 68.5 KB\nTotal Collateral (USD) is down 5.93%, from $10.96M to $10.31M.\nScreen Shot 2023-10-06 at 5.52.09 PM1798×466 69.3 KB\nUSDC Supply is up 9.16%, from $10.03M to $10.95M.\nScreen Shot 2023-10-06 at 5.52.32 PM1802×492 66.5 KB\nUSDC Borrows are down 5.06%, from $5.91M to $5.61M.\nScreen Shot 2023-10-06 at 5.52.50 PM1646×450 35.4 KB\nUSDC utilization decreased 11.64%, from 58.9% to 52.0%.\nSupply Caps\nScreen Shot 2023-10-06 at 5.54.21 PM3540×614 83.6 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-10-06 at 5.54.54 PM3544×844 145 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-10-06 at 5.53.20 PM1654×424 30.7 KB\nThe minimum USDC utilization was 51.9%, and the maximum was 63.3%.\nThe minimum USDC reserve growth was 2.1%, and the maximum was 12.5%. The average USDC reserve growth was 7.7%.\nScreen Shot 2023-10-06 at 5.53.41 PM1654×448 31.5 KB\nThe comet accumulated $0.31k USDC reserves while distributing $14.35k COMP rewards for a weekly Net Protocol Profit of $-14.05k.\\n[Gauntlet] Base v3 USDbC Update (10/06/2023 - 10/12/2023)\nGauntlet would like to provide the community with an update on metrics from the Base v3 USDbC comet over the past week.\nSimple Summary\n\nUSDC Borrows are up 9.06%, from $5.61M to $6.12M.\nUSDC Supply is up 11.94%, from $10.95M to $12.26M.\nUSDC utilization decreased 2.57%, from 51.3% to 50.0%.\nThe minimum USDC reserve growth was -4.8%, and the maximum was 1.0%. The average USDC reserve growth was -1.4%.\nThe comet accumulated $-0.05k USDC reserves while distributing $13.26k COMP rewards for a weekly Net Protocol Profit of $-13.31k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-10-13 at 2.34.23 PM1796×492 65.2 KB\nTotal Collateral (USD) is up 15.85%, from $9.8M to $11.36M.\nScreen Shot 2023-10-13 at 2.34.47 PM1800×478 65.6 KB\nUSDC Supply is up 11.94%, from $10.95M to $12.26M.\nScreen Shot 2023-10-13 at 2.35.17 PM1798×498 64.4 KB\nUSDC Borrows are up 9.06%, from $5.61M to $6.12M.\nScreen Shot 2023-10-13 at 2.36.06 PM1648×448 42.1 KB\nUSDC utilization decreased 2.57%, from 51.3% to 50.0%.\nSupply Caps\nScreen Shot 2023-10-13 at 2.37.59 PM3786×620 87.6 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-10-13 at 2.38.27 PM3792×792 139 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-10-13 at 2.36.30 PM1664×430 38 KB\nThe minimum USDC utilization was 45.7%, and the maximum was 51.3%.\nThe minimum USDC reserve growth was -4.8%, and the maximum was 1.0%. The average USDC reserve growth was -1.4%.\nScreen Shot 2023-10-13 at 2.36.50 PM1658×470 31.3 KB\nThe comet accumulated $-0.05k USDC reserves while distributing $13.26k COMP rewards for a weekly Net Protocol Profit of $-13.31k.\\n[Gauntlet] Base v3 USDbC Update (10/13/2023 - 10/19/2023)\nGauntlet would like to provide the community with an update on metrics from the Base v3 USDbC comet over the past week.\nSimple Summary\n\nUSDC Borrows are up 12.06%, from $6.12M to $6.85M.\nUSDC Supply is down 2.32%, from $12.26M to $11.98M.\nUSDC utilization increased 14.54%, from 50.0% to 57.2%.\nThe minimum USDC reserve growth was -0.1%, and the maximum was 9.3%. The average USDC reserve growth was 4.3%.\nThe comet accumulated $0.19k USDC reserves while distributing $12.71k COMP rewards for a weekly Net Protocol Profit of $-12.52k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-10-20 at 1.38.30 PM1800×492 68 KB\nTotal Collateral (USD) is up 8.29%, from $11.35M to $12.29M.\nScreen Shot 2023-10-20 at 1.38.55 PM1798×498 67.5 KB\nUSDC Supply is down 2.32%, from $12.26M to $11.98M.\nScreen Shot 2023-10-20 at 1.39.16 PM1790×482 67 KB\nUSDC Borrows are up 12.06%, from $6.12M to $6.85M.\nScreen Shot 2023-10-20 at 1.39.34 PM1656×466 36.9 KB\nUSDC utilization increased 14.54%, from 50.0% to 57.2%.\nSupply Caps\nScreen Shot 2023-10-20 at 1.41.21 PM3796×616 88.2 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-10-20 at 1.41.43 PM3796×848 152 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-10-20 at 1.39.50 PM1670×434 30.7 KB\nThe minimum USDC utilization was 49.9%, and the maximum was 59.5%.\nThe minimum USDC reserve growth was -0.1%, and the maximum was 9.3%. The average USDC reserve growth was 4.3%.\nScreen Shot 2023-10-20 at 1.40.06 PM1662×466 30.6 KB\nThe comet accumulated $0.19k USDC reserves while distributing $12.71k COMP rewards for a weekly Net Protocol Profit of $-12.52k.\\n[Gauntlet] Base v3 USDbC Update (10/20/2023 - 10/26/2023)\nGauntlet would like to provide the community with an update on metrics from the Base v3 USDbC comet over the past week.\nSimple Summary\n\nUSDC Borrows are up 9.94%, from $6.85M to $7.53M.\nUSDC Supply is up 1.57%, from $11.98M to $12.17M.\nUSDC utilization increased 8.14%, from 57.2% to 61.9%.\nThe minimum USDC reserve growth was 3.0%, and the maximum was 11.5%. The average USDC reserve growth was 7.7%.\nThe comet accumulated $0.38k USDC reserves while distributing $13.87k COMP rewards for a weekly Net Protocol Profit of $-13.49k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-10-27 at 5.41.53 PM1798×490 66.8 KB\nTotal Collateral (USD) is up 3.65%, from $14.05M to $14.57M.\nScreen Shot 2023-10-27 at 5.42.16 PM1794×462 68.3 KB\nUSDC Supply is up 1.57%, from $11.98M to $12.17M.\nScreen Shot 2023-10-27 at 5.42.36 PM1792×498 67.7 KB\nUSDC Borrows are up 9.94%, from $6.85M to $7.53M.\nScreen Shot 2023-10-27 at 5.42.51 PM1654×458 38.2 KB\nUSDC utilization increased 8.14%, from 57.2% to 61.9%.\nSupply Caps\nScreen Shot 2023-10-27 at 5.44.18 PM3786×610 88 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-10-27 at 5.44.38 PM3794×842 150 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-10-27 at 5.43.09 PM1674×436 37.1 KB\nThe minimum USDC utilization was 52.9%, and the maximum was 62.1%.\nThe minimum USDC reserve growth was 3.0%, and the maximum was 11.5%. The average USDC reserve growth was 7.7%.\nScreen Shot 2023-10-27 at 5.43.18 PM1676×470 30.6 KB\nThe comet accumulated $0.38k USDC reserves while distributing $13.87k COMP rewards for a weekly Net Protocol Profit of $-13.49k.\\n[Gauntlet] Base v3 USDbC Update (10/27/2023 - 11/02/2023)\nGauntlet would like to provide the community with an update on metrics from the Base v3 USDbC comet over the past week.\nSimple Summary\n\nUSDC Borrows are up 3.62%, from $7.53M to $7.81M.\nUSDC Supply is down 6.49%, from $12.17M to $11.38M.\nUSDC utilization increased 10.75%, from 61.9% to 68.5%.\nThe minimum USDC reserve growth was 11.1%, and the maximum was 17.3%. The average USDC reserve growth was 14.4%.\nThe comet accumulated $0.81k USDC reserves while distributing $14.58k COMP rewards for a weekly Net Protocol Profit of $-13.77k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-11-03 at 10.52.53 AM1796×488 65.9 KB\nTotal Collateral (USD) is up 1.48%, from $15.05M to $15.27M.\nScreen Shot 2023-11-03 at 10.53.09 AM1794×480 65.2 KB\nUSDC Supply is down 6.49%, from $12.17M to $11.38M.\nScreen Shot 2023-11-03 at 10.53.30 AM1794×476 66.1 KB\nUSDC Borrows are up 3.62%, from $7.53M to $7.81M.\nScreen Shot 2023-11-03 at 10.53.48 AM1656×460 39.3 KB\nUSDC utilization increased 10.75%, from 61.9% to 68.5%.\nSupply Caps\nScreen Shot 2023-11-03 at 10.55.14 AM3532×622 84.1 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-11-03 at 10.55.36 AM3538×838 150 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-11-03 at 10.54.05 AM1664×426 34.7 KB\nThe minimum USDC utilization was 61.6%, and the maximum was 69.4%.\nThe minimum USDC reserve growth was 11.1%, and the maximum was 17.3%. The average USDC reserve growth was 14.4%.\nScreen Shot 2023-11-03 at 10.54.21 AM1660×460 32.1 KB\nThe comet accumulated $0.81k USDC reserves while distributing $14.58k COMP rewards for a weekly Net Protocol Profit of $-13.77k.\\n[Gauntlet] Base v3 USDbC Update (11/03/2023 - 11/09/2023)\nGauntlet would like to provide the community with an update on metrics from the Base v3 USDbC comet over the past week.\nSimple Summary\n\nUSDC Borrows are up 4.47%, from $7.81M to $8.16M.\nUSDC Supply is down 12.93%, from $11.38M to $9.91M.\nUSDC utilization increased 17.57%, from 68.5% to 80.6%.\nThe minimum USDC reserve growth was 16.5%, and the maximum was 24.2%. The average USDC reserve growth was 20.8%.\nThe comet accumulated $1.27k USDC reserves while distributing $15.88k COMP rewards for a weekly Net Protocol Profit of $-14.61k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-11-10 at 1.36.18 PM1800×478 65.2 KB\nTotal Collateral (USD) is up 1.82%, from $15.67M to $15.95M.\nScreen Shot 2023-11-10 at 1.36.36 PM1798×474 63.5 KB\nUSDC Supply is down 12.93%, from $11.38M to $9.91M.\nScreen Shot 2023-11-10 at 1.36.54 PM1790×472 65 KB\nUSDC Borrows are up 4.47%, from $7.81M to $8.16M.\nScreen Shot 2023-11-10 at 1.37.10 PM1666×460 40.5 KB\nUSDC utilization increased 17.57%, from 68.5% to 80.6%.\nSupply Caps\nScreen Shot 2023-11-10 at 1.38.22 PM3792×622 88.1 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-11-10 at 1.38.40 PM3802×844 151 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-11-10 at 1.37.30 PM1662×438 37.8 KB\nThe minimum USDC utilization was 68.3%, and the maximum was 80.6%.\nThe minimum USDC reserve growth was 16.5%, and the maximum was 24.2%. The average USDC reserve growth was 20.8%.\nScreen Shot 2023-11-10 at 1.37.46 PM1660×472 31.2 KB\nThe comet accumulated $1.27k USDC reserves while distributing $15.88k COMP rewards for a weekly Net Protocol Profit of $-14.61k.\\n[Gauntlet] Base v3 USDbC Update: (11/10/23 - 11/16/23)\nGauntlet would like to provide the community with an update on the Base v3 USDbC comet over the past week.\nSimple Summary\n\nUSDbC Borrows increased 10.57%, from $8.18M to $9.04M.\nUSDbC Supply increased 5.5%, from $9.95M to $10.5M.\nUSDbC utilization increased 4.8%, from 82.17% to 86.12%.\nThe minimum USDbC reserve growth was -3.24%, and the maximum was 19.09%. The average USDbC reserve growth was 7.72%.\nThe comet accumulated $0.6K USDbC reserves while distributing $17.06K COMP rewards for a weekly Net Protocol Profit of $-16.46K.\n\nCollateral Asset Supply\nThis graph shows the time series of total supply of all collateral assets.\nSupply1920×1080 94.8 KB\nTo see updated statistics, please see the live version of this graph here.\nUSDbC Borrows\nThis graph shows the time series of USDbC borrows.\nBorrows1920×1080 60.3 KB\nTo see updated statistics, please see the live version of this graph here.\nUtilization\nThis graph shows the utilization (borrow / supply) of USDbC over the past week.\nUtilization1920×1200 202 KB\nSupply Cap Usage\nThis graph shows the supply cap usage (supply / supply cap) of all collateral assets over the past week.\nSupply Cap Usage1920×1200 141 KB"
  },
  {
    "number_of_comments": 43,
    "postid": "4cd9987d-b963-45c6-802d-9626dd67472d",
    "posturl": "https://www.comp.xyz/t/add-markets-mkr-aave-sushi-yfi/1977",
    "combinedcontent": "With the oracle improvement done, the protocol ready to list new markets. I am proposing to add MKR, AAVE, SUSHI, and YFI. I consider all four coins to be blue-chip DeFi tokens and very logical additions to the protocol. Each token will have a zero collateral factor (initially), the same interest rate model (as COMP/LINK), and a reserve factor of 25% for each market (which is standard).\nCredit to @elee for doing the dev work.\n\n\nMKR\nPrice: $2600\nFDV MCAP: $2.6B\nNote: 21% of MKR is the Goverance Contract and 8% in the MCD Pause Proxy\nMintable: True by the liquidation system or governance.\nInitial CF: 0\nCOMP rewards: 0\nBorrow cap: 25K (recommended by @monet-supply)\nRopsten cToken: 0x107ce37C9C40Ad204DFf182e82d886e53Bcb6e90 4\nRopsten underlying: 0xeef6de7ec2fae93769b1c2e725d698b8a5d0fd9e 3\n\n\n\nSUSHI\nPrice: $7.60\nFDV MCAP: $1.9B\nNote: 14% of SUSHI is in the Sushi Treasury\nMintable: False\nInitial CF: 0\nCOMP rewards: 0\nBorrow cap: 0\nRopsten cToken: 0x29f4542F050Eb3E31004a09AAdD89fF265487967 4\nRopsten underlying: 0x7c9243b94d2ccf40940a70171a8b868354822669 1\n\n\n\nAAVE\nPrice: $295.87\nFDV MCAP: $4.7B\nNote: 18% of AAVE is staked and 16% is held in the treasury.\nMintable: False\nInitial CF: 0\nCOMP rewards: 0\nBorrow cap: 200k\nRopsten cToken: 0xc840F7aB675fEe6E8Ce4E8D8923A8afE860075F9 1\nRopsten underlying: 0xe547acbd930e7a54e011a1fdd182933a823d826d 1\n\n\n\nYFI\nPrice: $32,943\nFDV MCAP: $1.2B\nNote: the tokens are spread out thanks to the initial distribution.\nMintable: True, via governance proposal.\nInitial CF: 0\nCOMP rewards: 0\nBorrow cap: 1500\nRopsten cToken: 0x67BCc4e98445fFccCf9D4f4A6CD14B4181b9449A 1\nRopsten underlying: 0x35362e4e7d23ac00b17c151e3b24d4e343e4c889 1\n\nInterest rate curve: 0x2341ba42eb00c63cf03559c9a2295a23ace7e4ad 55\nOnce the community confirms the ropsten cTokens deployments look correct, we will deploy the cTokens to mainnet, work with Chainlink to get the new oracle set up and post the simulation info for the new markets.\nSince this proposal follows historical precedents and sets the collateral factor to zero for all markets, I think it should be a straightforward addition to the protocol. Due to the ten governance action limit per proposal, I’ll likely propose them individually or in pairs if there is strong support. Once the assets are listed, I intend on circling back to make a proposal/forum post to increase their collateral factors.\\nGreat news! Reiterating that for SUSHI it would be nice to allow deposit of xSUSHI (i.e. staked SUSHI) to be used as collateral if oracle allows.\\nSince AAVE already supports xSUSHI, I think it would be beneficial to the DeFi ecosystem if Compound listed SUSHI. That being said, I do not see a reason why we could not list both of them.\\nIt’s great to see the integration already starting to pay off with the addition of new assets. I can’t see any reason against adding blue chips like these (especially w/ a zero collateral factor to start),\nExcited for more to come!\\nAdding these four markets makes complete sense, and I think these markets will have broad support.\nStarting these markets at a 0% collateral factor, 0 COMP distribution allows them to be added without any controversy; afterwards, the community should discuss & debate the introduction of collateral factors & COMP distributions (and how and why they might vary by asset).\nFantastic suggestion, and thank you for getting the ball rolling @getty \\nStoked for this one!  great work @getty\\nExcited to see more blue chips being added to Compound! I’m wondering what the major benefits are to starting with a collateral factor of 0 if it is just going to be increased in the immediate future? Would it not be beneficial to hash out those details now?\\nCollateral factors parameters are set on a coin by coin basis. The goal with my proposal is to keep it at a high level so we can add these relatively quickly (2-4 weeks). Once a coin gets added, I intend on creating a separate forum thread for each market to tune the collateral factor. Plus, this gives the protocol a little extra margin for whenever an asset is added.\\nUpdate: We have redeployed the contracts now that Compound 2.9 is posted 9.\nMKR: CErc20Delegator | 0xb9321ac47163d851d867a7e982cbf0c5ce22fb3f 7\nnpx saddle match 0xb9321ac47163d851d867a7e982cbf0c5ce22fb3f CErc20Delegator 0xeef6de7ec2fae93769b1c2e725d698b8a5d0fd9e 0xcfa7b0e37f5ac60f3ae25226f5e39ec59ad26152 0x2341ba42eb00c63cf03559c9a2295a23ace7e4ad 200000000000000000000000000 \"Compound Maker\" cMKR 8 0x2079a734292094702f4d7d64a59e980c20652cae 0x0295a48b76bc68662bd15bfaecedca075a4f568f \"0x\" -n ropsten\nSUSHI: CErc20Delegator | 0x1fcab19e3cfa9437648c240410a941c13b59ccc9 6\nnpx saddle match 0x1fcab19e3cfa9437648c240410a941c13b59ccc9 CErc20Delegator 0x7c9243b94d2ccf40940a70171a8b868354822669 0xcfa7b0e37f5ac60f3ae25226f5e39ec59ad26152 0x2341ba42eb00c63cf03559c9a2295a23ace7e4ad 200000000000000000000000000 \"Compound Sushi Token\" cSUSHI 8 0x2079a734292094702f4d7d64a59e980c20652cae 0x0295a48b76bc68662bd15bfaecedca075a4f568f \"0x\" -n ropsten\nAAVE: CErc20Delegator | 0x12671d7a90adacac177b4cb79183890284c0c431 1\nnpx saddle match 0x12671d7a90adacac177b4cb79183890284c0c431 CErc20Delegator 0xe547acbd930e7a54e011a1fdd182933a823d826d 0xcfa7b0e37f5ac60f3ae25226f5e39ec59ad26152 0x2341ba42eb00c63cf03559c9a2295a23ace7e4ad 200000000000000000000000000 \"Compound Aave Token\" cAAVE 8 0x2079a734292094702f4d7d64a59e980c20652cae 0x0295a48b76bc68662bd15bfaecedca075a4f568f \"0x\" -n ropsten\nYFI: CErc20Delegator | 0x829a3187954c7a87683212c6c0b2186af7a87500 5\nnpx saddle match 0x829a3187954c7a87683212c6c0b2186af7a87500 CErc20Delegator 0x35362e4e7d23ac00b17c151e3b24d4e343e4c889 0xcfa7b0e37f5ac60f3ae25226f5e39ec59ad26152 0x2341ba42eb00c63cf03559c9a2295a23ace7e4ad 200000000000000000000000000 \"Compound yearn.finance\" cYFI 8 0x2079a734292094702f4d7d64a59e980c20652cae 0x0295a48b76bc68662bd15bfaecedca075a4f568f \"0x\" -n ropsten\nOnce someone from the community confirms this is correct, we’ll deploy to mainnet.\\nCompletely in support of this - thanks to @elee & @getty for putting this together, makes total sense to have blue chip tokens as markets on Compound. Do you think it would be valuable to codify the general ways that we approach token listings for blue-chip, medium-risk and high-risk tokens? If so - would love to help out in any way we can.\\nThat’s not a bad idea to explore IMO. I know there have been discussions around things like Reserve Factor Standardization 3 for different asset types (which led to Prop 31 1). However I don’t think anything regarding collateral factors or interest rate models is official/codified AFAIK.\\nDefinitely makes sense to do imo - as DeFi expands what we consider to be blue-chip tokens will as well and it’ll speed up these proposals and require less discussion if we codify for collateral factors & interest rate models.\\nI’m in favor of adding these markets, but I’d strongly advocate for a different sequence of events here.\nI believe it’s time for the Compound community to come up with a framework for adding new assets so that additions don’t feel ad-hoc.\nI propose that prior to adding these new assets, we codify v1 of such a framework, and then evaluate these token additions by the terms of the new framework.\n@getty @rleshner I’d be happy to work with others on this work if there’s appetite to take this on.\\n+1\nI would definitely advocate for having a set proceedure (that you can maybe deviate off a little), which would definately help for adding more assets. We should make some sort of “whitepaper” for this\\nI can confirm that this set of contracts has been deployed correctly and matches the code at commit ae4388e780a8d596d97619d9704a931a2752c2bc 1. Be sure to enter the correct admin (timelock) and Comptroller when deploying to mainnet.\nI do agree with @brendan_dharma that a more formalized process here is definitely needed and now is the time to set it up.\\nAgreed, would be good to codify before and let these markets be a clear example of how v1 of the framework can be leveraged. Would love to help out here as well! We should set up a call sometime in the next few days to get the ball rolling.\\nIt is hard to make a formal framework when everything is changing as fast as it is in DeFi. That being said, I’m happy to give my input if someone wants to take the lead on this.\nHere is the general flow I am following/doing:\n\n\nBefore the forum post: The proposer really needs to ask themselves if their proposed market is ready to be on Compound and they need to ask themselves if they are willing to fight for it to be listed.\n\n\nForum post: It should include a summary/bio of the asset, some info on token distribution, mintability, liquidity/trading stats, why Compound should add it, proposed parameters, and testnet deployment.\n\n\nInitial community feedback: If the forum post has a warm reception and the testnet deployment is good then launch on mainnet.\n\n\nMainnet community feedback: If the mainnet token deployment is done correctly and the community is happy the proposer should make a governance proposal or CAP to add the asset.\n\n\nAfter initial listing: The proposer should post a recommendation for a collateral factor (and any other parameters they want to set) and rationale.\n\n\nParameter community feedback: Give the community an opportunity to give their input. If a rough consensus is reached then deploy a proposal/CAP for the changes.\n\n\nAs for what default parameters the proposer should use. I recommend finding the most similar asset on the protocol and using that as a starting place. Much of what has been done so far in this proposal is based on the recent cLINK and cCOMP additions.\nI think there is a balance between bureaucracy and efficiency. I don’t know what the answer is but I hope to set a good example for the community\\nI think this is a great sequence to follow, but I meant more a risk framework for determining if an asset should be added to Compound.\nSome of the factors that come to mind for me:\n\nFDV MCAP\nLiquid MCAP\nAvg Daily Trade Volume on DEXes\nAvg. Volatility (1hr, 6hr, 12hr, Daily, and Weekly)\nMintable?\nEconomic Concentration (Herfindahl Index? Known Treasury / Founder / VC holdings?)\nTime since Token Launch\nTime since last exploit\n\nThe idea being we can use this risk framework to make these decisions less arbitrary and more rule-based.\\nJust wanted to state full agreement here about Compound being overdue for an asset listing / risk assessment framework.\nFor context, Aave’s risk framework (Methodology - Risk 12) is quite extensive and actively used in discussions on new asset listings and selecting / adjusting collateral ratios. It’s hard to envision any of these new markets being listed and moving off of a zero collateral factor without a similar framework that Compound’s community is aligned on.\nHappy to help provide input too, from having been in those discussions within the Aave community. Any community members with risk analysis / actuarial expertise should absolutely jump in too btw, would be a great way to lead and contribute.\nGoing back to the topic at hand - in support of adding these markets @getty and IMO given that they’re being added at 0% collateral factor, OK to move forward. I don’t see a big risk of setting precedent here, since it seems like there’s enough interest to start on codifying an asset listing framework shortly.\nFranklin @ Pantera Capital\\nUpdate: We have deployed the contracts to mainnet.\nMKR: CErc20Delegator | 0x95b4ef2869ebd94beb4eee400a99824bf5dc325b 5\nnpx saddle match 0x95b4ef2869ebd94beb4eee400a99824bf5dc325b CErc20Delegator 0x9f8f72aa9304c8b593d555f12ef6589cc3a579a2 0x3d9819210a31b4961b30ef54be2aed79b9c9cd3b 0xd956188795ca6f4a74092ddca33e0ea4ca3a1395 200000000000000000000000000 \"Compound Maker\" cMKR 8 0x6d903f6003cca6255d85cca4d3b5e5146dc33925 0xa035b9e130f2b1aedc733eefb1c67ba4c503491f \"0x\" -n mainnet\nSUSHI: CErc20Delegator | 0x4b0181102a0112a2ef11abee5563bb4a3176c9d7 3\nnpx saddle match 0x4b0181102a0112a2ef11abee5563bb4a3176c9d7 CErc20Delegator 0x6b3595068778dd592e39a122f4f5a5cf09c90fe2 0x3d9819210a31b4961b30ef54be2aed79b9c9cd3b 0xd956188795ca6f4a74092ddca33e0ea4ca3a1395 200000000000000000000000000 \"Compound Sushi Token\" cSUSHI 8 0x6d903f6003cca6255d85cca4d3b5e5146dc33925 0xa035b9e130f2b1aedc733eefb1c67ba4c503491f \"0x\" -n mainnet\nAAVE:CErc20Delegator | 0xe65cdb6479bac1e22340e4e755fae7e509ecd06c\nnpx saddle match 0xe65cdb6479bac1e22340e4e755fae7e509ecd06c CErc20Delegator 0x7fc66500c84a76ad7e9c93437bfc5ac33e2ddae9 0x3d9819210a31b4961b30ef54be2aed79b9c9cd3b 0xd956188795ca6f4a74092ddca33e0ea4ca3a1395 200000000000000000000000000 \"Compound Aave Token\" cAAVE 8 0x6d903f6003cca6255d85cca4d3b5e5146dc33925 0xa035b9e130f2b1aedc733eefb1c67ba4c503491f \"0x\" -n mainnet\nYFI: CErc20Delegator | 0x80a2ae356fc9ef4305676f7a3e2ed04e12c33946 5\nnpx saddle match 0x80a2ae356fc9ef4305676f7a3e2ed04e12c33946 CErc20Delegator 0x0bc529c00c6401aef6d220be8c6ea1667f6ad93e 0x3d9819210a31b4961b30ef54be2aed79b9c9cd3b 0xd956188795ca6f4a74092ddca33e0ea4ca3a1395 200000000000000000000000000 \"Compound yearn.finance\" cYFI 8 0x6d903f6003cca6255d85cca4d3b5e5146dc33925 0xa035b9e130f2b1aedc733eefb1c67ba4c503491f \"0x\" -n mainnet\nOnce someone from the community confirms the deployment has the expected parameters, I will work with Chainlink to deploy the new oracle contract.\\nMKR, AAVE, SUSHI, and YFI are well established projects in the space and they will be valuable additions to Compound.  The decision to use a starting collateral factor of 0 also mitigates the risk posed by the asset listing and we look forward to discussing how we can raise those collateral factors over time.  LINK is also ripe for a CF update.\nBrendan makes some good points about potential risk factors, to chime in here, one other potential risk factor is the behavior of borrowers, which can change over time. The more aggressive people are with their collateral ratio, the more collateral is at risk for liquidation. At Gauntlet, we’ve thought about these risk factors quite 8 a 2  8bit 5, and I’d be happy to help with a listing framework as well.\\nManually verifying the following:\ncMKR: 0x95b4eF2869eBD94BEb4eEE400a99824BF5DC325b\ncSUSHI: 0x4B0181102A0112A2ef11AbEE5563bb4a3176c9d7\ncAAVE: 0xe65cdB6479BaC1e22340E4E755fAE7E509EcD06c\ncYFI: 0x80a2AE356fc9ef4305676f7a3E2Ed04e12C33946\n\nUnderlying addresses correct\nAdmin correct\nComptroller correct\nDecimals correct\nImplementation correct\nInitial exchange rates correct\nInterest rate models are good\nNames look good\nSymbols look good\nSource code looks good\nContract compiled with version v0.5.16+commit.9c3226ce and optimized using 200 runs as is standard\nEverything else looks good\n\nNow we just need some simulations similar to this one 10 to ensure that the proposal and market addition work as expected.\nGreat work @getty !\\nWould just like to advocate about the addition of xsushi vs sushi.\nThe idea is to bring in more liquidity to the protocol, it would be hard to draw individuals to bring in sushi itself and earn anything near the yield via sushi/Kashi.\nMost holders enjoy the ability to stake then have the ability of utilizing xsushi to use as collateral while still enjoying the staking apys and the platform fee kickback.\nBeen doing a deeper dive into other protocols allowing xsushi to be deposited as collateral.\nCream 0%\nAave .02%\nRari 0%\nKashi .01%\nI’m not familiar with the technicals of supporting the xsushi token, but would be worth a longer convo.\\nThe new price oracle has been deployed: UniswapAnchoredView | 0x6D2299C48a8dD07a872FDd0F8233924872Ad1071 8\nA simulation of the new price oracle and cMKR is ready as well. Here is the snippet. I’ll post the link to the Github fork with all of the simulations tomorrow.\n`#!/usr/bin/env yarn repl -s\nPrintTransactionLogs\n– Token holder addresses for mocking\nAlias CompHolder “0x7587caefc8096f5f40acb83a09df031a018c66ec”\nAlias MakerHolder “0x05e793ce0c6027323ac150f6d45c2344d28b6019”\nAlias CUSDCHolder “0x5e34bc93a7506ecc8562ade4d5c8b090247a6349”\nWeb3Fork “https://mainnet-eth.compound.finance/@12884400” (CompHolder MakerHolder CUSDCHolder)\nUseConfigs mainnet\n– update to the new oracle implementation\nAlias NewOracle “0x6D2299C48a8dD07a872FDd0F8233924872Ad1071”\n– Delegate and propose update\nFrom CompHolder (Comp Delegate CompHolder)\nFrom CompHolder (GovernorBravo GovernorBravoDelegator Propose “Update Oracle” [(Address Comptroller)] [0] [\"_setPriceOracle(address)\"] [[(address NewOracle)]])\nPrint “New Oracle Set”\n– Fast forward, vote, queue, execute\nMineBlock\nAdvanceBlocks 14000\nFrom CompHolder (GovernorBravo GovernorBravoDelegator Proposal LastProposal Vote For)\nAdvanceBlocks 20000\nGovernorBravo GovernorBravoDelegator Proposal LastProposal Queue\nIncreaseTime 604910\nGovernorBravo GovernorBravoDelegator Proposal LastProposal Execute\n– Delegate and propose update\nFrom CompHolder (Comp Delegate CompHolder)\nFrom CompHolder (GovernorBravo GovernorBravoDelegator Propose “Add MKR Market” [(Address Comptroller) (Address Comptroller) (Address cMKR)] [0 0 0] [\"_supportMarket(address)\" “_setCollateralFactor(address,uint256)” “_setReserveFactor(uint256)”] [[(address cMKR)] [(address cMKR) 0] [250000000000000000]])\n– Fast forward, vote, queue, execute\nMineBlock\nAdvanceBlocks 14000\nFrom CompHolder (GovernorBravo GovernorBravoDelegator Proposal LastProposal Vote For)\nAdvanceBlocks 20000\nGovernorBravo GovernorBravoDelegator Proposal LastProposal Queue\nIncreaseTime 604910\nGovernorBravo GovernorBravoDelegator Proposal LastProposal Execute\n– Assert cMKR market supported\nAssert True (Comptroller CheckListed cMKR)\n– Ensure accrue interest works\nCToken cMKR AccrueInterest\n– Mint test\nFrom MakerHolder (Erc20 MKR Approve (Address cMKR) 10e18)\nFrom MakerHolder (CToken cMKR Mint 10e18)\nAssert Equal (Erc20 MKR TokenBalance cMKR) (10e18)\n– Borrow test\nFrom CUSDCHolder (CToken cMKR Borrow 1e18)\nAssert Equal (Erc20 MKR TokenBalance CUSDCHolder) (1e18)\nAssert Equal (Erc20 MKR TokenBalance cMKR) (9e18)\n– Repay borrow test\nFrom CUSDCHolder (Erc20 MKR Approve (Address cMKR) 1e18)\nFrom CUSDCHolder (CToken cMKR RepayBorrow 1e18)\nAssert Equal (Erc20 MKR TokenBalance CUSDCHolder) (0)\nAssert Equal (Erc20 MKR TokenBalance cMKR) (10e18)\n– Redeem test (note: 50 cMKR == 1 MKR initially)\nFrom MakerHolder (CToken cMKR Redeem 50e8)\nPrint “cMKR integration ok”`\nExpect to see a governance proposal for adding MKR to the protocol tomorrow or in the next few days.\\nThe simulations for each asset can be found here 8\n\nsimulationData2377×1285 199 KB\n\\nThe proposal has been submitted on chain. Voting begins after the 2 day governance analysis period - you can set up or change your delegation during this period before voting goes live.\n\n  \n      \n\n      compound.finance\n  \n\n  \n    \n\nCompound 12\n\n  Compound is an algorithmic, autonomous interest rate protocol built for developers, to unlock a universe of open financial applications.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nThanks to Getty and Eddy for pushing this forward!\\nThe compound multisig has accepted ownership at 0x6D2299C48a8dD07a872FDd0F8233924872Ad1071. See the transaction here:\nTX details 6\\nDoes anyone mind if the next proposal groups SUSHI, AAVE, and YFI? I think they will all pass unanimously. To be timely and efficient, I think we should group them.\\nFirst off, kudos to everyone on this thread in getting cMKR proposed \n@getty if there are no strong arguments against the idea of grouping AAVE/SUSHI/YFI, I think a single proposal to add the remaining assets would make a lot of sense; the updated price feed will already be live, and the addition of three should add little risk over the addition of one.\\nBundled proposal to add AAVE, SUSHI, & YFI has been submitted. Voting starts Thursday at ~ 2300 UTC.\\nExcited to see these tokens potentially coming to Compound!\nI’d reiterate the benefits shared above of using xSushi over Sushi, which tends to get much higher engagement on other lending platforms. @getty you said potentially both could be added?\\nI think listing the underlying is more useful than listing xSUSHI, but we could certainly list it as well. Maybe in a month or two, we can evaluate the SUSHI market and consider adding xSUSHI.\\nAgree that listing SUSHI makes more sense from a borrower/shorter standpoint, but from a depositor point of view, this would be an irrational thing to do. Unless you can get a fairly high interest rate that offset the xSUSHI staker’ fees, you will be better off depositing your xSUSHI on AAVE or directly in the BentoBox/Kashi (Sushiswap lending platform). Currently xSUSHI’s APR 2 stands at 6.17% vs. supply-side interest rate of 0.43% for SUSHI on Cream 1, so I doubt listing SUSHI will attract much deposits.\\nWe may be able to use a different delegate contract that allows putting Sushi into the SushiBar. Maybe this could be similar to the delegate contract for Dai. 2\nIf this is done, we may not need to add xSushi as an additional asset.\nIf xSushi were the underlying asset, the functionality would be built in because the asset would already have been put into the SushiBar.\\nA few things that crossed my mind with this proposal, is that while I’d love to see new assets added:\n\n\nThe assets are quite similar to Compounds largest competitor (perhaps alternative is a better word) creating two markets with such collateral overlap may increase the risk of cascading liquidity problems if any asset ever had an existential issue?\n\n\nWith Sushi in particular, there is a lot more inflation to come, and a strong alternative for staking Sushi as xSushi, so it feels unlikely this market by itself will be competitive.\n\n\nIt may have been of value to have a per asset vote?\n\n\\n\n\n\n LawSirrah:\n\nIf this is done, we may not need to add xSushi as an additional asset.\nIf xSushi were the underlying asset, the functionality would be built in because the asset would already have been put into the SushiBar\n\n\nThis is a fantastic idea, the main drawback would be borrowed Sushi is not in the SushiBar but this is a small percentage of the total on other platforms.\n\n\n\n adamscochran:\n\nThe assets are quite similar to Compounds largest competitor (perhaps alternative is a better word) creating two markets with such collateral overlap may increase the risk of cascading liquidity problems if any asset ever had an existential issue?\n\n\nin my opinion assets should be evaluated in the context of how strong of a fit they are for Compound. If anything being present in other markets should be positive signal if the assets are doing well there for Compound to want to capture some of that market. The existential risks if severe enough should invalidate the token for inclusion on their own\\n\n\nThe existential risks if severe enough should invalidate the token for inclusion on their own\nhopefully i’m understanding you correctly, i think we’re on the same page?\n\n\nto me, as long as the inclusion of the asset doesn’t bank run compound, it should be fine to list. of course that includes a lack of liquidity, project security, etc, etc. idk what other criteria would be needed for a ‘strong fit’\nthe downside of, ‘not a lot of xyzzy coin will be deposited’ i don’t think hurts anybody. the thing i feel is most important for compound is giving control of personal finances back to the user, and if the system can handle it, we should list it.\nto the case of sushi - i agree that it would be the best solution in the long term, but writing then auditing that contract is gonna take some time, and for no great reason. given how modular things are, it should be pretty easy to add the improved sushi contract in the future when we have it.\\nNow that all four assets are listed, it is time to talk about collateral factors. I am specifying with “initial” because these are conservative numbers and likely can go higher once the market has a month or two to settle.\nMKR:\n\n\nTraded at a large number of exchanges: Binance, Okex, Coinbase, FTX, Houbi. Between them and legitimate others, the 24hr volume looks to be around $25m\n\n\nDEX liquidity: Uniswap v3 ~$12m volume 7-day, Uniswap v2 $20m liquidity 1, Suhsiswap $18m liquidity, Balancer v2 $32m liquidity\n\n\nNotable: 16% of MKR is the Goverance Contract and 8% in the MCD Pause Proxy.\n\n\nFDV: $2.9B\n\n\nInitial collateral factor: 35%\n\n\nSUSHI:\n\n\nTraded at a large number of exchanges: Binance/US, Okex, Coinbase, FTX/US, Houbi, Bitfinex. Between them and legitimate others, the 24hr volume looks to be around $86m\n\n\nDEX liquidity: Uniswap v3 ~$1.5m volume 7-day, Uniswap v2 $1m liquidity, Suhsiswap $193m liquidity\n\n\nNotable: 12% of SUSHI is in the Sushi Treasury\n\n\nFDV: $2B\n\n\nInitial collateral factor: 40%\n\n\nAAVE:\n\n\nTraded at a large number of exchanges: Binance/US, Okex, Coinbase, FTX/US, Houbi, Bitfinex. Between them and legitimate others, the 24hr volume looks to be around $150m\n\n\nDEX liquidity: Uniswap v3 ~$9m volume 7-day, Uniswap v2 $15m liquidity, Suhsiswap $7m liquidity, Balancer v1 $366m liquidity\n\n\nNotable: 19.4% of AAVE is staked, and 15.7% is held in the treasury.\n\n\nFDV: $5.3B\n\n\nInitial collateral factor: 50%\n\n\nYFI:\n\n\nTraded at a large number of exchanges: Binance, Okex, Coinbase, FTX, Houbi. Between them and legitimate others, the 24hr volume looks to be around $35m\n\n\nDEX liquidity: Uniswap v3 ~$1.5m volume 7-day, Uniswap v2 $2m liquidity, Suhsiswap $90m liquidity\n\n\nNotable: The tokens are spread out thanks to the initial distribution.\n\n\nFDV: $1.2B\n\n\nIntial collateral factor: 35%\n\n\nFor reference: COMP\n\n\nTraded at a large number of exchanges: Binance/US, Okex, Coinbase, FTX. Between them and legitimate others, the 24hr volume looks to be around $210m\n\n\nDEX liquidity: Uniswap v3 ~$17m volume 7-day, Uniswap v2 $3m liquidity, Suhsiswap $14m liquidity, Balancer v2 $7m\n\n\nNotable: The treasury holds 31.5% of minted COMP.\n\n\nFDV: $4.5B\n\n\nCollateral factor: 60%\n\n\nNext steps:\nIf we can establish a rough consensus that these initial numbers are safe, I’ll make a governance proposal on Monday. Please post comments and feedback, especially if you think a CF is too high.\\nGetty, thanks for proposing initial collateral factors. For comparison purposes, I’ve attached an apples:apples comparison with other protocols for the four assets:\n\n\n\n\nAsset\nMaker\nAave\nCream\nProposed\n\n\n\n\nMKR\nN/A\n0.65\nN/A\n0.35\n\n\nAAVE\n0.57\n0.65\n0.60\n0.50\n\n\nSUSHI\nN/A\n0.45\n0.65\n0.40\n\n\nYFI\n0.57\n0.55\n0.75\n0.35\n\n\n\n\\n\nWith the addition of these four new markets, I think we should adjust the current compSpeeds. The total amount of COMP distributed to the altcoin markets (non-stable with cf) remains unchanged. Each coin will be equally incentivized.\nThis will be a separate governance proposal, coming Monday, due to the 10 governance action limit. Please post comments and feedback.\\nWith adding compSpeeds to 4 new assets, we should really look at what we can do to reallocate compSpeeds away from the borrow side. If this isn’t done, then I think the COMP distributed will become too diluted across all market’s supply and borrow sides.\nWe need to analyze what the % of COMP rewards will be distributed across all markets before adding compSpeeds to those 4 assets. A simple adjustment across all other markets doesn’t seem like any type of calculation (except for averaging them out) or analysis has taken place in order to assure Compound remains useful, especially within the markets that are the most liquid.\nGiving YFI, for example, the same compSpeed as the UNI market doesn’t add up. It really doesn’t add up to give all 4 new assets 25% of the compSpeed as UNI’s $400M market. Those 4 assets combined are less than $10M.\\n\n\n\n CryptoCraig:\n\nwe should really look at what we can do to reallocate compSpeeds away from the borrow side\n\n\nOnce we have separate supply and borrow comp speeds, we can do it. @TylerEther and @arr00 are working on adding that functionality. RFP 16: Dynamic COMP reward distribution 10\\nThe pragma is not locked to a specific version for cErc20Delegator.sol.\npragma solidity ^0.5.16;\nHowever, it sounds like there’s a standard of using v0.5.16+commit.9c3226ce. This seems inconsistent.\nIs there a reason it needs to be that compiler version? If so, should the pragma be locked?\\n\n\n\n 3.1415r:\n\nAgree that listing SUSHI makes more sense from a borrower/shorter standpoint, but from a depositor point of view, this would be an irrational thing to do. Unless you can get a fairly high interest rate that offset the xSUSHI staker’ fees, you will be better off depositing your xSUSHI on AAVE or directly in the BentoBox/Kashi (Sushiswap lending platform). Currently xSUSHI’s APR  stands at 6.17% vs. supply-side interest rate of 0.43% for SUSHI on Cream , so I doubt listing SUSHI will attract much deposits.\n\n\n100% agree with this.\nForcing depositors to choose between cSUSHI yield and xSUSHI yield is unlikely to have outcomes as beneficial as simply letting them have both."
  },
  {
    "number_of_comments": 9,
    "postid": "8d4dd1c0-86af-486f-a98a-af054dcb7bc3",
    "posturl": "https://www.comp.xyz/t/reducing-mkr-borrow-cap/2959",
    "combinedcontent": "Hello Compound community!\nI’d like to gauge sentiment on reducing the cMKR borrow limit (currently set to 25,000 MKR). I think this could help mitigate risks to the Compound protocol and wider defi ecosystem, without negatively impacting protocol usage and financial KPIs.\n\nBackground\nMakerDAO controls the DAI stablecoin system through a governance mechanism directed by MKR token holders. Broadly, there are two governance mechanisms available for MKR holders to influence:\n\n\nGovernance module: This component has administrative control over the Maker protocol. The majority of assets and admin privileges are safeguarded behind the “pause proxy”, which imposes a delay period before passed governance proposals can be enacted similar to Compound’s governance timelock. During this delay period, proposals can be cancelled with a new proposal that overtakes the queued proposal’s voter support.\n\nEmergency shutdown module: This component allows MKR holders to burn their tokens to trigger global settlement (currently requiring a 100k MKR threshold of tokens burned to be triggered). All outstanding debts are settled against collateral at current oracle prices, and DAI becomes redeemable for a pro rata share of collateral (but may begin to shift in value from $1 due to market movements).\n\nThe use of a timelock delay (currently set to 2 days) with possibility to cancel malicious proposals significantly mitigates risk of attack on the governance module. But the emergency shutdown module has no delay on activation by design - the intent is that it could be used in case of a critical bug or oracle problem, where standard governance would be too slow to react. So the risk of malicious triggering of emergency shutdown has become the primary attack vector for Maker governance.\nMaker has tried to mitigate this risk by raising the emergency shutdown threshold (minimum amount of MKR required to trigger ESM), from 50k to 75k and now to the current 100k threshold (roughly 11% of circulating supply). Further increases might be untenable, because they would reduce the ability of MKR holders to trigger shutdown in cases where it’s really necessary (eg. critical bug that could lead to collateral being stolen/minting unbacked DAI).\nMaker primarily evaluates these thresholds from the perspective of rational attackers - eg. could an attacker earn a profit by maliciously shutting down the system? An attacker’s costs include cost of acquisition of necessary MKR tokens for the attack, while potential sources of profits include short positions in MKR, defi tokens, or other related crypto assets.\nThe availability of borrowable MKR tokens can significantly reduce the cost of attack - while the borrowed tokens would be burned as part of shutdown, the cost to repurchase MKR and close the loan could be much less after an attack vs purchasing MKR beforehand. By reducing the maximum MKR borrow limit, Compound could help reduce the risk of malicious shutdown. This would in turn make MKR and DAI less risky as collateral assets, which would reduce tail risk faced by the Compound protocol.\n\nProposal Ideas\nOption 1: Deactivate MKR borrowing entirely\n\nComptroller > _setBorrowPaused > cMKR > True\n\nThis would remove the ability to borrow MKR from the cMKR market (similar to setting the borrow limit to 0 MKR). Existing positions would be unaffected, but no new borrowing positions could be opened.\nOption 2: Reduce MKR borrow cap from 25,000 to 5,000\n\nComptroller > _setMarketBorrowCaps > cMKR > 5,000 E18\n\n5,000 MKR is lower than the current supplied amount, so this wouldn’t have any immediate impact on the cMKR market. This would still allow for some amount of borrowing against this market (and more than enough to meet historical borrow demand across Compound and Aave), so Compound can continue to earn MKR reserves.\n\nCost Benefit Comparison\nBenefits of limiting MKR borrowing:\n\nReduce tail risk of MKR and DAI collateral on Compound\nBenefit wider defi ecosystem by reducing risk to DAI stablecoin system\nImprove relationship with MakerDAO (partnership potential is already deepening with Compound D3M proposal 4, and this could further solidify friendly ties)\nGain business from MKR whales (many large holders may be reluctant to collateralize their MKR due to increasing governance risk, and reducing/eliminating borrowing could address these concerns)\n\nDrawbacks of limiting MKR borrowing:\n\nReduce reserve growth of cMKR market\nMarginally reduce utility of Compound money market (eg. users who wanted to borrow large amounts of MKR for non-malicious purposes like trading or market making wouldn’t be able to)\n\nOn balance, I think reducing or eliminating MKR borrowing would probably lead to an improvement in Compound’s financials. The cMKR market would see reduced reserve accumulation, but an influx of MKR collateralized borrowers could more than offset this with stablecoin and ETH reserve growth. Considering that the cMKR market has accumulated only around ~$1,000 worth of reserves in the 6 months since launch, this should not be a tough threshold to meet.\nAssuming 3% stablecoin borrow rate and 7.5% reserve factor, offsetting the lost cMKR reserve growth (~$2,000 per year) would require only about $900,000 in net new stablecoin borrowing, which seems easily attainable.\n\nInitial sentiment poll (non binding):\nShould Compound curtail MKR borrowing?100%Yes (Option 2): Reduce MKR borrow cap to 5,000 MKR    0%Abstain0%No: Don’t change current 25,000 MKR borrow cap0%Yes (Option 1): Eliminate MKR borrowing entirely4voters4total votes\n                \n                Results will be shown on vote.\n               \nIf this gets significant support from the community, I’ll push this forward through the Compound governance process (including any needed technical or risk review, along with making formal on chain proposal).\\nI’ve closed the forum poll after 2 weeks. It seems a low borrow limit (5,000 MKR) is preferred to fully disabling borrowing - this would provide most of the benefits while still allowing enough liquidity to meet foreseeable borrowing demand.\nI’ll try to schedule time to speak about this on a developer or community call and hear any comments. Stay tuned!\\nI agree with reducing the MKR borrow cap.\nI’m posting this idea so that it does not get forgotten: we should create borrowing and supply velocity limits for problems like these and various other problems. In that, I mean limiting the speed at which certain assets can be deposited or borrowed. If this approach were to be taken, we could slow down the speed of attacks and increase their costs.\\nThis is a great idea, Maker has implemented a similar mechanism with debt ceiling instant access modules, which allow for setting an absolute maximum debt limit while also limiting immediate utilization and daily increases in exposure.\\n@monet-supply thank you for this proposal. Gauntlet is conducting analysis from a market risk perspective and will provide results in ~2 weeks.\\nThank you @pauljlei and gauntlet team! Looking forward to this!\\nGauntlet Analysis:\nThe design intention for lowering borrow cap is more of a way to limit the losses related to an oracle attack, infinite minting, governance risks, and other technical risks. Gauntlet’s analyses focuses on market risk, which is different from these risks, so we advise that the community ultimately base their decision around their views on these risks listed above.\nFrom a market risk perspective, lowering the borrow cap of MKR can reduce the likelihood of insolvency driven by MKR borrows (all else being equal), although we’d note that under current conditions our analysis already predicts low chance of insolvency driven by MKR borrow. Currently, there’s roughly $20M collateral locked for MKR and essentially no MKR borrowed. MKR has an annual volatility of ~110% and an ADV of ~$20M. Even if the entire MKR borrow cap of ~$50M becomes maxed out, the main market risk would be if one user borrows the entire $50M and the position becomes liquidatable. At that point, given the close factor of 50%, the liquidatable account’s $25M MKR would need to be repaid. Even in this case there is likely to be enough MKR liquidity on the secondary markets to liquidate that position (and prevent protocol insolvency).\nTo summarize: our analysis predicts that the impact of lowering MKR borrow limit, from a market risk perspective, is likely to be minimal. The community should make their decision from the perspective of oracle, infinite minting, governance, and other technical risks.\\nThanks for this review!\nWith no indication that this proposal would add risk to the Compound protocol, and potential for it to reduce governance risk to Maker (and by extension reduce risk of DAI insolvency), I view this as a net positive from a risk perspective.\nI think it could also be positive from a business development perspective - large MKR holders could be more willing to collateralize positions on Compound (increasing stablecoin/eth borrow utilization and reserve growth) knowing that their collateral can not be used in governance attacks.\nNext steps:\n\nPresent to biweekly community or dev call (date TBD)\nConfirm operations needed to execute this proposal, and have community review to ensure no unexpected results\nPush proposal live on chain\n\\nAgree with this sentiment and next steps. I believe that the Compound community wants what is best for the Maker system and as such will readily lower the MKR borrow cap due to the arguments by Monet.\\nHappy to say this proposal passed and was executed a few days ago \n\n  \n      \n\n      compound.finance\n  \n\n  \n    \n\nCompound 4\n\n  Compound is an algorithmic, autonomous interest rate protocol built for developers, to unlock a universe of open financial applications.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nThanks to everyone who voted or participated!"
  },
  {
    "number_of_comments": 14,
    "postid": "cb53dd7e-fb63-48b2-abc1-d72abfeb8507",
    "posturl": "https://www.comp.xyz/t/refresh-comp-rewards-on-polygon/4253",
    "combinedcontent": "\nGoal\nAs pointed out in this post 8 on the Polygon cUSDCv3 thread, Compound Labs is proposing to refresh the supply of COMP rewards on Polygon by bridging over 12.5k COMP to sustain the current distribution for approximately another year.\n\nContext\nWhen the cUSDCv3 market initially launched on Polygon, a conservative amount of COMP rewards was bridged over to Polygon to reduce the risk of Compound’s first cross-chain deployment and proposal.\nIn more detail, 2.5k COMP was bridged over, which was worth ~72 days of rewards. Today, as the Polygon market steadily grows and approaches its 50th day of being live, the rewards contract has about 22 days of COMP rewards left to distribute.\nProposal 158 7 was created last week to bridge roughly a year’s worth of COMP rewards to Polygon, but ultimately failed due to not reaching quorum despite all the votes being for the proposal. This is likely because the voting period happened over the weekend, when Compound governance generally has a low amount of activity. Since there doesn’t appear to be any opposition to the proposal, Compound Labs intends to resubmit the proposal again next week, this time making sure the voting period happens over the weekday.\n\nNote\nBridging over a year’s worth of COMP rewards does not necessarily mean that all of it has to be distributed on Polygon. The community can always choose to turn off distributions on Polygon and bridge any remaining COMP back to Ethereum mainnet. This proposal is just to make sure that Compound users on Polygon can continue to claim the rewards that they accrue as long as the distribution is turned on.\\nI do not think the reason it failed is because of weekend. It is more likely because it’s controversal at best.\nWhile COMP distributions are useful tool for bootstraping markets and as well still good tool of decentralising governance process a bit. But Compound Polygon market kind of have no steam, and certanly it’s not for the lack of rewards. As it is still payed-to-borrow rates.\nAs i am not against general idea of refilling a reward pool on Polygon for a bit, but current reward speed is indeed excessive and not really bringing a new usage.\nBroader discussion could be held on the topic, but as it looks now, borrow distribution should be decreased as 9.3% distribution APR vs 4.9% borrow APR looks overshot by a lot. (that could be justified as temporary overshot to attract more usage, which however does not happened)\nThere can be several actions taken as i see it:\n\nIt makes sense to decrease borrow disributions, to bring it approx to 1-2% APR above borrow APR to maintain healthy initiative without overpaying.\nSupply distributions could be introduced to add 1-2%APR of rewards to supply side.\nOr a combination of both could be applied by cutting borrow distributions by about 30% and redistributing those to supply side instead.\n\nAfter observing the results for 2-3 months further adjustments could be taken if needed, but the current distributions doesn’t really look productive, market is relatively stagnant on polygon and likely need adjustments.\\n@Sirokko Gauntlet agrees that the current reward distribution in Polygon is inhibiting growth. Even though Net Borrow APR is positive (payed-to-borrow), the lack of USDC available to be borrowed seems to be what’s dissuading borrowers from joining the protocol.\nWe are going to publish a forum post tomorrow recommending allocating COMP rewards to USDC suppliers, which should increase USDC supply, decrease USDC utilization, and incentivize larger borrowers to join to the protocol.\\nGreat points raised by @Sirokko and @nlord. I also personally agree that the current distribution on Polygon does not appear very effective and could use some adjustments. Looking forward to the upcoming forum post from Gauntlet about this.\nThat being said, I still think we need to top-up the COMP rewards on Polygon to avoid any disruptions in the UX for users there. That way, users on Polygon can continue to use Compound v3 without any hiccups while the community discusses improvements to the distribution.\\nWe just created proposal 159 14. It will go into voting 2 days from now.\\nJust checked again and it looks like there’s roughly still 24h till the voting ends.\nCurrent outcome is great. Hopefully we’ll see a discussion about deploying on zkEVM mainnet soon.\\nAs @nlord said, “there’s no steam” because there isn’t enough USDC supplied to attract borrowers. There is definitely “steam” in the Polygon DeFi market, countless dapps show enough activity to prove that, the chain’s scalability and its fees have proven that it is a suitable place for frequent users who are on its DeFi ecosystem daily. There is no doubt that the Polygon ecosystem is home to plenty of active of users, there cannot be any doubt as it is backed by actual stats and there are plenty of examples of dapps that grew as a result of moving to the Polygon ecosystem.\nPoint is, Compound stands to gain a lot from participating in this market, and the only way it can grow its userbase is by allocating more resources, starting with topping-up the COMP rewards and increasing USDC supply.\\nA little too early to decide that as we still need to meet the required quorom. However I would really like to see that discussion being made, the zkEVM is exciting tech and will play a big role in the future. The earlier we get in on it, the better our chances are later on.\\nThis might be one of the most important proposals for the protocol yet. why hasnt it passed 400k yet? \nSuch a shame\\nWe still have 24 hours which is more than enough for more wallets to cast votes\\nWould really boost incentive and activity on the protocol. All in for it\\nI mean hopefully, cause the protocol really needs this\\nGauntlet plans on voting Yes for this proposal.\\nWe voted for it, though acknowledging that there may be a flawed incentive structure on Compound. The community should start re-consider the problem of redesigning the emission mechanism just as two years ago. It’s great to see that Gauntlet is picking up this problem again 6.\\nThe proposal has passed and been executed. The COMP has now been successfully bridged to the rewards contract on Polygon (txn 2)."
  },
  {
    "number_of_comments": 57,
    "postid": "dd0ce1b3-f3b8-4dfd-a6a1-86ca5d4b8444",
    "posturl": "https://www.comp.xyz/t/comp-reward-adjustments-v2/3074",
    "combinedcontent": "After many discussions with @getty, I’m pivoting my prior COMP Rewards Adjustments proposal 159.\n\nThe objective of the COMP rewards program was initially to distribute the token to our users. The sad truth is that an overwhelming amount of the COMP rewards are being farmed and instantly sold off, making the rewards program ineffective in achieving the initial goal. I believe there’s a clear need to re-evaluate how best to distribute the token to our community.\n\n\nWhen a market first launches, we don’t see much activity until the market receives COMP rewards. There’s little incentive to deposit into it when there’s no borrowing demand (or rewards), and there’s no borrowing demand because there aren’t enough tokens to borrow to make the cost of the borrow transaction worth it.\n\n\nA market needs to have great enough incentives for depositors to provide enough liquidity to make the cost of borrowing worth it.\n\nThe above statements I made in my prior proposal still hold. At the time of writing, I advocated for our users - who are also token holders - to continue receiving a share of the protocol. Decentralization of ownership matters. However, my perspective on the overall issue was slightly incorrect.\nSince most COMP being distributed by the current rewards program is instantly sold off, existing users and token holders are at a great disservice. Their share of the protocol is being diluted for nothing other than farming COMP for profit. This COMP farming behavior is not the kind of activity that will bring value to the protocol, or for the existing users and token holders. Incentives need to be used to grow the protocol to the benefit of the protocol itself and its users and token holders.\nGoing forward, I now believe it’s best to end the current COMP rewards program and to start a new one with the sole purpose of kickstarting new markets. To our users who have faithfully held the tokens received from the rewards program: thank you for trusting and believing in us. Thank you for sticking with us through thick and thin. It’s not fair that your share of the protocol is being diluted for profit while you faithfully hold on to the token. We will fix this.\nHere’s my new action plan.\n\nAction plan\n\n1. Cut existing rewards by 50%\nOn Friday, March 18, I will propose on-chain to cut the existing rewards by 50%. Rather than dropping rewards immediately to zero, we allow some time for adjustment.\n\n2. Cut existing rewards to zero\nAbout a month after, on Friday, April 15, I will propose on-chain to cut existing rewards to zero. This proposal will mark the end of COMP farming for profit at the expense of the protocol, our users, and our token holders.\n\n3. R&D in interest rate models\nWe’ll let supply and demand take over, rather than COMP rewards essentially dictating market sizes and activity. Now here’s the caveat; we’ve deeply neglected our interest rate models. While our latest version of the jump rate model with its current parameters works okay for stablecoins, they are far from optimal for other markets. These sub-optimal interest rate models can make it tricky to maintain various markets in a way that’s equally favorable for both suppliers and borrowers while maintaining enough liquidity for new borrowers to enter the markets.\nI already have a candidate in mind to help us with this who has experience in this area, having worked for one of Canada’s top banks. I will lead this initiative by reading and sharing the latest research papers on this topic (big thanks to my IEEE membership), defining a job scope with expectations and requirements, and coordinating with everyone involved.\n\n4. Introduce the replacement rewards program: kickstart rewards\nAfter upgrading our existing and future markets with optimal interest rate models, we’ll be in a great position to have an effective rewards program to kickstart new markets. Existing markets with low liquidity that haven’t received rewards before may also obtain these kickstart rewards.\nThis new rewards program addresses the issue mentioned at the top of this post - we need borrowing activity to make depositing activity worth it, and we need depositing activity to allow for borrowing activity to occur in the first place.\nThe plan is to incentivize $X market size at a rate of Y% annualized APR for three months to kickstart new markets. The X and Y variables are up for discussion. For example, we could incentivize $10M in deposits at an annualized rate of 8% APR.\n\nConclusion\nThis proposal aims to end the practice of COMP farming for profit which only hurts the protocol, our users, and our token holders. We need to incentivize behavior that benefits us rather than diluting our loyal COMP holders. This proposal paves a clear path for that, further strengthening the protocol.\\nTotally Agree!\nCOMP rewards should be used to stimulate the market when needed. Ends the inflation of COMP, and COMP holders are no longer diluted.\\nThis makes so much sense! Incentivizing the right form of engagement with the protocol is fundamentally important.\\nIn preparation for tomorrow’s proposal, here’s the passing proposal simulation: https://github.com/TylerEther/compound-protocol/blob/halve-comp-rewards/spec/sim/0012-halve-comp-rewards/hypothetical_proposal.sim 32\nAll COMP reward rates except for supplying COMP are halved.\n\nProposal\nComptroller._setCompSpeeds(\n  [0x4Ddc2D193948926D02f9B1fE9e1daa0718270ED5, 0x39AA39c021dfbaE8faC545936693aC917d5E7563, 0x5d3a536E4D6DbD6114cc1Ead35777bAB948E3643, 0xccF4429DB6322D5C611ee964527D42E5d685DD6a, 0xf650C3d88D12dB855b8bf7D11Be6C55A4e07dCC9, 0x35a18000230da775cac24873d00ff85bccded550, 0xface851a4921ce59e912d19329929ce6da6eb0c7, 0xB3319f5D18Bc0D84dD1b4825Dcde5d5f7266d407, 0x6C8c6b02E7b2BE14d4fA6022Dfd6d75921D90E4E],\n  [5375000000000000, 33500000000000000, 33500000000000000, 5375000000000000, 4825000000000000, 731250000000000, 731250000000000, 731250000000000, 731250000000000],\n  [5375000000000000, 33500000000000000, 33500000000000000, 5375000000000000, 4825000000000000, 731250000000000, 731250000000000, 731250000000000, 731250000000000]\n)\n\nI need someone to verify that the proposal actions are correct.\\nRewards for supplying COMP are to be halved as well.\nAdjusted proposal actions:\nComptroller._setCompSpeeds(\n  [0x4Ddc2D193948926D02f9B1fE9e1daa0718270ED5, 0x39AA39c021dfbaE8faC545936693aC917d5E7563, 0x5d3a536E4D6DbD6114cc1Ead35777bAB948E3643, 0xccF4429DB6322D5C611ee964527D42E5d685DD6a, 0xf650C3d88D12dB855b8bf7D11Be6C55A4e07dCC9, 0x35a18000230da775cac24873d00ff85bccded550, 0xface851a4921ce59e912d19329929ce6da6eb0c7, 0xB3319f5D18Bc0D84dD1b4825Dcde5d5f7266d407, 0x6C8c6b02E7b2BE14d4fA6022Dfd6d75921D90E4E, 0x70e36f6bf80a52b3b46b3af8e106cc0ed743e8e4],\n  [5375000000000000, 33500000000000000, 33500000000000000, 5375000000000000, 4825000000000000, 731250000000000, 731250000000000, 731250000000000, 731250000000000, 2500000000000000],\n  [5375000000000000, 33500000000000000, 33500000000000000, 5375000000000000, 4825000000000000, 731250000000000, 731250000000000, 731250000000000, 731250000000000, 0]\n)\n\\nagree, nice proposal.\\nIn support of this. Its vital for the protocol to incentivise the holders adequately\\nIs there a reason the COMP reward rates were also halved for supplying COMP ?\nSupplying COMP to the protocol is one of the use cases of the token and giving rewards for this would incentivize people to hold on to the token.\\nIt’s not a great way to reward holders when the function of depositing COMP this way is for lending and borrowing. Not only does rewarding holders this way have an undesirable effect on the market, but it also has additional risks.\nIn my opinion, there should be staking contracts with lock-up periods and fixed return rates.\\nSounds good to me, Tyler.   How much less COMP is getting sold each day as a result of this adjustment?  Does the protocol automatically sell rewards to pay down interest as it accrues?\\nYes!! This right here is what I’m talking about\\n\nSounds good to me, Tyler. How much less COMP is getting sold each day as a result of this adjustment?\n\nToo soon to say.\n\nDoes the protocol automatically sell rewards to pay down interest as it accrues?\n\nNope. Users must claim their rewards, then they can do as they wish with them.\\n\nThe above statements I made in my prior proposal still hold. At the time of writing, I advocated for our users - who are also token holders - to continue receiving a share of the protocol. Decentralization of ownership matters.\n\nNope, decentralization matters not. It’s cool story which easy to sell to crowd, but really nothing was done for decentralization. Actually every step so far was in opposite direction. Initial distribution directead most of the tokens to the hands of venture investors and team, with lesser portion to be distributed for a users over 4 years, but, oh well, that distribution was massively concentrated for big capital holders, and now even that going to stop, effectively stealing user portion of tokens and redirecting it into “trreasury” of protocol. Which in turn is largely controlled by those who got\ninitial distribution, and now they can vote what they going to do with tokens never actually belonged to them. Users on the other hand will have cool story that for their benefit distribution going to improve \n\nSince most COMP being distributed by the current rewards program is instantly sold off, existing users and token holders are at a great disservice. Their share of the protocol is being diluted for nothing other than farming COMP for profit.\n\nThat can’t be further from truth. Share of the protocol for token holder stays exactly same no matter what happens with COMP being farmed. 1 COMP is always 1/10M share of the protocol. Nothing is diluted as COMP is a fixed supply token. As for market\nvaluation it have literally nothing to do with share of the protocol. It’s just a speculation which matters not in the long run. The only thing protocol ACTUALLY own is reserves, and recursive farming is in fact benefitial to reserves, as it exchange comp tokens, which bear no value, for mostly stable coins in reserves which DO have a value.\nHowever, it’s true that COMP distribution mechanism isn’t that great. It worked somewhat, and users were actually able to get a little bit of it, though scraps.\nAs for your plan i don’t have a goot feeling about it, if only for a reason that normally you don’t dismantle anything, which works, to build something which might work better in future (or might not). You build something FIRST, and then you transition from old to a new model.\nAs for COMP tokenomics, as it was first, it obviously wasn’t ideal. First of all initial distribution had not created treasury of protocol, opting to allocate all COMP supply. And treasury is what should be used for bootstrapping new markets.\nBootstrapping is a good idea by itself.\nBut treasury isn’t the only thing DeFi discovered through it’s existance. Of course, treasury in protocol tokens isn’t really a treasury just like it also isn’t in traditional finance. Aside of having protocol native tokens for some initiatives treasury should mostly hold actual reserve assets, like stable coins, eth, wbtc maybe several other tokens. Compound protocol actually fits well for having that, as it does naturally collect some reserves from it’s pools, which could be managed by protocol instead of just sitting in the pools for kind of nothing.\nAnd another good discovery made by DeFi on the way, that it’s actually great to have protocol-owned liquidity. Not in kind of scammy way ohm forks did, but as a concept. For example, protocol can and should provide deep liquidity for COMP-ETH pairs\non important chains. Like it’s great to pay for that liquidity in protocol tokens like some dao do, but it’s even better when protocol actually owns it itself and instead of paying for liquidity, recieves trading fees, which slowly grow it’s treasury.\nAll that things are quite well-known by that point:\n\n\nHaving treasury supply of native tokens (COMP) for incentivize programs\n\n\nHaving deep treasury reserves, nominated in non-protocol tokens like stable-cons, eth, wbtc, possibly some others.\n\n\nHaving deep own liquidity of nativetokens pairs on importand DEXes on important chains. At least on eth, maybe some others, depending on presence/planned presence of protocol.\n\n\nIt’s controversal if protocol tokens should be distributed for locking COMP tokens, there is not much value for protocol when someone just holds tokens, especially if at the same time they neither can use it as collateral, nor even vote in governance. It’s a waste in a long run, incentivising liquidity pairs is by far better spending (and even that is debateful,\nas if protocol owns deep enough liquidity itself, there is no need to pay for external suppliers of liquidity, as they will come for trading fees anyway)\nThere might be some benefit in locking tokens for years and recieving weighted voting power for that, but we all know what it creates. CRV is good example. It’s just going to create another Convex. Which might be not bad.\nThe biggest question is while COMP model could be improved, should it? Do those VCs actually that useful for community to drag them with their bags, or it’s better to dump them and just use the code? Compound works fine as a protocol, price appreciation of COMP token isn’t really needed for anything and even if it plummet to zero pretty much only VC will be hurt, it’s irrelevant for small users, who hold pretty much nothing in vast majority.\nThat’s the thing: wide distribution of tokens is needed more for major bag holders, rather than for small guys. And yet they not even were able to push airdrop through governance.\nIt’s scientifically interesting to see what will happen when distribution will stop, but it’s not a big deal. Liquidity for COMP tokens provided by speculators, farming is done by speculators. There is nothing fundamental there, just one traders try to benefit at the expense of other traders. And VC mostly sit on their tokens and only marginally care about price at least mid-term. To have something more solid, Compound should be more of a DAO, which owns value, manages it, and is profitable in growing protocol-owned funds. Then shares in such enterprise will grow naturally.\\nUsing reserves to provide COMP-ETH liquidity is a great idea.\\nBancor is an good DEX for providing protocol owned liquidity. You can provide one-sided liquidity and you are the Bancor protocol protects you from impermanent loss.\nSee blog post below.\n\n  \n      \n\n      Medium – 10 Mar 22\n  \n\n  \n    \n\nNexus Mutual Joins 30+ DAOs Adopting Bancor’s DAO Treasury Management Solution 5\n\n  Protocol Yield Uniquely Protected by Bancor Single-Sided Staking\n\n  \n    Reading time: 3 min read\n  \n\n  \n\n  \n    \n    \n  \n\n  \n\n\\nPassing proposal simulation for step 2: https://github.com/TylerEther/compound-protocol/blob/halve-comp-rewards/spec/sim/0013-cut-comp-rewards/hypothetical_proposal.sim 8\n\nProposal actions\nComptroller._setCompSpeeds(\n  [0x4Ddc2D193948926D02f9B1fE9e1daa0718270ED5, 0x39AA39c021dfbaE8faC545936693aC917d5E7563, 0x5d3a536E4D6DbD6114cc1Ead35777bAB948E3643, 0xccF4429DB6322D5C611ee964527D42E5d685DD6a, 0xf650C3d88D12dB855b8bf7D11Be6C55A4e07dCC9, 0x35a18000230da775cac24873d00ff85bccded550, 0xface851a4921ce59e912d19329929ce6da6eb0c7, 0xB3319f5D18Bc0D84dD1b4825Dcde5d5f7266d407, 0x6C8c6b02E7b2BE14d4fA6022Dfd6d75921D90E4E, 0x70e36f6bf80a52b3b46b3af8e106cc0ed743e8e4],\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n)\n\\nSorry for being late to the discussion, but I believe there is a lot to unpack here. Let me start by saying thank you to Tyler and the team for working on these proposals and trying to move the community forward. I generally try to stay to the side-lines on these types of conversations, but I think there are some aspects worth clarifying for this proposal.\nFirst, Proposal 092 was executed less than a month ago. There simply has not been enough time for community members to analyze the effects of that proposal, and thus I believe it is too soon to make another significant change like this to the Protocol. For a proposal of this magnitude, I would expect significant analysis on the effects of that proposal, on the market health, etc. I see many thoughts on the genesis for making this change, but little on the analysis of the change, or a clear action plan on the future replacement.\nTo follow up on that, there is insufficient conversations from the community on this greater plan (discussed originally in this thread 11 and now here). We should spend more effort bringing in a larger set of voices from the community (e.g. suppliers, borrowers, liquidators), and not mistake a limited response for tacit agreement. This forum post, and its precessor, have garnered less conversation than many less signficant proposals. I do not believe that the community has been given sufficient time to grasp the change and contribute to the conversation. As far as I can tell, there is no reason to push this change now, as opposed to after more time with more people contributing their own thoughts and ideas.\nFinally, there are a lot of thoughts here that address fundamental questions for the existence of COMP and the distribution. From my personal view, I want to be very clear: decentralization of the Protocol has been, and should continue to be, the primary target of governance. I can understand the arguments raised by Tyler in the beginning of this conversation, but I do not see a concrete plan to achieve similar means of decentralization. Thus, until a future plan is hashed out and built, this proposal may actively work against continued decentralization. We should take more time to discuss and review these concepts, and I would suggest we table this proposal until after those conversations occur.\\nThanks for sharing this perspective @hayesgm . I am also planning to vote against proposal 100. My rationale was the practical issue that removing the first 50% of rewards and removing the second 50% of rewards is a far more asymmetrical proposition than it sounds. A regular schedule of slowing COMP emissions seems more likely to support healthy liquidity in the markets than a sudden curtailment (even though 50% of rewards have already been cut).\nI also think that the point about decentralization is an underappreciated one. Compound Labs’ deployment of COMP as a valueless governance token distributed pro rata to users is at the center of its identity as a non-security. I appreciate @TylerEther and others’ push to maximize the development and community-building work that the community can secure with its remaining COMP, but pro rata emissions to users is doing a different kind of work, call it political or legal, that still has value to the protocol. Retaining some pro rata distribution to users ensures that no one can claim control of the protocol isn’t being incrementally handed over to users (even if they farm and dump; we have no control over secondary markets).\\nThank you @hayesgm and @allthecolors for sharing your views.\nGeoffrey has highlighted a larger issue - it’s very challenging to get input from the various groups of stakeholders, and this has a big impact on decision-making and execution. @getty has been my biggest resource in planning this set of proposals as well as other proposals. Some Discord channels specifically for each stakeholder group would be a great first step at tackling this issue.\nI agree that there should be more focus on decentralization of the protocol. So far, most tokens have gone to LPs with a few very large accounts with recursive positions harvesting and selling their rewards, which doesn’t exactly lead to a higher level of decentralization. We must not let the reservoir dwindle while we come up with new decentralized distribution plans.\nWe need to put the tokens in the hands of people who provide value to the protocol, those who’ll hold the token, and those who’ll participate in governance. Rewards for community members, for those who add to discussion, rewards for those providing help to new users, rewards for data analysis, rewards for innovative ideas, rewards for development, rewards for business development, etc.\nThe community has been neglected as far as rewards go, and we lost some good people after the temperature check for retroactive COMP distributions was shut down. I cannot stress enough how important a community is in the longetivity of any project. This is one area I think we should focus on.\nAs for the effect of this proposal will have on the markets, we do have Block Analitica’s impact analysis 10. I’d be surprised if market utilizations or interest rates change significantly.\\n\n\n\n hayesgm:\n\nAs far as I can tell, there is no reason to push this change now, as opposed to after more time with more people contributing their own thoughts and ideas.\n\n\nI see no reason for pushing this either. I don’t believe even first part with 50% slash was great idea, as i believe that first new plan should be introduced and THEN old system might be deprecated.\nBut what is done is done. It expected much higher impact for TVL of stable coin markets, but so far they kind of holding relatively well. However as for second part i want to point one specific thing. After 50% slash there is now plenty of COMP released for presenting to community new and improved Kickstart rewards.\nAnd until they surface i see no reason for slashing current rewards further. Actually since most distribution was concentrated in stable coin markets we very possibly could have same or better results with leaving distributions for all markets intact and slash 50% only stable coins markets.\nAnyway i believe that phase two should be presentation of new rewards rather than removing remaining ones.\nAlso i don’t like potential collapse of TVL of stable coin markets, as these markes are pretty much sole actual contributors for protocol reserves with all other markets combined not contributing even 1/3 of reserves created by usdc and dai markets.\\n\nimage2290×828 99.2 KB\n\nOne month is enough. Why hasn’t the community discussed in depth about removing the last 50%?\nWhat do you think the community means? COMP holders? VCs? The person who spammed in Discord?  The community you imagined is leaving Compound.\nThe community has been silent for a long time. Many people realized that the rewards have been reduced after the first 50% were deleted.There doesn’t seem to be more attention paid to Compound’s governance, including media and Defi users, competitors.\nGovernance is still consuming a lot of COMP for third-party services. Large institutions control Compound, and they decide the direction of governance. Is this decentralization? .\nBut there are divergent interests among the institutions, and mutual restraint rather than cooperation.\nThe community is not a shield, and the proposal may fail, but TylerEther has worked hard.\\n@ClairvoyantLabs Since inception, Compound has been based around high-quality work that builds trust- even if that takes time. I cannot disagree more with sentiments such as “one month is enough.” No change in Compound, or any project, should go just based on the sheer fact that it was written. Every change should be applied only after there has been consensus that it is the right change, that it is likely to achieve its goals, and that everyone has had a chance to “speak now or forever hold your peace.” The community is everyone who puts their tokens into the Protocol, every single borrower, and everyone who is involved in the project’s code or ecosystem. This thread has exactly 10 participants and 21 comments. We should strive to do better.\\nThe compound community will only continue to exist if the token is valuable.  Nothing stays the same, it either grows or it’s dying.  The roadmap is robust and being kept secure for COMP led by community members who’re doing a great job.  My suggestion is pass this right away.  Then introduce further rewards that make sense and increase the value and attractiveness of owning COMP.  COMP has started appreciating nicely versus BTC and ETH since the first 50% cut.  The market and the analysis thus far is telling us that the impact of this is good.\\nHow to do better?\nPublishing Compound Treasury data / NO\nUpdating Medium / NO\nImprove the transparency of Compound /NO\nBoost market confidence /NO\nReturn to a dominant market position GFX LABS 10 /NO\nSends two tweets a month? / YES\nMost users don’t know the existence of this proposal, and most projects in the Defi ecosystem don’t even know about it. There is almost no twitter space discussing the reduction in Comp rewards.This is due to low-frequency marketing.\nFor institutions with a lot of COMP, isn’t one month long enough for them to make a decision? but they didn’t speak up, maybe they would vote.\nOther protocols relying on Compound will change their strategy. (like:Notional、Yearn、GRO)\n\"Aave polygon rewards have ended. Aave mainnet rewards end next month. \"\nTyler has prepared kickstart rewards to replace unpurposed incentives, and we can discuss new incentive distributions at a longer time.\\nCompound Labs has stated numerous times that they’re not the owners nor the sole leaders of the protocol. They’re allowing other members to come in and lead protocol development and management with them.\nThe most important thing in increasing the decentralization of the protocol is in decentralizing the governance, management, and development. The protocol isn’t going to become more decentralized if the majority of work is centralized around Compound Labs. We need more contributors… but contributors can’t be expected to work for free. This is where I think the majority of the remaining COMP should go.\n@JacobPPhillips @jamico care to comment? Your votes are needed to get anything done.\\nHey guys, I would like to put in my two cents. I made this argument on twitter, but I thought I would repeat it here for good measure.\nI’m the co-founder of Notional Finance - we are a fixed rate lending protocol that’s built on top of Compound. We are integrated tightly with the Compound protocol - every dollar that is deposited on Notional is in turn deposited on Compound. Today, we contribute ~$400M in TVL to Compound and any future growth will directly feed through to increased supply on Compound’s lending markets.\nI think Notional represents a core constituency for Compound’s strategy. Compound wants to be base-layer tech that contributors build on - Notional is one of the single largest such contributors.\nI am very worried about the effect that this proposal would have. We built our protocol on Compound for two reasons:\n\n\nIt was the largest and most secure money market, and it was growing.\n\n\nIt didn’t make big changes quickly.\n\n\nThe passage of this proposal would violate these two assertions. I believe that passing this proposal risks a substantial decrease in Compound’s usage and would threaten its competitive position. Additionally, passing such an important proposal so abruptly, and with so little time to consider and measure the consequences, makes me question the ultimate reliability of the protocol as base layer technological infrastructure.\nI am against this proposal, and I think that it’s important that the community present in this forum hear the perspective of an integrating protocol - one of Compound’s key user constituencies.\nIf the community is set on cutting liquidity incentives to 0, I would suggest doing so gradually over a longer period of time and closely monitoring the consequences. By cutting down to 0 in one fell swoop you lose all flexibility for questionable upside (in my opinion) vs. taking a more measured and gradual approach.\\nI agree with Tyler, and agree to this proposal. Comp has been on a continual decline. When BTC is halved, you see inflation. Let’s try something new and remove farming rewards that Bitcoin can only dream of in many many years. Respectfully, I agree on coming up with rewards for token holders as my self, but let’s cut the bleeding now.  It will be interesting to see what happens in 3 months of no rewards. As Comp has expanded to new markets recently, how many more individual investors have we seen? Let’s not lose them. Stock markets don’t have farming that decreases value, and they increase more than comp.\\nGFX will be voting in favor of proposal 100.\n\nThe protocol is currently set to spend $57m over the next year on COMP incentives which amounts to about 6% inflation when compared to the circulating supply (Coingecko). While the initial idea behind COMP rewards was to reward users for using the protocol with governance rights, that never caught on. There is significant evidence of users depositing funds, earning COMP, and selling COMP. Most incentives have been distributed to recursive positions, sometimes referred to as loopers, who deposit and borrow the same asset to inflate their position to earn more rewards. These users are seeking the best return for their capital.\nThe best available rate has decreased substantially from the beginning year, but the capital shift has not been as one might expect. Rates for dollars across DeFi have reduced, and for substantiated protocols, like Compound, it is time to end blanket subsidies. Aave ended Polygon rewards on the 13th and is set to end rewards on mainnet on May 22nd.\nProtocols need to get back to focusing on their core utility and look internally to improve capital efficiency. As Tyler and GFX have mentioned, this isn’t the end of COMP rewards. They can still play a role in bootstrapping new markets, but it needs to be refined and planned out.\nWe have put together a Google Sheet 14 for those who want to see how the protocol has changed since the beginning of the year, to the first reward cut, and compare it to the current market.\nThe proposal’s potential downside is a lower TVL and lower supplier rates. The upside of the proposal is eliminating the inflation of the COMP circulating supply and lower borrower rates. The data to date has indicated lenders on Compound are either rate insensitive or that Compound has consistently offered a good risk/reward for capital in the sea of DeFi.\nWorst-case scenario (unlikely given the data available), governance can always vote to turn COMP rewards back on.\\n\nimage1695×886 76.6 KB\n\nInflation rate above 31%\nActive Addresses Count  % change from 1 year ago   -73.00%\n\nimage1693×924 104 KB\n\\nTo me, it’s quite impressive how diverse the stakeholder group here is — users, tokenholders, the core team, and contributors are all part of the discussion.\nHaving said that, it seems to me that this very same feature — namely, the diversity of the stakeholders — is what complicates the discussion. That’s because the incentives of each group are quite different. Below, I’ve done my best to highlight the incentives of each party.\n\n\nUsers. Users benefit from COMP-denominated incentives. Why would they ever vote for less money to their pocket?\n\n\nTokenholders. I believe it’s safe to say that tokenholders do not benefit from prolonged liquidity mining programs. Liquidity mining is “profitable” when the lifetime value of the user exceeds the cost of liquidity mining. For Compound, this calculation requires estimating the lifetime value of the user, discounting that value by Compound’s cost of capital, and finally, subtracting the cost of liquidity mining from the discounted lifetime value of the user. Given the unfaithful nature of liquidity on money market protocols, users aren’t sticky, leading the lifetime value for user to go down (contrast this to “sticky” products with large barriers to exit, e.g., operating systems). Due to this dynamic, liquidity mining programs for un-sticky protocols are unprofitable for tokenholders: it costs more to acquire users than the money the protocol makes from the user. As a result, tokenholders should generally be against COMP incentives.\n\n\nCore team. This one is tricky. On the one hand, members of the core team are tokenholders and have the same incentive as tokenholders. On the other hand, as a core team member, you are judged by your protocols’ usage, and usage of a money market is typically measured by outstanding borrow. Reducing COMP incentives will reduce outstanding borrow, which may reduce the stature of the protocol, and as a result, the core team.\n\n\nContributors. In my experience, Compound’s active contributors are some of the best in the game: they are loyal to Compound, fiercely independent, and to top it all off, smart as hell. We are lucky to have them (they know who they are, so I’ll prevent the blushing by not tagging them). Generally, they are tokenholders that have a significant percentage of their net worth in COMP. For this reason, their incentives are typically that of a tokenholder.\n\n\nUnfortunately, these often directly-opposing incentives lead to factions, and factions lead to governance challenges.\nAs far as my personal bias goes, I believe tokenholders deserve the victory on this proposal.\nIf we all agree liquidity is “hot” and will chase the greatest yields, then the risk of turning off COMP incentives is liquidity will leave quickly. But the inverse of that is also true: if we decide to run this experiment, we can always change our minds and turn COMP incentives back on; since liquidity is hot, it will run back to Compound.\nIn short, the “cost” of running this experiment are the foregone profits from liquidity that leaves Compound on the margin. (It’s worth noting that since hot liquidity is unprofitable, having it leave may actually improve the profitability of the protocol on the whole)!\\n\n\n\n twoodward:\n\nBy cutting down to 0 in one fell swoop you lose all flexibility for questionable upside (in my opinion) vs. taking a more measured and gradual approach.\n\n\nI quoted that as a widespread opinion.\nAs a user myself, i can call myself somewhat holder, though quite small. So in that logic i should in theory be supportive for actions potentially leading to price appreciation.\nBut i strongly believe that as a protocol Compound should not care for price of the tokens whatsowever. Even if they go to zero tomorrow it doesn’t really impact protocol that much. They never costed anything to begin with, and while speculators are very welcome to play with price as much as they want, price really doesn’t matter still… Governance influence does. Ideally protocol shouldn’t really have token holders, who interested in price, rather than parties interested in development of protocol be those actual code writers, or big users or whatever…\nThere was a time when i was thinking that maybe Compound should evolve in some sort of DAO, aiming to increase the value for its participants. But now i think it’s unnecessary complication. DAO, even multiply ones, could be very well build on top of Compound protocol. And protocol itself could remain as it is, a protocol. Instrument, using which other entities, aimed to money creation could be created. Protocol itself doesn’t need to be burdened with money making. It might rather stay a tool, preferrably with many parties working to make sure it stays functional, safe, and useful for everybody.\nIt’s hard task to achieve somewhat “fair” distribution of governance. And even me myself, pointed many times at how Compound distribution isn’t optimal. But it works somewhat. I still think that users in vast majority getting scraps fronm distributions, which mostly still collected by big capital indeed.\nOpinions of small users never really mattered for governance decisions in Compound, sometimes i even wonder why i still bother voicing mine from time to time. Maybe because Compound was and still is useful tool for me and i want it to be better. Or maybe because i don’t like when people claim that they know what would be better for others and yet present nothing new, aside of claims that some big players constantly making money from COMP distributions and we should stop that.\nYet again i want to remind everybody that Compound protocol doesn’t really spend money at all with it’s distributions. COMP tokens are not money, they hold no value by themselves. The only reason some people making money is because other speculators made pools with comp pairs to eth. So the only money farmers getting are money originated from liquidity providers for COMP pairs. No actual money is coming from Compound protocol.\nAnd for that reason it is very bad for protocol to keep a lot of native tokens for itself. Because their value could literally evaporate overnight. Protocol should have treasury of reserve assets, aka eth, wbtc, stable coins. And pay for audits/development/bug bounties from that. It’s much more safe in the long run.\nAs for distribution stopping, i’m against it. We don’t really have any better mechanism for dispersing COMP tokens to users. I’m all ears to see actual suggestions how it could be done better. Kickstart rewars is all cool name, but no real numbers were presented.\\nWe will be voting for COMP100. The arguments about how the current token distribution mechanism is not effectively decentralising governance power or incentivising organic usage of the protocol have already been made in the thread above and we largely agree.\nDespite the many references to finding a better way to spend these COMP, one thing that hasn’t yet been mentioned is the work the Labs team has done on Compound III and its imminent launch. This is an exciting new product, and one that we as a community should thoroughly evaluate as an opportunity to continue to decentralise our governance, whether through new token incentives or otherwise.\nWe see the end of the COMP distribution as a forcing function and are excited to help craft, vet, and pass proposals from any interested party that can grow the value of the protocol. The space available for hard work and creativity is large, and we believe the protocol has the resources and talent available to make the best of it.\\n@TylerEther what do you think the rest of the undistributed COMP would be best used for? How should we address the issue that there isn’t a great way to get COMP into the hands of users without giving them as rewards to suppliers and borrow. We want to avoid a situation where someone who wants to participate in Compound governance has to buy COMP or ask for delegations\\nApologies for only jumping into this discussion now, after prop 100 has already been submitted.\nI will be voting against, reasoning below:\n\nRemoving incentives completely is likely to significantly reduce Compound’s liquidity and revenue\nWhile lower dilution rate should have positive impact on COMP performance, I fear that loss of tvl/revenue could outweigh this and lead to net negative impact on COMP value\nIf COMP is worth less, payments to contributors will tend to increase dilution over time\nIt is generally more costly to acquire users/liquidity than to retain them, so I disagree that this is a risk free experiment; if liquidity leaves and Compound community decides that removing rewards is not working as intended, we’re unlikely to gain back all of the lost liquidity be reinstating the previous rewards rate\n\nI’d be more willing to support a slimmed down / targeted rewards reduction (example with arbitrary numbers below):\n\nRemove borrow side incentives for all non-stablecoin assets\nReduce supply side incentives for non-core assets (LINK, UNI, ZRX, BAT) by 80%\nReduce supply side incentives for core assets (ETH, WBTC, COMP) by 50%\nReduce supply and borrow incentives for stablecoins (DAI, USDC, USDT) by 30%\n\n(above example would reduce COMP incentive spending by ~37%)\nThis would reduce or remove incentives that bring the least value to Compound, while preserving the incentives that are most important for usability and subsidizing organic usage.\nIt may also be fruitful to coordinate further rewards reductions / removal with Aave - both protocols likely have an interest in reducing rewards, but face a prisoner’s dilemma about being the first mover.\\nI’m with OUSD 2 - a yield earning stablecoin. We deposit funds to Compound, among other places. At one time we were one of the largest USDT lenders in Compound.\nWe allocate funds roughly by the expected yield, as long as the risk looks acceptably low. This means that our funds can move from AAVE to Compound to Curve/Convex, to follow the yield.\nI’ve been closely watching the discussion, votes, and outcomes on these two proposals. I firmly believe that the COMP holders have the right to vote and act in their best interest. I’ve just been sitting here with popcorn, waiting to see what happens.\nI’ve personaly been surprised that the funds draw down on moving from 100% down to 50% has been as small as it was. Maybe people don’t pay attention? Maybe they were just going wait until it went to zero, and then move out?\nFrom personal experience, we did ten week liquidity campaign late last year on our project that resulted in a 10x increase in our supply, and it took weeks after it ended for capital to begin leaving, and then another few months to get down to 3.5x our starting amount. I was surprised with the stickiness there too.\nI think chopping reward rates for either lenders or for borrowers to zero makes a lot of sense. Recursive borrowing purely exists to extract rewards from the protocol. Though it does provide some protocol “income” in the form increases to the pool reserves, this is less efficient than if the protocol just sold COMP for cash and sent it to the pool.\nIn the long run, from a theoretic perspective, lenders will lend where they good rates, and borrowers will borrow where they can can get good rates. In the long run, even a small edge matters a lot.\nSince lending rates determine borrow rates, and borrow rates determine lending rates, incentivizing either side should be equivalent. However, it feels like the stablecoin lending side is able to react much quicker to changes in incentives since many rewards there are immediately sold by the protocols, (making the gas expense of selling them minimal), whereas individuals selling reward tokens can be cost prohibitive to do. (Though this could be an argument for either side of incentivizing lending or borrowing, depending on if the overall system responding to incentives is more important, or people not selling is more important.)\nIn the short term though, I think capital is surprisingly sticky.\nIf the current proposal was voted in, I don’t think we’d see immediate massive changes outside of the unwinding of some recursive positions. It’s the long term I would be concerned about.\nWe really don’t know the full outcome of the 50% cut that happened. We have learned that’s it’s not immediately catastrophic, which is a nice learning, but we don’t know much about the long term effects. I would say that four-six months would be a more appropriate test period of time.\\nVoting No on Proposal 100 on behalf of Pantera Capital.\nWe think the proposal is directionally correct, but doesn’t need to be rushed.\nThe initial adjustment to COMP rewards provides enough breathing room for the protocol to begin introducing and testing kickstart rewards as an alternative destination for COMP incentives, as well as measuring the impact of the initial adjustment (Steps 3 and 4 in @TylerEther’s proposed plan). More progress is needed on these before we move towards a complete removal of the current program.\nThe impact of moving to 0% rewards is likely to be non-linear, in terms of:\n\nimpact to Compound’s supply side in core assets, which negatively impacts third-party contributors / partners in the COMP ecosystem\nmarket impact to COMP value, which significantly reduces the protocol’s ability to compete in the near / medium-term or to effectively test new incentive programs.\n\nIt’s clear from the minimal impact from the initial 50% adjustment that Compound has room to adjust its incentive program downwards, relative to how the market sees the risk-reward of supplying assets to the protocol. In other words, we’ve been over-compensating and the market \"trusts’ Compound more than we thought. This is why I believe the proposal is directionally correct in guiding us towards a more targeted, more productive use of COMP emissions, but needs to be adjusted in terms of timing and sequencing.\\n\n\n\n monet-supply:\n\nIt may also be fruitful to coordinate further rewards reductions / removal with Aave - both protocols likely have an interest in reducing rewards, but face a prisoner’s dilemma about being the first mover.\n\n\nI agree with much of @monet-supply’s post. But just going to point out that coordinating with AAVE would be almost certainly be illegal in the US 11.\\nThis is the deciding factor for me as well, just voted no.\\nReally appreciate the hard work from @TylerEther, @getty and others on this proposal, as well as the perspectives of everyone in this thread.\nHere’s why we voted yes: https://twitter.com/_jamico/status/1516902742513491968?s=20&t=tTwTgII363Jd5C58cjC5RA 31\\nSurprisingly the reserve factor hasn’t been brought up in this thread. From the perspective of a depositor the net result of this proposal is purely a capital efficiency loss. Why not balance this out by reducing the reserve factor?\nIt’s worthwhile mentioning that this exact logic was used to increase the reserve factor 2 years ago rather than reducing COMP distribution. It’s clear the two are related and should be considered together before making a decision, especially considering actionable events in the past: Compound | Proposal Detail #9 13\\nThe External Governance Team of ChicagoDAO 6 has reached a unanimous decision to vote against the COMP Reward Adjustments v2 Proposal with our 50k delegated COMP tokens from A16Z. The reasons for this vote are as follows.\n\n\nThere is no clear alternative for a reward mechanism. Although there are obvious inefficiencies in the current system, ending the current incentive plan would be an act of self destruction – especially when considering the initial benefits 1 from the reward mechanism. Additionally, a period with no reward mechanism could initiate a run from Compound to other competitors, permanently hurting the community.\n\n\nEnding distribution of the COMP token would impact the quality of decentralization in the Compound community. By capping the production of COMP, the community is more vulnerable to centralized overhauls. Thus, the current reward mechanism is not simply an incentive for further contributions, but it also provides a way of stability for the decentralization of the community.\n\n\nIn conclusion, ChicagoDAO 6 does not oppose the idea behind the proposal. The current reward system is outdated and needs further attention to promote the growth of the community. But, complete removal of the current structure would be premature causing fractures in the reputation of the community.\nWe will be sharing further reasoning, qualifiers, and rebuttals to common For arguments on our Twitter later this evening—find us at @chicagodao_io 2!\nMadhav Vats | University of Chicago\nOn Behalf of Chicago DAO - External Governance Team\\nGauntlet controls the setting of the reserve factor parameter\\nYou’re late, you’ve dispersed your 321,024 votes before, 256,007 is not enough, proposals have failed.\nPolyChain abstained, with a small gap of 1.5%, what else can Tyler do? Voting again?\\n\n\n\n ClairvoyantLabs:\n\nPolyChain abstained, with a small gap of 1.5%, what else can Tyler do? Voting again?\n\n\nIt would be logical to actually present his new vision of new rewards. There is more than enough COMP available already after first 50% reduction to start anything he might have in mind.\nAnd when comminuty could see results of new program we can revisit COMP distribution again but that time with much more data to back the proposal. There never was a reason to rush into anything. COMP rewards even before fist 50% reduction were scheduled to go for years. After reduction it can run for years AND have additional incentives implemented also.\nThere is very minor difference if the rewards stop already or stopped several months later. If new distribution model will be so good it would be easy to convince everybody with amazing results.\nSo far it’s really not that obvious and voting results reflect that. It’s very big difference in comparison with voting for something with what majority agrees. You can see that some chose to abstain, which kind of reflects lack of backing behind pitch for new model. Nothing stops Tyler from presenting his vision currently. I’d say actually failure to present it at this point just reinforce argument that there was nothing really, and the whole point was just to stop existing distribution “because we don’t like it” and there are farmers who make money on it. Well, wasn’t there analysis which pointed that vast majority of recursive farming positions are not in profit after first reduction already.\\nThis is an extremely strong way to state this. Gauntlet is being paid to run some simulations to give a more statistical basis for determining risk of insolvency, and where there is space for parameters to be tweaked without increasing risk they make a proposal.\nI would expect a statement from them if reserve factors were to be changed in a way that created risk of insolvency, but they do not control the reserve factor, nor are the tasked with finding creative new uses for it.\\nThanks for the feedback, everyone!\nHere’s my summary of why this second proposal failed:\n\nThere hasn’t been enough time to evaluate the effects of the first proposal and voters are fearful of the negative consequences of completely removing the existing rewards.\nVoters don’t see a clear path to decentralization without the existing program.\n\nThe action plan for kickstart rewards is really simple and I expect the first proposal for them (with the first new market) to achieve consensus fairly easily. It goes like this: incentivize $50M (or 10% of circulating supply, whichever is smaller) at an annualized rate of 8% over 3 months, then cut the rewards. We assume at least 20% (or $10M of that initial $50M target) will stay in the supply market (this is our starting hypothesis).\nFactors influencing the correctness of our assumption:\n\nInterest rate model(s)\nReserve factor\nUsefulness of borrowing the newly supported token\nCollateral factor (and borrowing factor when it’s implemented)\n\nWe’ll continually analyze the correctness of our hypotheses and will continue to test new ones, making adjustments to IRMs, RFs, and market addition priorities as warranted.\nThe action plan of the COMP rewards adjustments as per kickstart rewards isn’t exactly linear - we can and will go ahead without cutting the existing rewards program to 0. While everyone is paying attention to this thread, please speak now on the numbers I presented; otherwise, I’ll go ahead with those.\nI’ll loop back to cutting the rewards (or halving them again if that’s what the majority wants) in a matter of months.\nAs for additional rewards programs, those will take time to develop as they’ll change how we organize ourselves as well as the direction of the protocol. I have ideas in mind, but we’re subject to the inefficiencies of bureaucracy. This process can be expedited by having our voters (more importantly, our largest voters) provide quick and thoughtful feedback (which has proved to be very challenging).\\nGlad the thread will continue; the kickstart proposal here already goes a long way toward addressing some of the earlier questions about ambiguity of the plan forward.\n\n\n\n Tyler Loewen:\n\nWhile everyone is paying attention to this thread, please speak now on the numbers I presented; otherwise, I’ll go ahead with those.\n\n\nQuick question to check my understanding: if a new market attracts $200M in supply, the COMP kickstart reward to suppliers in this program would be diluted to 2% (4x dilution of the kickstart rewards on the target $50M); am I interpreting that correctly?\nKickstart rewards will still likely attract mercenary capital to farm-and-dump COMP, but given the evidence we’ve seen of stickiness in supply markets, this could still be an effective way to kickstart them.\nThere is at least one key difference between currently established markets and new ones that could affect stickiness: the currently established markets never had (and still don’t have) a pre-determined end date for their COMP distributions. I wonder if baking the 3-month timeline into the proposal in advance will make this capital less sticky by allowing suppliers to plan their next moves in advance? It feels like it should be possible to guess the answer with data from other protocols that have offered fixed-term rewards, but empirically it seems like every case has unique features making it difficult to generalize.\\nHello Compound community,\nI’m an observer of this extremely close round of voting regarding the Rewards incentives and I would like to suggest an alternative way of creating perhaps a more sustainable rewards solution. I can understand the limited (some people have commented that this is negative) value a continual rewards program brings; and that some are concerned about the system shock of an abrupt cut in rewards.\nPerhaps one way to redirect and focus rewards is the usage of KPI Options. This community values organic growth and behavior that is beneficial to the community, so perhaps if rewards can only be paid out upon additional criteria beyond the staking of capital, maybe this can also be considered. The payout/reward will be variable, but directly tied to how much of the KPI the community has achieved, which makes this an efficient form of incentivization. I envision that an application of KPI Options can replace the existing program, with each Option having an expiry of 1-3 months depending on the KPI measured, and addressing the potential need to wind down the existing rewards program and for bootstrapping new initiatives where a clear need for capital, governance, or even any sort of pain point has been identified.\nThe specific KPI can be further explored, and we will be more than happy to brainstorm with you on defining this further if there is initial interest among the community to proceed. A very simple example would be a TVL of a specific market or pool.\nKPI Options can also be used to mitigate whale effects on the ecosystem by considering a community-wide distribution in addition to the KPI Option itself being a source of emissions. In this case, the effects of community participation is partially socialized. This system is being trialed on other project integrations we are currently working with.\nFinally, KPI Options can also be used as a method of compensation for contributors. This is the model that the contributors at some DAOs are exploring or using.\nHappy to take any questions/feedback and appreciate everyone to read through this post.\nDisclaimer: I am a contributor at the SuperUMAns, a sub-DAO of UMA.\\n\n\n\n Tyler Loewen:\n\nIt goes like this: incentivize $50M (or 10% of circulating supply, whichever is smaller) at an annualized rate of 8% over 3 months, then cut the rewards. We assume at least 20% (or $10M of that initial $50M target) will stay in the supply market (this is our starting hypothesis).\n\n\nThis seems like a reasonable starting hypothesis from our experience.\\nThis was really close.  I wonder what would have happened if cCOMP token holders whose COMP is locked in the protocol as collateral could have voted?  I know there was some talk of this long ago but not sure what ever came of it.  It makes sense that a lot of COMP longs use the protocol to buy more COMP and also to borrow against gains tax free.\\n\nQuick question to check my understanding: if a new market attracts $200M in supply, the COMP kickstart reward to suppliers in this program would be diluted to 2% (4x dilution of the kickstart rewards on the target $50M); am I interpreting that correctly?\n\nThat’s correct!\n\nKickstart rewards will still likely attract mercenary capital to farm-and-dump COMP, but given the evidence we’ve seen of stickiness in supply markets, this could still be an effective way to kickstart them.\n\nPossibly, but it depends on various factors: exchange liquidity and slippage, holder distribution, ability to borrow the token elsewhere, etc. Buying large amounts of a token to temporarily farm COMP with may become unprofitable when it comes time to sell the token after rewards dry up.\n\nThere is at least one key difference between currently established markets and new ones that could affect stickiness: the currently established markets never had (and still don’t have) a pre-determined end date for their COMP distributions. I wonder if baking the 3-month timeline into the proposal in advance will make this capital less sticky by allowing suppliers to plan their next moves in advance? It feels like it should be possible to guess the answer with data from other protocols that have offered fixed-term rewards, but empirically it seems like every case has unique features making it difficult to generalize.\n\nYeah, this will be hard to generalize. It really depends on capital efficiency in two regards: the ability to borrow against it, and the interest rate on depositing compared to other avenues. This creates the need for a quality asset listing strategy.\\nThanks for keeping the momentum going here @TylerEther. I want to boil down my understanding of your kickstart proposal to make sure I’m on the same page.\n\nnew market launches\nthe first $50m of (supply? supply and borrow?) capital into that market earns an 8% annualized COMP incentive and anything over $50m dilutes the incentive linearly\nafter 3 months the rate of the incentive will be adjusted down to somewhere less than 8% and greater than or equal to zero\n\ncan you clarify what you mean by:\n\nWe assume that at least 20% (or $10M  of that initial $50M target) will stay in the supply market\n\nThanks!\\n\n\nthe first $50m of (supply? supply and borrow?) capital into that market earns an 8% annualized COMP incentive and anything over $50m dilutes the incentive linearly\n\n\nCorrect. Supply will be incentivized and borrowing activity will have to develop organically.\n\n\nafter 3 months the rate of the incentive will be adjusted down to somewhere less than 8% and greater than or equal to zero\n\n\nI’m thinking immediately down to zero after the 3 months. Then supply and demand will take over.\n\ncan you clarify what you mean by:\n\nWe assume that at least 20% (or $10M of that initial $50M target) will stay in the supply market\n\n\nSo assume that by the end of the three months reward period that there’s $50M in supply. The goal is to have at least 20% of that supply stay in the protocol.\\nGreat, super clear.\n$50m seems like a reasonable target based on the current market stats 3. Given the compressed yields market-wide, 8% should be enough to make lending appealing. And cutting to zero is a good idea from a discovery perspective - it’ll be super interesting to see attrition rates.\nLooking forward to voting for this one eventually.\\nAave Ends Rewards In Latest Blow To Yield Farmers\n  \n      \n\n      The Defiant – 26 May 22\n  \n\n  \n    \n\nAave Ends Rewards In Latest Blow To Yield Farmers - The Defiant 9\n\n  Aave has stopped offering incentives to Ethereum v2 users in its native token, AAVE after the protocol’s rewards program expired on May 22.\n\n  \n    Est. reading time: 2 minutes\n  \n\n  \n\n  \n    \n    \n  \n\n  \n\n\\n\n\n\n ClairvoyantLabs:\n\nhttps://thedefiant.io/aave-ends-rewards/\n\n\nA big argument against this proposal was “AAVE does it - so should we.” I hope this proposal gets revisited this year. The longer we wait the more the treasury drains.\\nYes, we should stop aimless liquidity mining and turn to incentives when necessary.\nFor example, to stimulate future multi-chain liquidity growth, provide more compound interest for COMP holders, veToken model, and protocol income distribution.\nAt the same time, the community should pay corresponding rewards to Tyler, who has worked hard for this.\\nIn order to be able to open the switch for distributing the profits of the protocol, the switch and ratio must be designed first.\nIt is easy to think of designing a veCOMP model for COMP, providing lock-up for 1 week to 4 years. (like Curve)\nveCOMP gradually replaces the voting rights of COMP, which is more effective in preventing governance attacks.\nveCOMP holders share the profits of the protocol, which is expected to exceed 30% APR in the early stage, and gradually decrease to 7% as veCOMP rises.\nObviously, the market circulation of COMP has decreased and the demand has increased."
  },
  {
    "number_of_comments": 11,
    "postid": "3f46d11c-e7fa-4937-9451-f5f6bd379c4f",
    "posturl": "https://www.comp.xyz/t/deploy-compound-iii-on-base/4402",
    "combinedcontent": "Deploy Compound III on Base\nPreamble:\nType: Multichain Deployment\nTitle: Deploy Compound III on Base\nAuthor: Compound Labs\nProposal Introduction\nPoint of Contact:\n@compound.finance (Compound Labs)\nProposal Summary:\nCompound Labs proposes the deployment of Compound III on Base for the community.\nOverview of Proposal:\nAs mentioned in Compound Lab’s roadmap updates during the community call, Labs proposes to deploy Compound III on Base 7. Base is an EVM-equivalent L2, incubated within Coinbase, built on the OP Stack in collaboration with Optimism. Base seeks to provide a secure, low-cost, and developer-friendly environment to build decentralized apps on-chain and bring the next billion users to web3.\nThe Base Goerli testnet has been live since February of this year and mainnet is planned to launch in 2023. More details can be found in their docs 5.\nCompound Labs sees two promising use cases for borrowing on Base and intends to launch two Comet markets on Base mainnet via governance proposals. The first will be a USDC market with crypto majors as collateral (e.g. WETH, WBTC, etc.). The second will be a WETH market with LSDs as collateral (e.g. cbETH, wstETH, etc.).\nCompound Labs has already deployed two instances of Comet (cUSDCv3 and cWETHv3) on the Base Goerli testnet (addresses in the docs 3) and welcomes the community to start trying them out 10.\nMotivation:\nAs Base gears up for mainnet launch in 2023, Compound Labs aims to position the Compound protocol as a core DeFi lego on the nascent chain. We believe that Coinbase’s popularity with retail users can onboard a new wave of users to the Compound protocol and community.\nGrant Application:\nDid not apply\nNon-Technical Evaluation\nAs of this post, 2.3M accounts have been created and 24.1M transactions have been completed 1 on Base Goerli. Base also has a comprehensive list of launch partners, as can be seen in the graphic below.\n1600×827 281 KB\nSource: How the Builders Met Base — Base 1\nSecurity Considerations\nBase is built on the OP stack and, as a result, will have a very similar security profile to Optimism, one of the largest L2s right now.\nSome of the risks that should be taken into consideration:\n\nBase’s sequencer is centralized\nThe fraud proof system is still under development\nThe core smart contracts can be upgraded 2 by a multi-sig\nFurther risk analysis can be found here: Optimism – L2BEAT 4\n\nTechnical Implementation\nThe smart contract system deployed on Base will be similar to that already deployed on Polygon and Arbitrum. The L1 Timelock will govern the Base contracts by sending messages to the L2 BridgeReceiver via the native message-passing bridge. More details of Compound III’s multi-chain governance can be found here.\nThe set of contracts that will be deployed on Base is detailed in the diagram below:\n439×1010 17 KB\nCopyright Waiver\nCopyright and related rights waived via CC0.\nLicense Exemption\nWe are requesting an exemption from the community that will allow the Base network to obtain a Compound Business Source License (BSL) to use the Licensed Work, update compound-community-licenses.eth, and deploy it on the Base network, provided that the deployment is subject to Ethereum Layer 1 Compound Protocol governance and control.\\nGauntlet Initial Parameter Recommendations - Compound V3 USDC and WETH Comets on Base (7/28/23)\nBASE is currently in Goerli testnet. Testnet liquidity, users, and TVL may not necessarily transfer over to mainnet, which means any parameters we recommend right now are based on assumed TVL and liquidity and MUST be revisited once mainnet is launched and liquidity flows in.\nSummary\nWe provide two options to the community below. Option 1 is very conservative to test out Compound V3 mechanics. As such, the conservatism is less so derived from market risk (which is Gauntlet’s focus) but more so from the smart contract and other technical risks. Option 2 is aggressive and assumes that the community does not need to test Compound V3 mechanics on a new chain.\nOption 1: Very Conservative (Test out Mechanics)\nUSDC Comet\nRisk Parameters\n\n\n\n\nParameter\nWETH\nWBTC\ncbETH\n\n\n\n\nLiquidation Factor\n50%\n45%\n45%\n\n\nCollateral Factor\n45%\n40%\n40%\n\n\nLiquidation Bonus\n5%\n5%\n7%\n\n\nSupply cap\n5500 ($10M)\n175 ($5M)\n3750 ($7.5M)\n\n\n\nStorefront Price Factor: 60%\nIncentive Parameters\nIR Curve: Same as Polygon USDC\n\n\n\n\nSupply COMP Rewards\nBorrow COMP Rewards\n\n\n\n\n10\n5\n\n\n\nWETH Comet\nRisk Parameters\n\n\n\n\nParameter\ncbETH\n\n\n\n\nLiquidation Factor\n85%\n\n\nCollateral Factor\n88%\n\n\nLiquidation Bonus\n5%\n\n\nSupply cap\n3750 ($7.5M)\n\n\n\nStorefront Price Factor: 50%\nIncentive Parameters\nIR Curve: Same as Ethereum WETH\n\n\n\n\nSupply COMP Rewards\nBorrow COMP Rewards\n\n\n\n\n5\n0\n\n\n\nOption 2: Aggressive (Assume mechanics are working, then increase the aggressiveness of parameters after inflow)\nUSDC Comet\nRisk Parameters\n\n\n\n\nParameter\nWETH\nWBTC\ncbETH\n\n\n\n\nLiquidation Factor\n84%\n76%\n80%\n\n\nCollateral Factor\n79%\n70%\n75%\n\n\nLiquidation Bonus\n5%\n5%\n7%\n\n\nSupply cap\n11000 ($20M)\n350 ($10M)\n7500 ($15M)\n\n\n\nStorefront Price Factor: 60%\nWe give supply cap recommendations based on assumed BASE TVL upon Compound deployment. Given it is unknown what the ratio of asset market cap proportions to total BASE TVL will be, the parameter values recommended here are an estimation based on assumptions, including cbETH having a higher circulating supply than wstETH on BASE, due to the Coinbase relationship. Again, these numbers depend on BASE TVL upon launch and may change.\nIncentive Parameters\nIR Curve: Same as Polygon USDC\n\n\n\n\nDaily COMP Supply Rewards\nDaily COMP Borrow Rewards\n\n\n\n\n30\n15\n\n\n\nOur COMP rewards recommendations are designed to offer appealing distribution APRs when the comet is first launched, and also when supply caps are highly utilized.\nThe relatively higher supply rewards incentivize a greater inflow of supply tokens into the protocol. This is important in the early stages of protocol growth since USDC supply is required before borrowers can join. Daily COMP rewards are subject to change as TVL rises and the markets evolve.\nGiven the recommended supply caps and liquidation factors, users can borrow at most $36.4M USDC. If users max out supply caps with an average borrow usage of 67% (current Polygon USDC borrow usage), then $24.6M USDC will be borrowed. If the protocol has $24.4M USDC borrows and distributes 15 daily COMP Borrow Rewards, the Borrow Distribution APR will be 1.61%. Now, assuming a 70% USDC utilization, the total supply required would be $34.8M USDC. If the protocol has $34.8M USDC supply and distributes 30 daily COMP Supply Rewards, the Supply Distribution APR will be 2.25%.\nNote that it will take time for the Base ecosystem to grow and Compound TVL to grow to this end state. In the meantime, this reward distribution amounts to a daily cost of $3,225 and $1.1M annually given the current COMP price. The table below shows an example of how distribution and net APRs could evolve based on the protocol’s TVL.\n\n\n\n\nUSDC Supply\nUSDC Borrows\nUtilization\nSupply APR\nSupply Distribution\nNet Supply APR\nBorrow APR\nBorrow Distribution\nNet Borrow APR\n\n\n\n\n$5,000,000.00\n$1,000,000.00\n20.00%\n0.65%\n15.70%\n16.35%\n2.20%\n39.24%\n37.04%\n\n\n$10,000,000.00\n$3,000,000.00\n30.00%\n0.98%\n7.85%\n8.82%\n2.55%\n13.08%\n10.53%\n\n\n$20,000,000.00\n$10,000,000.00\n50.00%\n1.63%\n3.92%\n5.55%\n3.25%\n3.92%\n0.67%\n\n\n$34,840,000.00\n$24,388,000.00\n70.00%\n2.28%\n2.25%\n4.53%\n3.95%\n1.61%\n-2.34%\n\n\n\nWETH Comet\nRisk Parameters\n\n\n\n\nParameter\ncbETH\n\n\n\n\nLiquidation Factor\n93%\n\n\nCollateral Factor\n90%\n\n\nLiquidation Bonus\n5%\n\n\nSupply cap\n7500 ($15M)\n\n\n\nStorefront Price Factor: 50%\nIncentive Parameters\nIR Curve: Same as Ethereum WETH\n\n\n\n\nSupply COMP Rewards\nBorrow COMP Rewards\n\n\n\n\n20\n0\n\n\n\nGiven the recommended supply cap and liquidation factor, users can borrow at most $14M WETH.  If users max out supply caps with an average borrow usage of 85% (current Ethereum WETH borrow usage), then $11.8M WETH will be borrowed. Now, assuming a 85% WETH utilization, the total supply required would be $27.5M WETH. A daily COMP supply reward distribution of 20 would yield a 1.90% Supply APR.\nNote that it may take for the Base ecosystem to grow and Compound TVL to grow to this end state. In the meantime, this reward distribution amounts to a daily cost of $1,433 and $523,191 annually given the current COMP price. The table below shows how distribution and net APRs evolve based on the protocol’s TVL.\n\n\n\n\nWETH Supply\nWETH Borrows\nUtilization\nSupply APR\nSupply Distribution\nNet Supply APR\nBorrow APR\nBorrow Distribution\nNet Borrow APR\n\n\n\n\n$5,000,000.00\n$1,000,000.00\n20.00%\n0.57%\n10.46%\n11.03%\n2.03%\n0.00%\n-2.03%\n\n\n$10,000,000.00\n$4,000,000.00\n40.00%\n1.14%\n5.23%\n6.37%\n3.06%\n0.00%\n-3.06%\n\n\n$19,000,000.00\n$8,000,000.00\n42.11%\n1.19%\n2.75%\n3.95%\n3.17%\n0.00%\n-3.17%\n\n\n$27,575,581.40\n$11,857,500.00\n43.00%\n1.22%\n1.90%\n3.12%\n3.22%\n0.00%\n-3.22%\n\n\n\nNext Steps\nWe welcome the community’s feedback and will create a poll below to gauge the community’s preferences.\\nTo gauge community sentiment, starting a poll here\n78%Option 2: Aggressive11%Abstain11%Option 1: Very Conservative9voters\n                    \n                    Closed Aug 4\n                   \\nAs mentioned in last week’s community call, Labs is planning to make the proposals to initialize the Base markets once a) all the assets (e.g. USDC) have been deployed on Base and b) Chainlink price feeds are available on Base. Until then, Labs is in a holding pattern because the proposal is blocked by these contracts being available.\\nThanks @Gauntlet we support the deployment of Compound III on Base!\nWe are in favour of the aggressive risk parameters to ensure Compound remains competitive with other lending protocol deployments on Base.\\nThanks Gauntlet team for the analysis. We also agree with the aggressive parameter setting.\\nFollowing the conversations on this thread, recent community calls, Discord, and GitHub, a cUSDbCv3 (USDbC 2 is the bridged version of USDC on Base) market has been deployed to Base from this pull request.\nThe parameters to enable the market are being finalized on this pull request. It includes Gauntlet’s recommended risk parameters (aggressive option) for the following assets:\n\nWETH: $20.2M supply cap (11K tokens), 79% CF, 84% LCF, and 5% liquidation fee\ncbETH: $14.4M supply cap (7.5K tokens), 75% CF, 80% LCF, and 7% liquidation fee\n\nWBTC is not included as an initial collateral asset because it does not exist on Base yet.\nThe interest rate model for cUSDbCv3 is currently configured the same as cUSDCv3 on Polygon/Arbitrum. After the launch of the market, Gauntlet will monitor and consider recommendations to the interest rate model based on preliminary utilization & growth.\nThe assets and price feeds of the deployment use the following inputs:\n\nThe base asset, USDbC, uses 0x7e860098F58bBFC8648a4311b374B1D669a2bc6B.\nFor WETH, 0x71041dddad3595F9CEd3DcCFBe3D1F4b0a16Bb70 1.\nFor cbETH, a multiplicative price feed is used to derive the cbETH / USD price using Chainlink’s cbETH / ETH and ETH / USD price feeds. The multiplicative price feed has been audited by OZ.\n\nOpenZeppelin has completed an additional audit 1 of the bridged governance contracts and flows. Note that the audit is for Optimism, which Base is a fork of.\nFinally, see the Initialization Proposal section below for more information about the next steps to enable the deployed market.\nDeployed Contracts\ncUSDbCv3: 0x9c4ec768c28520B50860ea7a15bd7213a9fF58bf\nThis is the main proxy contract for interacting with the new market. The address should remain fixed and independent from future upgrades to the market. It is an OpenZeppelin TransparentUpgradeableProxy contract.\ncUSDbCv3 Implementation: 0x75c07e9DF5B6EB30f6A40Fb124794aB0e0996322\nThis is the implementation of the market logic contract, as deployed by the Comet Factory via the Configurator.\ncUSDbCv3 Ext: 0x2F9E3953b2Ef89fA265f2a32ed9F80D00229125B\nThis is an extension of the market logic contract which supports some auxiliary/independent interfaces for the protocol. This is used to add additional functionality without requiring contract space in the main protocol contract.\nConfigurator: 0x45939657d1CA34A8FA39A924B71D28Fe8431e581 1\nThis is a proxy contract for the ‘configurator’, which is used to set and update parameters of a Comet proxy contract. The configurator deploys implementations of the Comet logic contract according to its configuration. This pattern allows significant gas savings for users of the protocol by ‘constantizing’ the parameters of the protocol.\nConfigurator Implementation: 0x83E0F742cAcBE66349E3701B171eE2487a26e738\nThis is the implementation of the Configurator contract, which can also be upgraded to support unforeseen changes to the protocol.\nProxy Admin: 0xbdE8F31D2DdDA895264e27DD990faB3DC87b372d\nThis is the admin of the Comet and Configurator proxy contracts. It is a ProxyAdmin as recommended/implemented by OpenZeppelin according to their upgradeability pattern.\nComet Factory: 0x27C348936400791b7350d80Fb81Bc61Ad68dF4AE\nThis is the factory contract capable of producing instances of the Comet implementation/logic contract, and invoked by the Configurator.\nRewards: 0x123964802e6ABabBE1Bc9547D72Ef1B69B00A6b1 2\nThis is a rewards contract which can hold rewards tokens (e.g. COMP) and allows claiming rewards by users, according to the core protocol tracking indices.\nBridge Receiver: 0x18281dfC4d00905DA1aaA6731414EABa843c468A\nReceives bridged governance messages from the Base cross domain messenger contract and forwards them to the bridge timelock.\nBridge Timelock: 0xCC3E7c85Bb0EE4f09380e041fee95a0caeDD4a02\nThe governor of the Comet deployment, exclusively receiving input from Ethereum mainnet governance through the bridge receiver.\nInitialization Proposal\nTo initialize the market, the deployment process is similar to the initialization of cUSDCv3 on Ethereum mainnet, the primary difference being the bridging of governance actions to Base, instead of taking place directly on the governance chain (Ethereum mainnet).\nThe initialization proposal will take the following actions:\n\n\nSet Comet configuration, deploy new Comet on Base, and set rewards configuration. This sends the encoded setConfiguration and deployAndUpgradeTo and setRewardConfig calls across the bridge to the governance receiver on Base. Rewards will be allocated according to Gauntlet’s aggressive suggestion: 30 COMP/day to the supply side and 15 COMP/day to the borrow side.\n\n\nApprove Base’s L1StandardBridge to take Timelock’s USDC, in order to seed the market reserves through the bridge.\n\n\nDeposit 10K USDC from mainnet to the Base L1StandardBridge contract to bridge to Comet.\n\n\nApprove Base’s L1StandardBridge to take Timelock’s COMP, in order to seed the rewards contract through the bridge.\n\n\nDeposit 12.5K COMP from mainnet to the Base L1StandardBridge contract to bridge to CometRewards.\n\n\nWrite the ENS TXT record v3-official-markets on v3-additional-grants.compound-community-licenses.eth containing the official markets JSON.\n\n\nReduce the COMP distribution to v2 cUSDC suppliers by 45 COMP/day, so as to keep the total COMP distribution constant.\n\n\nThe deployment and proposal migrations have been built using the Comet scenario framework and deployed using the Comet deployment manager. The scenario checks can be seen from the proposal branch CI checks.\\nThe proposal 9 has been created and will start voting in 2 day.\\nThanks everyone for voting. The proposal has passed and reached quorum. It will be executable on Base on Saturday.\\ncWETHv3 market on Base\nFollowing the conversations on this thread, recent community calls, Discord, and GitHub, a cWETHv3 market has been deployed to Base from this pull request 1.\nThe parameters to enable the market are being finalized on this pull request 1. It includes Gauntlet’s recommended risk parameters (aggressive option) and latest recommendations for the WETH market 2 for the following assets:\n\ncbETH: $14.5M supply cap (7.5K tokens), 90% CF, 93% LCF, and 2.5% liquidation fee\n\nThe community can add more liquid staking derivatives as collateral to the market once other options get supported on Base.\nThe interest rate model for cWETHv3 is configured to match Gauntlet’s latest recommendations for the WETH market 2. After the launch of the market, Gauntlet will monitor and consider recommendations to the interest rate model based on preliminary utilization & growth.\nThe assets and price feeds of the deployment use the following inputs:\n\nThe base asset, WETH, uses a constant price feed 1 of unity in terms of ETH with 8 decimals.\nFor cbETH, a scaling price feed 2 is used to convert the Chainlink cbETH / ETH 1 price feed’s 18 decimals to the constant 8 decimals that Comet expects.\n\nOpenZeppelin has completed an additional audit 1 of the bridged governance contracts and flows. Note that the audit is for Optimism, which Base is a fork of.\nThe cWETHv3 market has been deployed to 0x46e6b214b524310239732D51387075E0e70970bf 1.\nFinally, see the Initialization Proposal section below for more information about the next steps to enable the deployed market.\nInitialization Proposal\nTo initialize the market, the deployment process is similar to the initialization of cWETHv3 on Ethereum mainnet, the primary difference being the bridging of governance actions to Base, instead of taking place directly on the governance chain (Ethereum mainnet).\nThe initialization proposal will take the following actions:\n\n\nSend a cross-chain message to Base, triggering a series of actions. These actions are to set the CometFactory for the new Comet, set the Comet configuration, deploy a new Comet implementation, set COMP as the reward token for the deployment, wrap some ETH to WETH, and transfer WETH to the cWETHv3 contract as initial reserves. The initial supply speed will be 20 COMP/day and borrow speed will be 0 COMP/day.\n\n\nBridge 10 ETH from the mainnet Timelock to the Base Timelock, to be wrapped as WETH and seeded as initial reserves to the cWETHv3 market.\n\n\nWrite the ENS TXT record v3-official-markets on v3-additional-grants.compound-community-licenses.eth containing the official markets JSON.\n\n\nReduce the COMP distribution to v2 cUSDC borrowers by 20 COMP/day, so as to keep the total COMP distribution constant.\n\n\nThe deployment and proposal migrations have been built using the Comet scenario framework and deployed using the Comet deployment manager. The scenario checks can be seen from the proposal branch CI checks 2.\\nAre there any plans to launch a native USDC comet on Base? This would better integrate with exchanges  e.g. Coinbase where it is only possible to withdraw native USDC to Base and not USDbC.\nA similar process was implemented with USDC on Arbitrum.  I highly suspect over the coming months and years the majority of USDC coming onto Base will be native and not bridged.\\nAgree, it would be great to see a native USDC Comet be spun up on Base. As you said, native USDC provides the best UX due to the ability to transfer it directly from Coinbase.\nLabs is not actively working on this, since we’re currently focused on some other priorities. This would be a great project for the rest of the community to work on (e.g. via the grants program)."
  },
  {
    "number_of_comments": 13,
    "postid": "d74a0c5a-6d7b-45a1-a61a-a471445a4b22",
    "posturl": "https://www.comp.xyz/t/borrow-limits-for-each-market-comptroller-patch/65",
    "combinedcontent": "With the addition of WBTC collateral, many people have been talking about putting up more safeguards to avoid total loss in an infimint situation. While without migrating WBTC to a new cToken, I don’t think a supply cap is possible, but blck and I thought it’d be beneficial if we implement borrow limits for each cToken to reduce potential losses.\nFor example, ETH is rarely borrowed, so why should it be possible to borrow the whole supply when the only possible situation for that happening will be a compromised collateral infimint?\nBorrow limits will be set by the admin or guardian and borrows will gracefully fail if the borrow amount brings total borrow above the set limit. The limits will be set high enough to enable normal usage but minimize loss of funds. I still believe that a supply cap is a much better solution and this restriction can be disabled if a supply cap is implemented in the future.\nI have submitted a PR 5 on GitHub with the code. A preliminary version is currently the Comptroller 4 on Kovan. It has a borrow limit of 10,000 * 1e18 underlying for all assets. Currently, I am working on writing tests for the new patch.\nThis is a preliminary idea for starting borrow limits. These numbers are mostly random to help start discussion on what they should be. Setting the borrow limits will be successive calls in the same governance proposal after setting the new comptroller.\nThings to discuss: what should the initial borrow limits be for each asset, does this comptroller patch need to be professionally audited, do we like this idea, should the guardian have full reign over the borrow limit?\n922×212 14 KB\nPrevious discussion on comradery 5\\nIt’s nice to see borrowing caps; these seem logical, and could help standardize assets as collateral (e.g. ETH, BAT, ZRX) with low borrowing caps, and stablecoins with very large borrowing caps.\nA few questions:\n\nDoes this need a separate guardian? If it’s an “admin” function, it can be controlled through COMP Governance naturally. Would a 5 day proposal period be inadequate, or risky?\nWhat is the impact on gas costs of each type of transaction, to add these checks?\nCan you describe the logic, and how it works? Do transactions revert, if a transaction would exceed the cap?\n\nOn this thread, let’s focus on the development of the patch, and save the calibration of limits for future Proposals 2.\nThanks for getting this started - this is great!\\nIf this is implemented, it may also make sense to adjust the relevant interest rate models so rates are relatively high when borrowing value hits the cap.\\n\n\nDoes this need a separate guardian? If it’s an “admin” function, it can be controlled through COMP Governance naturally. Would a 5 day proposal period be inadequate, or risky?\n\n\nYes, controlling the borrow limits will definitely need to be controller by the pause guardian or a different guardian. Without borrowing ability, many liquidators may have issues liquidating larger positions. Additionally, we don’t want to be getting in the way of normal usage or clogging up governance proposals with these minor changes.\nWe were thinking of a guardian with ability of changing borrow limits being a community multisig like you recommended for the pause guardian. It would make sense for this to be a different guardian, as it will be used in somewhat normal situations.\n\n\nWhat is the impact on gas costs of each type of transaction, to add these checks?\n\n\nHaven’t looked into this yet.\n\n\nCan you describe the logic, and how it works? Do transactions revert, if a transaction would exceed the cap?\n\n\nI tried to make the implementation for the new feature match existing compound code.\nThe additional logic is placed in the borrowAllowed function in the Comptroller. If the market has a borrowLimit set which is greater than 0, it does a safe addition of the new borrow amount with existing total borrows. If the result is greater than the borrow limit, the borrow function fails gracefully, returning an error code.\nI return an error code because for almost all existing compound failures, error code are returned instead of reverting. The only time I see functions reverting in compound is authentication and when markets are paused. Additionally, returning an error code allows for the accureInterest to stay on chain.\\nI disagree. The idea of the new borrow limit is to restrict borrowing to expected amounts for normal usage. This is just extra security against an infimint situation and will potentially save hundreds of millions in funds.\nIf a market gets to the cap through normal usage, it should be raised to avoid obstructing normal usage of the protocol.\\nThis makes sense. The governance overhead of updating debt ceilings is not insignificant, but probably simpler with Compound’s proposal flow vs Maker’s executive votes.\\nI think that having changes to borrow limits all go through governance would be very tedious and annoying. That’s why I propose having a community multisig borrow limit guardian.\\nNot a bad idea. Is there a way for COMP holders to elect the signors? Gotta keep walking the path of progressive decentralization.\\nWhat would be cool to see is a Compound community board that manages the pause guardian and borrow limits. Would need to create another contract to allow for elections though.\nI think this is a very much needed aspect to the protocol. Pause guardian and borrow limits can’t wait for the normal process to finish.\\nA “community board” is a great idea; it could be an address associated with Pause Guardian and Borrow Limit Guardian responsibilities.\nGovernance sets this for the Pause Guardian, and could for the Borrow Limit Guardian as well.\nWhether folks want to volunteer, campaign, etc - the process to select a group should be relatively easy–which could then be put forth as a Proposal.\\nI committed a new change to my patch which adds a separate Borrow Limit Guardian. It would be nice if you could have our community board setup in time for this proposal. I’d love to volunteer. Maybe that should be a separate posting?\\nI just finished building up the tests for the new comptroller. Please review the changes.\n\n  \n      github.com/compound-finance/compound-protocol\n  \n  \n    \n  \n    \n  \n\n  \n    \n      Add borrow limit for each market 11\n    \n\n    \n      compound-finance:master ← arr00:master\n    \n\n    \n      \n        opened \n        \n          \n        \n        Jul 14, 2020\n      \n      \n\n      \n        \n          \n          arr00\n        \n      \n\n      \n        \n          +1774\n          -5\n        \n      \n    \n\n  \n\n\n  \n  \n    \n    \n  \n  \n\n\\nI’d love something akin to:\n\nCaps on Borrow / Supply (Quite generous)\nRate of change caps (most added per 24 hr etc)\nExpansion allowance (when at cap, this amount can be added per 24 hrs)\n\nI don’t think pause guardian fanciness is needed; you’ve got my vote for any meaningful risk reduction improvement that sets the borrow limits high on USDC, DAI, and ETH (perhaps 1B each is fine to start)\\n\n\n\n rad:\n\nCaps on Borrow / Supply (Quite generous)\n\n\nPersonally, I would prefer a supply cap over all everything, but in the current state of Compound, I don’t believe it is not possible to add one for most assets. Any method of tracking supply can be undermined for v1 cTokens. We would need to migrate to new v2 cTokens which would be a big deal. I don’t think this will be happening soon.\n\n\n\n rad:\n\nRate of change caps (most added per 24 hr etc)\n\n\nTwo main reasons not to do this from my point of view:\n\nIncreased gas costs\nThere are situations where the demand for assets in the market spikes overnight (ZRX price increase for example).\n"
  },
  {
    "number_of_comments": 16,
    "postid": "1e7c2411-ecd4-49e0-825c-8eebffd9d847",
    "posturl": "https://www.comp.xyz/t/community-multisig-4-of-6-deployment/134",
    "combinedcontent": "This post references this Discord message 40 posted in the #governance channel of the Compound Protocol Discord 19, and refers to Discord usernames from that channel.\n\nIn a community-driven effort with stewardship from @arr00, a Community Multisig 117 has been deployed that can, through the governance process, be voted into usage by the protocol for a variety of purposes: for example, to act as Pause Guardian ; or in the future, to act as Borrow Limit Guardian .\nSix community members have been proposed as signers in a 4-of-6 multisig:\n1 - @aaaaaaaaaaaaa: 0x57ded091cea8ffb590d6d72ba64a816bfc3521ff\n2 - @arr00: 0x2b384212edc04ae8bb41738d05ba20e33277bf33\n3 - @blck: 0x54a37d93e57c5da659f508069cf65a381b61e189\n4 - Compound Labs: 0x8b8592e9570e96166336603a1b4bd1e8db20fa20\n5 - Dharma (@0age) : 0x7e4a8391c728fed9069b2962699ab416628b19fa\n6 - @Jared F.: 0xF515DCb89e67bb5D52b857d11f6C0cC2aD7D0167\n@arr00 has already created a multisig at 0xbbf3f1421D886E9b2c5D716B5192aC998af2012c 117 using Gnosis Safe. @arr00 and @blck’s participation and addresses have already been confirmed, and been added as signers to that multisig.\nThe remaining proposed signers have been asked to confirm their addresses and participation in Discord. Following those confirmations, @arr00 will add all cofirmed addresses as signers to the multisig.\nSubsequently, governance may choose to vote in the multisig for usage in the protocol.\\nAs an update, you can see a list of current owners/signers on the gnosis safe multisig here: https://gnosis-safe.io/app/#/safes/0xbbf3f1421D886E9b2c5D716B5192aC998af2012c/settings 81\nSo far, Compound Labs, Dharma, @arr00 , and @blck 's public keys have been added as owners/signers. We await addition of @aaaaaaaaaaaaa and @Jared F. , and then the Community Multisig will be ready for use as desired.\\nUpdate: all addresses have been added and the Community Multisig is ready for use.\\n@aaaaaaaaaaaaa has nominated TennisBowling to take his place in the multisig.\n“Hey everyone. I was previously voted in to the compound multisig, see this post for details Community Multisig (4-of-6) Deployment\nI now don’t have enough time to dedicate to this, so I am nominating @1⃣:sunrise: to take my place”\\nWhy not @wario?\nHe is an active community member and is always available in discord.\\nWhile I agree that wario has been involved and active recently, he has only been active for the past two weeks. Tennis has been active on the Compound discord for the past year. I really do hope that wario remains active in the community and can take on future roles!\\nThanks @arr00!\nCompletely agreed, and I would add only involved in a pretty narrow topic. I hope to learn more about the protocol and its governance in the coming year!\\nTennisBowling (0xc3aae58ab81663872dd36d73613eb295b167f546) has replaced aaaaaaaaaaaaa in the multisig.\\nRegarding Compound proposal 57 16, can all of the multisig holders please confirm they are comfortable accepting the additional responsibility? I’ll vote for it, but I want to make sure we aren’t thrusting this additional responsibility if it is unwanted by anyone.\n@TennisBowling\n@arr00\n@blck\n@jared\n@brendan_dharma\nCompound Labs\\nI’m fine with it. It was discussed a long time ago, while the multisig was being created.\\nits ok for me.\n[extra chars]\\nI’m good with it.\n[extra chars]\\nWe are comfortable with it and appreciate the community’s entrusting us with this responsibility.\\nI accept\n[extra chars]\\nCompound Labs (0x8b8592e9570e96166336603a1b4bd1e8db20fa20) has stepped down from the multisig and they will be replaced by Gauntlet. Gauntlet will be using a 2/10 multisig at the address 0x683a4F9915D6216f73d6Df50151725036bD26C02 to participate in the community multisig.\\nNoting here for clarity. The present Community Multisig members are as follows:\nPaul L. (Gauntlet): 0xDD659911EcBD4458db07Ee7cDdeC79bf8F859AbC\n0age (OpenSea): 0x7e4A8391C728fEd9069B2962699AB416628B19Fa\narr00: 0x2B384212EDc04Ae8bB41738D05BA20E33277bf33\nblck: 0x54A37d93E57c5DA659F508069Cf65A381b61E189\nJared F.: 0xF515DCb89e67bb5D52b857d11f6C0cC2aD7D0167\nTennisBowling: 0xC3AaE58Ab81663872dd36d73613eb295b167F546\\nJared will be stepping down from the multisig and replaced by Compound Labs (0xd04e545c665741cd99d1B9D9CD96396F0f8166FB). Additionally, OpenZeppelin (0x2C96F0403eabC1F11FE737566dEDB183A019763B) will be added as a signer. The new signer set will be as follows:\nPaul L. (Gauntlet): 0xDD659911EcBD4458db07Ee7cDdeC79bf8F859AbC\n0age (OpenSea): 0x7e4A8391C728fEd9069B2962699AB416628B19Fa\narr00: 0x2B384212EDc04Ae8bB41738D05BA20E33277bf33\nblck: 0x54A37d93E57c5DA659F508069Cf65A381b61E189\nTennisBowling: 0xC3AaE58Ab81663872dd36d73613eb295b167F546\nCompound Labs: 0xd04e545c665741cd99d1B9D9CD96396F0f8166FB\nOpenZeppelin: 0x2C96F0403eabC1F11FE737566dEDB183A019763B"
  },
  {
    "number_of_comments": 10,
    "postid": "ef05f3ac-c32a-4bc5-aeeb-2e09a55f2a08",
    "posturl": "https://www.comp.xyz/t/gauntlet-monthly-updates/4038",
    "combinedcontent": "\nJanuary 2023\nSummary\nSharing a recap of Gauntlet’s market risk management work with Compound in January 2023. See below for this past month’s work summary and a preview of what’s coming up in February.\n\n\n\n\nCategory\nWork Completed\nCommmentary\n\n\n\n\nQuantitative Research\n\nInterest rate curve framework for Compound 11 \n\nOver the last several months, Gauntlet has conducted quantitative research on optimizing interest rate curves and applied to new Comet markets\n\n\nRecommendations\n\n2 1 sets 4 of Compound V2 Recommendations  2 1 sets 5 of Compound V3 USDC Comet Recommendations   1 1 set of Compound V3 wETH Comet Recommendation\nDaily risk simulations and stress tests\n\n\nRisk Modeling\n\nInitial parameter recommendations 4 on Compound V3 wETH Comet, including supply cap, CF, LCF, LF, and Interest Rate Curves    Conduct analysis and provide strategies for the community’s 4 Migration plan from V2 to V3 \n\nRisk modeling to provide insight into new market launches for Compound as well as protocol-wide initiatives\n\n\nOther\nPresented risk updates on 2 community calls  \n\nCommunity and contributor support\n\n\n\n\nHighlights\n\nProvide initial risk parameterization analysis for the launch of Compound V3 wETH Comet\nConduct quantitative research on interest rate curves, and provide interest rate curve parameterization for Compound V3 wETH Comet\nConduct analyses to provide migration plans for the community’s transition from V2 to V3\nOngoing daily simulation analyses and stress tests of Compound V2 and Compound V3 USDC Comet, including several collateral factor decreases for Compound V2 to mitigate outsized market risk\n\nUpcoming Selected Work\n\nConduct market risk analysis to support Compound’s launches on new chains such as Polygon\nPublish detailed migration plan from V2 to V3 following the community’s preference 3\n\nLaunch wETH Comet Risk Simulation Dashboard\n\nHappy to answer any questions or hear any feedback - feel free to comment below.\\nAppreciate this monthly report. Helps community understand what Gauntlet has been up to. Keep them coming!\\n\nFebruary 2023\nSummary\nSharing a recap of Gauntlet’s market risk management work with Compound in February 2023. See below for this past month’s work summary and a preview of what’s coming up.\n\n\n\n\nCategory\nWork Completed\nCommmentary\n\n\n\n\nQuantitative Research\n\nInitial recommendations for USDC Comet on Polygon market 5 \n\nGauntlet has conducted quantitative research on risk management for non-ETH chains and has provided initial parameter recommendations for the USDC Comet on Polygon to support Compound’s multi-chain strategy.\n\n\nRecommendations\n\n3 sets 1 of Compound V3 wETH Comet Recommendations  2 1 sets 1 of Compound V2 Recommendations   2 sets of Compound V3 USDC Comet Recommendations\nDaily risk simulations and stress tests\n\n\nRisk Modeling\n\nInitial parameter recommendations 5 on Compound V3 USDC Comet on Polygon, including supply cap, CF, LCF, LF, and Interest Rate Curves    Phase 1 of Compound Migration Plan from V2 to V3 1 - after detailed research, and after 3 different sets of recommendation options provided to the community following feedback, Gauntlet has published a proposal for migrating users from V2 to V3. The proposal has been approved and passed by the community.  \n\nRisk modeling to provide insight into new market launches for Compound as well as protocol-wide initiatives\n\n\nOther\nPresented risk updates on 2 community calls  \n\nCommunity and contributor support\n\n\n\n\nHighlights\n\nPublished updated detailed migration plan from V2 to V3 following the community’s preference. The migration plan includes parameter changes on V2 as well as V3 USDC Comet.\nProvide initial risk parameterization analysis for the launch of Compound V3 USDC Comet on Polygon to support Compound’s multi-chain strategy.\nOngoing daily simulation analyses and stress tests of Compound V2, Compound V3 USDC Comet, and Compound V3 wETH Comet.\nProvide supply cap updates on V3 wETH Comet to enable sustainable growth while mitigating outsized market risk.\n\nUpcoming Selected Work\n\nConduct market risk analysis to support Compound’s launch on Optimism.\nLaunch wETH Comet Risk Simulation Dashboard.\n\nWe are happy to answer any questions or hear any feedback from the community.\\n\nMarch 2023\nSummary\nSharing a recap of Gauntlet’s market risk management work with Compound in March 2023. See below for this past month’s work summary and a preview of what’s coming up.\n\n\n\n\nCategory\nWork Completed\nCommmentary\n\n\n\n\nQuantitative Research\n\nContinued research on V3 Migration 4   Parameter recommendations 2 for Compound V3 USDC Comet on Optimism\nGauntlet has conducted quantitative research on risk management for non-ETH chains and has provided initial parameter recommendations for the USDC Comet on both Polygon and Optimism to support Compound’s multi-chain strategy.\n\n\nRecommendations\n\nCompound V3 wETH recommendations 2  Compound V2 recommendations to adjust CF and deprecate legacy assets \n\nDaily risk simulations and stress tests\n\n\nRisk Modeling\n\nUSDC volatility firefighting 1, including risk modeling on USDC volatility’s impact on all Compound markets and risk mitigation recommendations  Initial parameter recommendations 2 on Compound V3 USDC Comet on Optimism, including supply cap, CF, LCF, LF, and Interest Rate Curves    Phase 2 of Compound Migration Plan from V2 to V3 4 - after the community has aligned on their preference 2, Gauntlet has published a proposal for migrating users from V2 to V3. The proposal has been approved and passed by the community.  \n\nRisk modeling to provide insight into new market launches for Compound as well as protocol-wide initiatives\n\n\nFeature Upgrades\nLaunched wETH Comet risk dashboard 2  Released version 2 of Gauntlet’s risk dashboard 4 in Beta including updated analytics  Daily simulation and risk modeling of USDC Comet on Polygon (dashboard to release soon)\n\nUpgrades to Gauntlet’s product suite\n\n\nRisk Alerts\nContinued market updates 1 from our alerting platform  \n\nRisk monitoring and alerting is part of Gauntlet’s market risk management offfering\n\n\n\n\nHighlights\n\nFirefighting when USDC lost its equivalence to the dollar. Provided risk modeling 1 to inform risk mitigation recommendations and triggers while communicating closely with Compound Guardian and community. Importantly, the Compound protocol achieved 0 new insolvencies during this unique tail market event.\nReleased version 2 of Gauntlet’s risk dashboard 4 in Beta, including updated analytics. We will continue to iterate on the dashboards to support Compound’s cross-chain strategy.\nPublished updated detailed migration plan phase 2 from V2 to V3 following the community’s preference 2. The migration plan includes parameter changes on V2 as well as V3 USDC Comet.\nProvide initial risk parameterization analysis for the launch of Compound V3 USDC Comet on Optimism to support Compound’s multi-chain strategy.\nOngoing daily simulation analyses and stress tests of Compound V2, Compound V3 USDC Comet on ETH, Compound V3 wETH Comet, and Compound V3 USDC Comet on Polygon.\n\nUpcoming Selected Work\n\nConduct market risk analysis to support Compound’s launch on Arbitrum.\nLaunch Risk Dashboard for Compound V3 USDC Comet on Polygon with both simulation and cross-chain analytics.\n\nWe are happy to answer any questions or hear any feedback from the community.\\n\nApril 2023\nSummary\nSharing a recap of Gauntlet’s market risk management work with Compound in April 2023. See below for this past month’s work summary and a preview of what’s coming up.\n\n\n\n\nCategory\nWork Completed\nCommmentary\n\n\n\n\nQuantitative Research\nImproved Value at Risk 4 methodology after months of R&D  Parameter recommendations 3 for Compound V3 USDC Comet on Arbitrum\nGauntlet has conducted research on improving Value at Risk for more transparency, accuracy, and consistency\n\n\nRecommendations\n\nPolygon USDC Comet Incentives & Risk Parameter recommendations 3 Implemented Phase 2 of Compound Migration 3 \n\nDaily risk simulations and stress tests\n\n\nRisk Modeling\n\nApril 7th Migration Update  April 14th Migration Update  April 21st Migration Update 1  wETH Comet cbETH Update \n\nRisk modeling to provide insight into new market launches for Compound as well as protocol-wide initiatives\n\n\nFeature Upgrades\nLaunched Discord Market Alerts from Gauntlet 1  Launched version 2 of Gauntlet’s risk dashboard including updated cross-chain analytics \n\n\n\n\nRisk Alerts\nContinued updates 1 from our alerting platform  \n\nRisk monitoring and alerting is part of Gauntlet’s market risk management offfering\n\n\n\n\nHighlights\n\nProvide incentive optimization and risk parameter recommendations for USDC Comet on Polygon\nAfter months of research and development, released updated Value at Risk (VaR) methodology\nProvide continuous updates on Compound V2 → V3 migration\nProvide initial parameter recommendations on Compound Comet launch on Arbitrum\nLaunched Version 2 of Gauntlet’s risk dashboard across all Compound markets and Comets\n\nUpcoming Selected Work\n\nOn-chain proposal for Polygon incentives and risk parameter changes\nPhase 3 of migration\nRisk modeling for upcoming Arbitrum launch\n\nWe are happy to answer any questions or hear any feedback from the community.\\n\nMay 2023\nSummary\nSharing a recap of Gauntlet’s market risk management work with Compound in May 2023. See below for this past month’s work summary and a preview of what’s coming up.\n\n\n\n\nCategory\nWork Completed\nCommmentary\n\n\n\n\nQuantitative Research\nExpanded Gauntlet’s continuous Incentive Optimization  3 for Compound’s incentive rewards \n\nIncentives are used to bootstrap liquidity, promote growth, and facilitate decentralized governance growth.\n\n\nRecommendations\nImplemented Risk Parameter Updates for Polygon Compound v3 USDC 1   Implemented IR Curve Recommendations for Compound v3 USDC 2 \n\nDaily risk simulations and stress tests.\n\n\nRisk Modeling and Incentive Optimization\n\nPolygon Compound v3 USDC Update 1  Arbitrum Compound v3 USDC Update 1  Migration Update \n\nRisk modeling to provide insight into new market launches for Compound as well as protocol-wide initiatives. Data-driven incentive optimization to help bootstrap liquidity, promote growth and facilitate decentralized governance growth.\n\n\nFeature Upgrades\nCap Usage Graphs (example) 1 for borrow and supply caps where applicable  Dashboard usability improvements, including full screen graphs, direct links to graphs \n\nGauntlet continuously upgrades its product offering.\n\n\nRisk Alerts\nContinued updates from our alerting platform  \n\nRisk monitoring and alerting is part of Gauntlet’s market risk management offfering.\n\n\n\n\nHighlights\n\nGauntlet’s Polygon Compound v3 USDC proposal 1 on I/O and supply caps increases resulted in a growth of 222% in USDC supply and 163% in USDC borrow.\nExpanded Gauntlet’s continuous Incentive Optimization  3 for Compound’s incentive rewards\nImplemented IR Curve Recommendations for Compound v3 USDC 2\n\nProvided continuous updates on Compound V2 → V3 migration\nImproved UX of Compound Dashboard\n\n\nUpcoming Selected Work\n\nConduct market risk analysis and incentive optimization to support Compound’s launches on new chains such as Base\nParameter recommendations for MaticX and stMATIC\n\n\nWe are happy to answer any questions or hear any feedback from the community.\\nJune 2023\nSummary\nSharing a recap of Gauntlet’s market risk management work with Compound in June 2023. See below for this past month’s work summary and a preview of what’s coming up in July.\n\n\n\n\nCategory\nWork Completed\nCommmentary\n\n\n\n\nQuantitative Research\nContinuous research on Incentive Optimization  1 for Compound’s incentive rewards \nIncentives are used to bootstrap liquidity, promote growth, and facilitate decentralized governance growth.\n\n\nRecommendations\nExecuted Phase 4 of the Migration proposal  Implemented interest rate curve and incentive changes on Compound v3 USDC  Proposal to pause supply for cAAVE, cCOMP, cLINK, cSUSHI, and cUNI on Compound V2 1  Proposal to increase UNI supply cap on V3 Ethereum USDC  Updates on ARB supply cap on Arbitrum USDC.e  Parameter Recommendations for stMATIC and MatixX \nDaily risk simulations and stress tests.\n\n\nRisk Modeling and Incentive Optimization\nCompound Incentive and Revenue Analytics   Migration Update  Topped up COMP Rewards on Compound V3 Mainnet Contract\nRisk modeling to provide insight into new market launches for Compound as well as protocol-wide initiatives. Data-driven incentive optimization to help bootstrap liquidity, promote growth and facilitate decentralized governance growth.\n\n\nFeature Upgrades\nAdded Correlated Liquidation Curves 1 for V2 assets on the Gauntlet Compound Dashboard 2  \nGauntlet continuously upgrades its product offering.\n\n\nRisk Alerts\nContinued updates from our alerting platform  \nRisk monitoring and alerting is part of Gauntlet’s market risk management offfering.\n\n\n\n\n\n\n\n\nHighlights\n\nExecuted phase 4 of the migration proposal.\nImplemented interest rate curve and incentive changes on Compound v3 USDC.\nPosted proposal and poll to pause supply for tail assets on Compound v2 1.\nRecord high inflow and borrow on Compound V3 in June, one month since the formal kickoff of Gauntlet’s Incentive Optimization  1efforts.\nImproved UX of Compound Dashboard 2.\n\nUpcoming Selected Work\n\nProvide incentive recommendations to support Compound’s launches on new chains such as Base.\nContinuous monitoring of utilization levels and supply cap usage across all Compound comets.\nContinue support of community initiatives and market alerts.\n\nWe are happy to answer any questions or hear any feedback from the community.\\nJuly 2023\nSummary\nSharing a recap of Gauntlet’s market risk management work with Compound in July 2023. See below for this past month’s work summary and a preview of what’s coming up in August.\nHighlights\n\nPaused supply for tail assets on Compound v2\nConducted market risk analysis on the mechanism design of the WETH comet 1\nProposed v2 Deprecation Strategy 1\nOn-chain proposal for Arbitrum USDC rewards changes passed\nInitial parameter recommendations for Compound V3 USDC and WETH Comets on Base\nPublished weekly market updates for all Compound comets: Ethereum USDC, Polygon USDC, Ethereum WETH, Arbitrum USDC\n\n\n\n\n\nCategory\nWork Completed\nCommmentary\n\n\n\n\nQuantitative Research\nMechanism Design Analysis on WETH Comet 1  Continuous research on Incentive Optimization 1 for Compound’s incentive rewards \nIncentives are used to bootstrap liquidity, promote growth, and facilitate decentralized governance growth.\n\n\nRecommendations\nExecuted pausing supply for cAAVE, cCOMP, cLINK, cSUSHI, and cUNI on Compound V2Implemented parameter changes to WETH, WBTC, UNI, LINK on Ethereum Compound V3 USDCOn-chain proposal for Arbitrum USDC recommendations 1Polygon USDC Reward Recommendations Proposed v2 Deprecation Strategy 1  On-chain proposal for risk parameter, IR curve, and Incentive change recommendations for Ethereum Compound v3 ETH 1\nDaily risk simulations and stress tests.\n\n\nRisk Modeling and Incentive Optimization\nInitial parameter recommendations for Compound V3 USDC and WETH Comets on Base Initial parameter recommendations for rETH on Ethereum Compound v3 WETH  Weekly market updates for Ethereum USDC, Polygon USDC, Ethereum WETH, Arbitrum USDC \nRisk modeling to provide insight into new market launches for Compound as well as protocol-wide initiatives. Data-driven incentive optimization to help bootstrap liquidity, promote growth and facilitate decentralized governance growth.\n\n\nFeature Upgrades\nImproved data freshness of the Gauntlet platform and alerts system. \nGauntlet continuously upgrades its product offering.\n\n\nRisk Alerts\nContinued updates from our alerting platform  \nRisk monitoring and alerting is part of Gauntlet’s market risk management offfering.\n\n\n\nUpcoming Selected Work\n\nGather community feedback and preference on v2 Deprecation Strategy 1\nRevisit Polygon USDC comet recommendations after the addition of MaticX.\nContinuous monitoring of utilization levels and supply cap usage across all Compound comets\nContinue support of community initiatives and market alerts\n\nWe are happy to answer any questions or hear any feedback from the community.\\nAugust 2023\nSummary\nSharing a recap of Gauntlet’s market risk management work with Compound in August 2023. See below for this past month’s work summary and a preview of what’s coming up in September.\nHighlights\n\nAligned community preference on the Compound V2 deprecation strategy 1 and published on-chain proposal for V2 USDC Deprecation (Phase 1)\nPosted Gauntlet <> Compound Renewal 2023 Proposal\nProposed Arbitrum v3 native USDC Reward Recommendations 1\nExecuted Ethereum WETH risk parameter, IR curve, and incentive updates\nPublished weekly market updates for  Ethereum USDC, Polygon USDC, Ethereum WETH, Arbitrum USDC\n\n\n\n\n\nCategory\nWork Completed\nCommmentary\n\n\n\n\nQuantitative Research\nAnalysis on WBTC price feed for Ethereum USDC Market 1  Continuous research on Incentive Optimization for Compound’s incentive rewards \nIncentives are used to bootstrap liquidity, promote growth, and facilitate decentralized governance growth.\n\n\nRecommendations\nAligned community preference on the Compound V2 deprecation strategy 1 and passed on-chain proposal for V2 USDC Deprecation (Phase 1). Proposed Arbitrum V3 native USDC Reward Recommendations 1  Executed Polygon USDC risk parameter updates  Executed Ethereum WETH risk parameter, IR curve, and incentive updates  Executed Arbitrum USDC rewards updates\nDaily risk simulations and stress tests.\n\n\nRisk Modeling and Incentive Optimization\nWeekly market updates for Ethereum USDC, Ethereum WETH, Polygon USDC, Arbitrum USDC. \nRisk modeling to provide insight into new market launches for Compound as well as protocol-wide initiatives. Data-driven incentive optimization to help bootstrap liquidity, promote growth and facilitate decentralized governance growth.\n\n\nFeature Upgrades\nAdded a new feature displaying interest rate curves on the recommendations page of the Gauntlet Compound dashboard, showing the expected impact of the recommendations. Note: these will only appear when a recommendation is live.  Added filters to all charts on the Gauntlet Compound dashboard, enabling users to select one or multiple assets to be represented in a clean, focused view.\nGauntlet continuously upgrades its product offering.\n\n\nRisk Alerts\nContinued updates from our alerting platform  \nRisk monitoring and alerting is part of Gauntlet’s market risk management offfering.\n\n\n\nUpcoming Selected Work\n\nProceed with the V2 Deprecation Strategy 1 following the execution of phase 1 of the V2 USDC deprecation\nOn-chain proposal for rewards recommendations for the Arbitrum native USDC comet 1\nContinuous monitoring of utilization levels and supply cap usage across all Compound comets\nContinue support of community initiatives and market alerts\n\nWe are happy to answer any questions or hear any feedback from the community.\\nSeptember 2023\nSummary\nSharing a recap of Gauntlet’s market risk management work with Compound in September 2023. See below for this past month’s work summary and a preview of what’s coming up in October.\nHighlights\n\nExecuted V2 USDC Deprecation (Phase 1) and V2 Deprecation (Phase 2)\nExecuted Gauntlet <> Compound Renewal 2023\nExecuted Arbitrum v3 USDC reward increase\nPublished weekly market updates for all Compound comets: Arbitrum USDC, Arbitrum Native USDC, Polygon USDC, Ethereum USDC, Ethereum ETH, Base USDbC, Base WETH\n\n\n\n\n\nCategory\nWork Completed\nCommmentary\n\n\n\n\nQuantitative Research\nContinuous research on Incentive Optimization for Compound’s incentive rewards \nIncentives are used to bootstrap liquidity, promote growth, and facilitate decentralized governance growth.\n\n\nRecommendations\nExecuted V2 USDC Deprecation (Phase 1) and V2 Deprecation (Phase 2) Executed Arbitrum v3 USDC reward increaseOn-chain proposal for increasing WBTC supply cap on Arbitrum Native USDC comet\nDaily risk simulations and stress tests.\n\n\nRisk Modeling and Incentive Optimization\nWeekly market updates for Arbitrum USDC, Arbitrum Native USDC, Polygon USDC, Ethereum USDC, Ethereum ETH, Base USDbC, Base WETH  \nRisk modeling to provide insight into new market launches for Compound as well as protocol-wide initiatives. Data-driven incentive optimization to help bootstrap liquidity, promote growth and facilitate decentralized governance growth.\n\n\nFeature Upgrades\nLaunched Compound V3 Base Dashboard \nGauntlet continuously upgrades its product offering.\n\n\nRisk Alerts\nContinued updates from our alerting platform  \nRisk monitoring and alerting is part of Gauntlet’s market risk management offfering.\n\n\n\nUpcoming Selected Work\n\nProceed with phase 3 of the Compound V2 deprecation strategy\nRevisit Polygon USDC rewards recommendations\nContinuous monitoring of utilization levels and supply cap usage across all Compound comets\nContinue support of community initiatives and market alerts\n\nWe are happy to answer any questions or hear any feedback from the community.\\nOctober 2023\nSummary\nSharing a recap of Gauntlet’s market risk management work with Compound in October 2023. See below for this past month’s work summary and a preview of what’s coming up in November.\nHighlights\n\nExecuted V2 Deprecation (Phase 3) and V2 Deprecation (Phase 4) 2 is on-chain\nLaunched the Compound <> BASE Comet Incentives Program\nETHx initial parameter recommendations on Ethereum V3 WETH Comet\nPublished weekly market updates for all Compound comets: Arbitrum USDC, Arbitrum Native USDC, Polygon USDC, Ethereum USDC, Ethereum ETH, BASE USDbC, BASE WETH\n\n\n\n\n\nCategory\nWork Completed\nCommentary\n\n\n\n\nQuantitative Research\nUpdated USDC - WBTC Oracle Analysis on V3 Ethereum  1Continuous research on Incentive Optimization for Compound’s incentive rewards \nIncentives are used to bootstrap liquidity, promote growth, and facilitate decentralized governance growth.\n\n\nRecommendations\nExecuted V2 Deprecation (Phase 3) Posted V2 Deprecation (Phase 4) and the proposal is on-chain 2Polygon v3 USDC - MaticX Supply Cap Update (10/20/23)Polygon USDC - Rewards Recommendations Executed on-chain proposal for increasing WBTC supply cap on Arbitrum Native USDC cometExecuted WBTC supply caps recommendations on V3 ArbitrumETHx initial parameter recommendations on Ethereum V3 WETH CometBase v3 WETH interest rate curve recommendations\nDaily risk simulations and stress tests.\n\n\nRisk Modeling and Incentive Optimization\nLaunched the Compound <> BASE Comet Incentives ProgramTop-up COMP Rewards on Compound V3 mainnet contract Published weekly market updates for all Compound comets: Arbitrum USDC, Arbitrum Native USDC, Polygon USDC, Ethereum USDC, Ethereum ETH, BASE USDbC, BASE WETH  \nRisk modeling to provide insight into new market launches for Compound as well as protocol-wide initiatives. Data-driven incentive optimization to help bootstrap liquidity, promote growth and facilitate decentralized governance growth.\n\n\nFeature Upgrades\nLaunched the Updated Compound Account Explorer feature on Gauntlet’s Compound Risk Dashboard to surface account level data for all markets on our dashboard. The feature includes visualizations and a table with key datapoints about the accounts. Interactive filters and a search bar allow anyone to do their own analysis of positions on the protocol. \nGauntlet continuously upgrades its product offering.\n\n\nRisk Alerts\nContinued updates from our alerting platform  \nRisk monitoring and alerting is part of Gauntlet’s market risk management offering.\n\n\n\nUpcoming Selected Work\n\nExecute phase 4 of the Compound V2 deprecation 2\nOn-chain proposal for Base v3 WETH interest rate curve recommendations\nContinuous monitoring of utilization levels and supply cap usage across all Compound comets\nContinue support of community initiatives and market alerts\n\nWe are happy to answer any questions or hear any feedback from the community."
  },
  {
    "number_of_comments": 15,
    "postid": "ac839f5d-f811-4609-a4ff-1ace47bbc65f",
    "posturl": "https://www.comp.xyz/t/setup-community-cuni-voting/440",
    "combinedcontent": "Assuming the success of proposal 25 31 we will soon have a significant amount of UNI locked up un the cUNI contract. We have the ability as a community to delegate this UNI to any address.\nThis will be the best UNI governance experience for the average user:\n\nEarn interest on UNI deposits\nBorrow against your UNI\nVote for free\nHelp the community gather enough votes to propose\n\nI propose delegating to the community multisig 32 (0xbbf3f1421D886E9b2c5D716B5192aC998af2012c) and having off chain voting to decide how to vote as a community.\nI have setup a snapshot space for cUNI at http://cuni.comp.vote 81. Shortly after every Uniswap proposal, we will setup voting where each address has votes according to their cUNI holdings at the time of the proposal. Before the Uniswap voting deadline, the community multisig will vote according to the cUNI holders’ decision.\nOnce cUNI accrues enough UNI to create proposals, we will allow for community member with at least x (maybe 100k?) UNI deposited in cUNI to start a vote on a new proposal. If the vote has 2/3 for yes, the community multisig will propose the requested proposal. For now, the multisig will not be proposing.\\nAny update on this ? Because the first uni Proposal is live at the moment.\\nUnfortunately, we’ve missed the boat on voting for the first proposal, but in the coming weeks, I hope to see this done. We will be able to participate in proposals starting after we delegate to the multisig.\\nI think this imposes some questions about the possible fee restructuring of Uniswap. With the V2 platform, the UNI governance might make the decision to redirect 0.05% to LPs. This means that the Compound contract might at some point start yielding fees from the sole ownership of UNI tokens. This would open new interesting ways to accrue funds for the Compound project. Nevertheless, it also opens some questions:\na) if I mint cUNI, would my ownership in proportion to non-used capital accrue the LP fees?\nThis is a relevant question since it might incentivize or disincentivize people to provide liquidity if they miss out on the 5.25% APY that would be produced by the following napkin math:Screen Shot 2020-10-14 at 2.57.27 PM1380×1286 291 KB\nIn other words, the yearly ROI on 400 UNI deposit would yield around 70 usd if kept within the account. Whether by depositing to Compound or not (and assuming the passive income on holding becomes true), you are essentially shorting UNI because you lose on the dividends in relation to what is the token utilization rate on Compound. If the whole APY from fees is lost, it would signal that only a very strongly opinionated shorters (and e.g., but not limited to, people who are very bullish on COMP) would be providing liquidity.\nI currently don’t have much more to say, but hope this aspect would be taken into concern in further reiterations of the cUNI token.\\nThis is a great point, and surprisingly, not the first time Compound has encountered a situation like this. A while back, MakerDAO introduced the DSR (DAI savings rate). Compound quickly updated cDAI to deposit market liquidity into DSR to make sure depositors don’t miss out on DSR.\nIf Uniswap starts disbursing fees to UNI holders, I’m sure that the community will update cUNI to distribute these fees to depositors if possible. A situation that could inhibit this would be requiring UNI to be locked in order to earn fees. I believe there is already a system setup for fee distribution, but I can’t look it up right now. Reading through that system would allow for a conclusive answer here.\\nThrough further thought on the topic and speaking to others in the field, I’ve decided that weighting the votes simply by cUNI held at the time of the proposal is not ideal. This would allow for any large proposer to easily buy the cUNI vote by entering a leveraged position on for a few blocks.\nThe solution I am proposing is checking cUNI balance about a week prior to the vote and at the time of the vote. The user votes gets weighted with the min of the two. Another idea I have for weighting is setting it to 0 if the address is borrowing any UNI. This way we have slightly more control in avoiding purchasing the vote by large UNI holders. Any more input on the method for weighting votes would be greatly appreciated.\\nI have updated the snapshot 8 strategy to invalidate any account borrowing cUNI for cUNI voting. Additionally, any new cUNI takes a week to vest for voting abilities. Beyond this, the cUNI voting weight for each account is simply their cUNI balance.\nBeyond this, I would like to set some grounds for the multisig Uniswap governance interactions if a proposal delegates to it.\n\nThe multisig will not vote based on cUNI snapshot voting unless there is a 10% quorum\nFor the time being, the multisig will not be proposing anything\nAny manipulation of the cUNI market in order to gain an oversized representation in the cUNI snapshot vote will not be honored.\n\\nHello @arr00\nI want to express some concerns I have regarding Uniswap governance processes with the introduction of this proposal.\nIf I understand correctly, cUNI voting is a binary situation, which means that all the cUNI tokens vote one way.\nIf you lend on Compound and lose the vote, your UNI tokens do the opposite of your will. It is much worse than if they did nothing.\nIt magnifies the whale dominance in Uniswap governance.\nThis system allows big UNI holders to hijack the UNI locked on Compound and leverage their voting power.\nMy estimates are that a whale could gain 2.5x boost on his/her UNI voting power with virtually no cost.\nHere’s how it’s calculated:\nThere are 15m UNI available to vote as cUNI. Bob adds 15m UNI to the fray and wins the vote, thus 2xing his voting power.\nAdditionally, Bob borrows DAI for 9m UNI (60% collateral) and lends it using another Lending protocol for 60% of UNI =  5.4m UNI. Bob repeats the process one more time and gets another 1.944m UNI.\nThis leaves us at 37.344m UNI gained from 15m UNI, i.e. ~2.5x boost.\n==\nI’ve pictured a scenario that is a little exaggerated, but it can easily be done on smaller, more subtle scale.\nThe important point here is that it can strongly feed into the whale power, or it can mildly feed into it.\nBut there’s no way it doesnt magnify the whale power, as the minority votes are turned against the minority with cUNI.\\nYour view is something I took very seriously along the way while setting up snapshot voting. Our strategy for calculating each score eliminates anyone borrowing UNI, which would mean they are attempting to get a leveraged voting position. Additionally, it takes a week for cUNI voting power to vest which should reduce the risk of votes getting hijacked by outside sources.\nWhen voting as a block, there is no way to fully eliminate the issues presented by it, but I believe what I implemented does help. If we see any manipulation, the multisig may not vote. This is an experement, which may work out well: hopefully we will see.\\nProposal 28 20 is live.\\nwhy was it canceled ?\\nThe CAP delegations dropped below 100k accidentally. It has been reproposed as proposal 29 10.\\nHey everyone!\nNow that proposal 29 has passed, cUNI can participate in Uniswap governance.\nI recently began a governance process for a new Uniswap liquidity incentive program: https://gov.uniswap.org/t/discussion-uniswap-liquidity-incentive-plan/8590 9\nI encourage cUNI depositors to check out the thread. If the initial snapshot poll process 5 is successful, we’ll likely move forward with a formal proposal in the next few weeks.\nAdditionally, cUNI holders and the Compound community should think about cUNI participation in Uniswap’s “soft governance 2” procedures, including snapshot polls. The policies in this thread aren’t 100% clear on how cUNI would engage with Uniswap snapshot polls.\\nIf the Uniswap community is interested, we could integrate cUNI scoring into the UNI snapshot polling.\\nWhere can we vote on the new uniswap proposal ?\\nGreat question! There is a vote live at https://cUNI.comp.vote 33."
  },
  {
    "number_of_comments": 10,
    "postid": "c7c4695c-7081-4387-b9fe-480ff7a93d04",
    "posturl": "https://www.comp.xyz/t/liquidity-mining-rewards-for-compound-liquidity-providers-on-bancor/1961",
    "combinedcontent": "Hi Compound Community,\nI created a proposal in the Bancor Governance forums that would add liquidity mining rewards on the COMP-BNT pool:\n\n  \n      \n\n      Bancor Governance Forum – 9 Jul 21\n  \n\n  \n    \n\nProposal: Activate LM Rewards for Compound (COMP) with a Six Week Duration +... 8\n\n  This proposal is due to appear on Snapshot on 2021-07-12T12:00:00Z (UTC). Make sure to stake your vBNT for voting before this date and time to participate in the DAO decision. Proposal to add mid cap LM rewards to the COMP-BNT pool with a six week...\n\n  \n    Reading time: 2 mins \uD83D\uDD51\n      Likes: 1 ❤\n  \n\n  \n\n  \n    \n    \n  \n\n  \n\n\nIt would be great for any BNT holders in this community or members in general to show their support. Voting will go live tomorrow on snapshot 2 at \n        \n          \n        \n        July 12, 2021 8:00 AM\n      . For anyone not familiar with Bancor, there are two key things that differentiate it from other AMMs:\n\nSingle-Sided Exposure: LPs can provide liquidity to a pool with single-sided exposure, either in an ERC20 token (“TKN”) or in BNT.\nImpermanent Loss Insurance: Impermanent Loss Insurance accrues over time, by 1% each day, until 100% protection is achieved after 100 days in the pool. There is a 30-day cliff, which means that if a liquidity provider decides to withdraw their position before 30 days passes, they’d incur the same IL loss experienced in a normal, unprotected AMM. If an LP withdraws any time after 100 days, they receive 100% compensation for any loss that occurred in the first 100 days, or anytime thereafter.\n\nIf the Bancor DAO approves Liquidity Mining rewards then single sided LPs in the COMP pool will also get BNT in addition to fees (payed in COMP) generated from trading in the pool.\\nVoting is live on snapshot 2. The proposal needs 40% quorum and 66.7% super majority in order for it to pass.\\nEnd of first day update:\n1.4M votes For\n0 votes Against\n7.2% quorum at the moment.\\nMiddle of the day update:\n3.12M votes For\n0 votes Against\n15.99% quorum at the moment.\\nEnd of second day update:\n3.76M votes For\n0 votes Against\n19.27% quorum at the moment.\\nMiddle of the day update:\n12.03M votes For\n0 votes Against\n61.71% quorum at the moment.\nSome large votes came in overnight and quorum has been reached. At this stage, it is going to be very hard to defeat this proposal. The proposal has 22 hours remaining.\\nEnd of third day update:\n12.07M votes For\n0 votes Against\n61.9% quorum at the moment.\\nThis is live now and rewards are high at the moment until liquidity starts coming into the pool:\n\\nSingle sided COMP liquidity has started to be staked into the pool and the rewards rate has dropped to a reasonable levels (but still very high):\n\\nPool is still growing in terms of liquidity\n\nand it is sitting at 1.2M at the moment. Rewards APR on the COMP side are 148%.\\nJust a quick note that there will be a renewal proposal for the Compound pool brought up to the Bancor DAO at week 5. The renewal will seek to extend liquidity mining rewards for an additional 8-12 weeks and chances are that the Bancor community will vote in favor given the large turnout for the initial proposal. If other renewals extensions for other communities such as SNX and AAVE that were recently passed are any indication then my guess is that these rewards will be renewed."
  },
  {
    "number_of_comments": 46,
    "postid": "69ae8f73-7075-4523-8909-e1244ec5640e",
    "posturl": "https://www.comp.xyz/t/zrx-bat-and-wbtc-parameter-update/1244",
    "combinedcontent": "We’d like to continue our efforts from last week to update parameters in the protocol. There was little on-chain opposition to our proposal but we failed to reach quorum and will be re-proposing along with our next change, to lower the WBTC collateral factor. We also got some great feedback from the community that they wanted to see more data from our stress tests and analysis, which we’ve included below. Obviously, higher capital efficiency for WBTC is something users really wanted (as we saw from the support for the proposal) but we believe this creates an untenable risk in the protocol.\nIn our next proposal, we plan to change the collateral requirements for ZRX, BAT, and WBTC.\n\n\n\n\nCollateral\nCurrent collateralFactor\nRecommended collateralFactor\n\n\n\n\nZRX\n.60\n.65\n\n\nBAT\n.60\n.65\n\n\nWBTC\n.75\n.60\n\n\n\nAs we mentioned on Twitter recently 38, an increase in volatility for WBTC and the overall market has increased risk substantially across many defi protocols.\nCurrently, outside of the circular borrowing with DAI and USDC, much of the collateral on Compound is ETH and WBTC being used to borrow stables:\n805×820 25.2 KB\nWhile there are similar levels of usage for both ETH and WBTC, ETH has much better liquidity, and poses less risk. On our dashboard 21, we track a few relevant statistics to collateral safety:\n1254×364 15.8 KB\nOne of these, which is admittedly a bit hard to grok, is the liquidity ratio. This is the ratio of the Real Daily Volume of the asset to the total amount of that asset used as collateral on Compound. Right now, that ratio is 1:5, which means that if markets were to take a tumble, Compound could attempt to sell 5x the current daily volume for WBTC in as little as a few hours. Now many people would say this sounds difficult, but the only way to understand if this could be done safely is to either 1) try it or 2) simulate it. Since option one is financially infeasible, we chose to try option two.\nWe have run thousands of simulations across a wide variety of price trajectories, but with updated data since our last post 3. The tail risk of WBTC continues to increase. We’ll focus on the main output of the simulations, which is “Net Insolvent Value” per collateral type. This is highly dependent on price volatility, which we vary across the x-axis below. A “Volatility Scalar” of 1 means that the stress tests used price paths that matched the historical volatility of each asset, a scalar of 7 corresponds to price paths that are 7 times as volatile as recent asset prices. For reference, the historically bad volatility of March 12/13, 2020 corresponds to a scalar of about a 7. In the graphs below, we show two sets of sample simulation price trajectories so you can all have a clear idea of what this looks like.\n1600×823 167 KB\nYou’ll notice that in the prices move together - this is because price trajectories are parameterized to match historical correlations. The expected loss to the protocol in an extreme market event has more than doubled since we first raised concerns:\n664×708 18 KB\nThis is due to adverse liquidations in the WBTC market. In our sim, you can see that the majority of protocol liquidations happen in the ETH and WBTC markets, but since ETH has so much better liquidity than WBTC, these do not contribute strongly to the risk of insolvency:\n816×840 25.8 KB\n@sirokko recently asked how we estimate slippage for liquidators, and while we briefly touched on it here 6, I wanted to elaborate on that. We use Coinbase data to determine the order book structure, which fits the curve we use for slippage. We then scale that by a factor Real Volume / Coinbase Volume to create a curve that represents the entire market. The volume on Uniswap is included in this, however assuming x*y=k style slippage in the market is a very conservative assumption. In fact, we’ve tried this, and then the simulation sees insolvencies in even pretty normal (but bad) market conditions, and we know that not to be the case, so we continued to develop our model that fits and scales real order book data. One thing that we do consider here are gas fees. We add gas price and transaction delay implications by building gas distribution curves on three month trailing data, and use random samples in our liquidator logic. In addition, we fit expected transaction delay times to liquidation calls.\nWe plan to create another proposal through the governance contract later this week, once people have had a chance to provide feedback on these proposed changes.\\nWhile i’m not opposing this proposal, there are several thinghs i wanted to higlight, hopefully for better picture.\nFor ZRX and BAT market this increase risk-taking ability for users, which i still think now isn’t a great time to allow in current market situation. However, taking in account the size of markets, and relatively small change introduced (proposed number of CF is still very conservative) i’m willing to take Gauntlet arguments, that it’s going to be fine.\nFor WBTC market though, since it’s involves decreasing collateral factor, i would like to see some sort of estimation of impact on current borrow positions before that’s put into vote. I believe no government decision should by itself put users underwater. So if decreasing CF put considerable amount of users into liquidation, or close to liquidation than it should not be executed. Or at least executed gradually, slowly taking CF down in several steps, rather than in one big cut of 15%.\nIn that particular case, while i don’t quite share Gauntlet concerns about how bad liquidity of WBTC is, in current market condition having more conservative CF would indeed reduce amount of potential risk for protocol, and for users as well. That’s why i do not feel strongly against decreasing CF here, though, i would probably suggest it higher than 60% still.\nNow why i still not convinced by results presented by model. I would like to see model accounting for on-chain liquidity, rather than just looking at daily trading volume.\nHere are just simple reference numbers.\nWBTC supplied to Compound : $2186M\nWBTC/ETH LP Sushiswap:      $1007M\nWBTC/ETH LP 1inch:            $553M\nWBTC/ETH LP Uniswap:        $416M\nThere’s hardly a liquidity problem to liquidate like even 30% of Compound WBTC supply using exclusively this 3 DEXes and not touching CEX liquidity at all. And i, by the way, think that if we see like 30% BTC price drop within a day, liquidations will go in majority via DEXes, rather than CEXes, as in times of violent price movement (especially downside) we could easily observe how they either completely go down or “experience degraded perfomance” making it not so much suitable as a liquidation instrument.\\n\n\n\n Sirokko:\n\nFor WBTC market though, since it’s involves decreasing collateral factor, i would like to see some sort of estimation of impact on current borrow positions before that’s put into vote.\n\n\nIf we dropped the CF of WBTC to 60% again, these 7 accounts would immediately be liquidatable, the top 5 hold about 26M in collateral right now:\n[‘0x4815b52fee7a689b000958bec010720d210377c1’,\n‘0x6513b81d8240cb94abc089f2cba60be6fda2f805’,\n‘0x8877889ff4a0fa0415d7572cbc871ede200e2f78’,\n‘0xc8ebbaaad5ff2e5683f8313fd4d056b7ff738bed’,\n‘0xfa8c56fd100c7c55ef39e0af075c618b59714f91’,\n‘0x90df02383e534ad007c27c673a4bfb92a1e6ba38’,\n‘0x96cb9c8d4d180b024274748b51fc18336d327e21’ ]\nWe absolutely want to minimize impact on borrowers, and that’s one of the reasons we built this dashboard 27.  Hopefully, borrowers who are pushing the limits of their collateral positions will be paying attention to governance at the least, but this dashboard should give people a further heads-up on changes. Changing parameters is essential to protocol safety and success over time, and while there are tradeoffs with the UX for users, we will continue to do what we can to make those tradeoffs as positive for Compound as possible.\nRe liquidity assumptions, If you were to imagine liquidating a mere 50mm of BTC on Uniswap:\nimage430×587 37 KB\nYou’d see a >20% price impact. This would push many other positions through the liquidation threshold. It’s not that our model doesn’t incorporate on-chain liquidity, it’s that looking only at on-chain liquidity is not a good assumption on this scale.\\nYes, that’s quite what i mean when i said the model should account for onchain liquidity.\nYour example with Uniswap follow that, but intentionally or not, you selected a place with lowest WBTC liquidity on chain for your example.\nLet’s take a look at sushi:\nsushi_c373×600 36.9 KB\nBut considering you want to deal with millions i seriously doubt anybody would use just a single pool of liquidity, so here is the same  trade on 1inch, routed through several pools:\n1inch_c478×609 48.7 KB\nOf course, telling that on-chain liquidity is only thing we should care was never my intention. I very much encourage using as much data, as available. The only thing i want to say, it shouldn’t be ignored either, as it is not negligble anymore. And volume at DEX is not good enough as ONLY metric, as they hold much more liquidity, than their daily volume is. Especially if we talk about suchi, 1inch pools, where liquididy is incentivised. Daily volume is a small fraction of their liquidity, most of it just sit there without trading. (untill price spike happen)\nTo make my point really short and simple. If we say:\na) ETH have no problem with liquidations (as shown by your model)\nb) WBTC have no problems in converting to ETH (as delivered from onchain  liquidity)\nThen if A and B is true, than\nc) WBTC should have little problems with liquidations also.\nOf course, there is additional step and transactional expenses, so it might not be as good as ETH, but certanly not 10 or 20 times worse in liquidity.\nThat’s not said to say that decreasing CF for WBTC is bad idea. It might be not. I believe after so many X in price some tokens done, lowering risk might be appropriate, even if it’s somewhat enforced on users. I just want you to maybe take a look at your model from outside perspective. Maybe DEXes, should not be just a single daily volume metric, and dex liquidity is not exactly quite same as cex orderbook either.\\n@jmo Is this the correct takeaway from your simulation results?\nIf we do nothing, and the markets are only as volatile than they have been in the last twelve months, then:\n\nWe should expect an event in which about 7% of the 2 billion dollars of WETH in compound is liquidated.\nSince a portion of profit for liquidators can come from the compound pool itself, this could negatively affect the solvency of one or more compound pools?\nThis liquidation event would have a very large, rapid, and self-reenforcing negative effect on the price of BTC worldwide.\n\nIs this in the ballpark of what you were intending to convey?\\n\n\n\n dvf:\n\nWe should expect an event in which about 7% of the 2 billion dollars of WETH in compound is liquidated.\n\n\nGiven “black thursday” levels of volatility, we expect the WBTC market on average to become underwater by 7%, much more than that would be liquidated. We’ll probably post some stats on the current liquidations once we get a chance as well.\\nI support raising the CF for ZRX & BAT. I do not support decreasing the CF for WBTC.\nWBTC is the most liquid coin on the Ethereum ecosystem that is not a stablecoin or ETH, and the CF should reflect that. Like one of @Sirokko’s points, I think Gauntlet’s models are not fairly taking into account the onchain liquidity available.\nThe top 10 cWBTC holders account for 60% of cWBTC. The top 100 cWBTC holders account for 91% of cWBTC. The top 10 users are highly incentivized to keep their wallets far away from liquidation, and many are high-sophisticated users. If I had more free time, I would love to analyze each of these user’s accounts because their activity and usage are likely valuable in our decision-making.\nI also think the CF should only move in one direction except for emergencies/significant changes to the underlying asset. It will be hard to build a community and user base if governance changes CFs up and down often.\nPerhaps we should advertise/support DeFi Saver and other services that protect users’ loans and better provide alert systems.\\n\n\n\n jmo:\n\nThere was little on-chain opposition to our proposal but we failed to reach quorum and will be re-proposing along with our next change\n\n\nI don’t think that not reaching quorum should be taken lightly. It is unlikely that many voters did not see the proposal/forgot to vote.\nAdditionally, I don’t believe that it is ideal to bundle the previous changes along with a change to the collateral factor for WBTC. Just decreasing the CF for WBTC is quite controversial and warrants its own proposal in my opinion.\n\n\n\n getty:\n\nWBTC is the most liquid coin on the Ethereum ecosystem that is not a stablecoin or ETH, and the CF should reflect that. Like one of @Sirokko’s points, I think Gauntlet’s models are not fairly taking into account the onchain liquidity available.\n\n\nWe should acknowledge that Gaunlet specializes in simulation and their models are likely very complex—far more complex than any anecdotal evidence which is generally offered. I don’t think it fair to say that based on limited research, their models are inaccurate. We should strive to generate parameters based on data attained from stress simulations and algorithmic optimization rather than finger in the air.\n\n\n\n getty:\n\nPerhaps we should advertise/support DeFi Saver and other services that protect users’ loans and better provide alert systems.\n\n\nCollateral factors do not affect risk of liquidation to the user whatsoever. If there is concern that mass liquidation could cause a system wide shortfall, then the collateral factor is definitely too high. Relying on borrowers to payback in time takes away from the trustlessness of the system.\\n\n\n\n arr00:\n\nWe should acknowledge that Gaunlet specializes in simulation and their models are likely very complex—far more complex than any anecdotal evidence which is generally offered. I don’t think it fair to say that based on limited research, their models are inaccurate. We should strive to generate parameters based on data attained from stress simulations and algorithmic optimization rather than finger in the air.\n\n\nI love the work that Gauntlet is doing and acknowledge they are highly skilled. Their simulations are certainly helpful for monitoring the risks the protocol has. That being said, I have read their risk reports, medium articles and discussed with them their methodology. In my opinion, as a long-time participant in Compound and generally DeFi, their methodology could better account for the liquidity available onchain. Currently, they focus on traded volume as the best indication for liquidity. Before Uniswap/Sushiswap and other AMMs had significant liquidity, I would have used the same trading volume stat they are using to measure liquidity; however, a significant amount of onchain liquidity has built up in recent months, and purely measuring trading volume is no longer sufficient.\\nTo stay focused on proposal itself, i think it’s worth to mention that despite of discussion going towards Gauntlet model, that isn’t main point of this discussion. I believe concerns, raised by Gauntlet certanly is enough as a reason to discuss lowering CF for WBTC, even if I might not share the scale of that concerns.\nI certanly hope that last couple days of market volatility allowed to gather more data, which might reinforce either position.\nThan surely WBTC liquidity isn’t quite equal to ETH liquidity, and in light of concerns from the model, governance can and should consider decreasing CF.\nThe scale of proposed changes should be conservative enough to not put users immediately into liquidation, regardless of “they should know better” arguments.\nSo, if putting CF from 75 to 60 puts users into liquidation, than change should be more conservative, to prevent it as much, as possible. If 65 doesn’t improve situation either, than we should go with 75 to 70. Which is probably most conservative approach possible. If decreasing CF by mere 5% cause liquidations, i don’t think governance could do anything about that.\nConsidering market volatility in last couple days, i’d say executing most careful and conservative approach in lowering CF might be appropriate. And certanly likely to get more support among voters rather than big 15% cut. Especially if we remember that increase to 75% was introduced just couple weeks ago, and even if we exclude Gauntlet voting “for” by mistake, still gathered quite a significant support.\nAnd yet, in current market conditions, having somewhat more conservative CF for WBTC than 75% might be appropriate.\\nI understand Gauntlet is a company and can’t give up the “secret sauce” to their analytical methods. But I’d love to see more specifics on model inputs (+ maybe outputs). Eg for the liquidity and volatility data, which specific sources are used and what is the method of computation for aggregate final numbers. Or any probability distributions of expected losses (net insolvent) so people can understand the nature of the risk to the protocol (eg thin or fat tail).\nIt’s tough for me to get fully behind the proposal when I can’t inspect the underlying model assumptions.\\nHey - we’re looking into the recent liquidations but in the spirit of sharing data, we published some quick numbers to Google Data Studio and made the link public for people who are interested:\n\n  \n      Google Data Studio\n  \n  \n    \n\nCOMP liquidations Feb 22-23 2021 8\n\nGoogle Data Studio turns your data into informative dashboards and reports that are easy to read, easy to share, and fully customizable.\n\n\n  \n  \n    \n    \n  \n  \n\n\nWe saw some liquidations, but just as a reminder, our stress tests are focused on worst-case volatility, like we saw last March. It was a tumultuous couple of days, but nowhere near as bad as we saw last March. I grabbed this quick chart from BitMex to help visualize:\n\nimage929×474 61.6 KB\n\nDefinitely let me know if you see anything interesting in the liquidations data!\\n@jmo\nI respect and appreciate your suggestions, which are always backed up by numbers.\nThis proposal is a big part of Liquidation\nI think we should take all possible measures to prevent users from liquidating, including publicizing it on social networking sites and displaying it on the website!\\nWe’re going to move forward with a proposal here. We realize that lowering collateralFactors can cause liquidations and should not be done lightly, however the risk posed to the protocol due to WBTC insolvency is too great not to take action. We’ve re-run the simulations with updated results:\n\nimage697×680 22.3 KB\n\nThese results show that risk in the protocol continues to increase as volatility continues to stay high and WBTC liquidity actually appears lower than it was last week - our last stress tests assumed 250mm daily volume for WBTC and these most recent results assume ~120mm:\n\nimage1753×984 32.9 KB\n\nWe’ve included all of the liquidity assumptions above as @monet-supply requested. We also have updated our suggest parameters to avoid liquidations, as more people have been taking advantage of the higher collateralFactor.\n\n\n\n\nCollateral\nCurrent collateralFactor\nRecommended collateralFactor\n\n\n\n\nZRX\n.60\n.65\n\n\nBAT\n.60\n.65\n\n\nWBTC\n.75\n.65\n\n\n\nBy lowering the CF for WBTC to .65 instead of all the way to .6, we can limit the immediate effect on borrowers. Some quick numbers here on liquidations for different values of WBTC:\n70: [‘0xfa8c56fd100c7c55ef39e0af075c618b59714f91’, ‘0x1702f2d0df7c99011a690461c34e51bd81cbab48’, ‘0xd762205476ea5649c689a8a703488572c8b3007d’]\nTotal collateral: 4,729,790.58 USD\nNOTE: FIRST ONE IS LARGEST (4.7MM) AND IS RECURSIVE WBTC\n65: [‘0x15200b08325069ea4edd0cec4baf75853b200b5c’, ‘0x3f9a8d828ebc441eb1ad6942118586f331454743’, ‘0x51a33a5bba82f8ebd0fe2192a25ca2e60b9dc32f’, ‘0x59149937397a77368a3a37ee359d1ff61d9df011’, ‘0x61f5d50cdf7813190d125c0e32b195c6fc0f6ffd’, ‘0x8877889ff4a0fa0415d7572cbc871ede200e2f78’, ‘0x8fd1fa7ba628a0a170ae53139552dbd1aec9c22b’, ‘0xd749940ac368792ffb33745653cb1e7b7d0b7a17’, ‘0xddef74bd1ef4915546ee9371928123475eb2a354’, ‘0xf14662b6b7edfebe645d783afef03a6ce615dfe0’]\nTotal collateral: 9,821,799.23 USD\n60: [‘0x071341196d71b14f04b84463f9b9b56f3b5ed333’, ‘0x2898e036dcaa7a94ea96761193cbe6c54d587bc5’, ‘0x29664e652ca8f8f2d95de3b33cc0ae7247fc0aa9’, ‘0x39f230c044c2a9a67ae95caac639e67145e533a7’, ‘0x3d624933c49df28059d07f950254abb92aa4dc8b’, ‘0x44428c4cfbe9e319ca00a5d955a417f4b6e080e9’, ‘0x444e01dcb3a1ec1b1aa1344505ed7c8690d53281’, ‘0x505008a654c321d93b7d2ebdf6389aaa876b16ab’, ‘0x5140c2e3be04d48e9b7c5fdf016bbea6a88b2b65’, ‘0x57d1e246d2e32f6f9d10ec55fc41e8b2e2988308’, ‘0x59f075a6a9a11b4437d33c0cddb6f5226a4bea87’, ‘0x63dcafd6b547e7cfb2e67386db3d06d67aa71c49’, ‘0x78b58e4197c04d320a33c582ecc3e7e885f26a58’, ‘0x7cd982fdbeaa7a3d13de73d50e2030fde2aba3e1’, ‘0x815afec2721dc50ca75c633906147a46c439e52e’, ‘0x8626e72944f62aad5d85e60b8d34471f7d5ef980’, ‘0x8b349d17392f266d87d518bf71426b2921cc35ee’, ‘0xb145ed57fdf5b6f4fc7d8ec9a2f03026a218f000’, ‘0xbececc0d46b3f06b0096c6b2bbc6e429fe7bd911’, ‘0xc8ebbaaad5ff2e5683f8313fd4d056b7ff738bed’, ‘0xcc60721776b70023ae3196b3a9994e911aa66854’, ‘0xd28bfaea8c886ff6424141278a928f3cde2741f1’, ‘0xe24286adfc053f76888aa51d9a94f6c1519b4cba’, ‘0xfd4bb054392dcd5acb07be4d9e287c7684c87606’]\nTotal collateral: 33,722,887.95 USD\nObviously, we could lower the parameter in smaller increments, but as proposals take 5 days, this is too slow as the current risk level is so high.\\nProposal is live: CP039 19!\\nJust a quick question. Since I don’t see adresses duplicate at 65 and 70 CF, i assume assets for liquidation at 65 CF do not include assets going for liquidation at 70 CF? So basically if CF is set to 65, then we going to put 4 729 790 + 9 821 799 for liquidation?\nI still believe decreasing CF in steps is more responsible action: lowering it to 70 first with second followup proposal lowering it to 65 going next would be a better approach.\nIf timing is critical, than second proposal could be put to vote before first one finish, giving it 24 to 48 hours delay after first proposal put, to give time for borrowers to adjust their positions after execution of first part to avoid being put for liquidations.\nAs for everything in general i think it should already be put to vote, as not much remained to discuss here really. I think all main arguments are already being presented. I still not really convinced about liquidity assumptions your model gives. As i look at that massive onchain liquidity for wbtc-eth (Uni 355M + Sushi 729M + 1inch 535M= over 1.5B) and it’s hard to believe that even 100M trading volume = low liquidity. Besides for last 24 hours only uni and sushi created 37M+38M = 75M of WBTC trading volume, so 125M doesn’t look accurate at all. Could all CEX combined generate less trading volume for WBTC than only 2 DEXes alone.\nYet, i believe, lowering CF is still good idea, as WBTC market is huge on Compound, and after all the bull run it would be smart to stay on safe side, and decrease amount of risk users could take on WBTC. So i’m for your proposal, just not quite convinced by the arguments of low liqudity.\nHowever, I’d say better be safe than sorry, so if your model say that we should derisk WBTC market, than we should. Maybe your model is somewhat right, who knows.\\nI’m very happy to see this proposal go live. I think switch to 65% for this was a good compromise on reducing the impact of this change.\\n\n\n\n jmo:\n\n~120mm\n\n\nCan you share what exchange/markets you are using to get the ~120mm daily volume?\\n\n  \n      \n      messari.io\n  \n  \n    \n\nCrypto Research, Data, and Tools 3\n\nGain an edge over the crypto market with professional grade data, tools, and research.\n\n\n  \n  \n    \n    \n  \n  \n\n\nBinance\nUniswap\nSushiswap\netc.\nThe only market with any real volume not included is Curve , which accounts for 10-30mm in volume. So best case, the current collateral would blow through the collective order book 4 times instead of 5 times, which is still, very very bad\\nLooks like it is missing the below:\n\n\n1inch\n\n\nCurve\n\n\nBancor\n\n\nBalancer\n\n\nDodo\n\n\nFTX\n\n\nBitgo mint/burn\n\n\nCoinlist\n\n\\n@getty How much liquidity do you think there is? We ran the earlier sims with an assumption of 258mm volume and the results were still clear - WBTC poses a huge risk to the protocol. We can talk about exchanges all day but the question is - why do you think the protocol is safe? Sure, we can add more features to our model, but shouldn’t the protocol move forward with the best information it has? The question I have for people who oppose this is not “what features should we add?” but “how could you possibly think this is safe?”\\n\n\n\n jmo:\n\nThe question I have for people who oppose this is not “what features should we add?” but “how could you possibly think this is safe?”\n\n\nFair question\nStepping back for everyone reading this thread, I want to explain the scenario where the protocol loses money. If any loan goes to liquidation and the collateral value (minus the liquidation incentive 8%) is below the value borrowed, the protocol will take on losses. An asset with 75% CF has ~17% of wiggle room. Although, once a loan goes to liquidation, it will likely be eating into a portion of the 17% cushion immediately.\nFor the protocol to lose money someone needs to have a loan reach the liquidation point, for there we either need a liquidator not to liquidate or we need the market to gap ~17%, and then the protocol starts taking on water. I am not going to say this is impossible, but it is very unlikely. Liquidators are highly incentivized to liquidate loans, and users are incentivized not to get liquidated. The primary concern that users/protocol can’t control is if the market does, in fact, gap ~17% in 10-30 minutes.\nHaving worked in crypto full-time for the last +3 years as a trader, I have seen significant volatility and understand the danger of cascading liquidations. I have also seen the market rapidly mature and develop liquidity.\nWhy do I think it is safe:\n\n\nThe available liquidity for WBTC onchain and offchain is unmatched by any coin on Compound other than ETH & the stables.\n\n\nI have researched the primary WBTC accounts, and they are all far away from liquidation, and I know many of them will go to great lengths not to be liquidated.\n\n\nI have minted and burned significant amounts of WBTC and am familiar with the significant OTC players.\n\n\nI have researched and participated in many of the WBTC onchain and offchain exchange/pools to know what is and isn’t possible.\n\n\nI deal in reality and function and less in theory. With the state of WBTC liquidity and the state of accounts using WBTC on Compound, I think it is safe. Are we taking a risk, yes. It is a risk worth taking, yes.\nIf I had more time I would love to write a more detailed post, maybe next time this goes to governance I’ll try to write something more formal.\n\n\n\n jmo:\n\nwhy do you think the protocol is safe?\n\n\nMy primary concern is that the oracle price does gap, which causes underwater loans and protocol debt. That is why I am pushing for oracle improvement because the current Coinbase and poster situation is not acceptable.\\n\n\n\n getty:\n\n\nI have researched the primary WBTC accounts, and they are all far away from liquidation, and I know many of them will go to great lengths not to be liquidated.\n\n\n\nI’ll go as far as to say that hoping users top up collateral during a downturn is not a great way for the protocol to mitigate risk.\n\n\n\n getty:\n\n\nI have minted and burned significant amounts of WBTC and am familiar with the significant OTC players.\nI have researched and participated in many of the WBTC onchain and offchain exchange/pools to know what is and isn’t possible.\n\n\n\nThese anecdotes are helpful in understanding your experience, but to be clear, we at no time intended to question anyone’s expertise as a trader.  What we are trying to do is build the best model possible, over time, and get community feedback while we do so. I think one piece of feedback that is clear is that we should include liquidity from Curve, and we’ll continue to look into that.\nNow our model focuses on liquidation cascades, which are a real concern given the quantity of WBTC on Compound. While you present interesting feedback on our model, it’s not clear to me how you have taken into consideration these cascades or any of the other things we included, like gas price spikes, price correlations between borrowed and lent assets, etc.  We know our model is no perfect - not model is - but it feels the counter argument is to throw the baby away with the bathwater in favor of finger-in-the-air reasoning.\\nI don’t take your comments personally, so no concern there. I know we all want what is best for the protocol. I kind of treat it like arguing with friends/family. We might get a bit dirty and harsh but generally, we all like one another.\nI truly wish I had the time and resources to build my own risk model so it does seem like “finger-in-the-air reasoning” because I certainly don’t feel like that is what I am doing but I can understand if that is how it comes off. I care deeply about these parameters and understand that what we do with them can have big implications.\nWhile I haven’t built a model to handle gas price spikes, looking at price correlations and user’s accounts is fairly easy to do. As complicated as this process might seem only a few things really matter: users’ accounts, liquidity, and liquidators.\nWe should focus on protocol growth and robustness instead of limiting it. I believe we can grow the protocol and make safer at the same time.\\nActually, i would like to suggest different approach with WBTC situation. There’s one thing i don’t parlticulary like about WBTC on-chain liquidity, is that ever since Uniswap introduced WBTC-ETH pair to their liquidity mining program, that became main focus, and most incentives introduced later incentivese that particular pair.\nAnd of course, everybody like it, btc and eth grow together, fees from trading accumulating and some incentives are on top. All great till it drops. And we know when it crashes it crashes all together. That pair isn’t really ideal for Compound liquidations, and since Compound WBTC market is so big, it actually create risk for all DeFi ecosystem. If Compound WBTC market could be pushed into liquidations deep enough, it can potentially send all DeFi on a quick ride to goblin town.\nTo create some sort of stability anchor for facilitation of liquidations Compound needs liquid enough WBTC-USDC pair on-chain. Like hundreds of millions size.\nI would advocate for Compound to create liquidity incentive program. To decrease distribution of COMP from USDC and DAI markets, and forward that portion to staking LP Uniswap WBTC-USDC. I think that is in best interest of Compound, as well as DeFi ecosystem overall.\\nSo I know there has been a lot of talk about liquidity assumptions, so we went and updated our data input pipelines to just assume much more liquidity for WBTC (350mm as opposed to 120mm):\n\nimage676×672 20.8 KB\n\nIf liquidity is actually much better than we assume, then the expected WBTC losses to Compound in a Black Thursday event are around $40mm instead of closer to $100mm. But this is still more than 10 times the risk across all other assets combined.\\nA couple of points, some may have been repeated in some form by others:\n\nYour model seems to be extremely sensitive to liquidity assumptions and your liquidity assumptions seem a bit off to me. Baseline you have COMP ($140MM) as having more liquidity than WBTC ($128MM) and UNI ($610MM) having 5x more liquidity than WBTC on chain. Checking 1-inch right now, WBTC has far lower slippage than either asset for large transactions (a $50MM trade to USDC incurs 14% slippage for WBTC, 40% for UNI, and 73% for COMP). This seems to be the biggest weakness in your simulations in my opinion. Going from $128MM to $350MM is a good step but arguably, WBTC should be rated at a liquidity level above UNI.\n\nAn additional consideration on the liquidity front: WBTC is a proxy asset off-chain because for most exchanges, people would prefer to trade BTC directly rather than WBTC. People only trade WBTC when they want to shift dollars or BTC onto the ETH chain. So WBTC has infinite ‘shadow’ liquidity because liquidators can hedge WBTC risk with BTC reasonably well, although during a stress event, that may be less true.\n\nRegarding your simulation mechanics, is it happening at the account level or the aggregate protocol level? I think the former approach would be a fair bit more accurate depending on the distribution of collateralization among top accounts.\n\nOverall, I think you make a reasonable point that Compound is taking on some extra risk with WBTC but I don’t think it’s as drastic as you think it is. And as a common sense test, assigning WBTC a factor of .6 or .65 (equivalent to BAT or ZRX) seems off as WBTC is probably more liquid than either asset by an order of magnitude at least.\nI think the real risk you’re highlighting is the size of the WBTC market, so you may want to consider managing risk by targeting that variable instead.\\n\n\n\n gczy:\n\nSo WBTC has infinite ‘shadow’ liquidity because liquidators can hedge WBTC risk with BTC reasonably well, although during a stress event, that may be less true.\n\n\nThat’s not really good argument. You see, thing is in the event of liquidation, liquidator need to quickly sell WBTC, not BTC, as that’s what they get from collateral, and while technically it’s possible to directly redeem WBTC for BTC and vice-versa, even non-custodial, via RenVM, it takes  some time, so liquidator need to have a lot of capital and willing to take bigger risks to be able to actually use that strategy. Usually it’s not the case, liquidators turn to go for “quick buck” by selling ASAP, getting their comission and be done with that. It’s not the most efficient, and likely not the most profitable way to do it, but it is how it is now. Thus they commonly sell literally at bottom and use huge gas fees also, sometimes far above any reasonable level (like customer pays, so who cares).\nAs for comparison with BAT and ZRX: size matters. WBTC dwarfs BAT and ZRX in size, even if you combine BAT and ZRX together and multiply it by 9, you still wouldn’t reach size of WBTC collateral. So it presents much more complicated task at liquidation.\nGauntlet simulation assumes there might be a need for market sell several hundreds of millions of WBTC, and trick here is it shouldn’t push price down that much, so 25% margin, collateral have, would be enough to cover liquidation and fee.\nThere is good reasoning behind Gauntlet concerns after all, even if we believe that their model isn’t quite accounting for on-chain liquidity.\nAnd i think if they wouldn’t try to push it too much and too fast, it would get much more community support. We all can agree that after all, WBTC liquidity isn’t quite good as ETH. Just for a simple reason that very big chunk of on-chain WBTC is tied to ETH, rather than to stable coin. So there going to be another hop, through ETH to reach stable coins, and ETH would be falling too in simulated conditions.\nI think it’s surely reasonable to decrease CF for WBTC from 75 to 70 just to get bigger margin in case of market crash. (actually it would probably be better there to begin with, and not going to 75 increase at all)\nAnd maybe even from 70 to 65 as a followup, though i feel less certain about second decrease.\nYou see, in theory, after such a big runup, we should expect people taking more caution, and keeping less leveraged positions, but even if decrease to 60% CF creates about 30M of liquidations already it’s clearly not the case. 30M might not look big in comparison to overall market, but market sell even of that size would still create noticeble down price movement.\\nIn my opinion it was a bit shortsighted from you to include the WBTC parameter update in the same vote as the ZRX and BAT adjustment, considering that the WBTC parameter was just recently adjusted 2 and the ZRX and BAT adjustments failed.\nIt would not surprise me to see this vote fail because of exactly this reason. Please consider to split numerous adjustments in separate votes in the future, especially if they alter recently deployed changes or include recently failed votes, as this will reduce obscuration and give a clearer picture of sentiment of voters in the protocol.\nConsider this also in regards to the health of the protocol, as your deployment of this vote will now delay future votes until this one has passed, and we will be starting again at square one, without a clearer picture of why exactly it failed (if it fails), other than to go by the forum sentiment.\\n\n\n\n Shubiwubi:\n\nPlease consider to split numerous adjustments in separate votes in the future\n\n\nOne thing to note - we can only propose changes every five days. This is how compound governance works. There are over 50 parameters controlled across the 8 lending markets (9 incl. USDT) and you need to change these more than once or twice a year.\nThe community needs to be able to vote on what’s best for the protocol. If on principle, you only vote for changes that affect one parameter at a time, you will be most likely voting against the best interest of the protocol. It’s not a vote against the changes, which would be fine, it’s a vote for stasis\\n\n\n\n gczy:\n\nAnd as a common sense test, assigning WBTC a factor of .6 or .65 (equivalent to BAT or ZRX) seems off as WBTC is probably more liquid than either asset by an order of magnitude at least.\n\n\nEven if that were true, there’s is also more than an order of magnitude more WBTC in the protocol. It’s the relative liquidity that matters, not total.\\nWould it be possible for you to post your graph trying out a range of WBTC CFs? Maybe 50%-80% on 5% increments?\nEdit: I was trying to reply to message 25 in the thread.\\nNot sure what exactly you mean, or even if we can do that in the next few hours, but happy to try once we have clarity. Also we can definitely try to include this information next time!\\nPolychain will be voting in favor of (FOR) Prop. 039.\nProp 039 puts Compound in a less than ideal situation. Either 1) We liquidate some users and reverse prop. 036 that was voted on only a few weeks ago, or 2) we leave the WBTC collateral factor where it is & ignore data suggesting that the protocol could become underwater in volatile markets.\nWe see #1 as the lesser of two evils. Security of the protocol is our top priority. The data here sufficiently demonstrates the notable risk posed by a 75% collateral factor, and as such, we are in favor of a collateral factor reduction. While this may end up liquidating users who don’t adjust collateral in time, this is a tradeoff we should be willing to make to ensure protocol security. (Forced liquidations shouldn’t be taken lightly, but the reduction to 65% seems like a good middle ground.)\nSetting collateral factors is about optimizing the tradeoff between safety & capital efficiency. There is an argument to make that, in light of certain risks, higher collateral factors may be justified to support growth/competitiveness. We’re open to more discussion on this to find the right balance. In this case, given the risks presented, we believe it is best to err on the side of caution.\nThis vote also brings up a number of issues worth further consideration (as have been discussed a bit in this thread): 1) Collateral factors shouldn’t see this much volatility; we need to iron out the process for setting these. 2) In extreme cases, when collateral factors need to be reduced, we need a more graceful method of deleveraging users who would otherwise be liquidated. 3) We need better pre-vote signaling to divide proposals into controversial issues that need to be separated & uncontroversial issues that can be bundled (we should start doing more of this through the forum).\\nAt this point it really doesn’t feel that this proposal in current form should be pushed. Primary because of expected forced liquidations, not because of the direction of action.\nIf that proposal leave ZRX and BAT aside of that controversal topic and was only to decrease WBTC CF from 75 to 70 i’m sure it would have much more support than it currently have.\nMaybe it makes more sense to let it fail, but instead use your massive voting power to create less radical proposal with WBTC CF going from 75 to 70, which majority could actually support.\nPutting users into liquidation with the argument that there is a possibility that protocol potentially might become insolvent in certain market conditions  doesn’t feels like a good practice after all.\nI believe it would be much more productive if Compound governance utilised snapshot voting to actually shape details of proposal prior to proceeding with proposal.\\nThe potential for liquidations is a concern, and we would prefer to see collateral factors change more slowly, but we agree that this is a significant risk for the protocol. We are voting in favor.\nIt would be great for a more transparent accounting of on-chain liquidity to allay concerns that the analysis is incomplete.\nThanks Gauntlet for rightly elevating this risk to a vote.\nDisclosures here. 12\\nHere’s a report 13 one of my colleagues on the MakerDAO risk team prepared recently. Digs into qualitative factors of WBTC liquidity and redemption cycle, and may offer some additional context for setting risk parameters.\\nLiquidating the users’ position via risk mitigation proposals is a horrible UX and will drive users away from the protocol. This is a competitive land scape and we as a community need to prioritize users.\nThe WHY matters – thanks John and the Gauntlet team for shedding light on your reasoning, though there are still lots to discuss, you guys are doing an admirable job presenting your case.\nThe HOW matters even more – we can’t dismiss users and liquidate their positions. We can’t present Compound as an unstable protocol that changes key metrics every few weeks and affect users in the process.\nThe risk should be mitigated but this is not the way. I’d like to call upon delegates who have yet to vote to step in and save the protocol’s face. We can mitigate this in a more methodical approach.\nWe can execute better than this.\\nFirstly, I want to highlight the growth in interest in community governance over the past few months—the number of substantive comments here is incredible to see. I believe that we all care about users and forcing a user into liquidation is terrible and should only be done as an absolute last case scenario. I hope that future proposals concerning protocol economic risk will have analysis to this extent and that we will be pushed to improve protocol infrastructure around risk management.\nSomething which comes to mind is supply caps—being able to monitor the rate of inflow into the protocol is essential at this stage. I’d be happy to implement it if there is community interest.\\nWith a16z and Polychain support that looks like it’s a pass regardless.\nI guess now it becomes important to higlight upcoming  CF change after it pass as much visible as possible, so anybody potentially impacted could adjust their positions in time.\nAnd i would like to specifically mention, that whoever is having a collateral with volatility like WBTC, and utilising it to such extend, that is put at liquidation at just 10% CF change is doing it wrong. Such risk isn’t worth a potential reward, and they really should take a deeper look at their position and either increase amount of collateral, or decrease amount of borrowing, even if that particular change not liquidate them immediately, but put them very close to it.\nGovernance can’t and shouldn’t babysit every user, and while we all try to minimise negative impact to users, but in general personal risk management  is what users should do themselves. Compound is a tool, and like every tool, needs a skill to be used efficiently.\\n@Sirokko\nI agree with you!\n75 → 70\nWe need to protect Compound users from liquidation!\\n\n\n\n tatsuzou12:\n\n75 → 70\n\n\nThat indeed would be more accurate approach, but too late, friend  It’s was all VC conspiracy in the end :)))) The only thing could be done at this point is to make sure that everybody is aware of upcoming decrease of WBTC CF to 65.\nIt’s almost impossible to outvote biggest COMP holders if they unite in pushing something.\nBut keep in mind, i support change itself, i think it makes sense to decrease CF for WBTC. (in current market conditions) Staged decrease would be more forgiving for users, but in theory if everybody who is in risk zone takes action in time, there might be no liquidations in the end, and then proposal wouldn’t look that much dramatic.\\nThat’s a very valid point which I wasn’t aware of, so thanks for pointing this out to me jmo! :))\nI suppose it was a bit unlucky the way these particular votes fell together, and granted this does change things slightly, but perhaps in this special case it would have worked in favour for most to have an exception made and kept things separate, considering the previous 2 votes and their outcome, together with the shaky sentiment around some of the proposed adjustments from before.\\nFirst, I’d like to acknowledge that the debate over these parameter changes is awesome. Everyone here is motivated to see the Compound protocol succeed, and everyone is bringing a different perspective to the conversation.\nGovernance needs to balance the needs of three different stakeholders; suppliers, borrowers, and COMP token-holders (governors). WBTC is the protocol’s largest collateral asset. As such, it is responsible for a large portion of borrowing demand (stablecoins), which in turn creates interest for the suppliers of assets. Adjusting risk parameters should be done with a holistic consideration of all stakeholders.\nThe Gauntlet team has done a great job analyzing WBTC market risk, and advocating for a reduction of risk associated with WBTC. It is the solution, reducing the collateral factor, that forms the basis of my Against vote.\nReducing a collateral factor is a significant event, since it directly degrades the user experience of borrowers. When SAI (which had been deprecated for 6 months, and was a small asset) had its collateral factor reduced, the community (1) fiercly debated the necessity of the change, (2) carefully managed the change, in timing and communication. SAI was iteratively reduced in proposals 3 1, 4, 5, 6, and 15 2, with significant effort put into informing users ahead of time in Discord, Twitter, the newsletter, etc. WBTC is approximately 3000x as large as SAI was.\nI strongly believe there are ways to reduce risk, without degrading the Compound user experience. These could include supply caps (per @arr00), protocol liquidation fees (to offset cascade risk), external liquidation protections (e.g. B protocol), resting protocol-level limit orders, real-time alerts to borrowers using an off-chain system (“margin calls”) etc – all ideas discussed here and in Discord, which could upgrade Compound so that a 75% collateral factor is safely managed.\nMy participation in this thread was late – I try not to involve myself in governance, to allow others to become the leaders of the protocol, and I could have done a better job raising concerns earlier.\nFrom here, let’s communicate these changes aggressively to users, and work towards building a system that is capable of safely supporting a 75% WBTC collateral factor, at massive scale.\\nEchoing the sentiments here on how fantastic it is to see such active discussion in Compound’s protocol governance.\n@rleshner I agree with all of your points on Proposal 39 being a sub-optimal process for reducing the WBTC collateral factor. In a different market environment, I would strongly urge a re-structuring of the proposal towards a gradual ramping-down - whether over a designated time period, e.g., 2 months similar to SAI, or over multiple rounds of proposals.\nThe Gauntlet team’s diagnosis is directionally correct based on the analysis that they’ve presented, but the limitations of the protocol as it stands today and the urgency of the risk are why Pantera Capital is voting FOR Proposal 39.\nRelative to past cycles and even this past summer (when the SAI change occurred), crypto markets are more leveraged, intertwined, and price-reflexive on an order of magnitude. Considering that a re-structured proposal could take another 5-7 days at minimum to pass and that a staggered approach would require multiples of that - I think we would generally agree that 5 days is a long time in crypto and 15 days is an eternity.\nI’d also note that we arrived at the same conclusion as other large investors, completely independently and from the perspective of a Compound user rather than an early-stage Compound investor (which we are not). IMO, this is a significant indicator of current market views and particularly the risk of a significant, rapid de-leveraging / price spiral in today’s environment.\nAs governors of the protocol, we should balance stakeholder needs and consider the impact to users in all decisions. At the same time, I’m keenly aware that one of the biggest open questions about Compound governance is whether decentralized governance can assess risk accurately and react quickly to protect the health of the protocol.\nIn a worst-case scenario, where the risk outlined in Gauntlet’s analysis is realized during this critical decision period, the perception that we as a community were aware and did not react in time is a potentially disqualifying event for those who aren’t convinced yet on decentralized governance. In this lens, the size of WBTC on Compound and the timing realities of a more progressive approach are the key factors in us voting to act quickly rather than delay for an optimal implementation.\\nMany of the great ideas from this debate will make it much easier to reduce risk without degrading the user experience — improving liquidations, supply caps, off-chain alerts. I’m excited to see proposals and developer grants.\nDisclosures here. 2\\nI’ve started a new thread to talk about liquidations now that the proposal has passed:\n\n  \n    \n    \n    CP039: Avoiding liquidations \n  \n  \n    One thing that all participants in the recent vote seem to agree on is that changes should try to avoid increasing the quantity of liquidations users face. As users of Compound only provide one piece of identifying information - their address - it’s not clear how to contact them to make sure they are aware of these changes. I’d like to suggest a few things that we could to minimize liquidations and wanted to get feedback on these ideas. \n\n\nTry to contact the users using ETH transactions. The ide…\n  \n\n\nI’ve put together a couple quick suggestions for how we could minimize the impact here, would love to hear everyone’s thoughts"
  },
  {
    "number_of_comments": 9,
    "postid": "5433078d-2619-46fd-8dd6-af7abcd5d884",
    "posturl": "https://www.comp.xyz/t/governor-bravo-proposal/1384",
    "combinedcontent": "Delegate to the Governor Bravo CAP here: 0xd122638eCa5bB644591fE660FCe0B85E2aB6186a\nIntroduction\nGovernor Bravo has been a community effort for the past few months to bring governance to the next stage. This proposal transfers Compound governance to the new Governor Bravo contract. This transfer is done by setting the pending admin of the Timelock to Governor Bravo, then calling initiate on Governor Bravo, which accepts the Timelock admin. If this proposal is successful, all future Compound governance calls should be sent to Governor Bravo 64.\nProposal Calls\nThe first two calls reimburses Compound Labs $27k that was paid to Open Zeppelin for the audit from cUSDC reserves. The following two calls award bounties to Arr00(160 COMP) and Blurr(80 COMP) for organizing and executing the Governor Bravo project. The following call sets Governor Bravo as the pending admin, and the last call initiates Governor Bravo, accepting the Timelock admin.\nGovernor Bravo Details:\nGovernor Bravo has been developed by members of the community in the open for the past few months. The new contracts have been fully audited by Open Zeppelin 15, are fully covered by extensive test suites, have been simulated by forking simulations, and have been running on the kovan testnet.\nThe new features are as follows:\n\nUpgradable implementation\nSettable parameters (proposal threshold, voting period, voting delay)\nAbstain vote option\nOptional string voting reason\nProposer can always cancel their proposal (until execution)\nRemoval of the guardian\nContinuous proposal id logic\n\nThe introduction of Governor Bravo allows for governance to create meta proposals to optimize the process and move the governing process forwards. The code changes of Governor Bravo are deliberately minimized but allow for endless exploration of governance modification in the future.\nFor more context, please see the Governor Bravo forum post Governor Bravo Development 47.\nImportant Notes:\nOnce the Governor Bravo proposal is live, no other proposals should be posted on Governor Alpha unless Governor Bravo fails. The Governor Bravo proposal will transfer the Timelock admin away from Governor Alpha which disallows future queuing and executing from Governor Alpha.\nFor platforms building on Compound Governance, note the Governor Bravo address and the ABI which can be found here 7. The first proposal on Governor Bravo will be one greater than this proposal. Any third party voting service should note the change of the support variable type from a boolean to a uint8. The support values are as follows: 0 = against, 1 = for, 2 = abstain.\nNFT:\nI plan to have a limited Compound NFT ready for the Governor Bravo initiation. The complete terms of who will be able to claim it are not concrete, but assume that only voters prior to this proposal will receive the NFT. It will probably be claimable through a merkle proof based distribution minting system. Claiming will be limited to ~2 days after Governor Bravo initiation, so be sure to check back here around Bravo initiation (the execution of this proposal) for claiming instructions.\\nverified the 2 contract bytecode, all good\n\nUsing network mainnet https://mainnet-eth.compound.finance 8\nMatching contract at 0xc0da02939e1441f497fd74f78ce7decb17b66529 to GovernorBravoDelegator with args [“0x6d903f6003cca6255d85cca4d3b5e5146dc33925”,“0xc00e94cb662c3520282e6f5717214004a7f26888”,“0x6d903f6003cca6255d85cca4d3b5e5146dc33925”,“0xaaaaaaaaaaaa8fdb04f544f4eee52939cddce378”,17280,1,“100000000000000000000000”]\n Successfully matched GovernorBravoDelegator to 0xc0da02939e1441f497fd74f78ce7decb17b66529 with args [“0x6d903f6003cca6255d85cca4d3b5e5146dc33925”,“0xc00e94cb662c3520282e6f5717214004a7f26888”,“0x6d903f6003cca6255d85cca4d3b5e5146dc33925”,“0xaaaaaaaaaaaa8fdb04f544f4eee52939cddce378”,17280,1,“100000000000000000000000”]\n\n\nUsing network mainnet https://mainnet-eth.compound.finance 8\nMatching contract at 0xaaaaaaaaaaaa8fdb04f544f4eee52939cddce378 to GovernorBravoDelegate with args []\n Successfully matched GovernorBravoDelegate to 0xaaaaaaaaaaaa8fdb04f544f4eee52939cddce378 with args []\n\\nConfirming my address is 0x27C79A01878962cE0b6126aF6E184c06D36c8f37.\\nI love the new contacts. In terms of bounties, where do you get your comp figures from? Have you spoken with the grants committee about compensation?\\nAs the grants committee only started after the completion of this project, I decided to complete the process using the old method. The COMP figures were reached through deliberation with many different members of the Compound community.\\nThank you for your response. Job well done!\\nThe Governor Bravo CAP has reached 100k votes and is now proposal 42 19. I just pushed a post propose forking simulation to GitHub 11, which confirms that Governor Bravo will take over as expected if the proposal is executed.\nGo vote! \\nThe Compound Community NFT is now live as mentioned in the Governor Bravo proposal. Follow the guide here 38 to claim yours once Governor Bravo is initiated. The NFT is claimable for 3 days from time that proposal 41 is executed.\nI’m hoping to setup a really simple frontend for claiming the NFT by Tuesday for those who are less comfortable directly interacting with the contracts via Etherscan.\\nWhat a cool idea. Thanks for all you do, arr00\\nCongrats on the success!"
  },
  {
    "number_of_comments": 15,
    "postid": "9f28da80-ea2f-4968-bae2-d5cb09e99392",
    "posturl": "https://www.comp.xyz/t/deploy-compound-iii-on-linea/4460",
    "combinedcontent": "[RFC] Deploy Compound III on Linea\nIntroduction\nHello Compound Community! This is Cameron from Consensys and Jun from PGov. We are excited to engage the compound community for feedback regarding a deployment of Compound V3 on Linea. This thread aims to be a starting point for discussion with a tentative timeline below.\nBefore jumping into the specifics of a potential deployment on Linea, it’s exciting to spotlight the collaboration between the protocol teams. Linea and Compound Labs have been working to integrate the protocol into Linea’s testnet voyage 3. This joint effort has led to exploring new functionalities, such as implementing gas-less sponsored transactions in compliance with EIP-4337 and adhering to guidelines set by Compound Labs.\nLast week’s testnet voyage, “DeFi week 2,” was live for 7 days. During this period, Compound Contracts have facilitated over 295,000 3 transactions. These transactions were conducted by more than 100,000 wallets, which collectively contributed to the 333M of testnet liquidity.\nGiven Compound’s success on the testnet, Linea is enthusiastic about guiding users toward the protocol, thereby bolstering the establishment of Compound V3 as a premier lending market for the network.\nTimeline and Scope of the RFC\nIn alignment with a commitment to transparency. This proposal outlines a tentative timeline below regarding a formal proposal to deploy Compound V3 on Linea mainnet. Linea Mainnet is expected to launch in mid-July.\n1. Current: Request for Comment Period - the request for comment is a timeframe where we can garner feedback from community members on the network and technical aspects of a deployment.\n2. July 17th: Formal Proposal - After incorporating community feedback and recommendations, we plan to post a formal proposal on the Compound forum. This proposal will adhere to the formal multichain deployment structure guidelines.\n3. July 28th: Voting phase - Pending positive community feedback and discussions with core stakeholders, we aim to advance the proposal to the voting phase.\n\nPreamble:\nType: Multichain Deployment\nTitle: Deploy Compound III on Linea\nAuthor: PGov / Consensys/Linea\nProposal Introduction\nPoint of Contact:\nPGov: @PGov\nLinea: @Cameron, @befa\nEmail: Governance@Consensys.net\nSet up a time to talk about the proposal: Calendly 4\nProposal Summary:\nThe Linea team from Consensys and PGov team are proposing the deployment of Compound III on Linea, a zkEVM incubated by Consensys, to continue broadening Compound’s multi-chain future and establish the lending market as a leading protocol on a promising zkEVM with a robust distribution network.\nOverview of Proposal\n\nAbout Linea\nProposed Markets\n\nAbout Linea\nLinea is a scalable L2 on top of Ethereum powered by a zkEVM tech stack incubated by Consensys, resulting from over 20 months of R&D from Consensys’s research team. This team has a track record of successful projects in the zk space, including the GNARK library and active involvement in the Merge.\nLinea processes native EVM bytecode for proving and verification. This approach allows for the execution of Solidity smart contracts without needing modifications or re-audit. This also means compatibility at the RPC level with widely adopted middleware and toolsets, providing developers with a familiar environment to build using well-established tools and infrastructure.\nMoreover, the network intends to align its onboarding and distribution through existing relationships with MetaMask, Infura, and Truffle communities. These products will help bootstrap the network and better serve developers, protocols, and builders.\nProposed Markets\nWe propose that the Linea deployment starts with an isolated USDC market with crypto majors as collateral. We also propose a conservative approach, after the applicable parties provide market and asset recommendations.\nMotivation\nLinea provides Compound access to an expansive and well-established user base through existing Consensys product integration. This connection facilitates a more efficient distribution and onboarding process for Linea, helping to catalyze Compound adoption on the network. This strategic alignment expands Compound’s market penetration and encourages new opportunities for user engagement.\n|624x403.983680466879431600×1037 105 KB\nLinea prioritizes User Experience (UX), aiming to eliminate barriers for MetaMask’s impressive community of 30M MAUs. By securing native support in the default MetaMask network dropdown list, Linea enhances user convenience and fully supports all MetaMask curated experiences. These include On/Off Ramp, MetaBridge, MetaMask Swaps, the Portfolio dApp, and the MetaMask SDK, presenting numerous opportunities for incorporating the Compound protocol.\nGrant Application:\nDid not apply\nNon-Technical Evaluation\nOver 100 global partners and protocols are committed to launching on Linea within the first week of mainnet, some of which are below.\n|624.5178423236515x395.999999999999941600×1016 192 KB\nLinea’s testnet has handled over 2 million transactions under private beta and over 40 million transactions under public beta. Many of these transactions are users of the ongoing Voyage quest, a crucial initiative designed to help stress-test the network. The network has also shown unprecedented developer activity, with over 4.3 M unique addresses deploying over 2M contracts, showcasing a vibrant and engaged developer community.\nSecurity Evaluation\nWe identified the following considerations to be aware of. These considerations resulted from consultation with our internal team, Linea, and external stakeholders.\n\nLinea’s sequencer and prover are centralized\n\nThe team is committed to decentralizing the sequencer and prover within the first 12 months of operation\n\n\nA multi-sig can upgrade core smart contracts\n\nThe team is looking to mitigate centralization risk through the use of a security council within the first 12 months of operation\n\n\nLow liquidity markets\n\nA primary variable for risk analysis is liquidity data. As Linea does not expect to have accurate liquidity data by the time this proposal goes live, the team is looking into how best to mitigate risk for protocol stakeholders\n\n\n\nTechnical Evaluation and Considerations\n\nTechnical Implementation\nCross-chain Governance\nOracles\n\nTechnical Implementation - Linea onboarding engineers will handle all technical deployment under advisement from all applicable stakeholders, including Compound Labs, OpenZepplin, and Gauntlet.\nPlease see a list of deployed testnet contract addresses here 2.\ncomet 0xa84b24A43ba1890A165f94Ad13d0196E5fD1023a 3\nconfigurator 0x079f68Fa440A0F04d741a5Aba7DA8fE9DfB0AA8B 1\nrewards 0x44411C605eb7e009cad03f3847cfbbFCF8895130 2\nbridgeReceiver 0x06F066A9C0633507EAc640D89442C77748C3a2a8 1\nl2MessageService 0xC499a572640B64eA1C8c194c43Bc3E19940719dC\nl2TokenBridge 0xB191E3d98074f92584E5205B99c3F17fB2068927\nbulker 0xad6729C101691A63F7d1e4CcbaD04bC8c6818a22 2\nfauceteer 0x953d0A78eeC15E76266792fE163dC5316F5c2aca 2\nCross-chain Governance - The smart contracts deployed on Linea will be the same as the mainnet contracts, using the compound cross-chain governance process. The Layer 1 (L1) Timelock will manage the Linea contracts, transmitting messages to the Layer 2 (L2) BridgeReceiver 1 through the native message-passing bridge. Please refer to the link provided for more comprehensive information on Compound III’s multi-chain governance approach.\nOracle - Linea currently does not have Chainlink integration. In looking for a suitable alternative for the deployment, the Linea team conducted a competitive analysis of oracles operating on the network. The analysis considered factors including Total Value Secured (TVS), data sources, aggregation methods, security, and market adoption. The evaluated on-chain oracle services were: Pyth, Redstone, and Umbrella (Deployed on testnet Compound integration).\nThe Linea team proposes to begin the discussion with Pyth as the oracle provider. Pyth’s first-party data is directly sourced from various trusted institutions, including exchanges, trading firms, and market makers. Pyth’s architecture and design enable secure and cost-effective high-frequency price feeds. The service has obtained audits for various networks, found here 4. Lastly, they have shown promising market adoption, including 1.7B TVS 3, and primary oracle integration at Synthetix and Venus.\nOther Considerations\nSponsorship – PGov did not receive any sponsorship to contribute to this proposal.\nMarket Backstop Discussions – The Linea team is assessing the viability of having a safety module to backstop protocol losses that occur directly from oracle or liquidity manipulation for the first 60 days of operation. This would not be smart contract enabled. These discussions are ongoing and not finalized. According to the feedback on this proposal, The team will provide more details on this topic.\nCopyright Waiver\nCopyright and related rights waived via CC0.\nLicense Exemption\nWe are requesting an exemption from the community that will allow the Linea network to obtain a Compound Business Source License (BSL) to use the Licensed Work, update compound-community-licenses.eth, and deploy it on the linea network, provided that the deployment is subject to Ethereum Layer 1 Compound Protocol governance and control.\nLinea References\n\nLinea Website 1\nLinea Block Explorer\nLinea Docs\nIntroduction into zkEVMs\nLinea Mirror\nConsensys: R&D\nConsensys R&D Gnark Library\nVortex: Building a Lattice-based SNARK scheme…\nEthresearch.ch - A ZK-EVM specification\nVitalik - A rollup-centric Ethereum roadmap 1\n\\n\n\n\n PGov:\n\nOracle - Linea currently does not have Chainlink integration. In looking for a suitable alternative for the deployment, the Linea team conducted a competitive analysis of oracles operating on the network. The analysis considered factors including Total Value Secured (TVS), data sources, aggregation methods, security, and market adoption. The evaluated on-chain oracle services were: Pyth, Redstone, and Umbrella (Deployed on testnet Compound integration).\nThe Linea team proposes to begin the discussion with Pyth as the oracle provider. Pyth’s first-party data is directly sourced from various trusted institutions, including exchanges, trading firms, and market makers. Pyth’s architecture and design enable secure and cost-effective high-frequency price feeds. The service has obtained audits for various networks, found here. Lastly, they have shown promising market adoption, including 1.7B TVS, and primary oracle integration at Synthetix and Venus.\n\n\nChainlink is maintaining price information on-chain, ready to be consumed by compound directly (also called a push oracle). In contrast to that Pyth and Redstone are known for their Pull architecture, where prices are available elsewhere and are pulled into the respective chain whenever needed.\nCompound lll is not build around this pull architecture and similarly in governance posts in the AAVE forum it was pointed out that AAVE cannot simply switch to a pull model, the same restrictions compound will be facing.\nI’d be interested in knowing what integration is being proposed here and how exactly e.g. Pyth is going to be used to function as an oracle for Compound with the pull vs push restrictions in mind.\\nThank you @sleezy - Great question! I circled back with a Pyth and Redstone representatives to get more technical details on push model integration.\nLet me know if this answers the above; happy to gather more specific information.\n\n\n\n sleezy:\n\nI’d be interested in knowing what integration is being proposed here and how exactly e.g. Pyth is going to be used to function as an oracle for Compound with the pull vs push restrictions in mind.\n\n\nProposed Pyth Integration\nPyth operates with both a pull and push model. For the push model, The Pyth EVM price pusher is a service that regularly pushes price updates to the on-chain Pyth contract. Any entity can run this service to push regular updates based on various conditions, such as a minimum update frequency or a price deviation threshold. This service is “Chainlink compatible” and would be leveraged for integration using Pyth.\nIn the context of operating and maintaining the pusher for Compound III on Linea, the Linea core team will fund, operate, and maintain the price pusher. After discussions with the Linea team, we suggest running the pusher on the same update parameters used for mainnet price feeds. This approach provides the same functionality as the standard chainlink integration.\n_\n*** The parameters for the price feeds are subject to change based on feedback from core stakeholders to ensure protocol and user safety. (i.e. if more updates are necessary)\nThe goal would be to mimic the chainlink price feed updates found here. 3\n\nRedstone Functionality\nRedStone Finance also offers a push model known as the RedStone Classic model 1. This model uses an off-chain relayer that pushes data on-chain based on customizable conditions such as time intervals or value deviations. The relayer is permissionless, but Redstone typically operates end-to-end and maintains the pusher. This model is also “Chainlink compatible.”\\nThanks @Cameron. Both of those imply heavy centralisation and are in stark contrast to what is currently in use with Chainlink.\nIn the case of Pyth, Compound will need to trust Linea as the sole entity of pushing prices and everything that comes with that (infrastructure, network costs, etc).\nIn the case of Redstone, the sole entity pushing the prices is apparently Redstone themselves, which again puts all the trust assumptions on a single entity.\nCompound utilities Chainlink because it is actually a decentralized oracle network, where every node is ensuring that prices get published through OCR. If one oracle node operator fails, another makes sure that prices are on-chain.\nChainlink DONs range from 11-30 nodes within a certain price feed.\nYour suggestion here in terms of oracles is a heavy downgrade in terms of security assumptions to what compound is used to and introduces risks that are currently not present.\nI also don’t want this to become a bashing post. I like both Redstone and Pyth as innovation within the oracle space is good. I think their pull models are a novel approach and allow for many new interesting use cases.\nWith that being said I want to reiterate that you don’t suggest using them in a way they are advantageous, meaning in an actual pull fashion natively by compound. Instead it is being suggested to turn these pull models into a Chainlink like push model, which creates major centralisation risks where there were previously none since the responsibility of price pushing is put into a single entity.\nEdit: you also refer to protocols like for instance synthetix as references for pyth, but synthetix integrates pyth in an actual pull model and not a makeshift push model.\\n@sleezy, Thank you for your thoughtful response. We appreciate your concerns around potential centralization. I was able to link up with internal teams at Consensys and the oracle providers to get a more thorough understanding of the concerns you shared and propose some ideas.\nFirstly, I want to acknowledge the value that Chainlink brings to the table with its decentralized oracle network. It’s a robust solution that has proven its worth in the industry and has our utmost respect. However, Chainlink is not deployed to the Linea network and thus is not an option for us at this time.\nThe goal of our discussion is not to downgrade security assumptions but to find an alternative oracle solution that enables support for a deployment on Linea. The core Compound contracts would require a push model oracle; this also was confirmed by core stakeholders. A pull model oracle would require a more significant development lift. One core benefit of deploying on Linea is that, as a type two zkEVM, existing contracts can be deployed without modification. @befa can speak more on the benefits of a type2 zkEVM if needed.\n\n\n\n\n sleezy:\n\nIn the case of Pyth, Compound will need to trust Linea as the sole entity of pushing prices and everything that comes with that (infrastructure, network costs, etc).\n\n\nA primary concern with the pushing solutions is a centralization risk associated with entities running the infrastructure. In the following, I will attempt to alleviate some of these concerns and propose ways to mitigate risk.\nAt Consensys, we take managing infrastructure seriously and have extensive experience in this area. Over the past 8 years, this experience has come from maintaining core Ethereum clients, hosting >20,000 Ethereum validators, Infura, operating infrastructure at Rocketpool and Lido, and more.\nSpecific Concerns\n\n\nIn all of the above scenarios, the price pushing is done permissionless, so any person or entity can set up an instance and push the prices under the same parameters. This is particularly interesting in the case of an entity that has an incentive to push prices more frequently. For example purposes only, a liquidator operating onchain might be incentivized to run an instance and update the price pusher more frequently. The permissionless nature of the price pushing provides much flexibility to address redundancy further as the ecosystem grows.\n\n\nUnder the example of Pyth, this specific instance of the price pusher would be running and monitored by our dedicated protocol & infrastructure teams. To help address redundancy, In the unlikely scenario that the price pusher would not update, the Pyth Data Association is committed to operating another pusher instance under the same parameters. Furthermore, Consensys will have backups ready to deploy under the same parameters if needed. Structures like this are common in the industry and closely related to how centralized sequencers are monitored across major L2 rollup ecosystems. (24/7 monitoring, response, backup)\n\n\nWe also understand that OpenZepplin manages monitoring for the DAO. Consensys/Linea is also open to collaborating on a bot that monitors price updates for the discord. This helps to keep all stakeholders informed.\n\n\nLastly, The Linea team is working to onboard Gelato’s automation relay network. This process is ongoing, with no finite ETA. However, when integrated, there is an option to run pushers (n instances/secondary/backups) on the Gelato relay network.\n\n\nI hope the above ideas and information help alleviate some concerns and find a middle ground for a deployment. We are open to working with community members like yourself to find solutions that benefit all involved parties. If interested, you can access our team’s calendar to discuss any ideas you have for the deployment above.\\nthanks for the thorough writeout @Cameron!\nI just wanted to highlight what you’ve written initially vs what you’re suggesting now are completely different things and as such turn the convo into an entirely different direction.\nIn the suggested pyth model you said, that Linea would be operating the sole price pusher\n\n\n\n Cameron:\n\nIn the context of operating and maintaining the pusher for Compound III on Linea, the Linea core team will fund, operate, and maintain the price pusher. After discussions with the Linea team, we suggest running the pusher on the same update parameters used for mainnet price feeds. This approach provides the same functionality as the standard chainlink integration.\n\n\nwhich now turned into Linea + Pyth + potentially Gelato, which obviously is a completely different conversation to be had, as you’ve now introduced multiple layers of potential decentralization (both in the form of more nodes and more entities running those).\n\n\n\n Cameron:\n\n\n\nUnder the example of Pyth, this specific instance of the price pusher would be running and monitored by our dedicated protocol & infrastructure teams. To help address redundancy, In the unlikely scenario that the price pusher would not update, the Pyth Data Association is committed to operating another pusher instance under the same parameters. Furthermore, Consensys will have backups ready to deploy under the same parameters if needed. Structures like this are common in the industry and closely related to how centralized sequencers are monitored across major L2 rollup ecosystems. (24/7 monitoring, response, backup)\n\n\nLastly, The Linea team is working to onboard Gelato’s automation relay network. This process is ongoing, with no finite ETA. However, when integrated, there is an option to run pushers (n instances/secondary/backups) on the Gelato relay network.\n\n\n\n\nA combination of the Linea team together with the oracle operator plus a potential Gelato Web3 Functions integration definitly sounds more appealing that what has been initially suggested.\\nH/T to the Complabs team, who worked diligently to get the forum back up\n@sleezy - Thanks for the comment above!\nYou are absolutely correct in your analysis. We approach these discussions with an open mind and to engage with community members like yourself, to spot [and address] any weaknesses or concerns that might be raised. Having multiple entities run the price pusher is an outcome of our direct conversation and a perfect example of how open discussion can strengthen proposals. This will meaningfully impact how the proposed oracle will operate and reduce risk if this deployment passes through governance.\nThank you for working through the above with us; looking forward to further discussion on the potential deployment!\\nI think we just need all the details on the table to make a fully informed decision. It is fine utilizing a new approach to oracles, but we need to know what precisely changes and what new/additional risks compound is taking on in this approach.\nThe push model we’re used to from Chainlink has the benefit that prices are maintained directly by the oracle node operators by the chain we consume them on. So the entities that fetch the price, come to an agreement off-chain and then put the price on-chain to consume are the same people.\nimage1934×575 82.4 KB\nCompared to that the suggested Pyth model will introduce multiple different parties that are currently not in the picture in the push model, since its main design is a pull design.\nWith Pyth:\n\noracle operators push prices to an aggregation contract on Pythnet\nWormhole (a bridge) then attests to these values and makes them available through an HTTP gateway\nThis gateway is then called by the Linea Team, the Pyth Foundation and potentially Gelato using a price pusher\nThis pusher checks the price reported by the gateway against the on-chain price on the contract on the target chain and pushes prices if certain conditions are met\nThen compound reads the value.\nimage1992×775 145 KB\n\nThe overall effect is the same, prices will be on-chain, readily available for compound to be consumed.\nThe difference however lies in how many trust assumptions compound takes on.\nIn the Chainlink push model, you trust the node operators. It’s the same entities coming to an agreement what the price is (aggregating off-chain through OCR) that are pushing the respective price onto the contract on the target chain to be consumed by Compound at will.\nIn the Pyth model you trust:\na) Pythnet being online/maintained\nb) the node operators to aggregate (on Pythnet) (*note this is the same as chainlink as you trust the oracle operators)\nc) Wormhole (a bridge) to attest to the values on Pythnet\nd) Linea to operate a price pusher\ne) the Pyth Foundation to operate a price pusher\nf) potentially Gelato to operate? a price pusher\nWhile the end result is the same, you’ve just introduced:\n\na bridge\nanother chain\n3 entities pushing prices\n\nalongside the node operators that were needed anyway.\n\nI know that my post might come across as severly negative, but that’s mainly because the topic of oracles was brushed aside as if it wasn’t an issue or a major change. You’re going to introduce a completely new model, with more parties involved, hence more trust assumptions, hence increased risk. The overall operation of such a system and the risk that compound takes on should be included if such a proposal is made, and should not be up to someone to point out.\\n\n\n\n PGov:\n\nLastly, they have shown promising market adoption, including 1.7B TVS , and primary oracle integration at Synthetix and Venus.\n\n\nAnother thing i wanted to mention here is that this decision is being justified by Pyth being used as a primary oracle and securing “billions” in value according to the Defillama TVS Dashboard.\n75% of Pyth’s TVS come from Venus and Synthetix.\nOn Venus, the primary oracle is Chainlink and Pyth values are used to santiy check Chainlink. (Read more here 1)\nPyth is not the primary oracle like being claimed here.\nOn Synthetix, Pyth and Chainlink are used in combination for Synthetix Perps only. (Read more here)\nPyth is not the sole oracle used on Synthetix. It is only used in combination with Chainlink.\nMost of the TVS that is used for justification of this provider is by projects that employ them alongside  Chainlink. None of the 75% (and even more if we go down the list) are secured purely by Pyth.\nThis is in stark contrast to what is being suggested and communicated here.\\nHi @PGov @Cameron @sleezy ,\nBefore this goes to the formal proposal phase I thought I’d throw the Tellor hat in the ring as another possible option. After reading the discussion, it might also be worth exploring a multi-oracle system here, similar theme to the Chainlink/Uniswap anchor that Compound uses on mainnet.\nTellor has multiple live implementations as a component in a multi-oracle design:\nA common use-case being a fallback to Chainlink for Liquity, Ethos, and more recently Raft. We’re also being used as a 1-of-3 median oracle for Ampleforth.\nIn terms of the push/pull dynamics we, too, are a pull oracle that can develop permissionless push solutions, i.e. our implementation on DIVA protocol.\nExciting to see Compound and Linea’s testnet progress so far and we’d love to help build out a resilient oracle solution that gives the community peace of mind. Happy to answer any questions.\nThanks!\nRyan\\nTo the best of my knowledge (happy to be corrected) the Uniswap Achor is a thing of the past and only used on Compoundv2.\nThis proposal is about v3, which is configured with a single oracle in mind (currently Chainlink). V3 doesn’t use UAV2 or UAV3 in any capacity to the best of my knowledge.\\n@sleezy - Thank you for your perspective. As we navigate these concerns, our objectives moving forward are twofold:\n\nAddress issues and concerns you’ve raised above through more detailed information/education about the oracle, and\nHave Pyth come to the forum to educate and further explain technical details on the oracle, including a walkthrough of how the oracle will function in a Compound integration.\n\nThis will help all stakeholders make a more informed decision when this proposal moves out of a RFC stage.\n\nFirst, You highlight +6 new trust assumptions [a-f], as we walk through below, this is not accurate. You also are proposing that the newly introduced entities are in fact bad. This could be misleading, as 2 out of the 3 are trustless systems, and the third is a permissionless price pushing process, with multiple instances running.\n\n\n\n sleezy:\n\na) Pythnet being online/maintained\nb) the node operators to aggregate (on Pythnet) (*note this is the same as chainlink as you trust the oracle operators)\n\n\nabove, A&B are separate, when they are the same. We could almost say this about any oracle. For Pyth, the prices are brought to Pythnet through blockchain transactions from each individual publisher, those are then automatically aggregated on Pythnet within the Pyth smart contract, following this algorithm.\nPythnet is the application-specific blockchain operated by Pyth’s data providers, in other words, each data publisher is also a validator. With more than 80 data providers today, any name listed here, are both a data provider and validator to the Network. Further, the network has onboarded various leading infrastructure providers to provide support to any of the 80+ data providers.\nThese node providers reinforce trustlessness across the network.\n\n\n\n sleezy:\n\nc) Wormhole (a bridge) to attest to the values on Pythnet\n\n\nWormhole operates trustlessly; there is no single entity that needs to be trusted. There are 19 guardians that operate within the system. These guardian nodes monitor the activities taking place on the chains to ensure the security of transactions. Further, and to be used as support, they have recently undergone an in-depth diligence regarding governance message passing. This process was carried out by an independent committee funded by the Uniswap Foundation, and Wormhole was one of the few providers to meet the committee’s standards for the DAO.\nWormhole Docs\nUF Bridge Assessment Report 1\n\n\n\n sleezy:\n\nd) Linea to operate a price pusher\ne) the Pyth Foundation to operate a price pusher\nf) potentially Gelato to operate? a price pusher\n\n\nThese concerns are not as applicable as they might seem at first glance. We’ve spent considerable time addressing the question of redundancy. The price pusher acts more like a scheduler, initiating transactions to broadcast prices through Wormhole’s trustless bridge. This process is permissionless, meaning any entity can spin up an instance of a pusher and start pushing prices.\nBecause the price pushing is integrated into a trustless process, the Pyth smart contract on every chain will verify all the prices ingested to ensure that (1) they’re prices attested by 2/3 of the Wormhole guardians; (2) and that it is sufficiently recent (both fresher than the current price in the Smart contract and within the last few seconds)\n\n\n\n sleezy:\n\nA combination of the Linea team together with the oracle operator plus a potential Gelato Web3 Functions integration definitly sounds more appealing that what has been initially suggested.\n\n\nWe acknowledge that the oracle implementation is one of the most critical aspects of this deployment. While Chainlink is not an option for this proposal, we strongly believe that what we are proposing is pragmatically the best option for this potential deployment.\nTechnical details on the oracle from the source will be beneficial for discussion moving forward, and we formally invite Pyth to the forum to provide more details, then further explain, educate, and answer technical questions around on the oracle.\\nHey Cameron, this is Mario, a Contributor from the Pyth Data Association 2\nThank you very much to all for this proposal and the thoughtful comments. Conversations like these are required for effective governance, and to ensure that Compound, among others, can expand to an exciting new chain like Linea while maintaining its high security standards.\nOracle Design\nPyth Network is an oracle designed to deliver accurate, reliable, and high-frequency prices to any blockchain. The protocol aggregates pricing data from a network of 80+ data providers, including some of the biggest financial institutions in both crypto and tradfi. The core of the protocol is an appchain called Pythnet operated by the data providers themselves. Data providers continuously stream prices for over 250+ assets to this blockchain, where their prices are robustly aggregated into the Pyth price. The aggregation procedure is robust and requires over 50% of data providers to manipulate the price.\nPyth prices are transferred from Pythnet to other blockchains via the Wormhole Network. Wormhole has 19 guardians who attest to the state of Pyth prices on the Pythnet blockchain. This process results in a continuously updating stream of signed price payloads. These payloads can be submitted to Pyth contracts on any supported blockchain; these contracts verify the validity of the signatures and store the price on-chain. Updating the on-chain Pyth price is a permissionless operation: anyone can submit a valid price payload at any time.\nThe entire system is designed for high reliability and performance. At every stage from data providers through wormhole, multiple independent entities would have to fail in order to cause an outage. Since Pyth’s launch in Fall 2022, Pythnet has had zero downtime, and the oracle has recently been delivering over 1 million price updates on-chain per day.\nCompound Integration\nThere are multiple ways for protocols to integrate with Pyth. The most common is a “pull” integration, where the users of the protocol perform the permissionless Pyth price update themselves. This integration guarantees that the protocol receives the freshest prices which can be critical for latency-sensitive protocols.\nHowever, for protocols such as Compound, it is technically simpler to perform a “push” integration. In this integration, an off-chain service called the Scheduler 3 continuously updates the on-chain Pyth price, allowing anyone to use the on-chain price without worrying about updating it themselves. In this particular case, any combination of Consensys, the Pyth Data Association, and decentralized infrastructure like Gelato could operate this service.\nNote that Pyth supports permissionless price updates even in a “push” integration. This approach eliminates the reliability risk associated with the off-chain service. Even if the service is offline, users will still be able to use Compound by simply sending a Pyth price update transaction first. While this may be awkward for retail users, sophisticated users — including liquidators — can and will integrate in this fashion.\\nThanks for the writeout and clearing some of my misconceptions! Took the liberty to excentsively dive into the pyth docs again.\nWhile some parts are cleared (e.g. that the oracle nodes are simultaniously the chain validators) others still remain, like e.g. the introduction of a bridge (even if that bridge is decentralized).\nOverall, a pyth integration will introduce two sets of new parties in addition to the oracle node operators:\na) wormhole - the decentralization aspect of wormhole is not in question. but compared to what compound is used to currently, you have to acknowledge that you’ve introduced another party into the data delivery chain and no matter how trustworthy that party is, it is added risk compared to a system that doesn’t require them (especially considering that there is history of such exploits with wormhole). I think it is pretty obvious that a solution that is utilizing a bridge carries more risk than one that doesn’t. I hope that isn’t up for debate here.\nb) Pushing - like outlined by @mariobern some entities are required to push prices for compound to function in a way that it is used to. Again comparing to what compound currently relies on are oracle nodes that do this. In the suggested pyth model 3 more entities (Consensys, Pyth and Gelato) are being introduced. We can sit here all day and debate how trustworthy these entitites are, which won’t change for one second that (similar to the bridge vs no bridge above) more parties need to be trusted and hence more risk is carried.\nJust from the logical standpoint of risk, it is pretty obvious and non-debatable that the suggested implementation carries more risk than what compound currently implies. I don’t think anyone here can and/or questions that. It’s just a matter of if this is a risk that is willing to be taken.\n\nAs a disclaimer since i’m seeing some talk on twitter from people involved in this from the sides of pyth and consensys alike. I’m neither a holder of oracle tokens, neither do i work for any oracle projects, so throwing accusations into those directions looks pretty weak. I simply try to bring transparency into this because not all facts (risks) are being presented by parties that obviously want a compound deployment. There are chains popping up left and right and everybody is doing everything in their power to force a blue chip defi deployment and in some cases without properly outlining what new risks these blue chip protocols are taking on. It took some anon asking the tough questions before you decided to dive deeper and you still haven’t even acknowledged that the suggested implementation requires more trust (something non-debatable).\\nI continued my research into various solutions that are being suggested here in the forums and i’ve stumbled upon another question for the Pyth and Consensys guys in this thread.\nFrom what i could gather from the docs and this proposal is the fact that:\n\nprices are aggregated on pythnet from oracle operators, who are chain operators directly\nwormhole attests to these values\nsomeone runs a gateway that allows these values to be fetched\nsome entities run a price pusher to push it to the desired chain\n\nI am now particularly interested in the third point, which you call the Price Service 4, that was not mentioned anywhere in this thread. According to your documentation anyone can run one of these, but Pyth is the primary entity that does so. Since there was no mentioning of Consensys also running a Price Service alongside the mentioned Price Pusher, this would mean that this entire setup would rely on a single Price Service, hosted by Pyth.\nSo the suggested implementation here is that multiple entities run a Price Pusher that gets data from a singular, pyth-hosted Price Service. Or is the plan that Consensys also hosts such a Price Service? If so, i’d be good for them to mention this as well, as this seems to be yet another angle of potential centralization that finds no mentioning in this thread.\nHere 2 is the Github repo for the Price Service, for anyone interested. If the Pyth-hosted Price Service fails, and nobody else runs an additional one, all of these Price Pushers that are mentioned here to be ran by Pyth, Consensys and Gelato, won’t be able to get any data and won’t be able to maintain price information on-chain.\\nConsidering the affiliation between Linea and Infura’s Chainlink node, it is plausible that the Chainlink infrastructure operated by Infura serves as the potential source of the data being provided by Linea. By incorporating other solutions like Pyth, Linea and Consensys could enhance the overall reliability and transparency of their data feed, potentially mitigating any concerns related to single-source dependency. This approach aligns with Consensys’ commitment to providing cutting-edge and secure services for their users, while also leveraging the speed and security of the Solana network through Pyth."
  },
  {
    "number_of_comments": 15,
    "postid": "dfea97f4-4d8d-4d38-9b39-2b290f6b0a73",
    "posturl": "https://www.comp.xyz/t/listing-maticx-on-compound/4306",
    "combinedcontent": "Topic: Add MaticX to Compound\nHi Everyone, I’m Gautam, business lead for Stader’s MATIC staking product, MaticX.\nBelow is a proposal for adding MaticX as a collateral asset on Compound. Using MaticX as a collateral asset has added significant value on other lending platforms like Aave v3 (Polygon) with supply caps being reached and a popular leveraged staking strategy increasing utilization rates for MATIC. For Compound, MaticX offers a good collateral option because of its extensive use and composability on Polygon.\nBackground - Stader Labs\nStader is a leading multi-chain liquid staking protocol that is currently live on 6 chains including Polygon and is soon going live on Ethereum.\n\n~US $110 Mn in TVL\n50k+ unique wallets staking\n\nBackground - MaticX\nStader’s staking solution for Polygon is MaticX, a liquid staking solution for MATIC. Stader lets users earn MATIC staking rewards and also enables users to participate in other Defi protocols using MaticX while accruing rewards.\nMaticX is a token that represents your share of the total MATIC pool deposited with Stader. As soon as you deposit MATIC on the Stader smart contract, you receive newly minted MaticX, based on the exchange rate at the time of staking. As the MATIC rewards get added the value of MaticX increases (w.r.t MATIC) To illustrate the above explanation, here 1 is a video.\nStader for Polygon gives you\n\nLiquidity through tokenization: Users can participate in Defi using the liquid token.\nEase of staking: User-centric design of Dapp to make staking seamless for users\nMaticX is the only solution that allows users to natively stake their MATIC on Polygon, allowing users to take advantage of the low transaction fee\n\nMore technical details are available here: Stader for Polygon - MaticX FAQs 2\nMaticX currently has a TVL of ~67.5M MATIC and ~$16M in MaticX supplied on AAVE (supply cap reached).\nThere is also a combined liquidity of ~$16M in MaticX based liquidity pools on leading DEXs split as below:\n\nBalancer: ~$11.1M\nQuickSwap: ~$3.4M\nMeshSwap: ~$1.4M\n\nAlso, the current rewards on the primary MaticX LP on Balancer are ~9.5% , about 2x+ those of comparable LPs, thus the size of the LP is expected to grow significantly\nMaticX Summary:\n\nScreenshot 2023-05-15 at 11.42.17 PM1221×858 42 KB\n\nMaticX Token Address:\n\nMaticX address on Ethereum is 0xf03a7eb46d01d9ecaa104558c732cf82f6b6b645\n\nMaticX address on Polygon is 0xfa68fb4628dff1028cfec22b4162fccd0d45efb6 2\n\n\nMaticX Security:\n\nStader on Polygon protocol contracts are dual audited:\n\nHere is the link to the Audit completed by Halborn\n\nHere 1 is the link to the Audit completed by Immunebytes\nStader’s contracts for MaticX are controlled by a multi-sig account (0x91B4139A2FAeaCD4CdbFc3F7B1663F91a54be237) managed by the internal as well as external parties. The confirmation count is 3 out of 5 signatures required\n\n$1Mn Bug Bounty on Immunefi 1.\nOngoing monitoring and on-chain security tracking by Forta\n\n\nExternal multi-sig and time-lock for staking contract\n\nMaticX - Key Reasons to List on Compound\n\nMaticX is deeply integrated with DeFI projects such as Aave and Balancer.\n\n\nThere is ~$16M liquidity in MaticX based pools cross leading DEXs.\n17.2M MaticX supplied on Aave - current supply cap is exhausted.\n\n\nIn order to integrate it into any lending market, MaticX has a Chainlink Oracle calculated price feed 6\n\nMaticX TVL has grown to ~$67M steadily ever since its launch in Apr’22.\nThere is about ~15mn MATICX on Polygon that haven’t been deployed on DeFi yet, which offers a large TVL opportunity for Compound\n\nSource: https://dune.com/stader_labs/MaticX-metrics 2, polygonscan.com\nParameters\nLiquidity - MaticX currently has a TVL of $67.6M and total liquidity of ~$16M in MaticX based liquidity pools on ecosystem DEXs with Balancer being the lead along with QuickSwap & MeshSwap.\nMy proposal is to set the parameters for MaticX on Compound as\n\nMaximum LTV:50%,\nLiquidation threshold of 65%\nLiquidation penalty: 10%\n\nFor reference current parameters for MaticX on Aave v3 Polygon market are:\n\nFor all assets: Max LTV: 58%; Liquidation threshold: 67%; Liquidation Penalty: 10%\nE-Mode (enabled for MATIC): Max LTV: 92.5%; Liquidation threshold: 95%; Liquidation Penalty: 1%\n\n—\nYour support and consideration of our proposal is greatly appreciated!\nI look forward to hearing from you soon, and I am happy to provide additional resources and support to the community if needed.\nBest regards,\nGautam\\nHi Gautam. I think this is a great idea and I am glad to see the community brainstorming new collateral assets for Compound III.\\nHey @Gautam_Stader, thank you for posting this!\nI am on the Fuji Finance 2 team, and we are building a cross-chain money market aggregator. Our product currently supports CompoundV3 on Polygon (as well as other Compound instances).\nHaving MaticX added as a collateral type for CompoundV3 would be quite advantageous from our perspective since we are already planning to support a vault on Polygon which rebalances positions with MaticX as the collateral type, while borrowing for USDC.\nHaving Compound added as a market that supports MaticX will improve the borrow rate that users are able to access from Compound as well as other money markets.\nThis would be a great opportunity for Compound, Stader, and Fuji to collaborate to help improve the borrowing capabilities for MaticX on Polygon and bring more TVL to the Compound ecosystem!\\nLiquid staking has been gaining traction in today’s DeFi landscape. If we look at AAVE even MaticX is also maxed out on AAVE and the balancer pools have a healthy liquidity to consider.\nAll in all, I believe that this is going to be a good add as there is a sizeable amount of MaticX that will flow into this market.\\nAs a sizeable MaticX holder, I support this proposal. It would allow me to borrow stablecoins from time to time or even open a leverage loop strategy if I feel an imminent price upside for $MATIC.\nHope this passes. Thanks!\\nI think it is a great proposal. Stader’s emphasis on security (with double audits) and providing convenience to Polygon users (staking on Matic available that lowers gas costs) makes MaticX a great LST to add to Compound market. Hope we can see leveraged staking loop with Matic soon on Compound.\\nI think it will be great to add MaticX as a collateral asset on Compound. Because it will be us with more DeFi plays on Polygon ecosystem.\\nReflecting on the success of MaticX on AAVE, we concur that it’s prudent to contemplate its listing as collateral. The two largest depositors (each over $2MM) of $MaticX on AAVE comprise Polygon Foundation (0x8d365687a75dc7688864822869ae0551bb6fc105) and a Smart LP (0xf8249b8d751f54dc7b317603ffffc4cbd093c543). The former primarily borrows stablecoins, while the latter engages in a leverage loop strategy on staked Matic.\nThere are two options for listing $MaticX: 1. the deployment of a distinct comet Matic market, or 2. its addition to the existing USDC market. The first option provides greater convenience to those implementing leverage strategies, albeit at the cost of increased complexity for stablecoin borrowers (as manual conversion from $matic to stables would be required).\nThough as suggested by @kevin,\n\nthis problem could be solved by compound or the community building a front-end that combines those steps into a single txn (similar to how we use the Bulker` contract to bulk your supply + borrows into one txn)\"\n\nAs the community agrees transitions gradually from v2 to v3, we should focus more on ensuring that comet remains competitive with AAVE, particularly in terms of user experience.\\nOnce this proposal has reached more community consensus, Gauntlet is happy to conduct analysis from a market risk perspective.\\nMaticX is currently maxed out as a collateral in the other money market in where MaticX is currently accepted as a collateral. Actually it went from 16M to ~29M since this original post.\nI think if there is a chance to still increase the global collateralization cap for MaticX in Polygon(Pos) that should be allowed now for CompoundV3.\\nPlease find Gauntlet’s analysis and parameter recommendations for MaticX here 13.\\nHi everyone, I’m from Stader Polygon team.\nPublishing the MaticX token checklist here for your reference.\n  \n\n      docs.google.com\n  \n\n  \n    \n\nMaticX token checklist - CompoundV3 USDC 5\n\n1. General 1.1 Token Asset Name:  MaticX 1.2 A description of the project and the token:  Stader is a non-custodial smart contract-based staking platform that helps you conveniently discover and access staking solutions. Stader’s staking solution for...\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nHappy to answer any questions on this. Thanks.\\nHi everyone,\nWe have raised the PR to list MaticX on compound\n  \n\n      github.com/compound-finance/comet\n  \n\n  \n    \n\n\n\n    \n      \n    \n\n  \n\n\n\n      \n        Add MATICX as collateral in Polygon USDC market 2\n      \n\n    \n      compound-finance:main ← compound-finance:manoj9april/add-maticx\n    \n\n      \n        \n          opened \n        \n          \n        \n        Jul 13, 2023\n      \n        \n\n        \n          \n            \n            kevincheng96\n          \n        \n\n        \n          \n            +291\n            -4\n          \n        \n      \n  \n\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nThe parameters have been set based on Gauntlet’s recommendations Gauntlet Recommendations: stMATIC and MaticX listing on Polygon Compound v3\nWould love the community to review and share any feedbacks.\nCC - @cylon\\nThe above PR has been merged into feature branch.\nUpdated PR to review\n  \n\n      github.com/compound-finance/comet\n  \n\n  \n    \n\n\n\n    \n      \n    \n\n  \n\n\n\n      \n        Add MATICX as collateral in Polygon USDC market 2\n      \n\n    \n      compound-finance:main ← compound-finance:manoj9april/add-maticx\n    \n\n      \n        \n          opened \n        \n          \n        \n        Jul 13, 2023\n      \n        \n\n        \n          \n            \n            kevincheng96\n          \n        \n\n        \n          \n            +291\n            -4\n          \n        \n      \n  \n\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\\nOpenZeppelin’s Security Team has read through PR 780 2, which aims to add MaticX as collateral on Compound’s Polygon USDC market. The goal of our analysis was not to provide a full security audit but rather to ensure that the data being used and the protocol interaction is accurate. We checked:\n\nthat the cited contract addresses for MaticX and the Chainlink price feed are correct\nthat the deployment strategy (of adding an asset and then redeploying the market’s implementation) is correct\nthat the configuration data for the new collateral asset matches the “aggressive” parameters found in Gauntlet’s analysis 1\nthat the proposed price feed today has a healthy amount of operators (fifteen)\n\\nGovernance Proposal is live for voting!\n  \n      \n\n      compound.finance\n  \n\n  \n    \n\nCompound 8\n\n  Compound is an algorithmic, autonomous interest rate protocol built for developers, to unlock a universe of open financial applications.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n"
  },
  {
    "number_of_comments": 85,
    "postid": "4ef9bb26-7850-4537-a685-fdbe59749c49",
    "posturl": "https://www.comp.xyz/t/cgp-2-0-delegated-domain-allocation-by-questbook/3352",
    "combinedcontent": "\nCGP 2.0 - Delegated Domain Allocation\nFirstly, thank you @adam , @sukernik , @TylerEther  @arr00 , Sovereign Singal 14 and Sam Simmons 8 for reviewing the proposal. It would not have shaped this way, if not for your valuable inputs.\n\nSummary\n\nWe propose a Compound Ecosystem Fund should be launched to find and fund a wide variety of teams building on Compound\n\nWe propose a budget of $1.5M spread across 2 quarters. We are open to iterating this budget amount based on the response from the community.\n\n\nWe additionally propose this budget be managed by 5 individuals - each managing $300K (domain allocator). Each of these individuals would have expertise and builder-networks in their respective domains.\n\nThe performance of each of these domain allocators will be publicly viewable and auditable using rich dashboards.\nEnd of every quarter the Compound community can come together to vote to replace, continue or increase budgets to each domain allocator.\n\nWe (Questbook.xyz 48) will help facilitate setting up these domain allocators and provide the tooling to run the grant program in an efficient and transparent way. We have previously/currently set up the grants process for Polygon, Solana, Celo and Aave.\n\n\nPurpose\nAn active grant program is a great way to attract and incentivize builders to build on Compound. The goal of this proposal is to benefit all the stakeholders - the builders, the token holders, and the people running the grants DAO.\nRunning the grants DAOs with a central committee run in to the following problems that this proposal aims to solve.\n\n\nCommittee blind spots - It is unfair to expect any grant committee member to have expertise across the various domains. It becomes impossible or inefficient to judge projects that may lie outside the committee’s expertise and may still be valuable to the Compound ecosystem. By delegating capital and decision-making to people on ground, we can engage the community to proliferate the ideas and projects that further the protocol.\n\nManager burnout - A single committee can handle only so many applications at any point in time. By not delegating and decentralizing the capital allocation, the grant program has a single point of failure.\n\nInadequate Tooling - Lack of tools and resources to effectively manage an on-chain governance process. Lack of tools and resources to effectively manage the grants workflows on-chain. Most of the grants programs currently use spreadsheets to manage their workflows with hardly any transperancy on the fund disbursals and application reviews.\n\nBy having domain allocators we will be able to delegate the outreach and disbursements to members of the community who are the closest to builders. Giving them access to capital and authority to disburse will go a long way in improving efficiency and reviving the grant program for Compound.\nSpecifically, this program aims to\n\n\nGrow Compound’s grant program - measured by the number of builders applying and building on top of Compound\n\nDelegate capital allocation - to identify, attract and fund projects/builders that the grant program would otherwise not have funded by delegating capital allocation to members of the community rather than a central disbursing committee.\n\nStrengthen the builder community in the bear market - a crucial component of the community is to keep the builder activity alive even during the bear market. The building keeps innovation and optimism in an otherwise grim market condition.  It also drives participation and transparency for the broader community.\n\n\nProgram Design\n\n“We also believe that more decentralized funding is important for the future of the Ethereum ecosystem. We continuously try to allocate resources to third parties that we believe can make better decisions than us within certain domains.” - Executive Director Ethereum Foundation [link] 6\n\nThis program focuses on getting the community to participate in the process of delegating capital.\nThe Compound foundation sets a budget of $1.5M to be disbursed by 5 domain allocators. These domain allocators will be led by one active community member each.\nEach domain allocator runs their program on-chain for full transparency. The data and performance across key metrics will be visible for the community to judge.\n\nGrants DAO Dashboard2880×1800 258 KB\n\nThe actual disbursement of the grant happens on-chain from a multi-sig wallet controlled by the domain allocator manager and one person from the foundation, whose responsibility will be to ensure that the application is aligned with the domain and in turn aligned with compound lab’s growth plan before signing the disbursal. The sole purpose of the multi-sig is to make sure capital is not being siphoned. However, the allocator is encouraged to make independent decisions.\nEvery quarter, the foundation & the community shall evaluate the performance of each of the allocators using publicly available data. The allocators can have one of the following outcomes from this review\n\nDiscontinued domain: The grants manager (initially Questbook), will share the required voting to discontinue a particular domain, once the domains are finalized along with the compound team.\nIncreased budget\nReplaced allocators: For replacement of allocators, or the resigned allocators, the grant manager (initially Questbook) will find the new allocator from the community, after discussing with the compound team. We are also working on making this process more decentralised.\n\nThe community members with certain credentials can also initiate a no-confidence motion to initiate a review off-cycle.\n\nProduct Screens\nPerformance Tracking\n\nGrants DAO Dashboard 12880×2312 351 KB\n\nEdit Member’s access control:\n\nScreenshot 2022-06-22 at 12.18.23 AM (1)1870×1032 116 KB\n\nInvite and Assign Reviewers\n\nReview2880×1600 283 KB\n\nApplication Review Screen:\n\nApplication2880×2968 368 KB\n\n\nProduct Demo Link: Demo Video 8\n\n\nCommittee\n\ndelegdom (1)701×271 25.6 KB\n\nWe have also identified various domains that are relevant to Compound. The committee members recommended below are also the ones best suited to evaluate applicants for the mentioned RFPs, evaluating them on axes like community participation on forums, discord and social media.\nWe propose the following set of individuals to run one domain allocator for each, we’re open to more domain, alterations to the suggestions and prioritization :\n\n\n\n\nDomain\nCredentials Needed\nIndividual / Orgs\nWhy it is relevant\n\n\n\n\nSecurity Analysis Tools, Security Bounties\nDone 100 Security Audits\nCertora,\nMore the security of the protocol, lesser the requirement from DApp developers to worry about security in their products\n\n\nRisk parameter update research\n\nPaul J, Gauntlet team\nCrowd sourcing  how and why the risk parameters should be adjusted\n\n\nDeveloper tooling\nCore contributor to Compound and/or popular web3 Defi applications\nArr00\nTools and libraries like hardhat plugins, gas optimizing libraries etc\n\n\nPrivacy-preserving transactionsI\nDeployed and audited zkSNARK, zkSTARK contracts\nTBD\nIncrease scale of users participating in compound because of privacy preservation\n\n\nMultichain Strategy\nDeployed 100 Solidity contracts\nTyler Ether\nWill help compound roll out to multiple chains with appropriate caps\n\n\nOpen Oracle\n\nGeoff Hayes\n\n\n\nCoding bootcamp\nDemonstrated technical writing and workshop organizing skills\nTBD\nScale DeFi contributors to Compound\n\n\nNew protocol ideas and dapps\nCore contributor to Compound Github\nAdam\nEncourage more people to do experiments and compose on Compound\n\n\n\n\nDomain Allocator Roles & Responsibilities:\nThe roles and responsibilities would be similar to the one used in CGP 1.0 7 incorporating the learnings from the learnings posted by the CGP 1.0 team here 3.\n\nTime commitment per week: Estimated 10 hours, might fluctuate based on the number of applications.\nReviewing applications: The grant manager (Inititally Questbook) will ensure a 2 week turnaround time for the applicants.\nAllocation of amount for the accepted grant proposals\nUsing the questbook tool to disburse the funds. It’s seamless, you can check it out here 1:\n\nQuestbook will proactively manage the tasks of the domain allocators, eventually this manager will be elected from the community itself.\n\nCompensation:\n\nDomain Allocator is paid at $10 an hour, maxed at $500 a week.\nGrant manager is paid at $15 an hour, maxed at $800 a week.\n\nAssuming 5 domains to start with, and a grant manager: We’re looking at ~40K operation cost for a quarter.  We are open to iterating this amount post the inputs from the community.\n\nConclusion\n\nWhat does success look like?\n\nObjective\n\nThe prime objective of this model is to have domains that align with the. compound’s priorities like Risk, Security etc. This way the contribution of the projects as part of the grants program is directly adding value to the DAO and the token holders.\nIncrease in number of builders & projects being funded\nIncrease in homegrown leadership to run grant programs (measured by the number of people running grant programs)\nIncrease in community members’ participation to keep grant programs accountable (measured by the number of people looking at the dashboard and participating in no-confidence motion votes)\nDiversity in projects being funded - across technologies, geographies, demographics, etc. We encourage the community to regularly review the project domains, during the compound developer call and also posting on comp.xyz 2 . The grants team will also propose few domains like public goods, if there is a major traction, we will double down by allocating a % of funds to such domains.\nIncreased engagement in builder communities\n\nDiscourse\nDiscord, Telegram\nSocial media (Twitter, Reddit)\nGitHub, Radicle\nDAO Tools - dework, gnosis, etc.\n\n\n\n\nSubjective\n\nAn increase in community involvement keeps the grant program accountable\nBuilders’ sentiment toward Compound\nCompound’s brand recognition in builder circles\n\n\nAbout Questbook & Compensation:\nCredentials\n\nQuestbook (YC-W21) is a decentralized grant orchestration tool, currently being used by Polygon, AAVE, Celo & Solana.\nSriharsha Karamchati (co-founder, Questbook) will initially take the role of the grants manager till we find the manager from the community. He has previously setup the Polygon Grants program from scratch to help DAO disburse $600K across 70+ applications and he is one of the signatories for the Polygon Grants Fund.\n\nCompensation\n\nWe are willing to share the tool for free with the compound labs.\nHowever, for any specific asks from the grants team in order to run the process more smoothly, we charge for the additional features based on the development overhead. From our previous work experience: We propose a budget of $50K to be kept aside for specific features.\n\n\nCommunity buy-in and next steps\nThis is a temp check to gather interest to execute such a grant program during the bear market. If the proposal is accepted by the community, we will chart out a more detailed plan on how to setup a domain allocators and the processes to review their performance.\nWe will also be involving more members from the community to identify the right domain allocators and the top 5 domains that are most relevant to Compound.\\nWe have listing standards 2, but these standards have not been successful in listing any new assets. I support proposing a fair standard for evaluating the priority of public chains, or publicly allowing public chains to provide matching incentives to compete and improve their priority, but I think the first support for public chains needs a deadline, within a month or within a quarter?\\nSummary of the first quarter\n\nTotal Proposals Received: 75\n29 proposals approved [38.6%]\n15 applications first milestone completed\n2 2nd milestone completed\n1  3rd milestone completed.\n\nThe total amount of funds committed\n\nProtocol ideas and dapps by @allthecolors : $247K\nSecurity tooling by @cylon : $100K\nMultichain Strategy by @Bobbay_StableNode : $82K\nDev Tooling by @madhavanmalolan : $22K\n\nTotal amount of funds disbursed\n\nProtocol ideas and dapps by @allthecolors : $144K\nSecurity tooling by @cylon : $16.25K\nMultichain Strategy by @Bobbay_StableNode : $25K\nDev Tooling by @madhavanmalolan : $16K\n\nUpdates\n\nDoo will be representing stablelab for the second quarter of the grants program instead of Bobby.\nThe allocation of funds for the domains in the second quarter will be proportional to the approved grant budgets in the first quarter.\n\nDetails, feedback and progress of all the proposals can be viewed here: https://questbook.app/ 10\\nThank you so much Sam for taking the time to review the proposal thoroughly and sharing your deep insights. \\nthank you for your tooling. we could have used it back in the day, but i am glad the future looks brighter\\nThank you. I think there should an exclusive domain for dApps/Protocol built on top of Compound.\nI’m thinking of something that can help solve issues and at the same time, benefits Compound by increasing borrowing/lending volume a significant percentage.\\nWe’ll surely consider having dapps as one of the domain Charlie. We’re planning to have an exclusive voting for selecting the domains soon.\\nThis looks a very promising proposal for the compound ecosystem \\nVery detailed proposal.\\ndomain allocators are the way to go! it’s a huge success with Ethereum.\nlove the proposal\nReopening compound’s grants program is much needed and would be very good for the community right now!\\nExcellent, great to see the fruits of Questbook’s investment in learning the ins and outs of Compound’s needs and experience with CGP 1.0, which has led to this detailed and compelling proposition. I agree with @monetsupply’s recommendation to bring compensation for program stewards to a level that reflects their responsibilities and experience.\nAs critical as privacy-preserving transactions are to the future development of money market protcols on public blockchains, it’s hard for me to see that domain rising to the top 5. However, if the community does decide to prioritize this domain, I would be open to taking on the allocator role for this domain on the basis of my experience contributing to Aztec’s bridge to Compound.\\nWhat do you think should be the credentials of the dApp domain allocator. @Charlie_Danern ? . Do you have someone in mind from the community who can potentially take up such a role?\\nThank you so much for coming forward with the domain suggestion, and contesting to be the domain allocator @allthecolors . We will definitely consider this domain while we take a vote on the domains. If prioritized, we can discuss your experience in this category for being the allocator. \\nThank you for this input @monet-supply . We will work on this and come up with the updated compensation number in the next iteration of this proposal. \\nhmmm, maybe being advisor either Marketing or Technical Smart contract for several projects? I’m not so sure, but I think they should be able to detect great ideas that could potential work.\nas for allocator, i don’t know since i only joined the community recently \\nGreat job with this proposal @harsha! It’s extremely well thought out and it addresses the pain points of the first grants program. I very much support this new grants program!\nThe only problem that I see is the compensation, but this is more so up to the Compound DAO in how much it is willing to spend.\\nThank you so much @TylerEther for those kind words & support. Yes, we’re working on updating the compensation and will take a vote before concluding on the final number.\\nNice to see the grants program being re-activated. Props to everyone involved!\\nI think this is a really cool initiative, and I definitely think that we really need a new Grants Program. I’m not sure about the approach (personally I would like to see a lightweight/cheap DAO voting mechanism for the whole community to participate in as opposed to pushing the decision-making to individuals), but it seems like a worthy experiment for 2 quarters, and the alternative right now is nothing. I don’t think the proposed set of domains and allocators makes a ton of sense however, and I’d love us to spend more time discussing that aspect of it. I’m glad to see that that’s the plan. I’d also be thrilled to see @allthecolors and @monet-supply supply on the list!\\nThank you so much for the support \\nAbsolutely, I will submit an iteration of the proposal this week to include updated compensation, domains, allocators, and the detailed processes/roles of the allocators. I will keep you posted on this thread  @0x7751  thanks for the support \\nThank you so much @jared for the support and feedback. I totally agree that we should be working towards a lightweight voting mechanism that involves the entire community to participate. The concept of delegated allocation to ensure builders from the community participate to give out the right grants, is a step intended to be in that direction. Hopefully, the learnings out of this 2 quarter experiment, should help us to drive towards it.\nRegarding the domains, I will submit an iteration of the proposal this week to include updated compensation, domains, allocators, and the detailed processes/roles of the allocators. Before this, I am figuring out a way with Adam to have a detailed discussion around the potential domains with the community. Yet to finalize if it would be an exclusive grants call or if it would be on the community developer call itself.  I’ll keep you posted. Thank you once again for taking time to go through the proposal \\nHi everyone,\nWe are going to have an exclusive call on CGP 2.0 on 15th July, 9:30 AM PT on the grants voice channel!! Link to the event: Compound 4\nIn reference to the earlier proposal submitted here: CGP 2.0 - Delegated Domain Allocation by Questbook - #4 by massnomis\nWe will be discussing the following:\n\nPotential Domains\nPotential Allocators\nUpdated compensation\n\nMost importantly, we will be brainstorming inputs from Community on the new grants process! I kindly request everyone to review the proposal on the forum before joining the call.  Looking forward! \n\nCompound Grants Program 2.0 (1)1080×1080 44.2 KB\n\\nI like it a lot.\nMy only request will be to incorporate and allocate someone from the Compound Labs team as a project guide. Project guide can spend between 30 mins - 1 hr with each of their projects every other week.\nThis will help in making sure the grantees can deliver stuff that is more relevant\\nThat is a really good suggestion Robin. Based on your suggestion, I have added more specifics to the role of a compound lab member in the iteration of the proposal. The role, will not be restricted to ensuring the alignment of a grant project with the core vision. But also, provides relevant guidance on a need basis. Arranges AMA calls, and sessions with the applicants on a bi-weekly basis to address the concerns.  You will see these inputs in the next iteration of the proposal in a couple of days. Thank you once again @robinnagpal\\nAs discussed on the grants call, I request everyone here in the community to please select the important domains which you would want to see as part of this grants program. This should help us finalize the 5 domains, and move towards finalizing the domain allocator accordingly.\n53%New protocol ideas and dapps20%Miscellaneous13%Multichain Strategy6%Coding bootcamp6%Open Oracle0%Developer tooling0%Risk parameter update research0%Security Analysis Tools, Security Bounties15voters15total votes\\nThe amounts proposed seem rather low based on the research I have done for similar programs.\nI would recommend a model that compensates more in line with the rates from other programs.  I can provide examples of these rates but, at a high level, this is in line with what other programs like Aave/Uniswap pay (some of these programs also offer a flat rate for certain roles and hourly for others)\nDomain Allocator (DA) : $75/hr\nGrant Manager (GM): $125/hr\nCompound Labs Member (CL): $100hr\nOne callout - I am not sure how the hours estimates that are provided here compare to other programs so there may be some differences that way in what is proposed here versus the programs I am comparing to.\nHope this helps!\\nThis is the new proposed compensation for the grant committee based on the community voting & the research on Uniswap and AAVE grant programs. - h/t: @Sov , @harshapakshi  thanks for the inputs! \n\nScreenshot 2022-07-25 at 2.33.10 PM1870×330 93.5 KB\n\\nHey, this is much better than the previous. Much better compensation for those putting in the hours!\\nLead: $150 an hour with a 30 hour/week cap\nReview committee: $150 an hour with a 10 hour/week cap\nDesigner: one-off based on specific design engagements\nOperations lead: $3.5k/month\nCommunity manager: $6k/month\nThese are the compensation rates for Aave team.\ncomparatively, the newly proposed compensation is pretty good. great job @harsha\\nI think for a test these proposed amounts make a lot of sense. We want highly motivated individuals to apply - not those looking for purely salaries. Of course compensation can be raised later on.\nIt’s way harder to lower comp than raise it.\\nThank you so much for this note @adam and your kind words. Excited to know the support from the Compound Labs team! I have taken these policies into consideration to come up with the iteration of the proposal and structuring. Will be sharing the final draft once the domain allocators are finalized. Thank you!\\nAbsolutely, that’s where the current efforts are going into. Finding the right domain allocators. Please do suggest a few names with whom we should speak to. That’d be really helpful!\\nI would make an open form that community members can share, that also includes a referral section. This is what I have seen work in previous similar instances.\\nHi everyone! We are currently hunting for domain allocators for this program. If you think you would be the right person for any of the domains, please fill up the form, and we’ll connect with you. Feel free to refer someone from the community too, if you think they might be a good fit.  Thank you for the support!\nLink to fill up the form: https://forms.gle/W9986NiMfp96VeGj9 71\\nHey - I’m @harsha 's cofounder at Questbook. Wanted to give an update - below is a blurb from Harsha :\n\nI have undergone surgery last week, and am away from the desk for a couple of weeks.\nWe are almost done with the interviews for the Domain Allocators. 2 more to go. We will be going for the voting on this as a proposal next week. The proposal will be open for voting for a week. Sorry about the delay here.\n\\nThanks for writing this Roger.\n1/ Domain Allocator weekly effort estimate and time commitments:\n\nThe roles and responsibilities of the domain allocators are similar to CGP 1.0 and incorporated the learnings from the learnings posted by the CGP 1.0 team.\nIf the volume of applications is much higher than expected, it is the grant manager’s duty to ensure that the workload is distributed. He/she might either distribute it to the existing domain allocators based on their expertise or add an additional domain allocator from the community to capture the overflow of the applications.\nThe teams collaborate online only, but on a regular basis. There would be a fixed schedule decided as per the convenience of all the team members, to discuss the progress and alterations to the process.\n\n2/ Scope of the “Risk Parameters update research” Domain:\n\nI have an update here, I have discussed this domain with @pauljlei - Protocol Program Manager at Gauntlet as part of Domain Allocator interviews, and we have decided to not have “Risk parameter update research” as one of the domains, since the team is already serving the DAO, conducting research on Risk Parameters and managing market risk for Compound,\n\nHope this answers your questions. Sorry for the delay in the response.\\nI’ve had a chance to talk with the Questbook team surrounding their grants program, and I look forward to seeing it implemented within Compound. I am currently an active delegate on behalf of StableNode within Optimism in which token holders vote to provide grants to protocols and have seen many issues in transparency and efficiency.\nGrant delegation and transparency are two key issues that need improvement, and hopefully, Questbook will provide those solutions.\\nHi everyone, here are the final updates on this proposal, before we go for the voting!\nFirstly, apologies for the delay, as I was away from work for a couple of weeks due to my medical treatment and it took some time to schedule all the calls with the domain allocator applicants.\nPlease find the updates as below:\n\n\nDomain Allocators\nThanks to the tweet 9 from @rleshner we had 20+ applications for the domain allocator roles. Post a thorough evaluation, the following domain allocators have been chosen as below.\n\n\nDomain: New protocol ideas and dapps, Miscellaneous\n\nAllocator: allthecolors (@allthecolors )\nDetails 14\n\n\n\nDomain: Security Analysis Tools & Security Bounties\n\nAllocator: Michael from Openzepplin (@cylon)\nDetails 5\n\n\n\nDomain: Developer Tooling\n\nAllocator: Madhavan, Questbook’s CEO (Pro-bono basis)\nDetails 4\nP.S: Madhavan is operating as an interim Domain Allocator for “Developer Tooling” since we did not find any qualified applicant yet. This role is still open for applications.\n\n\n\nDomain: Multichain Strategy\n\nAllocator : Bobby Bola from Stablenode ( @Bobbay_StableNode)\nDetails 9\n\n\n\n\n\nDomains\n\n\nThe finalized domains for the grants program are as follows, as per the voting results:\n\nNew protocol ideas and dapps\nSecurity Analysis Tools & Security Bounties\nDeveloper Tooling\nMultichain Strategy\n\nThough “Risk Parameters Updates Research” has been one of the top-5 voted domains from the community, we will not have it as one of the domains. Post the conversation with @pauljlei, Protocol Program Manager at Gauntlet as part of Domain Allocator interviews, we have decided not to have “Risk parameter update research” as one of the domains, since the team is already serving the DAO, conducting research on Risk Parameters and managing market risk for Compound.\n\n\nChanges in the Committee Org Structure:\nAs mentioned by @adam here and as per the guidelines 1 of the compound labs team, the organization structure of the grants program had to be changed. The onus is now on the program manager to communicate regarding the approved projects to the community and the lab’s team through bi-weekly community developer calls and regular communication over discord.\nSo there will be a Grants SAFE, with 3/5 multi-sig, between the program manager and the 4 domain allocators. We will then have 4 SAFEs for each of the domains with a 2/2 between the program manager and the specific domain allocator.\nGrants SAFE is where all the initial funds flow from the treasury for the grants program. This SAFE holds funds related to operational costs, committee compensation, and the grants budget initially. The domain level SAFEs will have funds only for the disbursal to the approved applications\n\n\n\nProgram Manager (1)1920×1080 108 KB\n\n\n\nBudget and Committee Compensation:\nThere are a couple of updates from the previously discussed compensation here:\n\nThe number of domains and domain allocators is reduced to 4 from the initially proposed 5.\nCompound Labs cannot take up a paid role.\n\nTherefore the updated grant budget is $1.2M with $300K for each domain.\nThe updated committee compensation is $200K for 4 Domain Allocators and one Program manager.\nP.S: Madhavan from Questbook will be operating on a pro-bono basis as an interim domain allocator for “Developer Tooling”, till we find another qualified domain allocator.\n\n\n\nScreenshot 2022-10-02 at 2.01.44 AM1970×274 85 KB\n\nThank you,\nHarsha\\nThe updates to the proposal look good to me.  Excited to see if this moves forward from here.\\nUpdate on Legal Compliances of CGP 2.0:\nWe’re working with BLG 2 - Blockchain Lawyers Group for the CGP 2.0 legal compliances.  Among the founding members of this group are:\n\n\neaglelex 1: Law Professor and EU licensed Attorney, Polygon Technology Advisor, Legal advisor for the Polygon Grants Program.\n\nMDLawyer 1: Corporate and Venture Capital Lawyer in London\n\nByianMienert 3: Crypto Lawyer, first-ever Ph.D. written on DAO law\n\nLarryFlorio: GC Delphia; Corporate Lawyer with Securities expertise\n\nThe BLG independent Legal Advisors will support CGP 2.0 for the contractual drafting, the application of KYC standards, and every legal issue that may arise during the grant disbursement phase. They will also be available for early-stage legal support to the selected projects.\nAccepted grant applicants of CGP 2.0 will sign a contract (sample 11) post the KYC to ensure that the grant fund isn’t misused. BLG will receive a contribution of $3K USD for the above-mentioned tasks, payment will be made from the operation fund budget received as part of this program. We are working with Synaps 5 for the KYC/KYB of the grant applicants.\\nThank you for the updates and consistent progress!\\nTalked with the team at Questbook and we’re excited to see this come to a vote! We missed the CGP 1.0 but are excited to see this newer revamped program come live!\\nI’m excited to see this grant proposal come to a vote so Compound can start building more security tooling as part of the domain I’ll be managing!\\nWhat’s cooking here, any progress?\\nawesome! looking forward to it.\\nThanks for asking these questions @rleshner , please find the responses below:\n\nOver the course of the original grants program, there was a shift towards “RFP” and deliverable based funding. This helped ensure that grants went towards outcomes, instead of being delivered up-front with varying (and in many cases, very little) progress. Beyond the contract (above), how will CGP 2.0 ensure that grant recipients deliver on their stated goals? Will there be presentations on community calls, progress reports, or incremental disbursements?\n\nSpot on. To precisely address the same issue, we would be giving out Milestone-based payouts through the tool. The entire grant amount would not be disbursed immediately when an application gets approved. The payouts will be done per milestone. Please find the detailed product wiki for making milestone-based payments here. 3\nMilestone-based payout product screens for the allocators:\n\nScreenshot 2022-11-15 at 10.57.49 PM940×1200 79.5 KB\n\n\nScreenshot 2022-11-15 at 10.58.11 PM894×910 162 KB\n\nThe grant proposal has a structure where the applicant must share the milestones and the deliverables to receive the specific payouts.\nGrant Application Sample:\n\nApplication2880×2968 368 KB\n\nEach approved applicant will be required to share their work & progress with the community on regular grants calls (Will be setup exclusively apart from the compound community dev calls if there are too many grant applications). They also use the grants channel on discord to share their work and take the feedback or support from the community.\n\n\nThe original grants program returned approximately half of its allocated COMP back to the community as unused after six months. If a domain is unallocated after six months, will it’s unused portion be returned to the community?\n\n\nThat’s right, all the remaining funds from the grants program will be returned to the treasury at the end of the program. Also, if there is a domain that is seeing a lot of applications over the other, we might be shifting the funds between the domains accordingly on a case-to-case basis.\n\n\nAs grants manager, how will the program be marketed? Will you be able to take over the compoundgrants 1 Twitter account and website, etc?\n\n\nIf it is possible to do that, that would be great. Since there is an existing audience for the compound grants twitter page already, Would love to take it over and re-activate it.  We planned to do twitter spaces with grant applicants and reviewers and also spread the word in the developer communities regularly. The chosen domain allocators have also mentioned how they would spread the word and source applications in their one-pagers. The Questbook tool also has 20K MAU who come to the tool to apply for the grants programs.\n\nScreenshot 2022-11-15 at 10.31.47 PM832×846 62.5 KB\n\n Ref: https://www.similarweb.com/website/questbook.app/#traffic 1\n\nHow will the time spent by the allocators and manager be tracked?\n\nThe allocators and the manager clock their hours and submit their weekly reports, which can be publicly viewed. This report has specific details like:\n\nProposal reviewed | Hours clocked\nActivities performed to market the grants program | Hours clocked\nMiscellaneous | Hours clocked\n\n\nIn general, I would strongly suggest a smaller initial budget for each domain (e.g. $200k over 6 months) knowing that the community can easily increase funding for any domain on an as-needed basis, especially if it is showing results.\n\nWe understand the suggestion of $200K instead of $300K for the initial budget for each domain for 6 months Robert, and we agree to it. Will update the proposal and in case more funds are needed, we will request them accordingly.\nMost of the concerns here are addressed in the Delegator Onboarding Documentation which will be shared once the program is live. Have shared the relevant parts above for now.\\nUpdate on the voting dates and the SAFE.\n\n\nThe proposal will be put up for voting on November 28th, which then goes through the voting cycle 3.The proposal link will be shared here once it is live. The dates of posting have been pushed keeping the Thanksgiving holidays in mind to ensure maximum voter turnout on the proposal.\n\n\nCGP 2.0 Grants SAFE (eth:0x8524B12CB7710C75B53bAa9ca72B420542d24C13) has been set up and the domain allocators : @allthecolors , @cylon ,  @Bobbay_StableNode , @madhavanmalolan , have been added as the multisigs. The allocators will also be commenting on this post from their profiles, to prove their ownership.\n\n\n\nimage (6)2260×1238 219 KB\n\\nThanks @harsha I’m hereby confirming my ownership over the address : 0xa2dDFc8a6C1F8868B80F2747D04532a6cDE9804d\nHere’s my signature\n{\n  \"address\": \"0xa2ddfc8a6c1f8868b80f2747d04532a6cde9804d\",\n  \"msg\": \"0x6d6164686176616e6d616c6f6c616e20666f7220636770322e30\",\n  \"sig\": \"491e61c56e6e5a22932fec4cc6b75e1aac12baa184ff653221c53e3dab843ad5053d8a78a38e153555069b50bd7f0b906c998cd3077a1fdb894849495ed5838c1c\",\n  \"version\": \"3\",\n  \"signer\": \"MEW\"\n}\n\\nConfirming that I control the key for the address 0x66cd62c6f8a4bb0cd8720488bcbd1a6221b765f9, to which allthecolors.eth maps.\n{\n“address”: “0x66cd62c6f8a4bb0cd8720488bcbd1a6221b765f9”,\n“msg”: “0x4920616d2040616c6c746865636f6c6f7273206f6e2074686520436f6d706f756e6420666f72756d2c2070656e64696e6720646f6d61696e20616c6c6f6361746f7220666f722043475020322e302c20616e6420492068657265627920636f6e6669726d2074686174204920636f6e74726f6c207468652070726976617465206b657920666f722074686520657468657265756d206164647265737320307836366344363263364638413442423043643837323034383842434264314136323231423736354639”,\n“sig”: “dab707c3d1b251f818178660c1804874a695dbad0dcb42f1eface38e07387ff0285c9a6327e01d51d1c2d85421e1824640ddb3c2a94747d68754613c1f2370591c”,\n“version”: “3”,\n“signer”: “MEW”\n}\\nConfirming my ownership of 0x98fdE0e52fd38eeE6D319B3E45bcaFF48237384c as a signer.\n{\n  \"address\": \"0x98fde0e52fd38eee6d319b3e45bcaff48237384c\",\n  \"msg\": \"0x4d69636861656c204c6577656c6c656e20666f722043475020322e30204d756c74692d7369672061743a20307838353234423132434237373130433735423533624161396361373242343230353432643234433133\",\n  \"sig\": \"84d11fff060e93ba720025c649efba06193f10111a223764040bba8709fd3b383386711545ac07c57c64c05f57a7df94b6e3cf7ff835ead6095567ee773dae6a1c\",\n  \"version\": \"3\",\n  \"signer\": \"MEW\"\n}\n\\nI control the key for stablelab.eth, which is 0xea172676E4105e92Cc52DBf45fD93b274eC96676.\n{\n“address”: “0xea172676e4105e92cc52dbf45fd93b274ec96676”,\n“msg”: “0x4920636f6d6669726d204920636f6e74726f6c20746865206b657920666f7220737461626c656c61622e6574682c20776869636820697320307865613137323637364534313035653932436335324442663435664439336232373465433936363736”,\n“sig”: “30b35ebeb92e1ae21ed7a266ecfdb04ee4cca0db79a89ba9ebda4f112c0729e914deb565252bfad1058b07d1c886a736eeb87b08d03638ebbc0fa9273a7992f000”,\n“version”: “3”,\n“signer”: “MEW”\n}\\nHere are my questions:\n\n\nWhich entity on the DAO side will be signing the contract (via the contract template that was proposed)?  Unless there is a legal entity representing the grantors, I don’t see how this would work.\n\n\nHow will Questbook’s compensation be working now (seems to be pro bono initially) and going forward?\n\n\nDoes the need for a grants program to be more decentralized justify this significantly higher cost? My initial thought is that it does not and that there are efficiencies gained from the standard centralized grants committee model that might be lost here (e.g., having a single committee review multiple grant proposals might garner better conversation and review).\n\n\nI think the tooling that Questbook offers is helpful, but couldn’t the same tooling be provided to a more centralized grants committee?\n\n\nWhat is the KYC for potential grant recipients meant to accomplish?\n\n\\nConfirming that I control the key for 0x3bf8b0acc8db7a8958d7f0ca0ead9e5bf4a47130\nSignature as below:\n{\n  \"address\": \"0x3bf8b0acc8db7a8958d7f0ca0ead9e5bf4a47130\",\n  \"msg\": \"0x486172736861206f6e20626568616c66206f66205175657374626f6f6b20434750322e30207465616d20\",\n  \"sig\": \"12937027e00d9d47b9fa072da9e03c79b8c3bea3853db998e597de46d4f71e6d1e91d152911ac8e0cb1cee1d39df4f32405a4800f53b9b4d292929a5651691d61c\",\n  \"version\": \"3\",\n  \"signer\": \"MEW\"\n}\n\\nThe proposal is up now!! 16\nThank you, everyone, for patiently following up on this proposal, and thank you @adam for all the assistance to help me navigate through the app. I’ll update here once the voting begins, it is currently in the review phase. This is the tentative voting cycle 3\\nVery excited to have this going!\\nThanks.  That all makes sense and is helpful.  I’ll vote for this proposal.\nI’m generally supportive of better tooling and decentralization.\nI’d be interested to know more about the KYC requirements and what will be charged for services surrounding that – are you referring to the need to screen under the OFAC list, etc.?\\nHi, I am part of the legal team which is assisting Questbook.\nThe KYC provider will apply the usual standards concerning the evaluation of risks (UBO, geographic area, type of activity). We want to avoid that Compound or Questbook get associated with illicit activities. That’s all!\nIt’s becoming a common standard within grants programs. Nothing new. Please tell me if you need further information.\\nThe proposal is live for voting now! 16\nIf you hold COMP, please vote. The voting ends in the next 48 hours. Thank you. \\n\nScreenshot 2022-12-03 at 12.03.06 PM2158×958 159 KB\n\nUpdate: The proposal has passed! 15\nThank you everyone for your consistent support throughout. \nWe are putting our heads down and working towards what we have promised.\nWill be sharing the next plan of action very soon on this thread.\nThanks once again for the trust the community has put in us.  \\nUpdate: Confirming here that the transaction is executed, and the tokens have been transferred to the SAFE.\n\nScreenshot 2022-12-06 at 3.54.31 AM1466×222 24.1 KB\n\\nAwesome to hear. Will reapply as we know previous weren’t seen. Thanks for all the dedication!\\nHere are the one-pagers from the domain allocators about their domains for CGP 2.0. The program is going live on 16th January, please feel free to share any feedback you have on any of the programs here. These documents have comment access to everyone with the link. Thank you!\n\n\nNew Protocol ideas and Dapps 8 by @allthecolors\n\n\nSecurity tooling 4 by @cylon\n\n\nDeveloper tooling 4 by @madhavanmalolan   :\n\nMultichain Strategy 6 by @Bobbay_StableNode\n\n\\nFor some ideas of multichain strategy, the deployment priority should be as follows:\nArbitrum>Polygon>Optimism>Avalanche>GnosisChain\nFor the above public chain, no introductory discussion is made and the topic is directly addressed.\\nThat’s a fair point, chains can provide incentives to provide a more attractive option, but it boils down to various factors; chain stats, team behind deployment, matching incentives, technical aspect of the chain etc.\nFortunately, these discussions will all occur publicly so we can discuss our preference to deploy Compound III on certain chains.\nA deadline to deploy compound on another chain? We don’t even have a process in place yet, so I don’t think we should rush this, by setting an arbitrary deadline. We rather focus on creating a sustainable process.\\nAave discussed its multichain strategy in February 2021, successfully deployed to Polygon in March, and deployed to Avalanche in October.\nCompound multichain strategy 4 proposal was presented in January 2022, but has yet to be truly implemented. Now, 12 months have passed and we are still in the process of determining standards and procedures.\nI am not really asking for a deadline and I do not think that Compound is worse than Aave. I think that efficiency should be valued, and we have delayed for too long. I look forward to your efficient work.\\nAs far as I know, I haven’t seen anyone initiate the process to create standards and procedures for going multichain until now. I’m sure we will implement this process swiftly, and Compound deployed on other chains.\nI will release a CIP tomorrow or Monday to simplify the multi-chain deployment process. I look forward to seeing your feedback on that, and feel free to reach out to me if you know anyone who wants to get involved in deploying Compound on another chain!\\nAll domains are now live and accepting proposals!\nProtocol Ideas and dApps 11 : Proposal form link 9\nDev Tooling 5: Proposal form link 3\nMulti-chain Strategy 5 : Proposal form link 4\nSecurity Tooling 5 : Proposal form link 2\\n\n\n\n Bobbay_StableNode:\n\nI control the key for stablelab.eth, which is 0xea172676E4105e92Cc52DBf45fD93b274eC96676.\n\n\nHello this is Doo from StableLab. As an update,\nWe would like to receive funds to the address 0x9c489E4efba90A67299C1097a8628e233C33BB7B\nAnd we confirm we control this address\n\n  \n      \n\n      etherscan.io\n  \n\n  \n    \n\nEthereum Verified Signed Message 1\n\n  The Ethereum BlockChain Explorer, API and Analytics Platform\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\\nPosting another confirmation from stablelab.eth, which is 0xea172676E4105e92Cc52DBf45fD93b274eC96676 to change the address to receive funds to 0x9c489E4efba90A67299C1097a8628e233C33BB7B\n\n  \n      \n\n      etherscan.io\n  \n\n  \n    \n\nEthereum Verified Signed Message 1\n\n  The Ethereum BlockChain Explorer, API and Analytics Platform\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\\nUpdate:\nHi everyone, as we are nearing the end of CGP-2 we are closing the applications on June 30th.  The TAT for these applications will be ~2 weeks, you’d receive the response on these applications before July 16th.\nAll unallocated funds (allocated funds = committed funds for the approved grant applications.) will be returned to the Treasury at the end of the program.\nWe will be having a demo day of all the approved grant applicants soon. Stay tuned  Thank you, everyone, for the kind support throughout! \\nWhat are the plans for after CGP 2.0? Will there be a request for 3.0?\\nThank you for your inquiry. At the moment, CGP 2.0 team is consolidating all the feedback and grantees’ project progress. We would like to evaluate the impact and incorporate the learnings from this program before proposing the next one. We will be sharing the renewal proposal with the Compound community soon.\\nUpdate:\nHi everyone, as we approach the end of CGP 2.0 after running it successfully for two quarters, we are excited to announce that the CGP 2.0 grants team is organising a demo day for all CGP 2.0 grantees. The demo day is scheduled for July 21st, 2023, at 10 AM PST and will take place in Compound’s Community Discord.\nThe primary objective of the demo day is to increase visibility for the accepted proposals and provide a platform for all grantees to present and showcase their projects. We kindly request all grantees to confirm their availability and inform us if they are unable to attend by replying to the announcement email we have sent to all grantees. If you have any questions, you can contact me or harsha at @sriharshakaramchati on TG or Discord.\nThank you, everyone, for the kind support and guidance throughout! \\nUpdate: The address for receiving the funds for the program manager for the second quarter is: 0x0a64f452D41991e3fDC5e1a45aCb94C7b5faF32A,\nFor the first quarter, it was 0x3bf8b0acc8db7a8958d7f0ca0ead9e5bf4a47130. We confirm that we own both the addresses. Signature as below:\n{\n  \"address\": \"0xa2dDFc8a6C1F8868B80F2747D04532a6cDE9804d\",\n  \"msg\": \"x19Ethereum Signed Message:\\nSigning for CPG Dev Tooling Safe at timestamp: 1692817396795\",\n  \"sig\":\" 0x444626d2123b559bb83b948a0306ded6424fe6fcef8cfece3852b2d50408540c57cfd2eb4b2474\n63c190f2305b64ae98332e4cc6370cd0f1702f5c946c5bebe01b\",\n  \"timestamp\": \"Wed Aug 23 2023 12:03:22 GMT-0700 (Pacific Daylight Time)\",\n  \n}\n\\nI spoke harsha about this proposal, and i have to say this is incredibly robust and well researched. This will be a great legacy for all of us \\n\n\n\n harsha:\n\nCompensation:\n\nDomain Allocator is paid at $10 an hour, maxed at $500 a week.\nGrant manager is paid at $15 an hour, maxed at $800 a week.\n\n\n\n\nConsidering that allocators/managers are not just reviewing/signing multisig transactions but have significant responsibilities within the process, I think higher compensation is appropriate. Otherwise I think the proposal looks pretty good, no other feedback at the moment \\nI’m generally supportive of someone taking a swing at refreshing Compound Grants, and appreciate the effort that went into this post.\n\n\n\n harsha:\n\nThis is a temp check to gather interest to execute such a grant program during the bear market. If the proposal is accepted by the community, we will chart out a more detailed plan on how to setup a domain allocators and the processes to review their performance.\n\n\nAs always, the devil will be in the details. I’d encourage you to move forward with scoping this out further, and am happy to be a sounding board as you do so.\\nAnother part of the proposal where the community had strong differing opinions was on team compensation. Proposed compensation of $100K per 2 quarters in the initial proposal, was pointed out as very low, by a lot of community members. Hence presenting the updated compensation options below. I request you to kindly vote on the option below as per your judgment.\nInitial Proposal: $100K for 6 months (24 weeks).\nDomain Allocator (DA) : 500 x 24 = 12000 [$10/hr , Max $500 per week]\nTherefore 5 DAs = 5 x 12000 = 60000 ($60K)\nGrant Manager (GM): 800 x 24 = 19200 ($19.2K) [$15/hr, Max $800 per week]\nCompound Labs Member (CL): CL: 500 x 24 = 12000 ($12K) [$10/hr, Max $500 per week]\nMiscillineous: $10K\n46%$250K for 6 months (24 weeks): 2.5 x (Intially proposed Amount)31%$100K for 6 months (24 weeks) - Intial proposed Amount23%$200K for 6 months (24 weeks): 2 x (Intially proposed Amount)0%$150K for 6 months (24 weeks): 1.5 x (Intially proposed Amount)13voters\\nHi Harsha. The Compound Labs team discussed our participation in the grants program internally. Each and every one of us is excited and supportive of a reignition of the program!\nI think this is exactly what the ecosystem needs in the long term. Seeing the community decide that this is a priority in a grass-roots manner is awesome. We are flattered that you envision that members of our team are a good fit for the domain allocator roles.\nWhen we created the system for community ownership back in 2020, we thought about decentralization and resilience. We asked ourselves: how do we make it so that if the Compound Labs team disappeared tomorrow, the protocol could still operate and be upgraded in perpetuity? We decided to take a step back and enable the community to take charge 15.\nWith this in mind, we have decided that members of Compound Labs can individually participate, so long as they are a minority of the allocators, and that participation is done on a voluntarily / pro bono basis.\nThat being said, I will respectfully pass on the proposed role of domain allocator. I am happy to provide feedback and guidance when needed with regards to the grants program.\\nCompound Grants Program 2.0 goes live on 16th January 2023.\nHere is everything you need to know!\nCGP 2.0 will be run by Questbook 8 and led by Harsha 2, who will be the program manager. This is a community-run grant program. Grants are spread across four domains, each led by a Domain Allocator(DA).\nEach DA will rigorously review all proposals and disburse up to $200K over the next six months. They will also mentor builders, give feedback on proposals and ensure milestones are met.\nThe domains and the respective DAs are:\n\nNew protocol ideas and dApps by allthecolors 1\n\nSecurity Analysis Tools & Security Bounties by Michael 2\n\nDeveloper Tooling by Madhavan 1\n\nMulti-chain Strategy by Bobby 3\n\n\nCGP 2.0 will prioritize:\n\nTransparency in fund disbursement, everything is on-chain.\nTurn-around time (TAT) of 2 days.\nOversight and mentorship to make sure milestones are hit.\n\nBlockchain Lawyers Group (BLG) will be working with us to ensure legal compliance. BLG’s independent legal advisors will support CGP 2.0 with contractual drafting, KYC standards, and any legal issues that may arise during the grant disbursement phase, and provide early-stage legal support to projects.\nYou’re all caught up now. If you or someone you know is passionate and believes they can contribute meaningfully to Compound, we encourage you to apply through the QuestBook tool on the 16th.\nWe are looking forward to your proposals.\nFor more information about the program and how it was developed, check out this thread 5.\nIf you have questions, please reach out to us in the #grants channel in the Compound Discord.\\nHi @harsha, thanks for your initiative.\nI’m not sure if the window for the applications for the current cycle is still open, however, a couple of questions that might be relevant for future cycles too:\n1/  Domain Allocator weekly effort estimate and time commitments:\n\nWhat is the basis for the estimate of 15 hours / week? Is it based on the expected volume of grant applications? Is it capped to 15 hours / week?\nWhat happens if the volume of applications is much higher than expected, and the Domain Allocator can’t commit more hours?\nDoes the grant team collaborate entirely online, or will there be routine meetings when team members are expected to participate live? This will be a big hurdle for people with other commitments.\n\n2/  Scope of the “Risk Parameters update research” Domain:\nThe name of the Domain sounds a bit narrow if it’s limited to researching the Risk Parameters. Also, Gauntlet is already deeply involved in that arena, so I’m not sure there’s much to be gained by limiting to Risk Parameters. There would be more value if this domain includes market risk management too. For example, products based on volatility recommend actions after big price movements. However, many quant firms are known to predict volatility upfront (using Machine Learning and technical / trend analysis) and take proactive actions before a volatility spike. Such a product can potentially help Compound carryout on-chain risk mitigation (without a governance vote for parameter changes in a limited range), or put out notifications alerting the community of liquidation risk levels (without necessarily adjusting risk parameters).\nSo, is the scope of this domain fixed (to Risk Parameters research), or is it more flexible? Expanding the scope would be a lot more interesting and valuable. I would like to know community’s thoughts too on this.\\nHi, yes, there’s been a lot of progress in the last 15 days, thanks for asking. This is what happened in the last 15 days:\n\nHad calls with blockchain clubs at various universities, and some of the major comp token holders to take their feedback on the proposal. Incorporated this feedback into the latest proposal.\nWith the help of the compound community members, evaluated various ways in which the proposal can be put up for voting, and met a couple of them at ethBagota to understand the process in detail.\nRequested to whitelist me in order to put up the proposal for voting, based on the last update I received from the compound team, we are almost ready. So most likely we will have the proposal up for voting next week!\n\\nHarsha, thanks for keeping the program moving forward.\nBefore a proposal is created & voted on, I have a few observations & questions to help ensure the CGP 2.0 is more successful than CGP 1.0:\n\nOver the course of the original grants program, there was a shift towards “RFP” and deliverable based funding. This helped ensure that grants went towards outcomes, instead of being delivered up-front with varying (and in many cases, very little) progress. Beyond the contract (above), how will CGP 2.0 ensure that grant recipients deliver on their stated goals? Will there be presentations on community calls, progress reports, or incremental disbursements?\nThe original grants program returned approximately half of its allocated COMP back to the community as unused after six months. If a domain is unallocated after six months, will it’s unused portion be returned to the community?\nAs grants manager, how will the program be marketed? Will you be able to take over the compoundgrants 3 Twitter account and website, etc?\nHow will the time spent by the allocators and manager be tracked?\n\nIn general, I would strongly suggest a smaller initial budget for each domain (e.g. $200k over 6 months) knowing that the community can easily increase funding for any domain on an as-needed basis, especially if it is showing results.\\nThank you for the questions, please find the responses below\n1 - Questbook International Inc will sign this contract on the grantor’s behalf. Yes, you’re right it’s a legal entity.\n2 - We’re providing the Questbook tool for free for CGP 2.0. We’re only charging for managing the program.\n3 - The focus is on running this grants program in a decentralized manner.\nRobert on one of the calls mentioned: \" At compound, we spend a lot of starting to push the community forward and developing the core protocol itself. A whole decentralized team of developers should be making improvements to the core compound protocol and adding new functionality\".\nRef: Welcome & Intro to DeFi Grants - YouTube 4\nApart from the problems this proposal aims to solve by running the grants program in a decentralized manner, having multiple allocators to run the individual grant programs can also help attract builders through their networks and allows faster decision-making.\n4 -  Yes, a centralized grants committee can also use Questbook as a tool to disburse grants. Polygon for example, has a more centralized grants committee - uses Questbook to run its entire grants program.\n5 - KYC is performed to understand to whom the payouts are being made, in order to be compliant.\\nI am currently mapping out a process for people to deploy Compound on other chains. We will provide a public evaluation metric to fairly evaluate the applications as they come."
  },
  {
    "number_of_comments": 17,
    "postid": "0de3413f-02a2-4f33-8aa7-a0e22ec0c286",
    "posturl": "https://www.comp.xyz/t/multi-chain-strategy/2903",
    "combinedcontent": "Over the past year, while the community has governed and grown the Compound protocol, Compound Labs has been been focused on two flagship priorities:\n\n\nLaunching and scaling Compound Treasury 46, an institutional on-ramp to Compound markets. Treasury is a centrally managed product built on top of the protocol, and the sustainable business model of our company.\n\n\nResearching new technologies and approaches for Compound markets to include assets and borrowing demand from blockchains (and L2s) beyond Ethereum. This work has centered around Gateway 86, a stand-alone distributed ledger with “starports” on external chains, used to connect these assets.\n\n\nSince originally announcing the Gateway project, time has moved quickly. We’ve released multiple versions of a Gateway prototype / testnet; countless teams have begun experimenting with multi-chain approaches; the Compound protocol on Ethereum suffered from a faulty upgrade (unintentionally rewarding COMP to some users); gas costs on Ethereum have risen dramatically; several cross-chain protocols lost assets to security incidents; and multiple blockchain ecosystems evolved rapidly, often with EVM-compatibility at the core of their growth.\nEach of these events has required us to adapt what we believe the right multi-chain strategy to be:\nRather than launch Gateway connected to starports on external chains, which will introduce significant new technology & risk, we believe it’s prudent to first launch a Compound presence on EVM-compatible chains as fragmented markets–and if/when the community decides to, upgrade these markets into a multi-chain shared liquidity pool.\nAt this week’s community developer call 66, Compound Labs will discuss the steps necessary to enable this strategy, and how we can best equip the community to govern a multi-chain future.\\nSuper hyped for this!\\nNice - looking forward to this.\\n\n\n\n rleshner:\n\nto enable this strategy, and how we can best equip the community to govern a multi-chain future.\n\n\nThis is an excellent step in expanding Compound protocol outside its homebase of Ethereum. With current market conditions and user profiles firstly being onboarded by L2s and Ethereum scaling solutions like Polygon PoS.\nThe Polygon community and team was very enthusiastic and welcoming of the Starport deployment - Polygon/Matic Starport - #4 by jared 23. Compound development team also saw the Polygon Starport integration successful and we would love to replicate that enthusiasm and interest again here.\nHappy to put up an extended proposal detailing benefits of a Polygon Compound deployment with more numbers and a detailed view and how it will benefit the Compound DAO.\nJust for reference - our team wrote a research article about Stablecoin utilization comparison 30 of Multichain AAVE that might help the community discussions.\\nThis is exciting! Lets go multi-chain\\nGoing Multichain would be great! Helps defy centralized blockchain platforms and powers decentralization on a whole new level simultaneously\\nI believe its high time Compound goes multichain. A welcome move towards various growth opportunities and becoming a one-stop platform.\\nI will contribute to this initiative by:\n\nProposing an L2 evaluation framework (started), and\nWriting automated deployment and configuration scripts (started)\n\\nHere’s my L2 evaluation framework so far: https://hackmd.io/@TylerEther/ryu5o1sl5 29\nIt’s interesting to see that Polygon has a higher degree of decentralization of active validators/miners (by address).\\nI personally dont care about other L1s. But what I would be interested to see is compound on rollups. Specifically Arbitrum/Optimism and later zk rollups.\\nPolygon is not an L2, atleast not in its current state. If a chain does its own consensus to validate transactions its an L1. If it relies on another chain to validate transactions its an L2.\\nI’m not looking to debate the definition of an L2.\nWhat’s important is being able to allow users to use Compound protocol without having to pay ridiculous gas prices relative to the amount of capital they’re working with. This is on top of baseline security, liquidity, on/off-ramps, and other requirements (to be established).\nHere are current swapping costs to benchmark transaction costs across different chains:\n\nEthereum: $16.34\nArbitrum: $1.16\nOptimism: $1.13\nPolygon: $0.005\n\\nI understand. I am not saying there shouldnt be compound on polygon. I am saying in its current state Polygon is not a L2.\nThe trend for transaction prices on L2s goes down, there is still a lot of optimization that can be done plus there is datasharding coming, reducing the prices even more.\nYour transactions are actually on the Ethereum blockchain with L2s. Which is way more costly to attack than Polygon.\nI am just observing the current trend. L2s will outperform any L1. Polygon will also be a rollup and a true L2 solution. Again I am not saying that Polygon shouldnt get compound I just think that L2 deployment should be also strongly considered because thats the future.\\nYeah, for sure!\nI’m personally very excited for Polygon Hermez 2.0 that offers a zkEVM using plonky2 as an Ethereum rollup.\\nMy evaluations for Arbitrum and Optimism are complete. We face a grim reality - decentralization is very questionable.\nArbitrum has one active validator, which should be fine provided there are active defensive validators and watchtower validators. However, I could not find any data on these validators.\nOptimism has one active sequencer which is run exclusively by Optimism PBC, and the fraud (“fault”) proof mechanism is temporarily disabled. Optimism is currently completely centralized and users have to fully trust Optimism PBC.\nShould the community wish to deploy on these rollups, I’d highly suggest aggressive supply caps until we see better decentralization, openness, and transparency in the block production and validation processes.\\nDecentralization with the sequencers is not the issue with rollups. The thing with optimistic rollups is that you have a long period to submit fraud proofs (optimism is overhauling their system right now). And you are able to interact with the smart contract directly from L1 and flush your funds to L1 incase the Sequencer goes inactive or acts maliciously. The transactions you do are stored on Ethereum so once settled its set in stone.\nOptimism/arbitrum are going to open up completly so anyone can run a sequencer and get paid for it. Possibly in combination with the release of their tokens. Aave will also deploy on Arbitrum and Optimism, if the only option is Aave on L2s and they blow up, it was a bad decision in hindsight for compound. Both L2 solutions are still a work in progress.\nBased on the fact that there is little liquidity at the moment on both. I dont see a problem with agressive supply caps. The risk reward ratio is in my opinion on the side of deploying compound there. I see an advantage in getting in early.\\nAre you thinking parachain on Polkadot or maybe using Acala’s EVM+?\\nI like this approach of prioritizing deployment on chains that are fully EVM-compatible (so the protocol “just works” out of the box) and that minimize centralization risk. Minimizing centralization risk is ultimately about security, which motivates the following suggestion:\nCompound’s security is currently underpinned by proof-of-work, but of course this will change to proof-of-stake at the merge. While most users will probably be comfortable with PoS security at the level of decentralization that the beacon chain and certain other large PoS chains enjoy, there’s a case to be made that a security-first money market protocol like Compound ought to offer users an option secured by PoW with a high hash rate for anyone who would prefer to keep their assets on a security model with similar characteristics / risk profile as what they have on ethereum today.\nOne EVM-compatible chain that will still be secured by proof-of-work after the merge is the Syscoin NEVM 4. The syscoin PoW base chain dates back about as far as ethereum and is actually mege-mined with bitcoin, so it has an enormous hash rate for the very modest TVL 1 its nascent NEVM smart contract layer currently secures. Being merge-mined with bitcoin also makes syscoin’s PoW security effectively carbon neutral: as long as there is intrinsic demand to mine bitcoin, attempts to ascribe emissions to syscoin’s PoW would be double-counting those emissions against bitcoin’s own carbon intensity.\nThe syscoin foundation will further scale up the NEVM with a zkrollup and validium this year, but fees on the NEVM layer are already dirt cheap, and the slower-than-average 2.5-minute avg block time of the NEVM isn’t nearly as much of an issue for money markets as it is for most other DeFi apps. Currently, ETH, WBTC, DAI, USDC, and USDT are available on syscoin through a single point-of-entry, Multichain’s router, which is not all that decentralized yet. This seems to be currently the weakest link in terms of potentially deploying Compound on syscoin NEVM. Compound would definitely be the project with the greatest name recognition on syscoin if it were to deploy there.\nSummarizing, I could see Compound on syscoin NEVM as our “PoW-secured offering” of the protocol, with ethereum as the default PoS-secured option and then rollups / alt PoS chains as desired. If folks know of other PoW-secured EVMs that should be considered alongside syscoin, feel free to add them here. The only other one that comes to mind for me is ethereum classic, but its hash rate is a tiny, tiny fraction (less than a millionth) of syscoin’s.\nDisclosure: it’s probably obvious given the level of detail here, but just so we’re clear, I hold some SYS."
  },
  {
    "number_of_comments": 28,
    "postid": "503638ba-b173-4bbc-9d71-5b94947ef6f4",
    "posturl": "https://www.comp.xyz/t/proposal-to-integrate-chainlink-price-feeds/685",
    "combinedcontent": "This post is response to the recent DAI Liquidation Event 22 and was originally intended to be a response to that thread, but is still awaiting moderator approval so I am posting my comment here on the proposal board.\nCompound needs to integrate Chainlink Price Feeds 7. I am writing this as a daily DeFi user and as someone who only wants the best for the DeFi ecosystem as a whole, especially as the value secured rises. The false liquidation of ~$90M in user funds recently was a serious issue that was directly caused by Compound’s centralized oracle solution which pulls market data from only a single exchange, Coinbase, with Uniswap TWAP used as a backstop. Compound’s price feeds provide data that only reflects a small subset of the total crypto trading market and fundamentally cannot provide sufficient market coverage. This in turn lowers the cost of market manipulation and exposes the protocol to inaccurate pricing from large trades.\nSpecifically, Coinbase has an extensive history of downtime and flash crashes, so I am surprised this was not immediately seen during development as being a huge single point of failure. Using Uniswap TWAP as a backstop is better than no backstop in this situation, but it introduces a false sense of security as it too can trivially be manipulated (as we saw during this event). This lack of market coverage allowed a malicious actor to manipulate just two exchanges to skew the price data delivered to the Compound protocol and falsely liquidate users and yield farmers using DAI as debt or collateral. The core issues of price feeds without market coverage are covered extensively in this blog post here 3 which provides context about the importance of data quality for oracles.\n1533×900 108 KB\nCoinbase was the only major exchange that experienced such a drastic price deviation, other major exchanges were unaffected.\nHowever, none of this information I mention above is new, as I have previously pointed out the numerous and specific vulnerabilities in the design of Compound’s oracle that were not and still have yet to be fixed. Here 3 is a tweet thread I wrote on July 21st 2020 on my concerns regarding the Compound oracle and the likelihood of Coinbase experiencing market manipulation/flash crashes, the ability to manipulate Uniswap TWAP, and why taking a simple median across pre-selected exchanges does not solve the issue adequately either. Compound’s price oracles are still highly vulnerable to these issues as we speak, leaving over $3B in user deposited funds at risk of further catastrophic losses, and needs to be fixed immediately. Compound’s price oracle simply does not provide adequate market coverage as it exists today. Moreover, because it requires exchanges to change their API infrastructure to provide signed data that is compatible with Ethereum, the Compound oracle will continue to be inherently limited in the amount of market coverage it can ever achieve.\nChainlink Price Feeds provide an immediate solution to this problem, allowing the Compound protocol to fully mitigate these oracle related issues going forward. Aave, another decentralized money market on Ethereum experienced no price oracle issues during this event or any false liquidations. There is a very simple reason for this; instead of rolling their own oracle, exposing them to wide range of nuanced attack vectors, they simply integrated Chainlink oracles, which has successfully provided Aave users with the true market wide price of both DAI and every other asset on the platform since launch, as well as during this Coinbase/Uniswap outlier flash crash. I implore you to consider the following sections as I describe how Chainlink is resilient to these attack vectors.\nChainlink’s Decentralized Price Feeds are highly accurate and resistant to exchange distortions because they provide full market coverage by using multiple layers of aggregation that smooth outliers and prevent manipulated data from being delivered to smart contracts. This ensures market manipulation on a select few exchanges have no effect on the final data point generated and delivered to contracts. Specifically, Chainlink has three levels of aggregation to prevent the exact issues Compound’s price oracles experienced today.\nimage1600×457 148 KB\n\nFirstly, Chainlink uses professional data providers (CoinGecko, BraveNewCoin, Amberdata, Kaiko, CryptoCompare, Alpha Vantage, CoinApi, CoinPaprika, CryptoAPIs, and more) who whose entire business model revolves around generating high quality data using refined aggregation methodologies. These data providers produce reference prices for cryptocurrencies that reflect the market-wide price by tracking hundreds of exchanges (both on-chain DEXs and off-chain CEXs), taking into account volume, liquidity, time, and other shifting differences across exchanges, preventing any single source of truth.\nSecondly, there are the security reviewed Chainlink node operators (T-Systems, LinkPool, Certus.One, Stake.fish, Chainlayer, Chorus,one, SNZ, Huobi, and dozens more) operated by professional DevOps and blockchain infrastructure teams who aggregate price data from multiple data aggregators and take the median off-chain before delivering the data point on-chain, preventing any single source of truth. These Chainlink nodes are paid for their services in LINK, not only covering their gas costs, ensuring timely and incentivized updates, but providing a source of profit. This creates crypto-economic security by creating a large opportunity cost for malicious activity. Additionally, multiple data providers already operate their own Chainlink oracle node and provide cryptographically signed data.\nThirdly, there are the Chainlink oracle networks (feeds.chain.link) which are on-chain reference contracts that aggregate data from multiple node operators, again preventing any single source of truth. Each Price Feed is updated based on a threshold deviation and a heartbeat frequency, ensuring fresh data that follows market volatility is always available to contracts. These Price Feeds are a shared public good funded by many DeFi projects and already secure over $4B in user funds.\n\nWhat I am proposing here is quite simple. By integrating Chainlink Price Feeds as the primary oracle solution for the Compound protocol, these market coverage issues simply disappear and users can be assured they will not be falsely liquidated (just as Aave can today). Chainlink already supports all of the price feeds the Compound protocol needs on mainnet and integration would be straight-forward, only requiring a few lines of code (docs.chain.link). Additionally, Chainlink Price Feeds can also be used in replacement of Uniswap as the backstop, providing a much more tamper-resistant solution, though being the primary oracle is ideal as it would completely stop these exploits from occurring and ensure there is no period without accurate data. I am writing this as a concerned DeFi user who does not want to see more user funds falsely liquidated due to entirely preventable oracle issues. We are all in this together and I believe that the DeFi community can come together to ensure all protocols are using oracle solutions that are sufficiently secure for the value they secure.\nPlease take what I say with consideration as the value locked in DeFi continues to grow in orders of magnitude. By fixing the issue at its source now, Compound development and governance can focus on and innovate around what assets should be listed and the risk parameters, rather than worrying about how to refund users in the wake of another price oracle exploit.\\nI think using Chainlink for price feeds is a very good idea. I would also like to add two additional benefits, which would come with the integration:\n\nWhen adding new coins to Compound you are not limited to the coins which are offered by the Coinbase API. Chainlink offers prices for a much large range of coins.\nYou are no longer dependent on volunteers to post the current prices to the blockchain, which leads to outdated prices. With Chainlink prices are updated regularly automatically.\n\\nI think using Chainlink price feeds can not only help us fix the immediate problems with Compound’s oracle mechanism, but I think it will help us scale Compound. We need Compound to be more nimble, and the current oracle mechanism is not developed enough to support an acceleration in asset listings across different cryptos and asset types. I’m not sure we should keep kicking this can down the road.\\nI whole heartedly agree that the liquidation event was caused by a fundamental lack of market coverage from the Compound oracle.  Full market coverage can be achieved by using Chainlink oracles because they pull from a range of data sources both CEX/DEX.\\nThis is simply the highest quality solution for the long-term. I wonder why it has not already been taken into consideration? Are there any negative implications for Chainlink integration?\\nAs already stated by ChainlinkGod above, Chainlink’s multiple forms of data aggregation at the data source, individual node response, and oracle network level will provide Compound with the strong levels of market coverage needed to mitigate price oracle attacks as experienced by the DAI liquidation event. Outside of the technical considerations around better market coverage, I think there are a few other benefits from integrating Chainlink that the Compound community should consider, which can help Compound scale to support larger lending markets across a wider array of assets, both at a cheaper price and with broader community support.\n\n\nChainlink provides an oracle solution that is already live on mainnet and proven to secure major protocols like Aave and Synthetix over an extensive period of time. As such, Chainlink can be quickly integrated across all of Compound’s existing markets to swiftly fix this price oracle vulnerability, saving the community considerable time, money, and mental energy that would otherwise be spent thinking about, debating, building, and testing an in-house solution that isn’t guaranteed to be successful.\n\n\nCompound will undoubtedly look to scale to support more assets in the future and potentially even go beyond traditional cryptocurrencies/tokens, such as digital commodities, NFTs, and more. Being able to quickly innovate and launch new markets will be important to all DeFi protocols as a means of retaining and growing market share in an increasingly competitive environment for yield. By offloading oracles to Chainlink, which specializes solely on oracles and has both a strong academic research team to continually innovate better oracle solutions and a full-time engineering team to ensure all projects have round the clock internal and external monitoring, Compound can focus on its core business model of lending/borrowing and launch more markets across a variety of different asset classes at an accelerated rate. This will help Compound retain its position as the leading lending protocol in an increasingly competitive DeFi market, which is only going to continue as traditional players enter and vie for market share.\n\n\nWith Chainlink Price Feeds already being widely used by the DeFi ecosystem, they offer a shared cost model where a variety of independent protocols/projects collectively support and fund commonly used price feeds. For example, the Chainlink ETH/USD Price Feed already has 26 projects collectively using and supporting it. This financing model lowers the per-user costs as more users join the network, which is likely to rapidly increase as Chainlink continues to be adopted across a variety of use case verticals and blockchain networks. This extends the shared cost model beyond just Ethereum projects, but to projects operating across all blockchains. Also, the Chainlink team has expressed their intention of employing a decentralized governance model in the future, in which Compound could and should clearly have a voice in should they integrate Chainlink.\n\n\nIntegrating Chainlink doesn’t mean that Compound has to remove its circuit breaker mechanism either. Compound can integrate Chainlink as its primary oracle solution, with its own oracle mechanism and/or the Uniswap oracle serving as its circuit breaker protection against the outside chance that Chainlink experiences any issues. This way Compound benefits from the market coverage that Chainlink provides, while still retaining the circuit breaker safety net should the community feel it’s important to keep in place. As such, Compound gets all the benefits of Chainlink, with the added guarantee that at the very least it won’t be any less reliable than your circuit breaker.\n\n\nWhile I know there has been some competitive energy between the Chainlink and Compound communities that may at times rub some people the wrong way on both sides, I think ultimately we are all in this together and have a lot of shared interests in seeing DeFi succeed as a whole as well as the success of each other’s protocols. Having been a part of the LINK community for a while, one thing I do know is that they are quick to put the past behind and support teams entering the Chainlink ecosystem, even if there was bad blood or mixed feelings beforehand. While the LINK Marines may be a little eccentric at times, I think the Compound protocol could benefit from the LINK community’s passionate energy as a means of furthering Compound’s brand and awareness as a leading on-chain money market. This clearly benefited Aave, as upon its Chainlink integration they were able to innovate quickly and grow in market share/awareness, with LINK becoming the largest market by value locked. Not only do I believe that Compound could benefit from this Link Marine effect, but I think it’s a perfect way to launch a LINK market on Compound that has a chance of being immediately adopted by a community with deep knowledge in the DeFi space and the funds to engage in these protocols. LINK is increasingly being seen as a reliable form of collateral, but the LINK community tends to only trust protocols that use Chainlink to secure their oracles.\n\n\nIn short, I think such a collaboration between the two would benefit both ecosystems. Compound could innovate / launch new markets faster, obtain more price oracle security, and utilize social support from the LINK marine community to help it reach wider audiences. At the same time, Chainlink could clearly benefit from supporting Compound, as Compound is undoubtedly an OG in DeFi, secures a large amount of value, and has a community of many smart and innovative thinkers that could contribute to the continued development of Chainlink to the benefit of both projects. While competition may be intense at times, it’s drawn from the passion and drive both communities have around winning, and I think combining those two forces would lead to an accelerated amount of winning for both ecosystems than what would otherwise occur as separate entities.\\nIf anyone is willing to start an Autonomous proposal, I’d be willing to delegate  my comp (although I do not have alot, some other’s might also join)  in line with this initiative.\\nAgreed, I think creating an autonomous proposal for replacing the Open Oracle System with Chainlink Price Feeds would be a good idea, especially given the Compound protocol as it exists today is still entirely exposed to these price oracle vulnerabilities and it only a matter of time before another market coverage related oracle issue occurs again. This is something I am seriously worried about, because as  Compound grows in TVL alongside the rapid growth of DeFi, the incentives to manipulate the oracle mechanism grows as well.\nChainlink provides an immediate solution today and there is really no downside because the Compound community can always revert back if it doesn’t end up working for whatever reason, but the status quo of doing nothing does not fix the issue at hand. Even if every exchange begins cryptographically signing their APIs (an idealistic but monumental and time consuming task), it still wouldn’t provide proper market coverage because taking a simple median across exchanges doesn’t take into account volume and liquidity differences like the data aggregators Chainlink uses do, which is required for actual market coverage.\nThe autonomous proposal can even keep Uniswap as the circuit breaker to protect against black swan events as it does today, providing the best of both worlds (Chainlink for full market coverage and Uniswap as an always-on circuit breaker).\nWhat would be the next steps to creating an autonomous proposal for a Chainlink Price Feed integration?\\n\n\n\n The_Crypto_Oracle:\n\nWhile I know there has been some competitive energy between the Chainlink and Compound communities that may at times rub some people the wrong way on both sides, I think ultimately we are all in this together and have a lot of shared interests in seeing DeFi succeed as a whole as well as the success of each other’s protocols.\n\n\nThat’s interesting to know. I always had the impression, the decision not to use Chainlink was not driven by technical aspects but by feelings and ego.\\nIn general it would be a good idea to create an autonomous proposal. But I think the chances, that it will go through, are very low. So I don’t think it’s worth the efforts.\nIf you want to understand the reason why, check here 11. Compound is not governed by a democracy, but is in the hands of a few. The top 6 together have more than 50% voting power. These are Compound investors, are affiliated with the Compound founders or are the Compound founders. It seems, that the Compound founders are not willing to switch to Chainlink and I’m pretty sure, that the other 4 won’t vote against them.\\nI would disagree, as your post seems to be merely a conjecture?\nAn autonomous proposal has already passed before, namely this\n  \n    \n    \n    Set WBTC Collateral Factor to 60% Proposals\n  \n  \n    2nd Autonomous Proposal: This change increases the Collateral Factor of WBTC from 40% to 60%. \nOn July 14th, proposal 16 was passed to raise WBTC’s Collateral Factor from 0% to 40%. According to Defi Pulse, on July 14th there were ~$100M of WBTC in circulation. Today there are ~$900M of WBTC in circulation. In addition to having a much larger supply today, WBTC liquidity has significantly grown since proposal 16 was passed. Uniswap.info shows WBTC on July 14th with a total liquidity of ~$1.15M w…\n  \n\n\nIf the top holders reject a proposal to use a better oracle without submitting an alternative solution, I would probably be weary of leaving and keeping holdings in the platform.\nSo certainly after this DAI debacle, a solution needs to be put forward.\\nI don’t say, that autonomous proposals haven’t a chance to pass in general. I meant this specific proposal. Perhaps you are right, that it’s worth trying it and make it transparent that way, what the different parties think about it.\nAnd yes, the oracle needs to be fixed asap.\nIn the first step at least 100,000 delegated votes are needed, so the autonomous proposal can be converted into a governance proposal. Not sure, if it is already necessary to have the needed code changes available. Would be great, if someone who knows this exactly, would give some hints.\\nThis has already been discussed and explained at length here.\n\n  \n    \n    \n    Fix the Compound oracle problem Ideas\n  \n  \n    The so called “DAI liquidation event” has shown, that the open oracle, which is used by Compound, has weaknesses. The root cause is, that the oracle uses only one price source, which is Coinbase. The effect is, that the Coinbase prices are used in Compound, even if they deviate largely from the rest of the market. The Uniswap price is used to define a +/- 20% bounding, but as seen, this doesn’t solve the problem. \nImo the only stable solution is, to use several price sources and calculate an ave…\n  \n\n\nChainlink is not a good oracle design, as the company is incentivezed to obfuscates a clear problem, by adding unnecessary layers (aggregators, nodes) that increase the attack surface of any system that integrates with them. With regards to improving the Open Oracle, the path forward is to add more high-liquidity exchange reporters to the current oracle view. This should not be the highest priority action, as it does not address the security issue at hand which can only be resolved by reducing the amount DAI outstanding debt in the system.\\n\n\n\n wario:\n\nChainlink is not a good oracle design, as the company is incentivezed to obfuscates a clear problem, by adding unnecessary layers (aggregators, nodes) that increase the attack surface of any system that integrates with them.\n\n\nAs I already explained to you here 2, none of these claims about Chainlink is true. I will repost my comment here to provide context to everyone.\nThis is simply not true and a bit disingenuous. Chainlink simply wants to provide the DeFi ecosystem with the most secure price oracle solution with the highest quality data and this involves pulling from data aggregators who have full time monitoring teams to ensure manipulation is prevented 24/7. Taking a simple median from a select few exchanges is simply not an adequate oracle solution as it will always be vulnerable to market coverage issues and manipulation attacks around volume shifts and consolidations. As I described in the post linked above, the three layers of aggregation (data level, node level, network level) prevents any single source of truth. Relying on a single source of true is what played a significant factor in the $100M false liquidations of Compound users, and at the very least made the attack much easier and cheaper to pull off.\nThese are not “unnecessary layers” but layers of redundancy to ensure smart contracts always receive price data that reflects the true market wide price and not that of a single or a few exchanges. As I described in my proposal, Chainlink is directly secured by cryptoeconomics through an opportunity cost of losing future income if a node is malicious, as well as losing their reputation as an DevOps infrastructure provider. This is why we have never seen a successful attack against the Chainlink network, because the incentives work and ensure correct data is always posted on-chain.\n\n\n\n wario:\n\nWith regards to improving the Open Oracle, the path forward is to add more high-liquidity exchange reporters to the current oracle view. This should not be the highest priority action, as it does not address the security issue at hand which can only be resolved by reducing the amount DAI outstanding debt in the system.\n\n\nAs I also described, taking a simple median across a select few exchanges who change their infrastructure to support signed data compatible with Ethereum still does not provide adequate market coverage because it doesn’t take into account volume and liquidity differences across exchanges like data aggregators do. Here is a repost of my comment for more context.\nI think you’re missing the key point of market coverage. The issue with Compound’s oracle during this DAI manipulation attack was that it did not report the true market wide price. If an attacker manipulates the entire market (across every single exchange), then yes all oracles would be affected at that point, but that’s only because the true market wide price was changed, but that’s not what happened during this event, only a single exchange (Coinbase) was manipulated. That’s the nuance here, market coverage raises the cost of attack to highest degree possible, and while it doesn’t prevent market manipulation altogether, but does make it as expensive as possible and ensures protocols always receive the true market wide price.\nLike we discussed at length in the governance discord channel, the DAI liquidation event was a mixture of two factors. Too much DAI debt taken out AND a lack of price oracle market coverage. Coinbase is not the only liquid market for DAI and the DAI/USD Coinbase trading pair only tracks 4.75% of DAI’s daily volume according to CoinGecko. The vast majority of the volume is from Uniswap, which Chainlink adequately tracks today through its usage of data aggregators. Both of these factors (debt and market coverage) need to be solved, but we shouldn’t be ignoring the latter whatsoever. We can solve both issues at the same time, because market coverage played a significant factor in the false liquidation of $100M in user funds. The longer the Compound protocol goes without ensuring market coverage, the more exposed user funds are to further oracle manipulation.\\n\n\n\n wario:\n\nWith regards to improving the Open Oracle, the path forward is to add more high-liquidity exchange reporters to the current oracle view.\n\n\nIf only alot more high-liquidity exchanges had/adopted the open oracle reporting api. Seems your solutions is to make this high liquidity exchanges provide tooling for us, what incentive is there for them to spend resources for this?? I know coinbase has, because they’re an early investor to compound.\n\n\n\n wario:\n\nhis should not be the highest priority action, as it does not address the security issue at hand which can only be resolved by reducing the amount DAI outstanding debt in the system.\n\n\nHow do you intend to do that? By artificially limiting market size?\\n\nHow do you intend to do that? By artificially limiting market size?\n\nThis is being discussed in this thread DAI Market Risk 3\\nThis is all incorrect and outright false advertising from a company with obvious interests. Adding layers of obfuscation to what should be a simple and transparent oracle design is detrimental to security. I’ll restate what I’ve said numerous times before “Full market coverage” is a silly idea, in reality most markets are low volume/liquidity and a lot of CEXs have fake volume and liquidity. You want to be very careful in selecting what markets to include in a price feed, and medianizing the set of carefully selected high-liquidity exchanges is the best possible way to do this.\nA bit off-topic, but It’s qiute lamentable, and time-consuming, that this industry has to deal now with a clear and obvious campaign of aggressive astroturfing by a company to promote a subpar technical solution constantly and everywhere. The arguments on this topic have become repetitive and tiresome, so I’ll just leave it at that.\\nI don’t the gaslighting here is necessary, this is simply a conversation where we can discuss market coverage and why Compound’s current oracle is not sufficient for the value it secures today. The Chainlink network doesn’t obfuscate anything, but is incredibly transparent into its operation (feeds.chain.link, market.link, reputation.link). The usage of data aggregators ensures price data tracks all trading environments, preventing market coverage issues and this includes tracking fake volume and market manipulation. You want multiple layers of redundancy to ensure there is no single point of failure.\n\n\n\n wario:\n\nI’ve said numerous times before “Full market coverage” is a silly idea\n\n\nI’m not going to repeat myself on the importance of market coverage because Chainlink has already proven its ability to properly secure a wide range of dApps like Aave and Synthetix for long periods of time, which have never experienced market coverage or data quality issues like Compound did. This isn’t a Chainlink shill, this is “please fix your oracle before it breaks again and users lose more money” plea that’s coming from someone who has seen these oracle manipulation issues time and time again. Taking a median across exchanges where volume can shift overnight to exchanges not being tracked doesn’t solve the problem.\\nMy biggest concern is that an event involving the current oracle system will happen again. If Chainlink provides a solution that prevents this from occurring in the future, we should take a hard look at it. As a user, I want to make sure that I’m protected from this from happening in the future and that my positions aren’t liquidated when they should not have been.\nChainlink had better market coverage, which is obvious because it did not report this crazy DAI price. The bottom line is Compound’s current infrastructure got us here, and something needs to change.\nI don’t know of any successful protocol that recognizes a problem and does not seek to address it.\\nI don’t understand your concerns about obfuscation. All nodes are run by known, established entities and you can see each price update they post on-chain. It’s not like they can post prices then hide from everyone, as they would be immediately identifiable. Full market coverage also doesn’t mean you are just taking an average of all exchanges. It means you are taking in data from all exchanges and then weighting it by volume, while also removing outliers, fake exchange volume, etc. By having such a setup, you can account for volume shifts between exchanges or if it starts flowing to a new exchange. This is much more scalable and doesn’t require constant shifts in the underlying price calculation to account for changes in the trading market.\nAlso, Compound can still keep a circuit breaker in place should they want a fallback mechanism, similar to what they have today in Uniswap.\\nGreat points @The_Crypto_Oracle. Honestly I just want to know my funds are safe, and Chainlink’s data sources/methods seem as reliable as it gets for mitigating risk in similar situations, and we can implement them quickly.\nI’m happy to see that we have a COMP distribution proposal 2 in the works. These sort of changes will be very useful. However, I can’t help but feeling like the oracle issue is a more pressing issue to resolve right now.\\nAgreed. @rleshner mentioned wanting to implement additional price feed safeguards on Discord last night, which is great, but I didn’t hear any specifics around what he is envisioning from a technical standpoint. It would be very helpful to get some more clarity here so we can get this conversation moving forward (and a fix in place ASAP).\\nHow can happen outstanding debt in DAI?\nThe Variable Interest Rate Mechanism was not effective?\nBecause DAI borrowing interest rate was around 4% all time. Higher interest rates on borrowing assets incentivize users to repay high expensive positions.\nI was personally borrow DAI because of lower interest rates than other stablecoins.\nCan you, if it is not a problem, put a link where the DAI market is analyzed precisely because of that anomaly? And where are the potential risks presented? I would like to take a closer look at your argument.\nThank you\\n\n\n\n ChainLinkGod:\n\nAave, another decentralized money market on Ethereum experienced no price oracle issues during this event or any false liquidations.\n\n\nno, but they have had problems with it earlier.\\n@TennisBowling Could you provide more context? I hadn’t heard about this, what oracle or liquidation issues did Aave have recently?\\n\n\n\nCompensation Proposal: Distribute COMP to Affected Users in the DAI Liquidations\n\n\nchainlink to me just isn’t the best option because it’s very prone to human error\n\n\n@tennisbowling please write also some details, what you mean by this.\\nI’ll echo the others in that it would be helpful if you could expand upon these concerns in any detail @TennisBowling.\nI feel it is pretty clear the current oracle framework needs a rethink (I’m not seeing many here argue against this), and so far Chainlink sounds like the best solution proposed thus far due to the massively increased market coverage w/ minimal development. There have been other ideas floated (more reporters, tightening bounds etc.) that could improve the current situation, but IMO only nominally by comparison.\nI’ve heard concerns about centralization, but it’s hard to understand those concerns when people seem ok using Coinbase Pro as our reporter, meanwhile CL is using a large network of independent data sources. I’ve also heard concerns about obfuscation, but again that is hard to understand when I can easily view granular node/feed data 2 and reputation info.\nI’ll admit the LINK marines/frogs can be a little much! Part of me feels like the negative sentiment here could be more a knee-jerk reaction to that than the technicals, which I feel that is a shame because from what I can tell the actual team is one of the most professional in the space, and the technicals are all that really matters.\nAgain if there is something I’m missing, I would love to know more details so we can discuss.\\n\n\n\n MasterofNonce:\n\nI’ve heard concerns about centralization, but it’s hard to understand those concerns when people seem ok using Coinbase Pro as our reporter, meanwhile CL is using a large network of independent data sources.\n\n\nChainlink is the best solution, but if in Chainlink exists some point of failure how we can improve that with the existing price discovery model?\nExisting price discovery solution is centralized and every argument against failed because at some point in time DAI price on Coinbase Open Price Feed wasn’t aligned with the global DAI price. We can’t say that is because of outstanding debt in DAI on the protocol because in that scenario protocol has big impact on price at existing oracle model. If Compound caused stablecoin volatility on an existing oracle solution then it just doesn’t work.\n\n\n\n MasterofNonce:\n\nPart of me feels like the negative sentiment here could be more a knee-jerk reaction to that than the technicals, which I feel that is a shame because from what I can tell the actual team is one of the most professional in the space, and the technicals are all that really matters.\n\n\nAgree, there may be more constructive reactions when something like this happens again\\nFYI - Coinbase experienced a 60% deviation from the market-wide price yesterday on the SNX/USD pair:\nimage (4)756×326 14.7 KB\nCompare this to Coingecko:\nScreen Shot 2020-12-23 at 3.57.41 PM786×501 26.5 KB\nHad SNX been a collateral type on Compound we likely would have seen another false liquidation event. We need to expand market coverage via Chainlink, plain and simple."
  },
  {
    "number_of_comments": 16,
    "postid": "b8afbdbe-0c1b-4139-a502-20ccc12a76d6",
    "posturl": "https://www.comp.xyz/t/gauntlet-weekly-market-updates-polygon-usdc/4539",
    "combinedcontent": "[Gauntlet] Weekly Market Update: Polygon USDC (7/21/23 - 7/28/23)\nGauntlet would like to provide the community with an update on metrics from the Polygon USDC comet over the past week and will include any relevant recommendations.\nSimple Summary\n\nIf the community believes that demand for the Polygon market has stagnated, Gauntlet recommends decreasing rewards for the comet to mitigate COMP losses.\nUSDC borrows are down 1.7%.\nUSDC supply is up 9.0%.\nThe comet accumulated $2.97k USDC reserves over the past week, with an average reserve growth of 21.1%.\nThe comet distributed $33.23k COMP rewards over the past week, for a Net Protocol Profit of -$30.26k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week.\nMarket Growth\nScreen Shot 2023-07-28 at 11.49.38 AM1804×500 64.8 KB\nTotal Collateral (USD) is up 0.8%, from $34.06M to $34.34M.\nScreen Shot 2023-07-28 at 11.49.59 AM1806×502 65.9 KB\nUSDC Borrows are down 1.7%, from $17.86M to $17.56M.\nScreen Shot 2023-07-28 at 11.52.59 AM1802×492 68.2 KB\nUSDC Supply is up 9.0%, from $22.73M to $24.78M.\nSupply Caps\nScreen Shot 2023-07-28 at 2.46.21 PM3548×656 92.8 KB\nAs seen above, WBTC (60.1%), WETH (37.7%), and MATIC (100%), all have supply cap utilizations < 75%. We do not currently recommend increasing the supply caps.\nScreen Shot 2023-07-28 at 2.47.51 PM3548×842 139 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-07-28 at 2.48.19 PM1670×432 29.7 KB\nThe minimum USDC utilization was 69.4%, and the maximum was 78.6%.\nThe minimum USDC reserve growth was 17.3%, and the maximum was 23.5%. The average USDC reserve growth was 21.1%.\nScreen Shot 2023-07-28 at 2.50.03 PM1654×462 31.9 KB\nThe comet steadily accumulated $2.97k USDC reserves, while distributing $33.23k COMP rewards, for a weekly Net Protocol Profit of -$30.26k.\nRecommendations\nThe Polygon comet offers an appealing 5.92% Net Supply APR and a pay-to-borrow rate of +0.96% Net Borrow APR. However, the protocol is still experiencing relatively stagnant growth compared to COMP distributions. If the community believes that demand for the Polygon market has stagnated, Gauntlet recommends decreasing rewards for the comet to mitigate COMP losses. Gauntlet will follow up with a proposal next week to gauge community preferences.\\nGauntlet Weekly Market Update: Polygon USDC (7/28/23 - 8/3/23)\nGauntlet would like to provide the community with an update on metrics from the Polygon USDC comet over the past week.\nSimple Summary\n\nGauntlet recently recommended decreasing rewards in the Polygon USDC comet 3 due to stagnant growth. We created a poll to gauge community preferences.\nUSDC Borrows are down 5.7%, from $17.56M to $16.56M.\nUSDC Supply is down 25.8%, from $24.79M to $18.40M.\nUSDC utilization increased from 70% to 90%.\nThe comet accumulated $640 USDC reserves, with an average reserve growth of 8.4%.\nThe comet distributed $37.46k COMP rewards for a weekly Net Protocol Profit of -$36.82k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week.\nMarket Growth\nScreen Shot 2023-08-04 at 8.21.18 AM1802×488 65.5 KB\nTotal Collateral (USD) is down 4.0%, from $33.81M to $32.47M.\nScreen Shot 2023-08-04 at 8.21.42 AM1806×496 65.3 KB\nUSDC Borrows are down 5.7%, from $17.56M to $16.56M.\nScreen Shot 2023-08-04 at 8.22.10 AM1802×484 70.5 KB\nUSDC Supply is down 25.8%, from $24.79M to $18.40M.\nScreen Shot 2023-08-04 at 8.31.55 AM1670×476 39.2 KB\nUSDC utilization increased from 70% to 90%, mainly due to decreased USDC supply.\nSupply Caps\nScreen Shot 2023-08-04 at 8.23.05 AM3530×620 89.9 KB\nAs seen above, all assets have supply cap utilizations < 75%.\nScreen Shot 2023-08-04 at 8.22.42 AM3542×828 144 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-08-04 at 8.28.22 AM1672×434 32.4 KB\nThe minimum USDC utilization was 70.1%, and the maximum was 100.0%.\nThe minimum USDC reserve growth was -14.0%, and the maximum was 24.4%. The average USDC reserve growth was 8.4%.\nScreen Shot 2023-08-04 at 8.28.51 AM1664×464 32.2 KB\nThe comet accumulated $640 USDC reserves while distributing $37.46k COMP rewards for a weekly Net Protocol Profit of -$36.82k.\\n[Gauntlet] Weekly Market Update: Polygon USDC (8/4/23 - 8/10/23)\nGauntlet would like to provide the community with an update on metrics from the Polygon USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are up 11.4%, from $16.56M to $18.44M.\nUSDC Supply is up 17.8%, from $18.40M to $21.68M.\nUSDC utilization decreased from 96.1% to 85.1%.\nThe comet lost $30 of USDC reserves, with an average USDC reserve growth of 1.2%.\nThe comet distributed $27.34k COMP rewards for a weekly Net Protocol Profit of -$27.37k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week.\nMarket Growth\nScreen Shot 2023-08-11 at 5.55.39 PM1808×512 67.6 KB\nTotal Collateral (USD) is up 9.8%, from $32.84M to $36.06M.\nScreen Shot 2023-08-11 at 5.56.00 PM1806×494 68.2 KB\nUSDC Borrows are up 11.4%, from $16.56M to $18.44M.\nScreen Shot 2023-08-11 at 5.56.20 PM1800×492 69.6 KB\nUSDC Supply is up 17.8%, from $18.40M to $21.68M.\nScreen Shot 2023-08-11 at 5.57.08 PM1662×450 46.3 KB\nUSDC utilization decreased from 96.1% to 85.1%.\nSupply Caps\nScreen Shot 2023-08-11 at 5.48.32 PM1746×602 32 KB\nAs seen above, only WBTC (80.3%) has a supply cap utilization > 75%. We will follow up with an update on increasing WBTC supply cap next week.\nScreen Shot 2023-08-11 at 5.48.02 PM1748×828 51.7 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-08-11 at 6.27.41 PM1662×420 41.2 KB\nThe minimum USDC utilization was 78.8%, and the maximum was 93.5%.\nThe minimum USDC reserve growth was -11.4%, and the maximum was 24.0%. The average USDC reserve growth was 1.2%.\nScreen Shot 2023-08-11 at 6.27.59 PM1660×444 31.6 KB\nThe comet lost $30 of USDC reserves while distributing $27.34k COMP rewards for a weekly Net Protocol Profit of -$27.37k.\\n[Gauntlet] Polygon v3 USDC Update (08/11/2023 - 08/17/2023)\nGauntlet would like to provide the community with an update on metrics from the Polygon v3 USDC comet over the past week.\nSimple Summary\n\nGauntlet recently proposed increasing WBTC supply cap from 1k tokens (~$26.5M) to 1.5k tokens (~$39.8M)\nUSDC Borrows are down 9.86%, from $18.44M to $16.62M.\nUSDC Supply is down 8.24%, from $21.68M to $19.89M.\nUSDC utilization decreased 1.72%, from 85.1% to 83.6%.\nThe minimum USDC reserve growth was 1.3%, and the maximum was 24.4%. The average USDC reserve growth was 10.1%.\nThe comet accumulated $1.68k USDC reserves while distributing $24.75k COMP rewards for a weekly Net Protocol Profit of -$23.07k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-08-18 at 1.33.49 PM1802×496 68.3 KB\nTotal Collateral (USD) is up 4.10%, from $34.89M to $36.32M.\nScreen Shot 2023-08-18 at 1.34.10 PM1792×484 69.2 KB\nUSDC Supply is down 8.24%, from $21.68M to $19.89M.\nScreen Shot 2023-08-18 at 1.34.35 PM1792×494 68.9 KB\nUSDC Borrows are down 9.86%, from $18.44M to $16.62M.\nScreen Shot 2023-08-18 at 1.35.00 PM1658×452 42.8 KB\nUSDC utilization decreased 1.72%, from 85.1% to 83.6%.\nSupply Caps\nScreen Shot 2023-08-18 at 1.39.16 PM3540×622 92.8 KB\nAs seen above, WBTC (60.1%), WETH (37.7%), and MATIC (100%), all have supply cap utilizations < 75%. We do not currently recommend increasing the supply caps.\nScreen Shot 2023-08-18 at 1.40.40 PM3536×828 149 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-08-18 at 1.36.52 PM1656×432 43.4 KB\nThe minimum USDC utilization was 73.7%, and the maximum was 85.4%.\nThe minimum USDC reserve growth was 1.3%, and the maximum was 24.4%. The average USDC reserve growth was 10.1%.\nScreen Shot 2023-08-18 at 1.37.18 PM1650×462 32.1 KB\nThe comet accumulated $1.68k USDC reserves while distributing $24.75k COMP rewards for a weekly Net Protocol Profit of -$23.07k.\\n[Gauntlet] Polygon v3 USDC Update (08/18/2023 - 08/24/2023)\nGauntlet would like to provide the community with an update on metrics from the Polygon v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are down 8.75%, from $16.62M to $15.17M.\nUSDC Supply is down 9.25%, from $19.89M to $18.05M.\nUSDC utilization increased 0.55%, from 83.6% to 84.0%.\nThe minimum USDC reserve growth was -3.1%, and the maximum was 24.3%. The average USDC reserve growth was 20.8%.\nThe comet accumulated $2.66k USDC reserves while distributing $20.56k COMP rewards for a weekly Net Protocol Profit of $-17.9k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-08-25 at 3.50.36 PM1802×480 68 KB\nTotal Collateral (USD) is down 11.83%, from $33.44M to $29.48M.\nScreen Shot 2023-08-25 at 3.50.52 PM1798×472 71 KB\nUSDC Supply is down 9.25%, from $19.89M to $18.05M.\nScreen Shot 2023-08-25 at 4.01.50 PM2504×490 69.1 KB\nUSDC Borrows are down 8.75%, from $16.62M to $15.17M.\nScreen Shot 2023-08-25 at 3.51.11 PM1652×458 45.3 KB\nUSDC utilization increased 0.55%, from 83.6% to 84.0%.\nSupply Caps\nScreen Shot 2023-08-25 at 4.05.31 PM3532×620 92.5 KB\nAs seen above, only WBTC (78.1%) has a supply cap utilization > 75%. We currently have an on-chain proposal to increase WBTC supply cap from 1k to 1.5k.\nScreen Shot 2023-08-25 at 4.05.53 PM3550×848 154 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-08-25 at 3.51.27 PM1652×422 39 KB\nThe minimum USDC utilization was 73.9%, and the maximum was 87.2%.\nThe minimum USDC reserve growth was -3.1%, and the maximum was 24.3%. The average USDC reserve growth was 20.8%.\nScreen Shot 2023-08-25 at 4.02.55 PM2364×450 36.3 KB\nThe comet accumulated $2.66k USDC reserves while distributing $20.56k COMP rewards for a weekly Net Protocol Profit of $-17.9k.\\n[Gauntlet] Polygon v3 USDC Update (08/25/2023 - 08/31/2023)\nGauntlet would like to provide the community with an update on metrics from the Polygon v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are down 1.72%, from $15.17M to $14.91M.\nUSDC Supply is up 6.42%, from $18.05M to $19.21M.\nUSDC utilization decreased 7.50%, from 84.0% to 77.7%.\nThe minimum USDC reserve growth was 5.6%, and the maximum was 24.3%. The average USDC reserve growth was 17.5%.\nThe comet accumulated $2.3k USDC reserves while distributing $20.48k COMP rewards for a weekly Net Protocol Profit of $-18.18k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-09-01 at 7.24.15 AM1800×498 68.9 KB\nTotal Collateral (USD) is down 2.82%, from $30.32M to $29.46M.\nScreen Shot 2023-09-01 at 7.24.38 AM1796×486 71.4 KB\nUSDC Supply is up 6.42%, from $18.05M to $19.21M.\nScreen Shot 2023-09-01 at 7.25.00 AM1800×492 67.4 KB\nUSDC Borrows are down 1.72%, from $15.17M to $14.91M.\nScreen Shot 2023-09-01 at 7.25.20 AM1652×462 42.6 KB\nUSDC utilization decreased 7.50%, from 84.0% to 77.7%.\nSupply Caps\nScreen Shot 2023-09-01 at 7.32.05 AM3534×620 93.7 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-09-01 at 7.33.57 AM3544×844 149 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-09-01 at 7.25.38 AM1658×424 47.6 KB\nThe minimum USDC utilization was 77.4%, and the maximum was 84.0%.\nThe minimum USDC reserve growth was 5.6%, and the maximum was 24.3%. The average USDC reserve growth was 17.5%.\nScreen Shot 2023-09-01 at 7.25.59 AM1652×456 34.4 KB\nThe comet accumulated $2.3k USDC reserves while distributing $20.48k COMP rewards for a weekly Net Protocol Profit of $-18.18k.\\n[Gauntlet] Polygon v3 USDC Update (09/01/2023 - 09/07/2023)\nGauntlet would like to provide the community with an update on metrics from the Polygon v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are up 19.04%, from $14.91M to $17.75M.\nUSDC Supply is up 21.46%, from $19.21M to $23.33M.\nUSDC utilization decreased 1.99%, from 77.6% to 76.1%.\nThe minimum USDC reserve growth was -10.4%, and the maximum was 24.3%. The average USDC reserve growth was 21.7%.\nThe comet accumulated $3.01k USDC reserves while distributing $19.41k COMP rewards for a weekly Net Protocol Profit of $-16.4k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-09-08 at 8.50.28 AM1796×488 67 KB\nTotal Collateral (USD) is up 18.11%, from $27.95M to $33.01M.\nScreen Shot 2023-09-08 at 8.50.54 AM1796×490 68.7 KB\nUSDC Supply is up 21.46%, from $19.21M to $23.33M.\nScreen Shot 2023-09-08 at 8.51.20 AM1800×494 66.3 KB\nUSDC Borrows are up 19.04%, from $14.91M to $17.75M.\nScreen Shot 2023-09-08 at 8.51.50 AM1650×456 35.6 KB\nUSDC utilization decreased 1.99%, from 77.6% to 76.1%.\nSupply Caps\nScreen Shot 2023-09-08 at 8.54.19 AM3536×620 95.1 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-09-08 at 8.55.21 AM3538×842 147 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-09-08 at 8.52.12 AM1654×422 31.4 KB\nThe minimum USDC utilization was 75.9%, and the maximum was 92.2%.\nThe minimum USDC reserve growth was -10.4%, and the maximum was 24.3%. The average USDC reserve growth was 21.7%.\nScreen Shot 2023-09-08 at 8.52.38 AM1646×454 33.8 KB\nThe comet accumulated $3.01k USDC reserves while distributing $19.41k COMP rewards for a weekly Net Protocol Profit of $-16.4k.\\n[Gauntlet] Polygon v3 USDC Update (09/08/2023 - 09/14/2023)\nGauntlet would like to provide the community with an update on metrics from the Polygon v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are down 6.24%, from $17.49M to $16.4M.\nUSDC Supply is down 1.19%, from $22.99M to $22.72M.\nUSDC utilization decreased 4.93%, from 76.1% to 72.3%.\nThe minimum USDC reserve growth was 17.1%, and the maximum was 22.1%. The average USDC reserve growth was 20.4%.\nThe comet accumulated $2.74k USDC reserves while distributing $18.47k COMP rewards for a weekly Net Protocol Profit of $-15.73k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nimage2244×626 87 KB\nTotal Collateral (USD) is down 4.29%, from $32.85M to $31.44M.\nimage2260×602 84.8 KB\nUSDC Supply is down 1.19%, from $22.99M to $22.72M.\nimage2250×636 84.3 KB\nUSDC Borrows are down 6.24%, from $17.49M to $16.4M.\nimage2074×556 51.6 KB\nUSDC utilization decreased 4.93%, from 76.1% to 72.3%.\nSupply Caps\nimage3409×352 20 KB\nAbove are the current supply cap utilizations for each collateral asset.\nimage2974×1024 162 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nimage2082×516 44.2 KB\nThe minimum USDC utilization was 69.2%, and the maximum was 76.4%.\nimage2048×556 40.9 KB\nThe minimum USDC reserve growth was 17.1%, and the maximum was 22.1%. The average USDC reserve growth was 20.4%.\nThe comet accumulated $2.74k USDC reserves while distributing $18.47k COMP rewards for a weekly Net Protocol Profit of $-15.73k.\\n[Gauntlet] Polygon v3 USDC Update (09/15/2023 - 09/21/2023)\nGauntlet would like to provide the community with an update on metrics from the Polygon v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are up 4.38%, from $16.4M to $17.12M.\nUSDC Supply is down 1.28%, from $22.72M to $22.43M.\nUSDC utilization increased 5.52%, from 72.3% to 76.3%.\nThe minimum USDC reserve growth was 18.2%, and the maximum was 23.2%. The average USDC reserve growth was 20.6%.\nThe comet accumulated $2.75k USDC reserves while distributing $19.15k COMP rewards for a weekly Net Protocol Profit of $-16.4k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-09-22 at 10.28.24 AM1798×480 67 KB\nTotal Collateral (USD) is down 0.08%, from $32.42M to $32.39M.\nScreen Shot 2023-09-22 at 10.28.41 AM1798×474 69.3 KB\nUSDC Supply is down 1.28%, from $22.72M to $22.43M.\nScreen Shot 2023-09-22 at 10.28.59 AM1798×492 66.1 KB\nUSDC Borrows are up 4.38%, from $16.4M to $17.12M.\nScreen Shot 2023-09-22 at 10.29.15 AM1648×454 44.7 KB\nUSDC utilization increased 5.52%, from 72.3% to 76.3%.\nSupply Caps\nScreen Shot 2023-09-22 at 10.30.50 AM3796×614 93.8 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-09-22 at 10.31.15 AM3794×842 177 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-09-22 at 10.29.32 AM1654×424 39.3 KB\nThe minimum USDC utilization was 70.6%, and the maximum was 78.1%.\nThe minimum USDC reserve growth was 18.2%, and the maximum was 23.2%. The average USDC reserve growth was 20.6%.\nScreen Shot 2023-09-22 at 10.29.57 AM1650×454 33.5 KB\nThe comet accumulated $2.75k USDC reserves while distributing $19.15k COMP rewards for a weekly Net Protocol Profit of $-16.4k.\\n[Gauntlet] Polygon v3 USDC Update (09/22/2023 - 09/28/2023)\nGauntlet would like to provide the community with an update on metrics from the Polygon v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are up 0.45%, from $17.12M to $17.2M.\nUSDC Supply is down 3.32%, from $22.43M to $21.68M.\nUSDC utilization increased 3.94%, from 76.3% to 79.3%.\nThe minimum USDC reserve growth was 14.8%, and the maximum was 24.4%. The average USDC reserve growth was 21.8%.\nThe comet accumulated $3.1k USDC reserves while distributing $19.95k COMP rewards for a weekly Net Protocol Profit of $-16.85k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-09-29 at 12.19.06 PM1800×500 67.1 KB\nTotal Collateral (USD) is down 0.05%, from $31.46M to $31.45M.\nScreen Shot 2023-09-29 at 12.19.26 PM1800×470 68.1 KB\nUSDC Supply is down 3.32%, from $22.43M to $21.68M.\nScreen Shot 2023-09-29 at 12.19.53 PM1796×494 65.4 KB\nUSDC Borrows are up 0.45%, from $17.12M to $17.2M.\nScreen Shot 2023-09-29 at 12.20.14 PM1656×450 42.7 KB\nUSDC utilization increased 3.94%, from 76.3% to 79.3%.\nSupply Caps\nScreen Shot 2023-09-29 at 12.21.48 PM3536×620 91.9 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-09-29 at 12.22.14 PM3538×848 172 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-09-29 at 12.20.44 PM1652×422 45.3 KB\nThe minimum USDC utilization was 75.2%, and the maximum was 81.7%.\nThe minimum USDC reserve growth was 14.8%, and the maximum was 24.4%. The average USDC reserve growth was 21.8%.\nScreen Shot 2023-09-29 at 12.21.04 PM1656×450 31.5 KB\nThe comet accumulated $3.1k USDC reserves while distributing $19.95k COMP rewards for a weekly Net Protocol Profit of $-16.85k.\\n[Gauntlet] Polygon v3 USDC Update (09/29/2023 - 10/05/2023)\nGauntlet would like to provide the community with an update on metrics from the Polygon v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are down 1.87%, from $17.45M to $17.12M.\nUSDC Supply is up 1.96%, from $22.03M to $22.47M.\nUSDC utilization decreased 3.76%, from 79.1% to 76.2%.\nThe minimum USDC reserve growth was 19.6%, and the maximum was 24.3%. The average USDC reserve growth was 22.2%.\nThe comet accumulated $3.07k USDC reserves while distributing $22.16k COMP rewards for a weekly Net Protocol Profit of $-19.09k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-10-06 at 5.43.45 PM1798×490 67.3 KB\nTotal Collateral (USD) is down 1.71%, from $33.48M to $32.91M.\nScreen Shot 2023-10-06 at 5.44.04 PM1798×476 70.4 KB\nUSDC Supply is up 1.96%, from $22.03M to $22.47M.\nScreen Shot 2023-10-06 at 5.44.24 PM1798×484 66.6 KB\nUSDC Borrows are down 1.87%, from $17.45M to $17.12M.\nScreen Shot 2023-10-06 at 5.44.45 PM1652×444 45.2 KB\nUSDC utilization decreased 3.76%, from 79.1% to 76.2%.\nSupply Caps\nScreen Shot 2023-10-06 at 5.46.29 PM3542×618 94.6 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-10-06 at 5.46.59 PM3542×848 180 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-10-06 at 5.45.22 PM1646×416 40.4 KB\nThe minimum USDC utilization was 72.7%, and the maximum was 79.9%.\nThe minimum USDC reserve growth was 19.6%, and the maximum was 24.3%. The average USDC reserve growth was 22.2%.\nScreen Shot 2023-10-06 at 5.45.49 PM1664×450 35.2 KB\nThe comet accumulated $3.07k USDC reserves while distributing $22.16k COMP rewards for a weekly Net Protocol Profit of $-19.09k.\\n[Gauntlet] Polygon v3 USDC Update (10/06/2023 - 10/12/2023)\nGauntlet would like to provide the community with an update on metrics from the Polygon v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are up 7.52%, from $17.12M to $18.41M.\nUSDC Supply is up 7.86%, from $22.47M to $24.23M.\nUSDC utilization decreased 0.29%, from 76.2% to 75.9%.\nThe minimum USDC reserve growth was -1.5%, and the maximum was 24.4%. The average USDC reserve growth was 21.0%.\nThe comet accumulated $3.05k USDC reserves while distributing $20.48k COMP rewards for a weekly Net Protocol Profit of $-17.43k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-10-13 at 1.19.18 PM1800×470 65.7 KB\nTotal Collateral (USD) is up 20.31%, from $31.65M to $38.08M.\nScreen Shot 2023-10-13 at 1.19.50 PM1796×494 64.2 KB\nUSDC Supply is up 7.86%, from $22.47M to $24.23M.\nScreen Shot 2023-10-13 at 1.20.14 PM1806×484 63.9 KB\nUSDC Borrows are up 7.52%, from $17.12M to $18.41M.\nScreen Shot 2023-10-13 at 1.20.31 PM1642×462 43.5 KB\nUSDC utilization decreased 0.29%, from 76.2% to 75.9%.\nSupply Caps\nScreen Shot 2023-10-13 at 1.23.25 PM3790×614 106 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-10-13 at 1.23.51 PM3784×790 210 KB\nAbove is a time series of supply cap utilization for each asset over the past week. The MaticX supply cap increased from 6.70% to 99.40% on 10/10/23 due to the user with address 0x1e5b92c66e4cad7963e8dacf1e8d642304c172c8 who supplied $3.08M MaticX. We will follow up next week with any next steps on the MaticX supply cap.\nUtilization and Reserves\nScreen Shot 2023-10-13 at 1.21.02 PM1648×432 38.1 KB\nThe minimum USDC utilization was 75.9%, and the maximum was 86.5%.\nThe minimum USDC reserve growth was -1.5%, and the maximum was 24.4%. The average USDC reserve growth was 21.0%.\nScreen Shot 2023-10-13 at 1.21.21 PM1654×464 32 KB\nThe comet accumulated $3.05k USDC reserves while distributing $20.48k COMP rewards for a weekly Net Protocol Profit of $-17.43k.\\n[Gauntlet] Polygon v3 USDC Update (10/13/2023 - 10/19/2023)\nGauntlet would like to provide the community with an update on metrics from the Polygon v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are up 5.59%, from $18.41M to $19.44M.\nUSDC Supply is down 0.49%, from $24.23M to $24.11M.\nUSDC utilization increased 6.11%, from 75.9% to 80.6%.\nThe minimum USDC reserve growth was 3.5%, and the maximum was 24.4%. The average USDC reserve growth was 16.7%.\nThe comet accumulated $2.8k USDC reserves while distributing $19.62k COMP rewards for a weekly Net Protocol Profit of $-16.82k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-10-20 at 1.30.37 PM1802×498 66.4 KB\nTotal Collateral (USD) is up 0.26%, from $39.43M to $39.54M.\nScreen Shot 2023-10-20 at 1.30.53 PM1796×504 67.3 KB\nUSDC Supply is down 0.49%, from $24.23M to $24.11M.\nScreen Shot 2023-10-20 at 1.31.11 PM1796×488 64.2 KB\nUSDC Borrows are up 5.59%, from $18.41M to $19.44M.\nScreen Shot 2023-10-20 at 1.31.30 PM1658×462 41 KB\nUSDC utilization increased 6.11%, from 75.9% to 80.6%.\nSupply Caps\nScreen Shot 2023-10-20 at 1.34.14 PM3800×620 107 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-10-20 at 1.34.48 PM3796×842 195 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-10-20 at 1.31.47 PM1670×426 45.3 KB\nThe minimum USDC utilization was 75.9%, and the maximum was 84.7%.\nThe minimum USDC reserve growth was 3.5%, and the maximum was 24.4%. The average USDC reserve growth was 16.7%.\nScreen Shot 2023-10-20 at 1.33.15 PM1654×466 32.9 KB\nThe comet accumulated $2.8k USDC reserves while distributing $19.62k COMP rewards for a weekly Net Protocol Profit of $-16.82k.\\n[Gauntlet] Polygon v3 USDC Update (10/20/2023 - 10/26/2023)\nGauntlet would like to provide the community with an update on metrics from the Polygon v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are up 3.49%, from $19.44M to $20.12M.\nUSDC Supply is down 1.46%, from $24.11M to $23.76M.\nUSDC utilization increased 5.00%, from 80.6% to 84.6%.\nThe minimum USDC reserve growth was -6.9%, and the maximum was 24.3%. The average USDC reserve growth was 12.1%.\nThe comet accumulated $1.86k USDC reserves while distributing $21.41k COMP rewards for a weekly Net Protocol Profit of $-19.55k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-10-27 at 4.59.41 PM1802×484 65.1 KB\nTotal Collateral (USD) is down 1.30%, from $47.9M to $47.28M.\nScreen Shot 2023-10-27 at 5.00.05 PM1800×488 67.3 KB\nUSDC Supply is down 1.46%, from $24.11M to $23.76M.\nScreen Shot 2023-10-27 at 5.01.00 PM1798×486 66.1 KB\nUSDC Borrows are up 3.49%, from $19.44M to $20.12M.\nScreen Shot 2023-10-27 at 5.01.20 PM1654×452 43.6 KB\nUSDC utilization increased 5.00%, from 80.6% to 84.6%.\nSupply Caps\nScreen Shot 2023-10-27 at 5.03.07 PM3792×616 106 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-10-27 at 5.03.32 PM3792×836 197 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-10-27 at 5.01.58 PM1660×422 38.6 KB\nThe minimum USDC utilization was 77.3%, and the maximum was 89.3%.\nThe minimum USDC reserve growth was -6.9%, and the maximum was 24.3%. The average USDC reserve growth was 12.1%.\nScreen Shot 2023-10-27 at 5.02.19 PM1666×466 33.5 KB\nThe comet accumulated $1.86k USDC reserves while distributing $21.41k COMP rewards for a weekly Net Protocol Profit of $-19.55k.\\n[Gauntlet] Polygon v3 USDC Update (10/27/2023 - 11/02/2023)\nGauntlet would like to provide the community with an update on metrics from the Polygon v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are up 4.90%, from $20.12M to $21.1M.\nUSDC Supply is up 4.61%, from $23.76M to $24.86M.\nUSDC utilization increased 0.28%, from 84.6% to 84.8%.\nThe minimum USDC reserve growth was -2.1%, and the maximum was 11.1%. The average USDC reserve growth was 3.4%.\nThe comet accumulated $0.68k USDC reserves while distributing $22.5k COMP rewards for a weekly Net Protocol Profit of $-21.82k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-11-03 at 10.20.54 AM1798×472 65.1 KB\nTotal Collateral (USD) is up 0.08%, from $48.93M to $48.97M.\nScreen Shot 2023-11-03 at 10.21.13 AM1794×486 65.2 KB\nUSDC Supply is up 4.61%, from $23.76M to $24.86M.\nScreen Shot 2023-11-03 at 10.21.35 AM1796×486 62.6 KB\nUSDC Borrows are up 4.90%, from $20.12M to $21.1M.\nScreen Shot 2023-11-03 at 10.21.53 AM1668×454 48.4 KB\nUSDC utilization increased 0.28%, from 84.6% to 84.8%.\nSupply Caps\nScreen Shot 2023-11-03 at 10.23.18 AM3534×614 104 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-11-03 at 10.23.39 AM3542×848 185 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-11-03 at 10.22.27 AM1658×430 50.5 KB\nThe minimum USDC utilization was 82.6%, and the maximum was 86.8%.\nThe minimum USDC reserve growth was -2.1%, and the maximum was 11.1%. The average USDC reserve growth was 3.4%.\nScreen Shot 2023-11-03 at 10.22.42 AM1668×456 35 KB\nThe comet accumulated $0.68k USDC reserves while distributing $22.5k COMP rewards for a weekly Net Protocol Profit of $-21.82k.\\n[Gauntlet] Polygon v3 USDC Update (11/03/2023 - 11/09/2023)\nGauntlet would like to provide the community with an update on metrics from the Polygon v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are up 4.08%, from $21.1M to $21.96M.\nUSDC Supply is up 3.22%, from $24.86M to $25.66M.\nUSDC utilization increased 0.76%, from 84.8% to 85.5%.\nThe minimum USDC reserve growth was -7.1%, and the maximum was 24.3%. The average USDC reserve growth was 0.9%.\nThe comet accumulated $0.09k USDC reserves while distributing $24.51k COMP rewards for a weekly Net Protocol Profit of $-24.42k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-11-10 at 1.28.58 PM1798×504 64.8 KB\nTotal Collateral (USD) is down 2.27%, from $51.05M to $49.89M.\nScreen Shot 2023-11-10 at 1.29.24 PM1800×492 65.3 KB\nUSDC Supply is up 3.22%, from $24.86M to $25.66M.\nScreen Shot 2023-11-10 at 1.29.50 PM1800×508 63 KB\nUSDC Borrows are up 4.08%, from $21.1M to $21.96M.\nScreen Shot 2023-11-10 at 1.30.09 PM1662×472 44.1 KB\nUSDC utilization increased 0.76%, from 84.8% to 85.5%.\nSupply Caps\nScreen Shot 2023-11-10 at 1.31.25 PM3782×626 105 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-11-10 at 1.31.48 PM3794×846 209 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-11-10 at 1.30.26 PM1682×444 40.1 KB\nThe minimum USDC utilization was 79.7%, and the maximum was 89.5%.\nThe minimum USDC reserve growth was -7.1%, and the maximum was 24.3%. The average USDC reserve growth was 0.9%.\nScreen Shot 2023-11-10 at 1.30.45 PM1660×468 29.7 KB\nThe comet accumulated $0.09k USDC reserves while distributing $24.51k COMP rewards for a weekly Net Protocol Profit of $-24.42k.\\n[Gauntlet] Polygon v3 USDC Comet: (11/10/23 - 11/16/23)\nGauntlet would like to provide the community with an update on the Polygon v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows increased 9.39%, from $22.0M to $24.07M.\nUSDC Supply increased 3.96%, from $25.72M to $26.74M.\nUSDC utilization increased 5.22%, from 85.55% to 90.02%.\nThe minimum USDC reserve growth was -9.95%, and the maximum was 0.94%. The average USDC reserve growth was -4.66%.\nThe comet accumulated $-1.38K USDC reserves while distributing $26.34K COMP rewards for a weekly Net Protocol Profit of $-27.72K.\n\nCollateral Asset Supply\nThis graph shows the time series of total supply of all collateral assets.\nSupply1920×1080 95.1 KB\nTo see updated statistics, please see the live version of this graph here.\nUSDC Borrows\nThis graph shows the time series of USDC borrows.\nBorrows1920×1080 58.4 KB\nTo see updated statistics, please see the live version of this graph here.\nUtilization\nThis graph shows the utilization (borrow / supply) of USDC over the past week.\nUtilization1920×1200 193 KB\nSupply Cap Usage\nThis graph shows the supply cap usage (supply / supply cap) of all collateral assets over the past week.\nSupply Cap Usage1920×1200 155 KB"
  },
  {
    "number_of_comments": 10,
    "postid": "81dd5ba8-e94f-4e93-9de9-bed6716462c2",
    "posturl": "https://www.comp.xyz/t/temperature-check-aera-for-compound/4091",
    "combinedcontent": "The Gauntlet team recently launched Aera 16, an autonomous treasury management protocol. This post is a temperature check to assess the community’s appetite to trial Aera for a portion of Compound v2’s reserve funds.\n\nIntroducing Aera\nAera is a solution for optimizing DAO funds autonomously and on-chain. For most DAOs, insurance funds (e.g., reserves, treasuries, safety modules, backstops) are not actively managed or adjusted based on market conditions. For DAOs, this can lead to an inability to maintain runway, cover liabilities, and benefit from growth in the market. Traditional institutions can allocate funds to more nimble managers who make day-to-day decisions, but DAOs face numerous challenges with this model including governance and creating strong incentive alignment with external managers.\nAera provides DAOs with a one-stop solution for managing insurance funds efficiently and transparently. The Aera protocol consists of vaults, which are constructed on a per-protocol basis and can hold a combination of stablecoins, native tokens, and other cryptocurrencies. The objective function of the vault is determined by each DAO and is highly customizable ranging from simply keeping fund proportions in line with borrows, to complex hedging strategies using on-chain options. Vaults are automatically rebalanced by multiple actors (Guardians and Arbitrageurs) who compete on-chain to propose the best combination of assets in the portfolio. This ensures that the vault objective is met across a wide range of market scenarios and time horizons.\nFor more detail, here 13 is a quick video that walks through how things work.\n\nProduct___Aera_Finance3088×1246 148 KB\n\n\nGuardians are experienced risk analysts and can be institutions or individuals. They compete by periodically submitting asset weights to rebalance the vault, which evolve over time to keep up with the objective and market variables. Their submissions are aggregated amongst other Guardians and weighted based on historical performance. To participate, Guardians must stake their own assets and reimburse the vault if their decisions underperform.\nArbitrageurs in the open market execute transactions to rebalance to the Guardians’ preferred allocation to earn profit\n\n\nKey Benefits\nThe Compound v2 market on Ethereum has accumulated over $40M of reserves, which are now primarily in the form of USD stablecoins ($22M DAI, $14M USDC, $3M USDT), WBTC ($3M), ETH ($1M), and BAT ($1M). These reserves are static and don’t track risk exposure. In previous discussions on reserve management, the community has shown interest in reallocating reserves to a more optimal asset mix 8. With the launch of Aera, there is now a purpose-built solution that simplifies and automates much of this optimization.\nA few key benefits for Compound:\n\nAera helps to minimize bureaucracy. The DAO doesn’t need to plan strategies, but rather just pick assets.\nThe Aera protocol continuously rebalances Compound’s fund portfolio based on actual liabilities and market conditions\nAera allows for coordination of a decentralized network of actors working together transparently on-chain to optimize the vault\n\n\nHow It Works\n\nCompound governance deposits a portion of reserves into an Aera Vault\nCompound governance works with a Guardian to set the objective and selects a set of assets for the vault. Gauntlet will serve as the initial Guardian with all fees set to zero. Later in 2023, a new version will launch with the ability to assign new Guardians and enable vault fees to promote Guardian specialization and competition.\nGuardians and Arbitrageurs continuously rebalance the vault based on the objective and market conditions\nCompound can view vault performance at any time through the public dashboard and have instant access to funds due to the self-custodial design of Aera\n\n\nFAQs\n\nWhat does this cost?\n\nFree during trial phase. Fees enabled in late 2023 during scaled rollout phase.\n\n\n\n\nNext Steps\nIt would be great to hear the community’s feedback on trialing Aera and happy to answer any questions – please comment below. If the community is supportive of a trial, we will follow up with a detailed proposal to vote on.\\nWe support the idea of diversifying and optimizing the treasury, as this is crucial for the sustainability of the protocol in the long run. From our perspective, it would be great to first launch it with one specific treasury asset (like the one llama proposed for $CRV on AACE).\nWe would be happy to vote for it if the mechanism  has been tested by the market while generating consistent yields.\\nThis is  a very innovative product with the potential to automate treasury risk management and other novel use cases. I would  support the idea of  Compound exploring it initially on a limited basis. However, as Compound gets its feet wet, I would also like to see additional details as follows:\n\n\nFrequency of weight submissions by the Guardians has an impact on transaction costs incurred by the DAO. I think it would help to know the comparative cost estimates, say for daily submission  vs. weekly submissions.\n\n\nThe AERA concept is fairly straight-forward under normal market conditions. However, things can get complicated under volatile markets. It would be helpful to see examples addressing scenarios involving multiple rounds of mid-epoch trading by Arbitrageurs, price ”whipsaw”, reversion to the mean etc.\n\n\nAdditionally, one general observation regarding the use of options in a framework such as AERA’s is the following. (I understand that Gauntlet is not proposing to use options in the initial trial for Compound, so this discussion may not be immediately relevant)\nMy concern is that having options as part of the asset mix during volatile markets can lead to adverse results (in terms of portfolio value), given that Arbitrageurs can trade any number of times as prices whipsaw, with their primary concern being correct asset allocation and arbitrage profits, and not overall portfolio value. To prevent loss in portfolio value, Guardians would be required to predict not only the underlying asset prices correctly (where they would end-up by the end of the epoch), they would also need to predict the trajectory of prices through the epoch, so that they can estimate implied volatility and option prices. When underlying prices revert to the mean, or if volatility suddenly subsides, option prices can collapse, locking in losses. This means that Guardians may be correct in predicting underlying prices, but can still incur slashing. In theory, they are supposed to be experts and be able to predict even options implied vol, but it’s not clear how this will work in practice if the options markets are illiquid and bid-ask spreads are too wide. However, on the other hand, without using such hedging instruments, protecting against crashes may be difficult.\nOptions are generally cheap just before big price moves, and not optimal to deploy once volatility has already taken afoot. One way to deal with this situation is to run the epoch based on some chart events on a reference asset such as ETH (say, a moving average crossover, which is blunt with false positives, but more sophisticated ones can be constructed) rather than on a periodic basis such as weekly epochs. This is just a thought.\\nThanks @dtalwar for the post!\nWe are in support of trialling Aera for Compound especially as multi-chain deployments increase and subsequently, so will reserves.\nAssuming the temperature check passes, we look forward to a detailed proposal outlining the initial TVL, objective function, and asset selection!\\nThanks for this initiative. I’m generally supportive of having DAO treasuries actively managed to allocate capital efficiently and help maintain runway. However, I’m not sure whether this specific implementation would be the best way to do it. DAO treasuries should work more like endowments: focusing on keeping the principal amount intact through close management of underlying risk factor exposure.\nThe concept of a “decentralized network of actors working together” seems appealing at first sight, but complexity opens the door to unexpected outcomes 3:\n\nWhat happens in an extreme market event e.g. sudden price drop; depeg; protocol hack? What if none of the depositing DAO executors was aware on time to withdraw the funds?\nWhat happens when a vault guardian amasses a significant amount of voting weight on the aggregation function to a point where it becomes economically viable to collude with arbitrageurs and drain the CFMM pool?\n\nHas Aera been tested before? If so, could you please share any meaningful results?\\nThank you for all the thoughtful questions and comments!\n\n\n\n Michigan_Blockchain:\n\nWe would be happy to vote for it if the mechanism has been tested by the market while generating consistent yields.\n\n\nThe objective function (e.g. yield generation, maintain runway, etc) will ultimately be decided by the community for the pilot. We’ll share more details on this in an upcoming forum post. Aera has been audited by Spearbit and has been in testing for the past 6+ months on Polygon.\n\n\n\n RogerS:\n\nFrequency of weight submissions by the Guardians has an impact on transaction costs incurred by the DAO. I think it would help to know the comparative cost estimates, say for daily submission vs. weekly submissions.\n\n\nFor context, there are two transaction costs - (1) gas fees, which are paid by Guardians and Arbitrageurs and (2) arbitrage loss, which is incurred by the DAO. It’s difficult to provide cost estimates for daily vs weekly submissions given the amount of variables involved (e.g., the asset, along with its historical volatility and on-chain liquidity, etc). Once the community selects the assets for the pilot (more details on assets and the selection process will be included in the upcoming forum post), we will determine the optimal epoch length and can share this data with the community as well.\n\n\n\n RogerS:\n\nThe AERA concept is fairly straight-forward under normal market conditions. However, things can get complicated under volatile markets. It would be helpful to see examples addressing scenarios involving multiple rounds of mid-epoch trading by Arbitrageurs, price ”whipsaw”, reversion to the mean etc.\n\n\nYou are correct that things get complicated under volatile conditions. It’s worth noting a few things about the incentives and roles of Guardians and Arbitrageurs during these conditions. Guardians are incentivized to predict future volatility and incorporate this into their submissions to avoid incurring losses. Arbitrageurs are constantly rebalancing the vault to achieve the optimal weights - this is similar to a Balancer pool. Arbitrage loss would be equivalent to a Balancer pool and Arbitrageurs pay fees to the DAO.\n\n\n\n RogerS:\n\nOptions are generally cheap just before big price moves, and not optimal to deploy once volatility has already taken afoot.\n\n\nAs you mentioned, options will not be part of the pilot. Worth noting that options will be over collateralized, which reduces the likelihood of adverse events resulting in volatility. There would also be constraints on the percent of the portfolio allocated to options - this would also depend on the DAO’s objective function.\n\n\n\n naokiohno:\n\nWhat happens in an extreme market event e.g. sudden price drop; depeg; protocol hack?\n\n\nOur highest priority with Aera is building a trusted product with a strong track record given that we are working with treasuries. For this reason, we are launching small pilots (like this one we are proposing with Compound) to complement 12+ months of rigorous research and testing where we’ve simulation extreme market events and built an incentive mechanism that responds to volatility.\n\n\n\n naokiohno:\n\nWhat happens when a vault guardian amasses a significant amount of voting weight on the aggregation function to a point where it becomes economically viable to collude with arbitrageurs and drain the CFMM pool?\n\n\nThe design of the protocol is such that the staking and slashing costs of collusion are extremely high (the whitepaper unpacks the math behind when and how this would happen if you’re curious). Aera will start off with a high collateral ratio for the amount staked by Guardians and will reduce this ratio over time as the protocol matures.\\n\n\n\n dtalwar:\n\nAera has been audited by Spearbit and has been in testing for the past 6+ months on Polygon.\n\n\n@dtalwar Thanks for your reply. Could you please share the results of this testing period so far?\\n\n\n\n dtalwar:\n\nFor context, there are two transaction costs - (1) gas fees, which are paid by Guardians and Arbitrageurs and (2) arbitrage loss, which is incurred by the DAO.\n\n\nThanks @dtalwar! Can you elaborate your point 2 above? Is it just price slippage or there other reasons too? An example would help.\nWhitepaper (p. 19) says the following: “The goal of the parameter submitters\nis to enable the strategy utilized on-chain to be dynamic and competitive, reducing the\noverall adverse selection and transaction cost for a DAO.\nA unique challenge faced by vaults offering dynamic rebalance strategies is deciding on an\noptimal rebalancing frequency. If a vault rebalances too frequently, then it will realize losses\nfrom excessive transaction costs.”\nThe above seems to be referring to the impact of submission frequency on the transaction costs and related losses. Can you provide more details?\\n\n\n\n dtalwar:\n\nAs you mentioned, options will not be part of the pilot.\n\n\nEven without using options, volatility can wreak havoc. Last 24 hours is an object lesson. Due to Cirlce’s exposure to SVB, USDC ( and thereby DAI) have depegged from dollar. Now, FDIC is known to act with extreme efficiency 1, and the most likely outcome on Monday or Tuesday morning will be an announcement that SVB depositors will be able to recover most, if not all, of their deposits. This will probably cause the peg to be restored at a very rapid clip. Under AERA’s regime, if Arbitrageurs had rebalanced due to depeg, they may not have enough time to rebalance to adjust to the repeg when the announcement hits, and can cause losses to the portfolio. This again means that even if the Guardians are correct by the end of the epoch in their price predictions, they may still get slashed because of loss in portfolio value.\nThis points to the core limitation of AERA model: the incentives of Guardians and Arbitrageurs are different / misaligned. Guardians want to protect the value of the portfolio over a longer timeframe (duration of the epoch), while Arbitrageurs only care about asset allocation weights and immediate profits.\nOf course, losses to the portfolio are recovered by the DAO (not sure if this make-whole?) by slashing the Guardians. But then, this raises the question if Arbitrageurs are truly adding value or making things worse, and if it’s possible to create a model where only Guardians exist. In this model, if the Guardians are wrong, they get slashed, but Arbitrageurs’ actions won’t cause additional losses for them.\nIn my view two aspects of AERA are worth examining further:\n\nIf it’s possible to create a system where only Guardians exist, or with two classes of them: Submitters vs. Executors, and both do staking.\nIf it’s possible to run the epochs not periodically, but based on some chart events that approximately portend a significant price move.\n\\nHi, I’m Ignacio from Avantgarde Finance 2.\nWe’re excited about Aera. However, we’d also like to put forward an approach to managing these funds and I’d imagine others would like to put forward other approaches too. We’re wondering if an open application process would make more sense.\nWe’d recommend that an RFP goes out for a period 2-3 weeks giving people the time to propose alternative approaches. At that point, the DAO votes on the 2-3 approaches it likes best with a path to ramping up exposure over time once the approach is proven & tested.\nTo further support the idea of an open application process, we can draw inspiration from the successful approach taken by the Auditing Compound Process 1, which was won by OpenZeppelin.\\n\n\n\n naokiohno:\n\nCould you please share the results of this testing period so far?\n\n\nOf course. We have been running a vault on Polygon (submitting weights daily) with $220k of our own funds. This vault models the loanbook of another client and was positioned as insolvency coverage. The portfolio has largely maintained a composition of 90% ETH and 10% USDC as the risk in the loanbook of the client was low and thankfully had no insolvencies or drawdowns that caused a liquidity crunch during this period. The vault is valued at around $250k today.\n\n\n\n RogerS:\n\nCan you elaborate your point 2 above? Is it just price slippage or there other reasons too?\n\n\nYes - it is price slippage and loss versus rebalancing.\n\n\n\n RogerS:\n\nThis points to the core limitation of AERA model: the incentives of Guardians and Arbitrageurs are different / misaligned.\n\n\nThe point is for them to have opposite incentives. The Arbitrageurs have a low time preference and the Guardians have a high time preference. By being as close to zero sum as possible, you ensure that colluding cannot drain the vault. If they had the same incentives, that would effectively mean they could drain the vault.\n\n\n\n ignaciorsg.eth:\n\nWe’re wondering if an open application process would make more sense.\n\n\nWe have deliberately chosen to bring Aera to market slowly through rigorous research and testing both internally and through very small (and free) external pilots like this one to inform development and get feedback from the community as we build. Given Aera’s stage (pre-launch, mechanism and fee model still in development), there isn’t a great way to compare it to other options."
  },
  {
    "number_of_comments": 13,
    "postid": "f3dfb69f-4fcd-4e2f-957b-2129b1cf43b0",
    "posturl": "https://www.comp.xyz/t/add-market-matic/2179",
    "combinedcontent": "1.Summary:\nWe would like to ask the Compound community to consider adding Polygon (MATIC) token as a collateral.\nMany of the content is copied from MATIC collateral application on MakerDAO governance:\n\n  \n      \n\n      The Maker Forum – 25 May 21\n  \n\n  \n    \n\n[MATIC] - MIP6 Collateral Onboarding [updated] 16\n\n  1. Who is the interested party for this collateral application?  Polygon Network  2. Provide a brief high-level overview of the project, with a focus on the applying collateral token.  References  MATIC is the ecosystem token of Polygon Network...\n\n  \n    Reading time: 4 mins \uD83D\uDD51\n      Likes: 74 ❤\n  \n\n  \n\n  \n    \n    \n  \n\n  \n\n\n2.Applicant:\nStable Node. Stable Node was founded by Gustav Arentoft and Doo Wan Nam who are seasoned business development and growth professionals. We both spent about three years growing the MakerDAO protocol in the European and Asian markets. Stable Node invests and also assists with business development and governance.\nStable Node Twitter: https://twitter.com/StableNode 6\n3.Interested Collateral:\nERC-20 Polygon (MATIC)\n4.High-level overview of the project:\nMATIC is the ecosystem token of Polygon Network (previously Matic) - a 100% EVM compatible L2 scaling solution with current TVL of more than $6B and 350+ dapps deployed across DeFi, NFT, Gaming and DAO space. With the recent rebranding Polygon has now expanded in scope and vision and have transformed into Polygon with the aim of becoming an Ethereum scaling aggregator - thereby providing developers with L2 solutions in addition to the POS/Plasma chain (mainnet launched April 2020), zk and Optimistic Rollups and Validum chains (part of the roadmap - as seen on website http://polygon.technology/ 7).\n5.A brief history of the project:\nMatic team has been making valuable contributions to the Ethereum ecosystem for a long time, even before it’s public surfacing as Matic Network. This includes working on implementations of Plasma MVP, developing the WalletConnect protocol and the Ethereum event notification engine Dagger.\nMATIC token has now been listed over 30 exchanges globally. In terms of market cap MATIC is valued at $9.5B and FDV of $14.7B (as of 8/28/2021). 24H trading volumes of >$800M make it one of top 30 traded coins across DEXs and CEXs.\nPolygon POS/Plasma hybrid chain is a production ready L2 scaling solution that is 100% EVM compatible, decentralized with 88 validators and battle-tested with 200+ dapps building on it. Average txn/day around 200k and 260k+ wallet addresses. These numbers are increasing at a faster rate than competitors owing to the NFT craze and DeFi initiatives launched by dapps on Polygon.\nYou can check all the dapps building on Polygon here on awesomepolygon.com 7\n6.Relevant documents:\nProject - Website http://polygon.technology/ 7\nWhitepaper - Papers - Polygon Papers - Polygon | Ethereum's Internet of Blockchains 2\nDocument portal - Official documentation https://docs.matic.network/\nMatic Asset codebase - Github GitHub - maticnetwork/contracts: Smart contracts comprising the business logic of the Matic Networ\nEthereum addresses - Etherscan $1.4021 | Matic Token (MATIC) Token Tracker | Etherscan 1\n7.Audits:\nAudits - Quantstamp audits 5_6190238193657316409.pdf - Google Drive 2\n8.Community links:\nDiscord: Polygon 1\nReddit: https://www.reddit.com/r/maticnetwork/\nOfficial forum: https://forum.matic.network/\n9.The use case for the token:\nThe MATIC token lies at the heart of the Polygon ecosystem with multiple use-cases. Primarily the MATIC token is used for paying gas fees on the Polygon network. It runs the ecosystem, supporting top Dapps. Additionally you can participate in the Proof-of-Stake consensus of the blockchain as a validator node and earn 12-14% APY in MATIC tokens.\n9.1 DeFi use case for the token:\nMATIC is also used as collateral for various DeFi protocols including MakerDAO and QiDAO.\n10.Where does exchange for the asset occur?\nMATIC is one of top traded assets on all CEXs. On-chain pairs are most liquid on Uniswap and Bancor.\n11.Oracle Integration:\nChainlink which supports Compound protocol also supports price feed for MATIC\n12.Next steps:\nWe would like the Compound community to consider the application and assess collateral factor, reserve factor, and borrowing limit and put into a formal vote.\nFor reference, on Maker protocol, minimum collateralization ratio is 175%, stability fee at 3% and debt ceiling at 3 million, but are in discussion to increase to 50 to 100 million.\nThank you and let us know any questions and comments.\\nHey Doo,\nI’ll look into the Risk Assessment Questions more thoroughly tomorrow.\nRisk Assessment Questions:\nMoved to a seperate reply.\nAddressing your questions:\nCollateral Factor:\nCollateral factor provides a buffer in which the borrower can be safely liquidated before becoming insolvent. Generally, assets are first added with 0% collateral factor.\nOnce the asset is added, we can compare MATIC to benchmarks to find the right CF.\nCollateral factor should be lower for assets that are more risky for liquidators or Compound to hold. Collateral factor should be lower for assets with large centralized holdings compared to DEX liquidity. Collateral factor can be higher for assets with high DEX liquidity.\nWe may say MATIC is less risky than USDT (0% CF), but more risky than ETH (75% CF).\nMKR (35% CF)\nUNI (60% CF)\nZRX (65% CF)\nBased on these numbers, MATIC may end up around 35% to 65% CF.\nReserve Factor:\nReserve factor directs some of the interest to insurance for the protocol. Over time, the growing reserves protect the protocol from insolvency. This could be needed if liquidation is too slow. For volatile assets, the standard reserve factor is 25%.\nBorrow cap:\nEach asset has a borrow cap. Once that much of the asset has been borrowed, no more can be borrowed by anyone.\nBorrow cap accomplishes two things:\n\n\nIt limits damage from a vulnerability leading to infinite minting of another collateral asset.\n\n\nIt limits governance sway using borrowed funds.\n\n\nBorrow cap may not matter until there are greater funds in the market. I’ve heard the Community Multisig has the ability to adjust the borrow caps, so we may not need to address borrow cap right now. Does Polygon use MATIC for governance? If not, we can leave borrow cap unlimited.\\nThanks for this follow up and flurry for questions. Happy to answer all.\nQuick clarification - Will existing cToken contracts work with MATIC?\nIf cTokens work with other ERC20s like MKR, wBTC - pretty sure they should work with MATIC (also a simple ERC20).\\nRisk Assessment Questions:\nQ: How much CEX/DEX liquidity is there?\nA: Coingecko provides info about liquidity/market depth 4. There’s much more liquidity across CEXs: ~$25mm -2% liquidity. Coingecko puts UniswapV3 -2% depth at $223k.\nUniswap depth comes from $7.62mm 1 of MATIC liquidity on Uniswap V3.\nQ: Which DEX has the most liquidity?\nA:\n\nUniswap and Bancor\n\nQ: How does liquidity compare to the Market Cap?\nA:\n\nIn terms of market cap MATIC is valued at $9.5B\n\nQ: Can the MATIC token contract be upgraded?\nA: No. 4\nQ: Does the collateral token contract have a fixed supply?\nA: Yes. MATIC was initialized with the whole 10 Billion supply sent to msg.sender. 4\nQ: Are there any large MATIC holders?\nA: Ownership concentration: Here. 8\nContracts:\nEach of these contracts has been audited.\nMaticTokenVesting funds are controlled by the Polygon Foundation Multisig below.\nStakeManagerProxy and Plasma DepositManagerProxy are upgradable by this 2 day timelock 1.\nA 5/9 Multisig 1 can execute from the timelock.\n\n\nStakeManagerProxy 1 holds 19.6% ($2,844,303,343).\nPolygon (Matic) Vesting Contract : MaticTokenVesting holds 16.6% ($2,412,407,277).\nPolygon (Matic) Plasma Bridge : DepositManagerProxy holds 6.2% ($929,535,671).\n\nMultisig Wallets:\n\nPolygon (Matic) Foundation Contract : 2/5 MultisigWalletWithDailyLimit 2 holds 12.7% ($1,899,262,975).\n\n2/3 GnosisWallet 1 holds 7.8% ($1,159,644,837).\n\n2/3 GnosisWallet holds 1.8% ($270,857,271).\n\n3/6 GnosisWallet holds 1.0% ($149,587,119).\n\nOther Wallets:\nTODO: Are there any security procedures protecting these accounts?\n\n\nWallet holds 4.7% ($695,787,036).\nPolygon (Matic) Mining & Seeding : Wallet holds 4% ($596,388,888).\n\nWallet holds 1.8% ($262,675,456).\nPolygon (Matic): Marketing & Ecosystem : Wallet holds 1.5% ($230,363,933).\n\nWallet holds 1.5% ($224,381,950).\n\nQ: Will existing cToken contracts work with MATIC?\nA: Existing cToken contracts should work because MATIC is pretty standard.\nThe tokens are pausable. This could prevent liquidation if MATIC is added as a collateral asset.\nTODO:\nWho has the power to pause MATIC?\nUnder what circumstances will MATIC token be paused?\nMATIC token is mostly standard OpenZeppelin contracts. The only new code appears to be the MaticToken constructor.\n// File: contracts/MaticToken.sol\n\ncontract MaticToken is ERC20Pausable, ERC20Detailed {\n    constructor (string memory name, string memory symbol, uint8 decimals, uint256 totalSupply)\n    public\n    ERC20Detailed (name, symbol, decimals) {\n        _mint(msg.sender, totalSupply);\n    }\n} \n\nQ: How long has it been since the last exploit?\nA:\n\nThere have been no exploits till date on the MATIC token thankfully. \n\nQ: How long has it been since launch?\nA:\n\nmainnet launched April 2020\n\nAccording to etherscan analytics, MATIC token was deployed April 2019. I guess the system was deployed in pieces.\nQ: What security issues were raised in the collateral token’s audit reports?\n\nAre any of these relevant to its use as collateral in Compound?\n\nA:\nThe Quant Stamp audit focused on Plasma Contracts. I don’t see any issues here about MATIC token.  Nothing in the report seems relevant to its use as collateral in Compound.\nI found a Nomic Labs audit 2 of the MaticTokenVesting and MaticToken contracts.\nAccording to the audit summary,\n\nAll issues have been properly addressed by the Matic team.\n\\nThere have been no exploits till date on the MATIC token thankfully. \\nThanks for progressing with the application. And while the process is different, Maker Risk Team has done a lot of work on looking at MATIC address and movement and etc as well. Hopefully, it will help with the process as well: [MATIC] Collateral Onboarding Risk Evaluation - Domain Work - The Maker Forum 2\nThank you.\\nIt would be good to have some feedback on my deployment before deploying to mainnet.\nRopsten CToken: 0xdD3F345DdbB38ED1926B738bc9C026a5883d8051\nnpx saddle match 0xdD3F345DdbB38ED1926B738bc9C026a5883d8051 CErc20Delegator -n ropsten 0xc4c8c1125A91dA265d6bbcC868Fd115c5cfc40D4 0xcfa7b0e37f5ac60f3ae25226f5e39ec59ad26152 0x2341ba42eb00c63cf03559c9a2295a23ace7e4ad 200000000000000000000000000 \"Compound Matic Token\" \"cMATIC\" 8 0x2079A734292094702f4D7D64A59e980c20652Cae 0x0295a48b76bc68662bd15bfaecedca075a4f568f 0x\nLabeled Parameters:\nUnderying: 0xc4c8c1125A91dA265d6bbcC868Fd115c5cfc40D4\nComptroller: 0xcfa7b0e37f5ac60f3ae25226f5e39ec59ad26152\nInterestRateModel: 0x2341ba42eb00c63cf03559c9a2295a23ace7e4ad\nInitialExchangeRateMantissa: 200000000000000000000000000\nName: Compound Matic Token\nSymbol: cMATIC\nDecimals: 8\nAdmin: 0x2079A734292094702f4D7D64A59e980c20652Cae\nImplementation: 0x0295a48b76bc68662bd15bfaecedca075a4f568f\nbecomeImplementationDate: 0x\\nI think MATIC would be a great addition to the protocol.\nGood work on deploying to ropsten; know the implementation you are using is an old version. Here is the current 4 version.\\nI deployed cMATIC to mainnet.\nCErc20Delegator : 0x944dd1c7ce133b75880cee913d513f8c07312393 6\nRunning saddle match, I get an error message about gas fees:\n\nError: Returned error: err: max fee per gas less than block base fee: address 0x9889EBA2bbbF0c7B11442d55397303D25d88F7Dc, maxFeePerGas: 50000000000 baseFee: 72015953262 (supplied gas 6000000)\n\n@adam says:\n\nThis seems like eip 1559 bug. I think the solution is to wait for the gfx team to implement hardhat in place of saddle.\n\nnpx saddle match 0x944DD1c7ce133B75880CeE913d513f8C07312393 CErc20Delegator -n mainnet 0x7d1afa7b718fb893db30a3abc0cfc608aacfebb0 0x3d9819210a31b4961b30ef54be2aed79b9c9cd3b 0xd956188795ca6F4A74092ddca33E0Ea4cA3a1395 200000000000000000000000000 \"Compound Matic Token\" \"cMATIC\" 8 0x6d903f6003cca6255d85cca4d3b5e5146dc33925 0xa035b9e130F2B1AedC733eEFb1C67Ba4c503491F \"0x\"\\nYour deploy looks good. I’ll try to see if I can get the match to work on my side.\\nAny updates on this? ETA on when MATIC goes as collateral and users can deposit funds?\nHappy to help if any issues along the way.\\nHere’s the new oracle contract 3.\nIt appears the old oracle contract code is the same 1 as the new oracle contract code.\nThe ETH price returned by the oracle seems to match the old oracle and the MATIC price seems to roughly match coingecko.\nThe simulation 2 passes:\n\n…\nResult {\n‘0’: ‘10000000000000000000’,\n‘1’: ‘384’,\n‘2’: ‘1003578883971642670’,\n‘3’: ‘40429985162’,\nlength: 4,\ncashPrior: ‘10000000000000000000’,\ninterestAccumulated: ‘384’,\nborrowIndex: ‘1003578883971642670’,\ntotalBorrows: ‘40429985162’\n},\nundefined,\nResult {\n‘0’: ‘0x2f7e209e0F5F645c7612D7610193Fe268F118b28’,\n‘1’: ‘1000000003032248887’,\n‘2’: ‘5000000000’,\nlength: 3,\nredeemer: ‘0x2f7e209e0F5F645c7612D7610193Fe268F118b28’,\nredeemAmount: ‘1000000003032248887’,\nredeemTokens: ‘5000000000’\n}\n]\nAction: CToken cMATIC: MaticHolder (0x2f7e…) redeems 5e+9 tokens\nCommand: Print “cMATIC integration ok”\ncMATIC integration ok\n\\nChainlink just finished setting up the new oracle contract with LUSD, RAI, & FRAX support. While MATIC is included in the current oracle contract, we’re going to take the opportunity to add MATIC to the protocol and switch the oracle contract over to the new one.\nThe new oracle contract 2 is the same contract as the existing oracle contract. The config is the same for all current markets and includes the three new markets. Each new market uses a Chainlink oracle and a Uniswap v2 pool.\nWe expect the Matic addition to pass unanimously with a 0 CF listing. To reduce the amount of time Chainlink needs to update two oracles, the current and the proposed oracle, we’re submitting the changes together.\nGFX Labs is introducing the oracle, so the groups/individuals interested in adding the assets have the opportunity to bring them to a vote. This is not a statement of support or opposition to their additions. Please review the new oracle contract and the cMATIC market. We are targeting to submit the proposal on Monday.\\nHello everyone,\nI wanted to follow-up on the recent proposal 9 to list MATIC and expand on this statement:\n\nIt should be noted this asset addition has not been reviewed by Open Zeppelin. Given that MATIC is a well-established contract and the ctoken contract utilizes the current standard, there is a good precedent to support the asset addition. As usual, the proposal is to list MATIC with a 0 collateral factor. This allows Compound to onboard assets incrementally and safely.\n\nAs the proposal text says, the asset listing proposal hasn’t been reviewed by the OpenZeppelin security team. I discussed this proposal with @getty yesterday and conveyed my previous recommendation 3 that the community wait to list new assets until OpenZeppelin could do an asset review and define a more secure listing process. However, it could be several more weeks before we have a complete plan in place for securing asset listings as we are wrapping up the Compound Protocol Audit this week.\nThe security risks here appear to be low for the reasons mentioned in the Proposal but I would advise the community to proceed with caution and with an understanding that integration risks are always present. As Security Advisor, I want the community to be aware of the risks in unaudited proposals so that the community can make an informed decision with the best interests of the DAO in mind."
  },
  {
    "number_of_comments": 21,
    "postid": "a305d0fd-13e9-443a-b437-85acc8d3d5f4",
    "posturl": "https://www.comp.xyz/t/insane-fees-to-collect-compound-distributions/593",
    "combinedcontent": "Hope someone can clarify this for me on what’s going on.\n\n\nAfter supplying my coins to compound as a lender/borrower, I managed to earn 0.3 compound coin.\n\n\nI click on “collect” under Vote to collect those distributions.  Then in my wallet it pops up that this requires a fee of 0.1333 ether!\n\n\nAt current market of $570 per ether, this is ~$75, which is more than the cost of the compound distribution itself. I tried a couple more times, sometime i get a fee of 0.0344 ether instead which is still a lot at $17\n\n\nI am looking for some tips how this works,  should I just not collect the compound distribution and wait until it reaches a much higher number then do it, to save on the ether fee?  It seems this high fee for collecting compound coin kind defeats the profitability of this whole enterprise.\nThanks\\nCOMP is automatically distributed to your wallet when you interact with a market; check out the FAQ on https://compound.finance/governance/comp 123\nThe “claim” function attempts to withdraw COMP earned from every market – it’s not very efficient. You can use it when gas prices are low, or if you’re feeling skills in smart contracts, call function 20 (claim) via Etherscan 116 with your address and the cToken’s address - this is a “cheap” way to collect the COMP.\\nThanks for the info.  Is there any risk to leave the comp uncollected for long time - like 6 month to 1 year, then do some activity like add $10 to the supply or borrow $10 to trigger the withdrawal.\\nThat would work fine, and there should be no additional risk to that!\\neasy and cheap way for fee should be implemented automatically, most user cant directly interact with smart contract\\nhi guys, want to revisit this topic. So i have a decent amount of comp left unclaimed, and i tried to enable comp as collateral (paid ~$2 in ether for fees).  Per above, the unclaimed comp should then automatically become claimed right?  However it is still sitting as unclaimed even after the transaction has been confirmed.\nDoes anyone know what’s going on, or can give a direct cheapest way to claim your unclaimed comp without have to code against the api. It’s kind silly it doesn’t have an easy way or automatically to claim comp you earned…The claim button on the website is so expensive in term of fees, it might as well be useless.\nthanks\\nAutomatic claims on interactions are now removed, so by supply/borrow you wouldn’t now be collecting COMP automatically.\nAnd it wasn’t working like you imagine before. Enabling something wouldn’t claim anything. For example, if you had some USDC borrowed, than borrowing more or repaying some would claim COMP accrued from USDC borrow only, but COMP accrued from, for example, supplied DAI wouldn’t be collected.\nBut again, that function is now removed, i don’t think there’s any way now to relatively cheap claim through UI on website.\\nAh thanks for the info. Is there any plans to make claiming comp cheaper,\nHow do you guys do it, everyone is just coding against the api? I am sure there are lot for casual users like me who just use the website,  to claim 3 comp I have to pay $20 that’s just horrible.\nShould I just leave the comp unclaimed for like a year then claim it all at once, is there are risk leaving it unclaimed for extended period of time.\\nYou kind of first one to raise the topic since removal of auto-claim. It was executed less than 48 hours ago.\nHowever i highligted that issue when talks about removal of auto-claim just started month or couple month ago. That users would have problems with that, as only expensive claim all will remain in UI. But haven’t met enough support in that opinion  So i decide that i’ll just wait for outcry on forums when enough people discover gas costs of claim function.\nThough maybe there is something on the way to the UI, who knows, maybe Robert can answer that.\nCurrently you can just wait untill something arrives, or learn how to call claim function manually via etherscan UI.\\nI agree.  The fees are way out of control.  I can’t even claim my accumulated comp to supply to the pool becuase the fees are ridiculous.\nIt would be great if there was a way to do a partial claim of the accumulated COMP.  then I could claim whatever amount I wanted.  right now, I’d have to claim comp every day to be able to afford to claim the COMP I’ve accumulated.\nAny thoughts… or can someone tell me how to learn how to interact directly within etherscan.\\nHi there,\nIndeed, this fee policy seems to be quite inappropriate.\nTo claim $1 worth of COMP, the price is ~$100!\nWith the COMP accumulating the claim prize is increasing as well over the last days/weeks…\nHence I support the idea of enabling a cheaper way of claiming the earned COMP (I don’t mind the speed for that at all; it could take hours, if that was an option to reduce the fee)\nAlternatively, I would also be happy to learn how to do this directly via etherscan. Is there an actual and targeted tutorial for that?\nThank you very much and keep up the good work!\\nHow do we get this on the radar of the devs to have a vote. This directly affects adoption of the platform, which we all want. It affect especially the new user base who are not technical and just want to use defi for its intended purpose via the web gui.\nThis high fee is a blocker issue.\\nI totally agree.\nMaybe it’s not desired that the ordinary user claims his/her COMP? \nThis comes with a very bad taste, of course, since the earned COMP are apparently included in the “Net APY” value presented centrally on the dashboard.\nLooking forward to hearing any other thoughts on this.\\nThis is disgraceful. Over $100 to claim $25 worth of Comp.\nAnd don’t claim it’s all ETH and Gas fees fault. If I want to remove my coins I’ve loaned you, it costs half that much.\nSort this out, put in a plan of action and look at other protocols that plug in and speed up or look at ways to reduce fees as this is untenable.\nI have to pay YOU to loan YOU money AND to claim my interest.\nComp is looking more and more like a scam.\\nIs there any word on moving to a L2 solution to mitigate these high fees?\\ncheckout compound.cash 136\\nThanks @getty!\nWhile this was an interesting read and it also touches on the point of high transaction cost, it unfortunately did not provide applicable suggestions to me as an ordinary user (i.e. a noob ); at least I could not figure out a recipe of how to claim COMP at a reasonable fee.\nAny further suggestions?\\nCan you hint to a video or written manual explaining how to “cheaply” claim COMP using Etherscan? I tried too look it up, but couldn’t find any. Thank you!\\n@rleshner where can I find:\n\nholder (address)\ncTokens (address[])\n\nI checked a transaction via Etherscan but was unable to find it.\\nAlright looks like I got it.\nLet’s say you would claim COMP for you DAI->\nuse function “19 claimComp”\n\nholder (address)-> your Ethereum wallet address\ncTokens (address[]) → can be found in the docs e.g. DAI 0x5d3a536e4d6dbd6114cc1ead35777bab948e3643\n\\nThis thread keeps on receiving attention as its content is still effective, i.e. high price for claiming COMP via the website. Thanks @silver456 for outlining the manual procedure!\nHowever, in the interest of a community based environment keeping in mind the balance between “whales” and small users/investors (which have been e.g. actively decoyed through the Coinbase activities) that is reffered to more often in recent posts and the governance aspects of the COMP, it would be nice, if an automated COMP withdrawal (without costs for the user) could be introduced again(?).\nAny thoughts or comments on this?\nHow about linking the automated withdrawal/transfer to a threshold, e.g. 0.1 COMP, or have an automated “pay out” after a certain time, e.g. every 3 months?\nWould be great to read some further ideas and eventually see a proposal stemming from this.\\nI’m looking at having to pay 3x my capital to extract from this scam, it’s the right word when they hold your capital hostage like this"
  },
  {
    "number_of_comments": 15,
    "postid": "cadc4955-8445-4fcd-862a-b99986deb378",
    "posturl": "https://www.comp.xyz/t/ctoken-sweeptoken-function/1085",
    "combinedcontent": "Following up on the idea of returning accidental funds sent to cTokens when possible, I wrote a quick code change which enables us to do this. The new function sweepToken(address) is a very simple addition which allows anyone to sweep the entire balance of any ERC20 (except for the underlying) to the admin (timelock).\nThe new function is as follows for CErc20:\n/**\n  * @notice A public function to sweep accidental ERC-20 transfers to this contract. Tokens are sent to admin (timelock)\n  * @param token The address of the ERC-20 token to sweep\n  */\nfunction sweepToken(EIP20Interface token) external {\n  require(address(token) != underlying, \"CErc20::sweepToken: can not sweep underlying token\");\n  uint256 balance = token.balanceOf(address(this));\n  token.transfer(admin, balance);\n}\n\nAnd as follows for CEther:\n/**\n  * @notice A public function to sweep accidental ERC-20 transfers to this contract. Tokens are sent to admin (timelock)\n  * @param token The address of the ERC-20 token to sweep\n  */\nfunction sweepToken(EIP20Interface token) external {\n  uint256 balance = token.balanceOf(address(this));\n  token.transfer(admin, balance);\n}\n\nI wrote scenario tests for the new function and a forking simulation ensuring that the new code performs as expected. Please take a review the pull request and leave feedback.\n  \n      github.com/compound-finance/compound-protocol\n  \n  \n    \n  \n    \n  \n\n  \n    \n      Add sweepToken functionality 11\n    \n\n    \n      compound-finance:master ← arr00:Sweep-erc20\n    \n\n    \n      \n        opened \n        \n          \n        \n        Jan 28, 2021\n      \n      \n\n      \n        \n          \n          arr00\n        \n      \n\n      \n        \n          +685\n          -2\n        \n      \n    \n\n  \n\n\n  \n  \n    \n    \n  \n  \n\n\\nI’ve seen your Github.\nI don’t know the technical details, so please tell me.\nWhat, exactly, is this proposal?\nPlease include the benefits!\\nThanks for asking.\nThis function will allow for ERC-20 tokens which were accidentally sent to cToken contracts to be recoverable by governance. For example, in this transaction 11, cUNI was accidentally sent to the cUNI address. Adopting this new code will allow for governance to return these currently bricked funds.\\nQuick reply, thanks!\nI think it’s a great suggestion.\nI’ll be rooting for you!\\nThat was my transaction! My good sir, if there is any portion of those coins returned to me I will certainly be sending you your fair share. Words can not describe the appreciation and gratitude that has come over me. There are still good people in this world :'D\\n@arr00 this is a great feature to add to the “base” cToken; it will help return accidentally trapped / incorrectly transferred funds, by routing them to the Timelock contract for redistribution (through governance proposals) to users.\nGiven that the admin and underlying are defined on each token, these functions should be generally safe and low risk. It would be great for the community to verify (and test-case), but this looks logically correct & simple.\nNote: this will not apply to cETH, cBAT, cZRX, cUSDC, and cWBTC, which are not upgradable–but can apply to all newer cTokens, including cDAI, cUSDT, cUNI, and cCOMP.\nAfter deploying a new cToken implementation (and upgrading DAI, USDT, UNI, and COMP) the community should develop some heuristics or policy for how funds are returned to users – governance proposals are time-consuming and onerous, and the distribution process shouldn’t be taken lightly.\\nI will be glad to tribute my transaction of cDAI that has become trapped in a contract in limbo as a guinea pig for the initial test. If something goes wrong and they become erased or burned and leave them unrecoverable, then so be it, it would be worth the effort put forth by the community.\\nCorrect me if I’m wrong, seems idea would be for the missent tokens to be swept to the timelock and decided on what to do with it via a governance proposal (returning to respective owners would be the most altruist option).\nThis is actually very  much needed!\\nCan you please send the tx id?\\nYes, that is exactly the thought.\\n0xeafecc481b37c1f1688842f41ce8fb1264Screenshot (46)3840×2160 515 KB d12a0da292b6f26bad511a512d2365\\nJust for extra proof of ownership I added a screenshot. Anything else that would help just let me know!\\nI am sorry, but this is not something that would be recoverable by this effort. You sent cDAI to an address unassociated with the compound protocol. That is a permeate, irreversible transaction, and the compound community can not help you recover it. We can help recover tokens sent to upgradeable contracts managed by the Compound community.\\nOof, I was so excited. It is no worries and I still genuinely appreciate how much time and effort you put into this proposal, regardless of the outcome. I wish you a good day, sir.\\nI have completed development on this patch. Please review the PR linked below. The changes are quite minimal and have full testing and simulation coverage. In addition to adding the sweep function, I removed the unused verify hooks which will result in gas savings on most interactions. I don’t plan on having a formal audit, so please take a look!\n\n  \n      github.com/compound-finance/compound-protocol\n  \n  \n    \n  \n    \n  \n\n  \n    \n      Add sweepToken functionality\n    \n\n    \n      compound-finance:master ← arr00:Sweep-erc20\n    \n\n    \n      \n        opened \n        \n          \n        \n        Jan 28, 2021\n      \n      \n\n      \n        \n          \n          arr00\n        \n      \n\n      \n        \n          +713\n          -15\n        \n      \n    \n\n  \n\n\n  \n  \n    \n    \n  \n  \n\n\\nI have deployed the new implementation here 4. Please match the contract to ensure the code is as expected. I plan to make a proposal post soon and get this to a proposal."
  },
  {
    "number_of_comments": 10,
    "postid": "dca6e8e9-5714-46c5-8203-71581f09381f",
    "posturl": "https://www.comp.xyz/t/update-ccomp-parameters-borrow-cap-and-interest-rate-model/3614",
    "combinedcontent": "Hi all,\nRecently the v2 cCOMP market has seen high utilization, with total borrowings currently at the 90,750 COMP borrow cap. This implies that market parameters are not well optimized and some users are taking advantage of artificially compressed borrowing costs.\nThe borrow cap on COMP was initially set to prevent governance manipulations, but with recent votes showing strong participation (typically 500,000 COMP or more) and proposal threshold already well below the cap (25,000 COMP required to submit a proposal) without negative effects, I think that it may be safe to increase borrowing caps to allow for more utilization and greater cCOMP yields (particularly valuable considering recent removal of COMP incentives to cCOMP suppliers).\nAdditionally, I think adjusting the interest rate model could help improve market dynamics. Higher base rate could help improve cCOMP yield, and higher optimal rate with a lower kink point could help avoid situations where borrow rate is kept artificially low when hitting the cap.\n\nProposed changes:\n\nSet cCOMP borrow cap to 150,000 COMP\nUpdate cCOMP interest rate model:\n\nBase rate: 5%\nKink: 50%\nOptimal Rate: 25%\nMax Rate: 250%\n\n\n\n\nImpact in existing market conditions:\nUtilization is currently at 8.6% and running up against the 90,750 COMP borrow cap, with 5% borrow rate and 0.32% supply rate. After implementing the above changes, the borrow and supply rates would be 8.3% and 0.53% respectively. If borrowing returned to the new cap (150,000 COMP, 14.3%), this would result in borrow and supply rates of 10.7% and 1.1%.\n\nNext Steps\nIf anyone has input on ideal rate model or borrow cap parameters please give your input! If there’s support for adjusting the cCOMP market, I’ll push forward a proposal for the final consensus parameters.\\nPlanning to move this proposal on chain in the next week unless there are any objections!\n\nUpdate on market status and impact on proposed changes:\nWhile total borrowed remains at the 90,750 COMP borrow cap, utilization has increased to 15.8% due to recent withdraws. This yields a current borrow rate of 7.18% and supply rate of 0.82%.\nAssuming constant utilization, the new interest rate model would set a borrow rate of 11.32% and supply rate of 1.34%. If borrowings increased to reach the new cap without any change in supply, this would lead to utilization of 25.8%, borrow rate of 15.32%, and supply rate of 2.96%.\nGiven current high yields for LPing COMP on Balancer/Aura, it seems possible that borrowing may hit the new cap of 150,000 COMP despite higher borrow rates.\n\nOn Chain Actions:\n\nComptroller > _setMarketBorrowCaps > cCOMP > 150,000E18\n\ncCOMP Interest Rate Model 3 > updateJumpRateModel > (0.04E18, 0.2E18, 2E18, 0.5E18)\n\nNote that baseRatePerYear, multiplierPerYear and jumpMultiplierPerYear values above have been normalized to account for the 15 second block time hard coded into the interest rate model contract (versus current 12 second block time in Ethereum PoS). Values were multiplied by 0.8, which accounts for the difference in block times and will result in the intended APRs being achieved in the rate model contract.\\nThanks for the proposal, @monet-supply - Gauntlet is looking into the market implications of the above.\\nCOMP’s borrow balance has been hitting the 90,750 cap since early September. Gauntlet has limited market risk concerns with Monet’s borrower cap increase, but we want to note that around 90% of the borrowing is being driven by this one account 6. This user’s borrowing behavior will probably be elastic to the interest rate parameter changes.\n\nImage3800×1396 234 KB\n\nAs noted by @monet-supply, the borrow cap on COMP was initially set to prevent governance manipulations. Gauntlet doesn’t assess governance risk, but governance risk has been partially mitigated by stronger voter participation (550k+). From a market risk perspective, it is unlikely that increasing cCOMP borrower cap to 150k will add outsized risk to the protocol.\\nLooks like the proxy owner 8 is farming the balancer comp/eth pool. They’ll probably earn positive yield unless borrow cost increases above 20-30%.\\nThis is correct from a historical perspective — the cap was set to prevent malicious proposals. If the borrowing capacity is fully utilized by organic borrowing demand this is less of a risk.\\nSo, @arr00 pointed out that cCOMP rate model may be used in other assets as well, and after checking it looks like it was indeed used for cAAVE, cMKR, cYFI, and cSUSHI. Given that the existing rate model seems suitable for these other assets, I don’t think it would make sense to submit COMP rate adjustments in the existing IRM contract.\nInstead, I plan to submit a proposal just for updating the cCOMP borrow cap (from 90,750 to 150,000). If the new borrow cap is reached, we can then consider creating a new rate model specifically for cCOMP to allow for adjusting rates separately from the other assets.\\nOk last update on the proposal implementation before I put it up on chain!\nThe interest rate model currently in use for cUNI (0xd88B94128Ff2B8Cf2d7886cd1C1E46757418cA2A 3) is not used by any other markets, and has a somewhat higher path of interest rates compared with the cCOMP rate model (0xd956188795ca6F4A74092ddca33E0Ea4cA3a1395 3) which is also used across AAVE/MKR/YFI/SUSHI markets. While higher rates for cUNI were initially warranted to avoid unsafe utilization, it now makes sense to move the cUNI market over to match other defi tokens. On the other hand, cCOMP’s capped borrowing capacity makes it more suitable to switch to a higher rate model such as the one currently in use for cUNI.\nThe most expedient and simplest way to effect this is to move cUNI to the prevailing defi token interest rate model currently used for AAVE/MKR/YFI/SUSHI, and then move cCOMP to the rate model previously used by cUNI.\nIf the COMP market reaches the newly increased borrow cap despite the higher interest rate model, then further changes to the rate model or borrow cap can be submitted to alleviate excessive utilization and allow the market to reach equilibrium based on cost.\n\nImpacts in Current Market Conditions:\ncUNI\nAt current utilization of 8.75%, borrow and supply rates would decline from 6.5% and 0.42% to 5.1% and 0.32% respectively. Reserve accrual would decline by ~22% ($9,800/year) assuming no change in utilization.\ncCOMP\nAt current utilization of 15.7%, borrow and supply rates would increase from 7.1% and 0.82% to 9.8% and 1.1% respectively. If borrowing volume increased from current 90,750 to the new cap of 150,000 without any change in COMP supplied, this would push utilization up to ~25.7%, with corresponding borrow and supply rates of 14.5% and 2.6%.\nWith LP rewards on Balancer/Aura continuing to exceed these projected borrow rates, there is decent likelihood of the new cap being reached.\n\nimage1054×1122 88 KB\n\n\nOn Chain Actions:\n\n\nComptroller > _setMarketBorrowCaps > cCOMP > 150,000E18\n\ncUNI > _setInterestRateModel > 0xd956188795ca6F4A74092ddca33E0Ea4cA3a1395 3\n\n\ncCOMP > _setInterestRateModel > 0xd88B94128Ff2B8Cf2d7886cd1C1E46757418cA2A 3\n\n\nSorry for the twists and turns getting this proposal into a finalized state! \\nProposal has been submitted and voting begins in ~2 days: Compound 15\\nvoted. fully agreed with your points. \nBy the way, does it have some table or dashboard to see the current IRM status? in compound homepage(including document), it is hard to recognize which markets are using the same or different model. (it would be helpful to understand compound structure for voters and new developers.)\nex) current IRM categories\n\n\n\n\nsector\nmarket\ncontract address\n\n\n\n\nDeFi\ncAAVE, cMKR, cYFI, cSUSHI, cUNI\n0xd956188795ca6F4A74092ddca33E0Ea4cA3a1395 1\n\n\nStableCoin\nUSDC, USDT, DAI\naddress\n\n\n\n\\nProposal has been passed and executed! Currently the cCOMP borrow total is about 85,000 COMP, comfortably below the new 150,000 COMP borrow cap.\nIt looks like incentives for the balancer pool may not be renewed, so there is a possibility of borrow utilization declining once these existing rewards run out. Will be worth monitoring.\n\nimage1040×1110 86.9 KB\n"
  },
  {
    "number_of_comments": 11,
    "postid": "8e060380-1b61-4ac9-94d7-038d88433372",
    "posturl": "https://www.comp.xyz/t/compound-dsr-proposal/3856",
    "combinedcontent": "The MakerDAO community recently reactivated the DAI Savings Rate (DSR) providing a 1.00% variable yield natively streamed by the Maker protocol to all DAI holders that opt in. This effectively offers any DAI holder the opportunity to earn a low risk yield by depositing their DAI into the DAI savings rate (DSR) smart contract. The DAI remains in the DSR contract until the user withdraws (at any time they choose) and is not rehypothecated or lent out. The DSR doesn’t change depending on the amount of funds pooled (different from every single other DeFi protocol that rewards staking).\nThe DSR was first introduced in 2018 and Compound was an early supporter but has since deactivated it as the DSR rates have been negligible for the past few years. We propose the reactivation of the DSR in Compound so users can have direct access to this low risk base DeFi yield.\nHere is the link 13 to the post where the DSR compliant implementation was phased out.\nThe MakerDAO DSR contracts are still at the same addresses - but in the interest of clarity, the address set that should be configured for the Compound cDAI Delegate Contract should be:\n\nDai: 0x6B175474E89094C44Da98b954EedeAC495271d0F\nVat: 0x35D1b3F3D7966A1DFe207aa4514C12a259A0492B\nPot: 0x197E90f9FAD81970bA7976f33CbD77088E5D7cf7\nDaiJoin: 0x9759A6Ac90977b93B58547b4A71c78317f391A28\n\nTo implement these items need to be completed: the cDAI implementation to go back to the DSRSweep. Change the DAI interest rate model to account for the DSR rate.\nThese addresses can (and should) be checked against our chainlog 4.\nIt should be noted that the DSR annual yield is determined by Maker Governance participants via Executive vote, and is subject to periodic future adjustments.\\nExcited by this proposal !\nJust wanted to note that the old DaiInterestRateModelV3 3 available at 0xfed941d39905b23d6faf02c8301d40bd4834e27f 5 may not be reusable as is, because of the number of seconds between each blocks that it uses for getSupplyRate (see issue 230 2).\\nI’m currently taking a look into this and there is a nuance to the existing implementation which seems off.\nThe current DSR IRM enforces a max borrow rate pre-kink that is equal to the stability fee of ETH-A + a small gap, which is 1.5% (below current cDAI market rate). This might have been comparable years ago when it was used; however, currently, the min collateral ratio of ETH-A is 145% while cETH has a collateral factor of 83% (ratio of ~120%). ETH-B has a closer ratio of 130% and 3% stability fee although I’m reluctant to say this is on par with cETH-cDAI pair.\nI think our options are either remove the peg to a stability fee or use the higher rate of ETH-B instead. Thoughts?\\nWhile I agree that they are not equivalent, it makes a lot of sense to take into account ETH-B stability fee in the Dai IRM. However, I may be cleaner to have an IRM independent from the stability fee, but whose parameters have been chosen in order to take it into account.\\nI’m not really seeing any downsides to supporting this proposal.  If someone knows of any, please let me know.\\nWe changed the DAIInterestRateModel to V4 to fix this: Wrong seconds per block in the DAI interest rate model · Issue #230 · compound-finance/compound-protocol · GitHub 9\\nPersonally, I think it would be better to remove reference to Maker vault stability fees (borrow rates). There are pretty significant differences between Maker vaults and Compound (Maker oracle delay, differing liquidation ratios, etc), and Maker has a bit less liquidity constraints so benchmarking optimal borrow rate to Maker vault rates might lead to the optimal rate at Compound being too low to preserve market liquidity in some cases.\nSpitballing here but I think a reasonable DSR IRM could be something like:\n\nBase rate: DSR (currently 1%)\nOptimal rate: DSR + 3% (currently 4%)\nMax rate: DSR + 40% (currently 41%)\n\\nHi, we have sent a PR to Compound’s Comet GitHub repository 9 containing our proposal’s data.\nHere is a list of the proposed actions:\nAction 1 - Set the cDAI implementation back to the cDaiDelegate contract:\n\nTarget: 0x5d3a536E4D6DbD6114cc1Ead35777bAB948E3643 (cDAI)\nFunction called: _setImplementation(address,bool,bytes)\nParams:\n\n0: 0xbB8bE4772fAA655C255309afc3c5207aA7b896Fd (cDaiDelegate 1)\n1: true\n2 (params for cDaiDelegate._becomeImplementation(daiJoin, pot)\n\n0: 0x9759A6Ac90977b93B58547b4A71c78317f391A28 (DaiJoin)\n1: 0x197E90f9FAD81970bA7976f33CbD77088E5D7cf7 (Pot)\n\n\n\n\n\nAction 2 - Set the IRM for cDAI back to DAIInterestRateModelV3:\n\nTarget: 0x5d3a536E4D6DbD6114cc1Ead35777bAB948E3643 (cDAI)\nFunction called: _setInterestRateModel(address)\nParams:\n\n0: 0xfeD941d39905B23D6FAf02C8301d40bD4834E27F (DAIInterestRateModelV3 2)\n\n\n\\n@MakerGrowth Thanks for posting these details.\nI was asked to take a look at this proposal by other community members to see if it warrants an audit before being submitted. While the prior implementation was actually audited by OpenZeppelin back in 2020, a lot of time has passed since then. We’re currently exploring whether this warrants another audit to avoid any potential integration issues.\nI’ll let you know by end of the week if an audit is warranted and propose a timeline for completion if so. You can reach out to me directly at my contact details below if you have questions or would like to coordinate further:\n\nEmail: michael@openzeppelin.com\n\nTelegram: @cyloncat\n\nDiscord (in the Compound server): Michael L#3462\n\\nNote that an other path is still possible: using the same IRM as for other assets, with parameters that has been determined with the DSR in mind, like the one given by @monet-supply:\n\n\n\n monet-supply:\n\nSpitballing here but I think a reasonable DSR IRM could be something like:\n\nBase rate: DSR (currently 1%)\nOptimal rate: DSR + 3% (currently 4%)\nMax rate: DSR + 40% (currently 41%)\n\n\n\nThis has the benefit of not introducing new code to the Compound protocol. It would obviously not react dynamically to the update of the stability fee of ETH-B and to the DSR rate, but it may not be a big deal.\\nAfter consulting with the OpenZeppelin audit team, we’ve decided that another audit is warranted, primarily to ensure no integration issues are present and that the process of rolling back an upgrade does not introduce unexpected behavior. We are starting an audit of the Optimism Comet deployment next week so we are planning to conduct an audit of the DSR proposal afterwards, starting Feb 20th.\nIn the meantime, I’d like to propose the following timeline, based on our recent draft CIP for contribution policies 1 that lays out a process for community feedback and quality assurance. I start by retroactively recognizing steps that have already been taken in this forum thread.\n\n\nIdea Review (started Dec. 16th)  - The idea was posted in the forum to solicit community feedback and support.\n\nDeveloper Review (started Jan. 18th, current stage) - Code to implement the idea was shared with the community to solicit feedback.\n\nWe are currently at this stage as the community reviews the implementation and discusses alternative paths such as those suggested by @MathisGD above. An external audit is now being scheduled.\n\n\n\nCommunity Review (suggested to start by Jan 30th) - This starts once @MakerGrowth  finalizes the design of their implementation and shares tests and simulations of the proposal changes with the community.\n\nIn this case, if @MakerGrowth does not intend to change the current proposal design, they just need to provide simulations for community review. To ensure community feedback is received and everything is finalized one week before the audit starts on Feb. 20th, I’d recommend reaching this stage by Jan. 30th.\n\n\n\nExternal Review (scheduled to start Feb 20th) - This stage consists of receiving an external audit from OpenZeppelin and then addressing any issues that might be raised. A frozen commit of the code should be provided one week prior. We estimated that the audit and fix reviews should be finished by Mar. 10th at the latest.\n\nFinal Review (estimated by Mar. 10th) - The final state of the PR and simulation testing is shared with the community following the audit ahead of the on-chain submission of the proposal and vote.\n\nWhile this process might seem long, I believe it is a necessary one for ensuring a contribution has incorporated every possible quality assurance check, audits are successful and community feedback has been addressed. The community is always grateful for contributions such as this and so this process is intended to ensure that it has the highest chance of success for adoption.\nI’ll also note that this is all a suggestion based on a CIP draft that has not yet been adopted by the community. The timeline in this case is also longer than it might otherwise be due to our current audit backlog. In the future, similar proposals could pass through each stage rather quickly, especially if any necessary audit was already scheduled in advance.\nI’m happy to discuss this plan further in this thread or directly with @MakerGrowth over chat or video call. @jbass-oz will also be available to answer general questions about the Contribution CIP in the community call next Wednesday.\\nHi everyone,\nI’m happy to share that OpenZeppelin’s security assessment of the DSR reactivation is complete. This took more time than expected due to a need to investigate potential edge-cases but the report is now ready to review here.\nWhile no major security issues were found, the surface area of the Compound V2 and\nMakerDAO protocols is quite large so the security of this upgrade does depend on a few specifics that should be carefully considered. We’ve communicated these considerations to both the @MakerGrowth and @compound.finance teams over the course of our security assessment. I encourage the community to read our conclusions below and refer to the report for additional details before voting to approve any upgrade.\n\nAfter further discourse and fixes, there are two lingering issues that will need attention\nfrom the community:\n\n\nAs highlighted in M01, the Dai Savings Rate proposal has many variables that will affect\ninterest rates. Passing any such proposal should include thoughtful decisions for these\nvariables’ values. Parameters such as blocksPerYear and SECONDS_PER_BLOCK , while\nseemingly simple at first glance, are used in conjunction with other variables to determine how\ninterest rates behave (particularly, see gapPerYear and its derivative gapPerBlock ). Below\nis a chart of different interest rates computed from the getBorrowRateInternal function\nunder different parameters. This chart is not comprehensive and meant only to demonstrate a\nfew sample parameter differences. We encourage the Compound community to discuss and\ndecide on the global values within this proposal.\n\nimage695×364 43.2 KB\n\n\n\nIn reference to L01, an error in setting the interest rate model would catastrophically impact\nthe cDai market. No such error is caught or handled. And while the risk of this occurring is\nminimal, we encourage the Compound community to make sure that any DSR proposal\nensures that such an error will revert the deployment or immediately return the model to its\noriginal setting.\n\n\n"
  },
  {
    "number_of_comments": 9,
    "postid": "ae7f9655-5143-4786-8498-e03ada0de827",
    "posturl": "https://www.comp.xyz/t/comp-distribution-speeds-the-end-of-compspeeds/2778",
    "combinedcontent": "Over the last few weeks, we have analyzed Compound and particularly compSpeeds for each market. We have modeled a wide range of speeds and did our best to estimate the resulting liquidity. While we have learned an immense amount about the protocol, its interest rates, and the effects of adjusting the supply and borrow compSpeeds, the result wasn’t what we hoped for.\nWhen we jumped down this rabbit hole, the goal was to optimize the existing compSpeeds so that the protocol gets more bang for its buck. That is, we wished to find a way to reduce compSpeeds without decreasing market liquidity too much. Unfortunately, it wasn’t as meaningful as we hoped.\nThe questions we asked ourselves went something like this:\n\nHow do we optimize the compSpeeds such that the protocol gets more bang for its buck?\nDo legacy markets (and markets with a clear excess of capital) require compSpeeds at all?\nIf the goal is liquidity for borrowers, are there more efficient methods to achieve that? (eyeing DAI)\nHow much participation is Farmers vs. Borrowers? (Farmers are seeking the best yield by parking their capital. Borrowers are users who supply a different asset than they are borrowing)\nWhat are the target yield rates for each market? Why do the yield rates differ on DAI vs. USDC & ETH vs. WBTC?\nHow much liquidity does each market require? (Isn’t the kink supposed to ensure there is sufficient liquidity?)\nIs COMP farming influencing interest rates and utilization?\nWhat is the cost-benefit of compSpeeds?\nWhat is the objective of a compSpeed?\nDo we need compSpeeds on substantiated markets?\n\nMany of these questions are rabbit holes in themselves. We have found answers to several of them. (Abbreviated answers below. We’re happy to go more in-depth)\n\nHow do we optimize the compSpeed such that the protocol gets more bang for its buck?\n\n\nA complicated question that is difficult to prove. If we assume all participants are Loopers (users supplying and borrowing the same asset for profit), then incentives change in tandem with market participation. Hard to say what will happen with all of the external factors involved.\n\n\nDo legacy markets (and markets with a clear excess of capital) require compSpeeds at all?\n\n\nNo.\n\n\nIf the goal is liquidity for borrowers, are there more efficient methods to achieve that? (eyeing DAI)\n\n\nYes, lobbying MakerDao to implement the Direct Deposit Module would be a good example. (Something GFX Labs is working on 6)\n\n\nHow much participation is coming from Farmers vs. Borrowers? (Farmers are seeking the best yield by parking their capital. Borrowers are users who supply a different asset than they are borrowing)\n\n\nFocusing on the ETH market and its participants shows significant natural activity.\n\n\nWhat are the target yield rates for each market? Why do the yield rates differ on DAI vs. USDC & ETH vs. WBTC?\n\n\nWe know the market is happy with the current rates, but beyond that, it is a guess. The next best data points are the much larger perpetual futures and futures markets.\n\n\n\nHow much liquidity does each market require? (Isn’t the kink supposed to ensure there is sufficient liquidity?)\n\n\nIs COMP farming influencing interest rates and utilization?\n\n\n\nNot if we assume all participants are Loopers (half-answer).\n\n\nWhat is the cost-benefit of compSpeeds?\n\n\nAt a high level, the protocol pays in COMP and collects via the reserve factor revenues from the additional funds deposited.\n\n\nWhat is the objective of a compSpeed?\n\n\nIn our opinion, the compSpeed is for bootstrapping new markets.\n\n\nDo we need compSpeeds on substantiated markets?\n\n\nProbably not.\n\nQuestions 8-10 are the ones to focus on. The protocol is currently spending ~$160m in compSpeeds. If we agree the primary use case of compSpeeds is incentivizing liquidity (also known as TVL), it can be said that the protocol has more than enough liquidity in most markets and is thus overspending on liquidity.\nWhile we wish there were a clear optimal proposal for the community to consider, we are left with a much harder question: how valuable is liquidity?\nIf reducing annual compSpeed spend from $160m to $80m would result in the TVL dropping by $2b, would this be problematic? What about a reduction from $160m to $40m that causes TVL to drop by $4b? What if we got rid of it altogether and TVL dropped by $6b?\nInstead of projecting future activity, we can also look at historical performance. Over the last month, COMP has dropped nearly 40%, and thus the dollar value of rewards dropped by nearly 40%. However, TVL only dropped 12% over the same period, while ETH dropped by 11% at the same time. This, along with a significant decrease in yields, demonstrates that Compound participants are inelastic to yield.\nWe have spent close to 100 hours trying to find an objective conclusion to present to the community. At this point, it is clear we could spend hundreds more hours researching and analyzing the protocol but we wouldn’t be addressing the main point anytime soon if we did that.\nWhat is the primary objective of Compound? We think it is to provide a place for people to borrow assets. To achieve that objective, Compound has users who supply assets, and to operate without a central entity, interest rates are chosen programmatically.\nWhat is the difference between the interest rate curve’s purpose and compSpeeds?\n\nThe interest rate curve has one main goal: to ensure there is liquidity available to be borrowed and for lenders to withdraw from the pool. The invention of the v2 interest curve (in use by modern ctokens) includes a “kink” parameter. Simply put, when the market utilization exceeds the kink (generally 80%), the curve steepens to incentivize new deposits and repayments. The interest rate curve’s subgoal is to provide an opportunity for lenders and borrowers to utilize the protocol by providing competitive interest rates.\ncompSpeeds distribute a fixed amount of comp per block to a market and thus its participants. The COMP distributed acts as an incentive to participate in the market. Markets with low participation (like new markets) may benefit from an incentive to increase participation.\n\nWhat should the protocol do?\nIn an ideal world, we go back to a neutral state and adjust from there. Reduce compSpeeds to zero and let the market rebalance. Once the market is at its neutral state without incentives, we can update the interest curves so that it targets a sufficient amount of liquidity while offering competitive interest rates. Then, for markets with low participation, we can add compSpeeds until there is sufficient participation.\nHowever, that path is more of the “rip the band-aid off and do some aggressive physical therapy.” Perhaps a more balanced approach for the protocol to consider is to update interest rate curves and simultaneously and linearly reduce compSpeeds across the board. The primary downside is that it will take a long time to linearly reduce compSpeeds, and it introduces noise into the process of updating the interest rate curves.\nSo we leave the community with a decision to make:\n\nOption A: Rip the band-aid off and aggressively perform physical therapy, at the cost of shocking market participants but with the benefit of getting healthy faster.\nOption B: Carefully remove the band-aid and build a long-term physical therapy schedule.\nOption C: Y’all crazy. Compound is fit af. Nothing is wrong.\nOption D: Other - comment below\n\nHere is a link 12 to a Google Sheet with the entire protocol modeled. If you are interested in playing around with the protocol and seeing how it functions, feel free to make a copy. We have it configured so it pulls the current info straight from the blockchain into the sheet. If you notice the info on the sheet is dated, send Getty a message and he can call the function to update it. Unfortunately, it is not at a state where we can make the function publicly available.\nSeparate from the options above but related, GFX Labs has a long list of improvements it wants to bring to Compound, and this post is the first big project we’re interested in tackling. We think Compound is in need of significant maintenance and improvement and we’re interested in allocating significant resources towards the protocol. Getty is currently receiving 500 COMP/year as a part-time contributor. If the community is interested in GFX Labs (Getty & crew) participating in a more meaningful fashion, we would love to talk about canceling Getty’s grant and opening a new one for GFX Labs. More information to follow at a later date, just want to begin floating the idea.\\n\nOver the last month, COMP has dropped nearly 40%, and thus the dollar value of rewards dropped by nearly 40%. However, TVL only dropped 12% over the same period, while ETH dropped by 11% at the same time.\n\nWhales and protocol are often making their decisions relative to other options for using capital. If everyone else’s reward tokens went down in price at the same time, then one would expect little change.\nIn the medium run, capital is indeed sticky, because as a large holder, when you transfer your funds from one protocol to another, the rates go down on the one you moved to, and up on the one you left, just by virtue of your move. And chasing a half percent is going to cost you a bucket of gas, and only benefit you a week or so, maybe. In general it’s better to pick a risk profile, and let other people’s money balance things out between protocols. Now if gas prices were tiny, sure, move aggressively, but not in the world we currently live in.\nSo any small changes in rates are probably going to take months to see the effect of. Lenders will over time balance lending across platforms based on rate and a little on risk. I’m less concerned about the lenders (even though I am one). Large lenders will just shift to roughly equalize yield, over longer timeframes.\nBorrowers are the primary concern to me. In the current scheme, borrowers are doubly incentivized. First the base rate they are paying is reduced because of lender’s willingness to lend at reduced rates for rewards, and then regular rewards they get are on top of that. I’d be interested to see any guesses as to how this would affect things.\nAs for loopers, perhaps they come in two kinds. Leverage loopers are looking to gain exposure on coin prices moving up, by lending eth/btc and borrowing stables. I don’t think they would be very affected by changes, since most of their expected profit is from elsewhere. COMP farming loopers, who are borrowing and lending stables, would be the most affected by dropping yields. A lot of the apparent TLV in the system comes from them (judging only by the size of the loop winds and unwinds that I see).\\nIn your spreadsheet, what do you mean by loop costs and loop rate? I would have assumed that there were more loopers than 10% of lending and 10% of the borrowing (if that’s what it means).\n[Edit: Or is this just saying how profitable it is to loop?]\\nGreat writeup @GFXlabs, and welcome to the community.\nBroadly speaking, I think the protocol should be willing to experiment with rather drastic changes if the downside of those changes isn’t permanent. For example, if for one week we cut “growth spend” from $160M to $0M in COMP subsidies per year, we would clearly lose some TVL. But if the week after that we go back to spending $160M (or more) per year, TVL would likely return rather quickly.\nIn short, the cost-benefit of running an experiment like this one is rather skewed towards the benefit. More specifically, the cost is losing some TVL in the short-run in return for the benefits of (i) gathering data on Compound’s users, and (ii) potentially, learning that the protocol is overpaying for TVL, cutting COMP growth spend, and saving dozens of millions in the process.\nI’m hugely in favor of choosing Option A or B.\n\n\nOption A: Rip the band-aid off and aggressively perform physical therapy, at the cost of shocking market participants but with the benefit of getting healthy faster.\nOption B: Carefully remove the band-aid and build a long-term physical therapy schedule.\nOption C: Y’all crazy. Compound is fit af. Nothing is wrong.\nOption D: Other - comment below\n\n\nEntirely separately, the game theory of running this experiment could be fascinating. No protocol loves handing out what is now hundreds of millions of dollars in its native token to get users to use the project; it’s simply a necessary evil if the guy you’re competing with is doing it. If Compound stops issuing so much COMP, it’s possible others will follow, which may improve the economics of the entire category of lending and borrowing protocols. On the flipside, it’s also possible that competitors will spend more in an effort to take advantage of what Compound is doing (spending less). Either way, the only way we’ll find the outcome is if we play the growth spending/no spending game. I think we should play ball.\\nLoop costs is the interest paid & earned on the base market.\nLoop comp supply earnings are the comp earnings from the supplying side of looping.\nLopp comp borrow earnings are the comp earnings from the borrowing side of looping.\nTotal loop rate is the sum of the three.\nI just broke it down because it was helpful for some of the work we were doing. Generally, the entire sheet could be simplified, but I think it is beneficial to exam each component and be able to manipulate them.\nPS: I think if you click on the cell, you should see the math/query.\\n\n\n\n GFXlabs:\n\nGFX Labs\n\n\nFinally saw some constructive discussions on fixing the dying token economics.\\nI believe it’s worth reminding, that it isn’t protocol-owned tokens are being distributed to begin with.\nI’ll refer you to source:\nDistributing COMP\nA collection of Compound’s most important stakeholders share the ability to upgrade the protocol:\n\n2,396,307 COMP have been distributed to shareholders of Compound Labs, Inc., which created the protocol\n2,226,037 COMP are allocated to our founders & team, and subject to 4-year vesting\n372,707 COMP are allocated to future team members (we’re hiring 4!)\n4,229,949 COMP are reserved for users of the protocol\n775,000 COMP are reserved for the community to advance governance through other means — which will be announced at a future date\n0 COMP will be sold or retained by Compound Labs, Inc.\n\nSo, those approx.  4.2M COMP tokens, are tokens of users of Compound protocol, not protocol-owned tokens. And they are being distributed to users of protocol over 4 years. There’s nothing about subsidising anything at the core. So yes, of course it’s always make sense to talk about more efficient distribution, but stopping it completely is kind of out of the question, unless we talking about team and VC portion of tokens reduced proportionally as well and returned back to protocol.\nIt’s kind of easy to say let’s not pay someone else share and keep your own intact. \nIndeed, there are certain issues with distribution like recursive farming, which kind of makes distribution kind of unbalance in favor of big capital owners, but things like some markets are doing well by themselves and don’t need distributions is kind of slippery road. They getting it not so much because they NEED for something, but rather because it’s a mechanism of distribution governance tokens to protocol users.\nAgain, it’s not some mystery unallocated protocol-owned treasury tokens, that is portion of governance tokens specifically allocated to users of protocol, and primarily not for financial benefits, but rather for a governance rights.\\nWhile we agree the original intention by Compound Labs was to use the compSpeeds to distribute COMP to users of the protocol, we think there are more efficient methods to distribute COMP to users. Currently, the majority of COMP is accruing to farmers who are selling COMP. As the protocol decreases its spending on compSpeeds, we can fund new initiatives to drive growth and distribute COMP.\nWe don’t seek to end compSpeeds altogether but suggest reconsidering their current usage and changing to a more targetted approach.\\nGFXLabs has brought up some really good questions and points.\nOkay, sure, COMP token is intended to be distributed to its users for governance rights, but how do we evaluate the extent to which we are succeeding in this? What does success look like? What does failure look like?\nHow can Compound grow when the metrics to self-evaluate are so unclear?\\nI believe option A - dropping compSpeeds to 0 - is the best option. I don’t see the purpose of continuing such a large liquidity mining program. We should focus liquidity mining on new initiatives like starports and Compound Gateway not the OG protocol on Ethereum.\nIf the primary concern is the continued distribution of COMP to users, then refocusing towards new initiatives will solve that."
  },
  {
    "number_of_comments": 12,
    "postid": "7e049fc8-397e-4f22-ae07-bba8f080afe4",
    "posturl": "https://www.comp.xyz/t/add-market-fei/2241",
    "combinedcontent": "Fei Protocol is a rapidly growing algorithmic stablecoin built natively for the Defi ecosystem. Fei Protocol has growing support across DeFi, and its governance token, TRIBE, has recently been listed on leading exchanges including Coinbase, Binance, Huobi, etc.\nThe Fei community (Tribe) has demonstrated strong partnership with Compound by depositing more than 75k ETH and 50M DAI to date, a significant portion of our PCV (Protocol Controlled Value).\nFei Protocol:\nFEI is a highly scalable and decentralized algorithmic stablecoin that utilizes protocol controlled value (PCV) for peg stabilization, while maintaining highly liquid secondary markets. Users can mint FEI from ETH and other bonding curves, while FEI is always redeemable at $1 USD (with a 1% fee) for ETH at the peg price. Fei Protocol consists of two tokens, FEI the stablecoin, and TRIBE, the governance token, governing the PCV currently valued at $850 million at the time of writing.\nFei Protocol continuously develops both technical and security improvements through a well-defined governance methodology for Fei Improvement Proposals (FIP), which uses the same governance infrastructure as Compound. The same FIP methodology also extends into management of PCV (Protocol Controlled Value) and development of additional use cases and applications.\nWhy FEI:\nFEI is one of the most decentralized and scalable stablecoins in DeFi. As a matter of its monetary policy, Fei Protocol is actively subsidizing and bootstrapping liquidity to its partner protocols with the aim of lowering rates and increasing traffic. The FEI community’s deep commitment has been already evidenced by its supply of 75,000 ETH (~$283 M) and 50m DAI into Compound. The Fei DAO can support FEI markets on Compound by providing FEI liquidity, and has already signaled approval to bootstrap 25-50m worth (Snapshot 1). Traders would be able to tap into the large FEI-ETH liquidity on Uni V2 ($330 million) as well as any future protocol owned or incentivized liquidity pools.\nFollowing the example of FEI’s listing on other lending protocols; upon listing of $FEI on Compound, the Fei will be committed to introduce a proposal for a liquidity injection of FEI. Fei protocol has already deployed over 20M FEI between Rari, Kashi, and CREAM.\nThe inclusion of FEI will allow Fei and Compound’s dedicated and robust communities to increase their exposure and utility.\nMarket Details:\n\nUser circulating FEI: 240,956,125\nProtocol Owned FEI: 193,051,159\nProtocol Controlled Value: $885,761,132\nCollateralization Ratio 368.7%\n\n\nw1046×517 24.2 KB\n\nFEI protocol’s IDO was one of the seminal events of Defi this year, plagued with early price fluctuations and liquidity issues. But through robust DAO action and community voting, FEI firmly restored its peg and pursued many integrations that have allowed its ecosystem to flourish.\nFEI’s peg has remained steadfast through multiple market downturns, and there is now a reserve stabilizer fund in place to buy back FEI at $0.99. Currently the protocol can redeem all FEI three and a half times over, and with the inclusion of other assets to hedge against an ETH plunge, the risk of continued departure from peg is minimal.\n\n865×886 50.6 KB\n\nFei Protocol has been the subject of multiple professional audits by premier firms such as Open Zeppelin and ConsenSys Diligence, and subject to further extensive auditing throughout its security updates.\nFei Protocol also maintains an active bug bounty program which offers up to $1.1M for critical vulnerabilities to the smart contract.\nAsset Onboarding Framework Collateral Information:\n\n\nOverview\n\nWhat is the token name and ticker symbol?\n\n\nFEI\n\n\nWhat does the token do?\n\n\n$1 USD pegged algorithmic stablecoin\n\n\nWhat additional risks might supporting this token create?\n\n\nProtocol getting hacked\nSignificant disruption in underlying collateralization assets (ETH)\n\n\nWhat audits, if any, have been done?\n\n\nOZ - https://blog.openzeppelin.com/fei-protocol-audit/ 1\n\nDili - Fei Protocol | ConsenSys Diligence\n\nOZ 2 - Fei Protocol Audit - Phase 2 - OpenZeppelin blog\n\n\n\nHave there ever been any protocol hacks? If so, when? How were they addressed?\n\n\nThe protocol has never been hacked, but two critical bugs have been reported shortly after the protocol launched via the protocol-sponsored bug bounty program. After identifying and validating the vulnerabilities, the Fei Labs team immediately stopped the affected protocol functionality with the Guardian (held in a multisig by the Fei Core team), which prevented the vulnerabilities from being exploited. No funds were lost.\n\nPost mortems:\n\nFei Protocol Vulnerability Postmortem | by Immunefi | Immunefi | Medium\nFei Bonding Curve Bug Post Mortem | by Brianna Montgomery | Fei Protocol | Medium 2\n\n\n\n\n\n\n\nMarket Risk Information\n\nWhat venues allow for the trading of this asset?\n\n\nDEX\n\nUniswap V2, Uniswap V3, Sushi, etc.\n\n\n\n\n\n\nHow much liquidity is there on each of these venues?\n\n\n2048×454 133 KB\n\n\nHow has that liquidity changed over time? One way to show this is with rolling 30/60/90 averages.\n\n\n1424×844 161 KB\n\n1416×806 127 KB\n\n1440×816 140 KB\n\n1436×824 137 KB\n\n\n1436×810 143 KB\n\n1412×820 162 KB\n\nSource: Dune Analytics 1\n\nWhat is the historical volatility of this asset?\n\n\n1046×517 24.2 KB\n\n\n\nDecentralization\n\nHow is this asset distributed amongst token holders?\n\n\nGINI Coefficient\n\nLow (highly distributed asset)\n\n\nLargest 10 positions and the percent of total float they constitute\n\nFloat = 250M FEI\n\n\n\n\nHow is the supply of this currency controlled?\n\n\nMint: User minted via bonding curve purchase, protocol deployments\nBurn: Stabilizer (guaranteed $1 redemption), Reweights\n\n\nCentralization scale (Centrally Backed → Permisionless)\n\n\nPermisionless\n\n\n\nAsset Listing Request - to be defined with COMP community\n\nCollateral Factor - will be 0 for all assets until after launch\nReserve Factor (probably should start pretty high)\nBorrow Cap -\nInterest Rate Curve\n\n\nStart with an existing interest rate curve\n\n\n\nEngagement and Consultation Request\nWhile expressing our desire to incorporate FEI as a collateral asset on Compound Finance’s platform, the FEI community would also like to solicit suggestion and ideas from the COMP community regarding vital details pertaining to a potential inclusion:\n\nCollateral factor (we are initially applying for a 0% CF)\nReserve factor - 20%\nBorrow Cap\nBase borrow APY - 0%\n\n(FEI’s interest rate at Rari has flowed beneath 3% with subsidies, averaging ~45% utilization)\n\n\n% utilization of kink - 80%\nAPY at kink - 5%\n\nAny comments and insights would be greatly appreciated!\nCommunity and Resources:\nFEI Contract address: 0x956F47F50A910163D8BF957Cf5846D573E7f87CA 1\nTRIBE Contract Address: 0x514910771af9ca656af840dff83e8264ecf986ca\nFei Protocol Contract Addresses: Contract Addresses - Fei Protocol\nProject Website: https://fei.money/ 2\nDocumentation: http://docs.fei.money/ 1\nDiscord: Discord 2\nCommunity forum: https://tribe.fei.money/\nTwitter: https://twitter.com/feiprotocol\nGithub: Fei Protocol · GitHub 1\nSecurity Audits: https://docs.fei.money/audit\n\\nDisclosure: I am an investor in FEI & TRIBE through Robot Ventures 3.\nHistorically, stablecoins have been the lifeblood of the Compound protocol, and comprise the majority of borrowing demand (and interest paid to suppliers). I personally believe that adding additional stablecoins that have achieved scale/demand to be the right course, and FEI feels ready.\nThe primary risk is that (@brianna correct me if I’m wrong) FEI can be infinitely minted by its protocol, bonding curves, and contracts; which poses collateral risk to other users/markets.\nGiven the fact that stablecoins are often borrowed and supplied to earn interest, not use as collateral, adding them to the protocol without a collateral factor is an easy decision that poses little risk, and has significant benefits.\nQuestions:\n\nIs there price data availability for FEI on-chain, and via Chainlink?\nGiven the similarity to DAI as a collateral-backed stablecoin, does it make sense to use DAI’s rate model? Does it warrant deploying a unique rate model?\n\\nYes, FEI is mintable by the protocol through our governance process. The Fei DAO is forked from the Compound Governor Alpha and Timelock. For this reason, we start with a CF of 0.\nThere are Chainlink oracles for both FEI-USD (1% deviation threshold) and FEI-ETH (2% deviation threshold). Here are the addresses: Contract Addresses - Fei Protocol 3\nWe can use DAI’s rate model, or we can use a more conservative model if the community thinks that is appropriate, given the necessarily smaller market size during market formation.\\nHere’s a price feed: FEI / USD price today | Chainlink 9\\nFEI is a nice addition to the protocol. I reviewed their minting/upgrading functionality. Most functionality is controlled by their Governor Alpha and there is one multisig “guardian” used (0xB8f482539F2d3Ae2C9ea6076894df36D1f632775) which is a 2 of 4. We can add it with a 0 cf for now, and once we have supply caps and they secure the guardian, I think it can be moved to >0.\\nFEI is also burnable by governance, which is somewhat more expansive power than other decentralized stablecoins (eg. DAI) that are also mintable - tokens could be removed from a single target address. But the risk still should be very low for adding FEI as a borrowable asset.\\nHey @getty, thanks for taking the time to review Fei’s governance functionality.\nThe “guardian” multisig has limited functionality intended to be used in the event of time-sensitive emergencies, like vulnerability reports. The guardian can only:\n\n\nRevoke any role from any contract, except Governor\n\n\nPause and unpause contracts\n\n\nForce a reweight\n\n\nFor those interested in learning more, check out our docs on the Guardian role here 2.\\nHi everyone! I’m from Blockchain at Berkeley, and on behalf of the organization, we’d like to announce our support for this initiative.\nFEI is one of the fastest growing stable coins right now, and having it on Compound would definitely help grow Compound’s user base.\\n@getty shared with me the cToken address: cFEI 7.\nI haven’t checked the FEI token contract’s safety.\nThe cFEI sim 8  passes.\n\n…\nResult {\n‘0’: ‘10000000000000000000’,\n‘1’: ‘0’,\n‘2’: ‘1000000004756468807’,\n‘3’: ‘4756468796’,\nlength: 4,\ncashPrior: ‘10000000000000000000’,\ninterestAccumulated: ‘0’,\nborrowIndex: ‘1000000004756468807’,\ntotalBorrows: ‘4756468796’\n},\nundefined,\nResult {\n‘0’: ‘0xc803698a4BE31F0B9035B6eBA17623698f3E2F82’,\n‘1’: ‘1000000000356735159’,\n‘2’: ‘5000000000’,\nlength: 3,\nredeemer: ‘0xc803698a4BE31F0B9035B6eBA17623698f3E2F82’,\nredeemAmount: ‘1000000000356735159’,\nredeemTokens: ‘5000000000’\n}\n]\nAction: CToken cFEI: FeiHolder (0xc803…) redeems 5e+9 tokens\nCommand: Print “cFEI integration ok”\ncFEI integration ok\n\nCErc20Delegate (implementation) 3 matches cDAI.\nCErc20Delegator 7 matches cMKR.\nCErc20Delegator parameters look good:\n\nUnderlying: 0x956F47F50A910163D8BF957Cf5846D573E7f87CA\nComptroller: 0x3d9819210A31b4961b30EF54bE2aeD79B9c9Cd3B\nInterestRateModel matches Dai IRM: 0xfb564da37b41b2f6b6edcc3e56fbf523bd9f2012\nInitialExchangeRate: 200000000000000000000000000\nName: Compound Fei USD\nSymbol: cFEI\nDecimals: 8\nAdmin: 0x6d903f6003cca6255D85CcA4D3B5E5146dC33925\nImplementation: 0xa035b9e130F2B1AedC733eEFb1C67Ba4c503491F\n\nThe oracle 10 is the same as the oracle for the PAXOS (USDP) and MATIC proposal. The FEI price roughly matches coingecko.\\nI’ve written an additional, up-to-date simulation (it passes as expected): https://github.com/TylerEther/compound-protocol/blob/add-market-fei/spec/sim/1001-add-market-fei/hypothetical_proposal.sim 4\nThe contract source code and configuration are exactly as expected.\nThanks @LawSirrah for also verifying and writing a simulation.\nTime to submit this proposal on-chain!\n\nProposal\nComptroller._supportMarket(0x7713DD9Ca933848F6819F38B8352D9A15EA73F67)\nAdd the cFEI 4 market.\ncFEI._setReserveFactor(250000000000000000)\nSet the reserve factor to 25%.\\nThanks @TylerEther \\nI see that FEI is configured to use a variable Chainlink oracle price. This is different from other non-collateral stablecoins like USDT and USDP, which are assigned a fixed price of $1. The rationale for a fixed-price of $1 for non-collateral stablecoins is twofold:\n\nThe price is only used to determine the liability of a borrower, never the value of collateral.\nThe true market price is expected to never be significantly above $1.\n\nThat combination means that a fixed $1 price is conservative for protocol safety, since the protocol may overestimate a borrower’s liability but will never underestimate it.\nIn contrast, using an oracle price that can fall below $1 means that borrower liabilities may be marked down during a loss of peg. When a stablecoin loses its peg, its price tends to become more volatile, which means that oracle price errors are larger. If the oracle price is significantly below the true market price, the protocol becomes vulnerable: the entire pool of FEI could be borrowed against an insufficient amount of collateral that is then abandoned.\nIf we intend to eventually promote FEI to a collateral asset, this risk comes with the territory. But until then, is there any advantage to having a variable price? Are we concerned about a market price deviation above $1?\\nGauntlet will be voting “For” on this governance proposal to add FEI to Compound. From an economic risk perspective, FEI is safe to add to the protocol. Following this listing, Gauntlet will conduct economic risk simulations to determine whether FEI should be enabled as collateral, and if so, specify its risk parameterizations dependent on market conditions and ecosystem dynamics. From a smart contract risk perspective, we trust that the technical risks have been assessed by the FEI team and the community developers above."
  },
  {
    "number_of_comments": 15,
    "postid": "a341ee41-a76e-4e56-93f4-82dce1bddd30",
    "posturl": "https://www.comp.xyz/t/initialize-compound-iii-usdc-on-polygon-pos/3611",
    "combinedcontent": "{Parts of this proposal have been taken from rleshner’s proposal 19 for USDC on Ethereum.}\nBackground\nCompound III is a next-generation collateralized borrowing protocol, designed for security, capital efficiency, low gas costs, and streamlined governance.\nEach deployment of Compound III features a single borrowable asset. Borrowers supply collateral, which is isolated and remains their property–it is never rehypothecated or withdrawable by other users (except during liquidation).\nBy removing every unnecessary feature and use-case, upgrading the risk engine (and capital efficiency), and focusing on a single borrowable asset, the protocol has the potential to be the safest & most appealing tool for borrowers ever designed.\nPolygon PoS is a proof of stake chain built on Ethereum. WIth over $1.8 billion in TVL  2across DeFi protocols, Polygon is the largest DeFi ecosystem across all Ethereum scaling solutions. Alongside Compound, Polygon acknowledges Ethereum as the premier blockchain for decentralization and developer talent, however the gas fees can be prohibitive for many users. With a mission to provide #DeFiForAll, Polygon aims to provide an extremely cheap yet secure DeFi experience for users across the globe, regardless of wealth status. In order to create the best DeFi ecosystem across blockchains, Polygon hopes to see Compound III launch so that users can benefit from a robust lending system in an affordable manner.\nProposal\nInitialize Compound III on Polygon with USDC as the borrowable asset. Seed the market with 500k USDC.\nAt launch, we propose accepting:\n\nWETH\nWBTC\nMATIC\nUSDC\nDAI\nUSDT\n\nProtocol Launch\nReferencing rLeshner’s proposal, the launch of a new compound III market first begins by raising the supply caps of the collateral assets (each individual). The function called is updateAssetSupplyCap in the Configurator 5 5. This will be a total of 8 actionsThe supply cap changes are summarized below:\n\n\n\n\nAsset\nWETH\nCOMP\nDAI\nUSDT\nMATIC\nWBTC\n\n\n\n\nSupply Cap\n5400\n40000\n7500000\n5000000\n3000000\n420\n\n\n\nThe next (9th) action is to deploy and upgrade to the newly configured implementation for the market. The proposal calls the deployAndUpgradeTo function of the ProxyAdmin contract 3, which is the only contract capable of changing the implementation of the proxy. This wrapper function also takes care of deploying the new implementation from the factory contract 1 2, using the previously set configuration in the Configurator.\nThe final (10th) action is a simple ERC20 transfer from the Timelock to the new market, adding 500,000 USDC of reserves.\nReasoning:\nAs an EVM compatible chain, the deployment of Compound III markets on Polygon will be seamless. Given the contracts have been tested, audited, and deployed to production for the USDC market on Ethereum, there should be little security concern for also deploying on Polygon.\nHaving USDC as the borrowable asset brings two major benefits, namely in security and utilization. Referencing the IMF’s Global Financial Stability report 1 from April of this year, over 90% of borrowing in DeFi is denominated in stablecoins. Thus, to ensure sufficient utilization of borrowed assets, USDC presents a highly demanded asset. Additionally, having USDC as the borrowable asset enables safer borrowing for users. By accepting volatile assets such as WETH, WBTC, MATIC as collateral, users need only track their collateral value to ensure they avoid liquidation, rather than worrying about the borrowed asset increasing in value while the collateral decreases. While Compound has a robust liquidation mechanism, this brings benefits to users of the platform so that they can practice healthy borrowing.\nSeeding the market with 500K USDC at launch will help create a sustainable market from Day 1. Reserves create a liquidity and loss cushion for users, and enable new dynamics introduced by the upgraded interest rate models in Compound III.\\nbeen waiting for compound on polygon, this looks like the perfect implementation\\nIn general i believe it would be good to expand v3 to polygon. Though there are some concerns in that proposal. I don’t see issues with WETH, WBTC as being collateral, as well as other smaller tokens like Uni, Comp can be also added.\nUSDT isn’t a collateral asset on Compound it needs a discussion even if should be a collateral at all.\nDAI is also up to debate if it should be supported as collateral, as bringing stable token as a collateral to v3 is also debatable proposal. It’s possible, but up to governance.\nUSDC can’t be a collateral, as you propose it as a base asset.\nMATIC isn’t a supported token in Compound so far. It’s a perspective one though, and it might be benefitial to onboard it, but it requires a governance decision by itself.\nSo, i’d say one thing is to initialise comp v3 on polygon with already supported selected assets, and the other thing is to have the proposal in it’s full. I’d suggest to split it in several stages, with first one possibly launching on polygon with same configuration as on eth mainnet. And following it with additional proposals concerning other perspective assets.\\n  @hamzahkhan! Great to see momentum towards a Polygon PoS proposal \nThe Compound team is actively working on testing a Mumbai deployment, governed from Goerli; analogous to the setup between mainnet Ethereum and Polygon PoS. While true that the protocol contracts will be the same, there are a couple of new contracts involved (a bridge receiver contract and timelock), and the assets should all be vetted and price feeds confirmed/discussed.\nThere are restrictions on what assets can be listed as collateral in v3 (i.e. non-rebalancing, no transfer fee), so the assets should be triple and quadruple checked by the community. The price feeds must be reliable and governance should deem each of them as non-manipulatable, understanding the power each price feed address has in the protocol.\nI also agree with @Sirokko that the initial set of assets should be relatively minimal, to ease the governance decision-making for launch.\\nReally excited for this.\\nThis is awesome news, been waiting for this for a very long time!\\nThe cross-chain governance aspect is especially exciting as it is new for Compound. Are the bridge receiver contract and L2 timelock publicly posted in the comet github yet? I didn’t see them there but I might have been looking in the wrong place. Maybe this isn’t the right place for questions about the mechanism, but I’m curious whether the bridge receiver is specifically for governance-initiated asset transfers, like seeding the USDC reserves from the Comptroller on mainnet, or if there would be any interfacing with the bridge receiver contract by third-party contracts / EOAs.\\nGreat questions! It’s all being worked on in the public GitHub repository 24 (warning: this PR is under active development ).\nOverall, the current scheme for multi-chain would cover any deployment which can be governed directly by Compound Governance on mainnet. There are other possibilities besides that, but those are probably further in the future and require other development. In general this means, using a message-passing bridge which Governance decides to trust to deliver messages. Within that set, the idea is to start first with L2 deployments (which essentially by definition have a native bridge), since this reduces the Governance decision to trusting just the L2 chain. The step after L2s, would be to build a receiver for some other (Governance trusted) message-passing bridge, which could then be used on whatever chains that bridge is bridged to.\nThe interface needed by governance is the ability to send a message to a contract on the L1, which encodes a set of actions (targets + calls) for the remote chain. The interface needed by the protocol, is a ‘governor’ (contract) address (which is the Timelock contract on mainnet). In general, we think there should also be a Timelock in front of the protocol (on the receiving side of the bridge), which is its governor, and the admin of that should in turn be some provable governance proxy address, which depends on the details of the message-passing bridge interface, which is effectively what the bridge receiver contract is.\nSo, the bridge receiver doesn’t have any knowledge or details about assets like USDC. That would be handled by Governance deciding what the USDC address is and constructing the protocol deployment using it, and using whatever mechanisms are available for bridging or providing that asset are already/externally available for it. The bridge receiver is the local address for a contract which Governance believes to be a reliable proxy for its messages on the bridged chain.\\n\n  \n\n      github.com/compound-finance/comet\n  \n\n  \n    \n\n\n\n    \n      \n    \n\n  \n\n\n\n      \n        Polygon deploy supply cap proposal 17\n      \n\n    \n      compound-finance:silver/polygon-deploy ← compound-finance:silver/polygon-deploy-proposal\n    \n\n      \n        \n          opened \n        \n          \n        \n        Jan 31, 2023\n      \n        \n\n        \n          \n            \n            scott-silver\n          \n        \n\n        \n          \n            +126\n            -12\n          \n        \n      \n  \n\n\n  \n    - [ ] add description\n- [ ] add actual supply cap values\n- [ ] update ENS text… record\n  \n\n  \n\n  \n    \n    \n  \n\n  \n\n\n\\nYes please  . Would be massive.\\nFollowing the conversations on this thread, recent community calls, Discord, and GitHub, a cUSDCv3 market has been deployed to Polygon from this pull request 6.\nThe parameters to enable the market are being finalized on this pull request 4, and will be discussed next week on the upcoming developer call, as we await Gauntlet’s recommendations on the following risk parameters:\nWBTC: $_ supply cap (_ tokens), _% CF, _% LCF, and _% liquidation fee\nWETH: $_ supply cap (_ tokens), _% CF, _% LCF, and _% liquidation fee\nWMATIC: $_ supply cap (_ tokens), _% CF, _% LCF, and _% liquidation fee\nThe interest rate model is currently configured the same as cUSDCv3 on mainnet. After the launch of the market, Gauntlet will monitor and consider recommendations to the interest rate model based on preliminary utilization & growth.\nThe assets and price feeds of the deployment use the following inputs:\nThe base asset, USDC, uses 0xfE4A8cc5b5B2366C1B58Bea3858e81843581b2F7 2.\nFor WBTC, 0xDE31F8bFBD8c84b5360CFACCa3539B938dd78ae6 2.\nFor WETH, 0xF9680D99D6C9589e2a93a78A04A279e509205945 1.\nFor WMATIC, 0xAB594600376Ec9fD91F8e885dADF0CE036862dE0 2.\nConsiderable effort has gone into testing this deployment, however this will be the first multi-chain deployment of Comet, as well as the first proposal executed across the bridge on mainnet, we therefore recommend relatively conservative parameters when it comes to seeding reserves and rewards for the new market. We do recommend seeding these values though, to account for the current interest rate model and bootstrapping phase.\nOpenZeppelin has completed an additional audit 3 of the bridged governance contracts and flows.\nFinally, see the Initialization Proposal section below for more information about the next steps to enable the deployed market. Note that the proposal includes the actions from the recent Initialize ENS proposal (which failed to reach quorum). Maintaining the official markets list is an important precedent to establish as governance moves forward into governing a multi-chain world.\n\nDeployed Contracts\n\ncUSDCv3: 0xF25212E676D1F7F89Cd72fFEe66158f541246445 5\n\nThis is the main proxy contract for interacting with the new market. The address should remain fixed and independent from future upgrades to the market. It is an OpenZeppelin TransparentUpgradeableProxy contract.\n\ncUSDCv3 Implementation: 0x58db165A3CC86A2955f7A270120E68236b57D819\n\nThis is the implementation of the market logic contract, as deployed by the Comet Factory via the Configurator.\n\ncUSDCv3 Ext: 0xbdE8F31D2DdDA895264e27DD990faB3DC87b372d\n\nThis is an extension of the market logic contract which supports some auxiliary/independent interfaces for the protocol. This is used to add additional functionality without requiring contract space in the main protocol contract.\n\nConfigurator: 0x83E0F742cAcBE66349E3701B171eE2487a26e738\n\nThis is a proxy contract for the ‘configurator’, which is used to set and update parameters of a Comet proxy contract. The configurator deploys implementations of the Comet logic contract according to its configuration. This pattern allows significant gas savings for users of the protocol by ‘constantizing’ the parameters of the protocol.\n\nConfigurator Implementation: 0x9c4ec768c28520B50860ea7a15bd7213a9fF58bf\n\nThis is the implementation of the Configurator contract, which can also be upgraded to support unforeseen changes to the protocol.\n\nProxy Admin: 0xd712ACe4ca490D4F3E92992Ecf3DE12251b975F9\n\nThis is the admin of the Comet and Configurator proxy contracts. It is a ProxyAdmin as recommended/implemented by OpenZeppelin according to their upgradeability pattern.\n\nComet Factory: 0x2F9E3953b2Ef89fA265f2a32ed9F80D00229125B 2\n\nThis is the factory contract capable of producing instances of the Comet implementation/logic contract, and invoked by the Configurator.\n\nRewards: 0x45939657d1CA34A8FA39A924B71D28Fe8431e581 3\n\nThis is a rewards contract which can hold rewards tokens (e.g. COMP, WMATIC) and allows claiming rewards by users, according to the core protocol tracking indices.\n\nBridge Receiver: 0x18281dfC4d00905DA1aaA6731414EABa843c468A 1\n\nReceives bridged governance messages from the Polygon FxChild contract and forwards them to the bridge timelock.\n\nBridge Timelock: 0xCC3E7c85Bb0EE4f09380e041fee95a0caeDD4a02 1\n\nThe governor of the Comet deployment, exclusively receiving input from Ethereum mainnet governance through the bridge receiver.\n\nInitialization Proposal\nTo initialize the market, the deployment process is similar to the initialization of cUSDCv3 on Ethereum mainnet, the primary difference being the bridging of governance actions to Polygon, instead of taking place directly on the governance chain (Ethereum mainnet).\nThe proposal is currently prepared and tested using best guesses for final parameters and decisions from the community, which are intended to be discussed next week.\nThe initialization proposal will take the following actions:\n\nSet Comet configuration and deploy new Comet on Polygon. This sends the encoded setConfiguration and deployAndUpgradeTo calls across the bridge to the governance receiver on Polygon.\nApprove Polygon’s ERC20Predicate to take Timelock’s USDC, in order to seed the market reserves through the bridge.\nDeposit USDC from mainnet to the Polygon RootChainManager contract to bridge to Comet.\nApprove Polygon’s ERC20Predicate to take Timelock’s COMP, in order to seed the rewards contract through the bridge.\nDeposit COMP from mainnet to the Polygon RootChainManager contract to bridge to CometRewards.\nSet up the ENS subdomain v3-additional-grants.compound-community-licenses.eth,  with the Timelock as the owner.\nWrite the ENS TXT record v3-official-markets on v3-additional-grants.compound-community-licenses.eth containing the official markets JSON.\n\nThe deployment and proposal migrations have been built using the Comet scenario framework 1 and deployed using the Comet deployment manager 1. The deployment was run through a GitHub action 1 using seacrest 1 and the scenario checks can be seen from the proposal branch CI checks 3.\\n\nGauntlet Initial Asset Recommendations - Compound v3 Polygon USDC Comet\n\nSummary\nWe provide two options to the community below. Option 1 is very conservative for the purpose of testing out Compound V3 mechanics. As such, the conservatism is less so derived from market risk (which is Gauntlet’s focus) but more so on the smart contract and other technical risks. Option 2 is less conservative and assumes that the community does not need to test Compound V3 mechanics on a new chain.\n\nOption 1: Very Conservative (Test out Mechanics)\n\n\n\n\n\nWETH\nWBTC\nWMATIC\n\n\n\n\nSupply Cap\n1800 ($3.02M)\n80 ($1.97M)\n1M ($1.41M)\n\n\nLiquidation Factor\n50%\n45%\n40%\n\n\nCollateral Factor\n45%\n40%\n35%\n\n\nLiquidation Bonus\n5%\n5%\n7%\n\n\n\nStorefront price factor: 60%\nIR Curve: Same as Ethereum USDC comet\n\nOption 2: Conservative (Assume mechanics are working, then gradually increase aggressiveness of parameters)\n\n\n\n\n\nWETH\nWBTC\nWMATIC\n\n\n\n\nSupply Cap\n11k ($18.5M)\n400 ($9.9M)\n10M ($14.1M)\n\n\nLiquidation Factor\n82.5%\n75%\n70%\n\n\nCollateral Factor\n77.5%\n70%\n65%\n\n\nLiquidation Bonus\n5%\n5%\n7%\n\n\n\nStorefront price factor: 60%\nIR Curve: Same as Ethereum USDC comet\nThe supply caps are set as a function of on-chain liquidity and can be increased after the initial launch. The proposed LFs for the initial listing are set conservatively while still being capital efficient enough for usability.\nBy approving this proposal, you agree that any services provided by Gauntlet shall be governed by the terms of service available at gauntlet.network/tos.\\nThanks @pauljlei and Gauntlet team for preparing these recommendations! We’ve updated the parameters according to Option 2 in the proposal pull request 1. We were going to make this proposal today, but thought better of it given the late in the day timing and what would be Saturday execution on Polygon. This will also allow time for the community to review the parameter options, and final proposal details before making it.\\nTo update this thread, the proposal 25 was made yesterday and will be in review for another 18 hours until voting begins!\\nAs the cUSDCv3 Polygon market continues to grow steadily, Compound Labs intends to propose refreshing the supply of COMP available for rewards on Polygon. The initial proposal transferred a relatively conservative amount of COMP, as was deemed prudent for the first multi-chain market instance and proposal. The draft proposal can be found on this pull request 7, and aims to renew the supply for an additional year at the current speeds (approximately).\\nI previously recommended deploying a native USDC comet on Base 1, I am now recommending the same for Polygon as Circle now support native USDC on Polygon.\nI would however state that we should allow some time to pass for the supply of native USDC to grow, as it pales in comparison to that of bridged USDC, before such a comet is introduced.\nIn the meantime, it may be prevalent to rename the market for bridged USDC on Polygon to show that it is for bridged USDC, as it currently doesn’t."
  },
  {
    "number_of_comments": 19,
    "postid": "df90ead0-f4c8-4881-a3f0-8c381d1350de",
    "posturl": "https://www.comp.xyz/t/new-listing-proposal-paxos-stablecoin-pax/1894",
    "combinedcontent": "Introduction\nHi Compound community! We are Vishal Kankani and Spencer Applebaum from the Multicoin Capital team.\nWe are investors in Paxos through our Venture Fund and own COMP in our Hedge Fund. We would like to propose adding Paxos Standard (ticker: PAX) as a stablecoin on the Compound.finance 6 platform.\nAs of 24th June 2021, as per CoinGecko 5:\n\nPAX is the 7th largest USD-backed stablecoin with a market capitalization of approximately $900 million.\n\nPAX ranks 5th in terms of number of global exchange listings (105).\nThere are 4 stablecoins that have more exchange listings: USDT, USDC, DAI and TUSD—all of which are listed on Compound already.\n\nBased on the above data, PAX seems to be the next logical choice for stablecoins on Compound.finance 6. We propose adding support for PAX. We welcome comments and participation in this proposal from the community.\nOverview\nPaxos Standard (PAX) was launched in 2018 and is a USD stablecoin that is 100% backed by U.S. dollars. Paxos issues PAX, and both the company and the token are regulated by the New York State Department of Financial Services (NYDFS). PAX tokens are minted/burned based on client demand. The process for minting and burning has been approved by the NYDFS and is regularly audited by Withum, an advisory, tax and audit firm. Full PAX attestations are completed monthly and posted to www.paxos.com/attestations.\nPaxos raised $142 million in December 2020 in its Series C investment round, bringing their total funding to more than $240 million. As stated by the company, Paxos’ main 2021 goals include increasing tokenized assets under custody and trustworthiness. Paxos not only has a NYDFS state Trust charter but also is the first crypto-native company to receive preliminary conditional approval from OCC for a de novo national Trust Bank charter. Furthermore, Paxos was the first digital asset company to get SOC2 Type 2 verified 3 for custody, exchange and stablecoin product platforms. This follows their SOC1 Type 2 certification 1, which was granted in December 2020. These certifications ensure users’ data is compliant with their company policies, controls and regulatory requirements.\nPAX Adoption Across DeFi and CeFi\nPAX is included in MakerDAO’s pool and is available for trading on DeFi Apps, including Curve and Uniswap. It is used for lending and borrowing on known CeFi institutions such as BlockFi and Celsius. Additionally, it is traded on many centralized exchanges, including but not limited to Binance, Bitfinex, Bitstamp, Huobi Global, and OKEx. PAX daily traded volumes are greater than USD $50 million across CeFi & DeFi.\nProposal\nWe would like to extend PAX’s functionality in the DeFi space through listing on Compound.finance 6. We believe adding PAX to Compound.finance 6 will provide users a trustworthy alternative crypto asset to post collateral and borrow. PAX is an excellent source of collateral because it is one of the most reliable, regulated stablecoins. PAX is valuable to borrow due to its trustworthiness and range of use cases, including store-of-value, settlement, collateral, trading, and peer-to-peer transfer.\nToday, nearly 75% of the crypto assets deposited into Compound.finance 6 are fiat-backed stablecoins, stablecoins partially backed by other fiat-backed stablecoins, and centrally wrapped BTC. Adding PAX diversifies Compound.finance 6’s exposure to any one centralized token issuer, while at the same time adding functionality and utility for a very trusted asset in the ecosystem. It would further diversify the menu of centralized stablecoins and stablecoins with differentiated utility on Compound.\nWe welcome comments and participation in this proposal from the community.\nPrice Feed Mechanism & Other Parameters\nCurrently, PAX is traded on Uniswap and Curve and we could use those price feeds. Also, if there are concerns around the daily traded volumes on these specific venues, we could start with a 0 collateral factor to mitigate short term concerns. An alternate approach would be to peg it to USDC as these two assets come with a very high level of assurances.\nInitial Parameters (Open to suggestions)\n\nCollateral Factor: 0-25%\nReserve Factor: 20-30%\nCOMP Speed: None\nBorrow Cap: 80% of supply\nSupply Cap: None\nPrice Feed ideas: Uniswap and/or Curve OR Peg to USDC\nInterest Rate Models: USDC\n\nMarket data\n\n\nMarket Cap 3: ~$900 million\n\n24hr trading volume: $50 - $100 million\n\nExchanges: Binance, itBit, Bitfinex, Coinsbit, Atomars, etc\n\nRelevant Links\n\n\nWebsite 1\n\n\nWhitepaper 1\n\n\nGithub\n\n\nCompany\n\n\nTwitter 2\n\n\nAudits\n\n\nSmart Contract 3\n\n\nSOC 2 Type 3\n\n\nAttestations 2\n\n\nEthereum Addresses Contracts 3\n\n\\nAs a general matter, Compound benefits from having as many legitimate markets as possible. It increases the number of markets supported by Compound, grows its reserves, and makes it a more competitive product in the marketplace.\nIn short, I’m very much in favor of adding PAX with a 0% collateral factor to start with. Good proposal @kankanivishal!\\npm me or @TylerEther lets get this done\\nWe should do this. Also, I think we should include collateral factor in initial proposal - it’s most regulated and trusted stablecoin on the market (NYDFS, OCC, SOCII, etc). Has potential to be big amongst institutions especially\\nI’ve spoken with Spencer Applebaum from Multicoin Capital and I am in support of this proposal.\nPAX is one of the most regulated stablecoin around being regulated by the New York State Department of Financial Services (“NYDFS”). This has the following benefits (taken from here 1):\n\nThe value of each stablecoin token is tied directly to the value of the US dollar, and the amount of “reserve” dollars equal or exceed the number of stablecoins outstanding.\nRegulators are overseeing the establishment and maintenance of reserves backing the stablecoins.\nReserves may only be held in the safest forms, such as FDIC-insured bank accounts and in short-term maturity US Treasury instruments.\nReserves are fully segregated from corporate assets, specifically for the benefit of token holders, and are held bankruptcy remote pursuant to the New York Banking Law.\n\nWe compare the breakdowns of PAX vs. other stablecoins:\n\nimage2024×1012 217 KB\n\n\nimage1912×2048 127 KB\n\nThe underlying assets of PAX are much more liquid and stable compared to USDC and even more so than USDT.\nI argue that the price of PAX will hold to a 1:1 ratio against the US dollar much better than USDC or USDT which are both listed on Compound for the reason of being strictly regulated by the NYDFS.\nI’ve also gone ahead and looked into the smart contract code and its audits which all seem fine.\nThe first version of the PAX contract (upgradable contract) was audited 3 times and those audits can be found here: https://github.com/paxosglobal/paxos-gold-contract/tree/master/audit-reports/pax-audits 1.\nAlthough the second (latest) version of the contract hasn’t been audited, the contract format is based on PAXG which has been audited twice: https://github.com/paxosglobal/paxos-gold-contract/tree/master/audit-reports/paxg-audits.\nI’ve also asked about how the owner account of the contract is secured to which Spencer replied that it’s secured via a multisig contract: https://www.paxos.com/simple-multisig-how-it-works-and-why-its-awesome/ 1.\nAll-in-all, PAX seems like a very strong stablecoin to add to Compound.\\nWhat are everyone’s thoughts on starting off with a conservative 10% collateral factor?\\n\nDeployment\nnpx saddle -n mainnet script token:deploy '{\n  \"underlying\": \"0x8E870D67F660D95d5be530380D0eC0bd388289E1\",\n  \"comptroller\": \"$Comptroller\",\n  \"interestRateModel\": \"0xFB564da37B41b2F6B6EDcc3e56FbF523bD9F2012\",\n  \"initialExchangeRateMantissa\": \"2.0e26\",\n  \"name\": \"Compound Paxos Standard\",\n  \"symbol\": \"cPAX\",\n  \"decimals\": \"8\",\n  \"admin\": \"$Timelock\",\n  \"implementation\": \"0xa035b9e130F2B1AedC733eEFb1C67Ba4c503491F\",\n  \"becomeImplementationData\": \"0x\"\n}'\n\nInterest rate model: same as cTUSD, cUSDT, and cDAI\nImplementation: same as cTUSD, cUSDT, and cDAI\\nDefinitely not. We have a pretty clear precedent of starting with 0% collateral factors. Additionally, there is massive risk from the centralization here which must be accounted for.\\nDeployment looks good.\nSeparate note: start the collateral factor at 0.\\nThe collateral factor will start at 0, as is standard. Thanks for the input @getty and @arr00.\\ncPAX deployed 2 to 0x3cDb0115b5CCd1D5Cb631c2946Abf55816Ad61a2.\nnpx saddle match 0x3cDb0115b5CCd1D5Cb631c2946Abf55816Ad61a2 CErc20Delegator 0x8E870D67F660D95d5be530380D0eC0bd388289E1 0x3d9819210a31b4961b30ef54be2aed79b9c9cd3b 0xFB564da37B41b2F6B6EDcc3e56FbF523bD9F2012 200000000000000000000000000 \"Compound Paxos Standard\" cPAX 8 0x6d903f6003cca6255d85cca4d3b5e5146dc33925 0xa035b9e130f2b1aedc733eefb1c67ba4c503491f \"0x\" -n mainnet\n\\nmy man, so happy to see this progress. May we walk forth towards the horizons with delight.\\nWhat are the next steps regarding this proposal? Has there been a vote yet? thx\\nThe cToken contract has been redeployed to match the new name and symbol.\ncUSDP - “Compound Pax Dollar” 3 - 0x041171993284df560249B57358F931D9eB7b925D\nThis new market will be proposed on-chain soon \\nSimulation 6 - currently fails because the price oracle hasn’t been configured for USDP yet. I’ll modify the simulation to use SimplePriceOracle within the next 24 hours to get a passing simulation.\\nOh awesome! Do you have enough COMP delegated for a proposal?\\nYup, I have the power to make a proposal. Just waiting on Chainlink for the new oracle contract.\\nThe new USDP oracle is here EACAggregatorProxy | 0x09023c0DA49Aaf8fc3fA3ADF34C6A7016D38D5e3 12\\nI’m gearing up to submit the proposal to add a USDP 3 market.\nProposal simulation (it passes): https://github.com/TylerEther/compound-protocol/blob/add-market-usdp/spec/sim/1000-add-market-usdp/hypothetical_proposal.sim 9\n\nProposal\nComptroller._setPriceOracle(0x046728da7cb8272284238bD3e47909823d63A58D)\nUse the latest UAV price oracle 4.\nComptroller._supportMarket(0x041171993284df560249B57358F931D9eB7b925D)\nAdd the cUSDP 2 market.\ncUSDP._setReserveFactor(250000000000000000)\nSet the reserve factor to 25%.\\nProposal 73 18 is live! "
  },
  {
    "number_of_comments": 9,
    "postid": "b6722dd3-5b98-4f20-8003-33b8303eb2d6",
    "posturl": "https://www.comp.xyz/t/add-market-rai/2611",
    "combinedcontent": "\nWhat is RAI?\nRAI is an incarnation of the early design for Single Collateral Dai: it is solely backed by ETH, meant to have less governance as time passes, and uses a funding rate to automatically balance market forces and keep itself stable.\nThis funding rate can be either positive or negative. A positive rate is similar to the stability fee in SAI or DAI, meaning that RAI borrowers would pay a fee by having their RAI debt increase in value. A negative rate can compel market participants to sell RAI when it is traded at a premium compared to its floating “target price” — something that is missing from all pegged coins currently on the market. The target price is the price that the protocol wants RAI to have on the open market.\nFor more details on RAI, you can check out the videos and FAQs on the Reflexer website 4.\n\nWhy should RAI be added to Compound?\nRAI is currently the only real stablecoin that is not 1:1 pegged to fiat and is also solely backed by ETH. It offers a unique value proposition to protocols that want to diversify their stable asset risk and maintain censorship resistance.\nHistorically, Compound proved to be extremely prudent in choosing which assets to include in the protocol. RAI seeks to minimize risk on all fronts by removing human governance, maintaining purity with ETH collateral, and automating the vast majority of parameters in the protocol. For these reasons, we believe RAI fits Compound’s approach to add safe assets to the system.\n\nTechnicals\n\nToken Etherscan link: 0x03ab458634910aad20ef5f1c8ee96f1d6ac54919\n\nContract code verified and open-source? Yes\n\n\nTeam: People · Reflexer Labs · GitHub 6\n\nExperience in the respective field: Certain team members received grants for stablecoin R&D and all team members have prior experience in building or auditing L2 scaling solutions or DeFi protocols\n\n\nWhitepaper: whitepapers/rai-english.pdf at master · reflexer-labs/whitepapers · GitHub 3\n\nAudits: GitHub - reflexer-labs/geb-audits: List of audits for the whole suite of GEB related smart contracts 2\n\nGovernance structure: 3/5 multisig\n\n3 core team members and 2 anons\n24h delay on changing anything\n\n\nNumber of contract transactions: More than 20,000\nAge of the token: 8 months\nSocial channels:\n\nTwitter: https://twitter.com/reflexerfinance\n\nDiscord: Discord 1\n\n\n\nPausability: Can trigger settlement for the whole protocol after a 24 hour governance delay\nUpgradeability: Can upgrade the whole protocol besides the ERC20 RAI contract and the core contract that contains data about each position minting RAI\nRestrictions on transfers: No\nTotal supply: 33,169,412\nCirculating supply: 32,583,538\nToken distribution\n\nNumber of unique holders: 2476\nAddresses holding >= 10% of supply: 3\n\n\nCEX liquidity: $400K\nDEX liquidity: $53M (as at November 15, 2021)\n\n\nUniswap V2 1\n\nTotal liquidity: $16M\nVolume (24h): $635k\nTransactions (24h): 20\n\n\n\nUniswap V3 1\n\nTotal liquidity: $37M\nVolume (24h): $891k\nTransactions (24h): 35\n\n\n\n\nMarket cap: $98M (as at November 15, 2021)\nPrice volatility: Sub 1% per day\nSupporting oracles: Chainlink RAI/ETH, Chainlink RAI/USD\nUnderlying assets\n\nHow quickly can the amount or value of any underlying assets change?\n\nRAI is solely backed by ETH. ETH experienced sharp 30%+ drops over a couple of days\n\n\n\n\nInsurance:\n\n\nNexus Mutual 1\n\nWhat’s covered:\n\nContract bugs\nEconomic attacks, including oracle failures\nGovernance attacks\n\n\nAmount staked: 96.194k NXM = ~$16.747M\nCapacity: $19.1M\nMax individual coverage length: 365 days\nCost: 2.60% annually\n\n\n\n\n\n\nProposal parameters\n\nCollateral factor: 0%\nBorrow cap: 4M RAI\nCOMP borrow/supply speeds: 0\nInterest rate model: Same as DAI\nReserve factor: 25%\n\\nI’ve been working with @stefan on this new proposal, taking consideration points from Compound’s developing formal process for new collateral assets 4, other proposal threads, as well as processes outside of Compound.\nI think this is a great asset to add, further diversifying the stablecoin markets on Compound with this innovative gem.\nRelated: https://www.comp.xyz/t/add-market-rai-reflex-index/2045 10\\nI am a big fan of RAI and a member of the community but not the team.\nRAI diverges from other stablecoins in a few key ways that I think should make it of interest to Compound.\n\n  \n\n      twitter.com\n  \n\n  \n    \nJake Chervinsky 5\n@jchervinsky\n\n\n  RAI is vastly underutilized within ETH DeFi. For a variety of reasons, we should change that.\n\n\n\n  11:06 AM - 16 Sep 2021\n\n    \n      \n        \n      \n      391\n    \n\n    \n      \n        \n      \n      47\n    \n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\n1. Not pegged to $1\nDAI, USDC, USDT, TUSD, USDP, GUSD, sUSD, UST, etc., are all pegged to $1.00. If the price diverges from $1.00 then arbitrageurs mint or redeem coins to offset the price – this is usually facilitated by centralized minting authorities (Circle, Gemini, Tether, etc) or a PSM.\nRAI, on the other hand, is stable relative to USD, but is not pegged to 1.00 USD. Currently, the “redemption price” is $3.0275 and the market price is ~$3.025. If the market price diverges significantly from the redemption price, then the system changes the redemption price via the “redemption rate”.\nIf RAI is over-sold, e.g. it’s at $2.99 when the redemption price is $3.0275, then the system slowly begins increasing the redemption price via the “redemption rate” (recently, redemption rate has always been between -7% and 5% APY).\nThis means people who have minted RAI are seeing their collateral requirements increase and are incentivized to buy up cheap RAI to cover their debts. People who don’t have RAI debts may also choose to buy, hold, and lend RAI as reserves because they expect it to outperform USD when the redemption\nConversely, if RAI market price is above the redemption price then people are incentivized to sell or short RAI. This means there is significant demand to borrow and lend RAI, which will lead to profits for Compound that would normally be captured by arbitrageurs in the case of $1-pegged coins.\n2. Ungovernance\nForming DAOs is a hot thing to do right now. We’ve all recently seen crazy and/or controversial government shenanigans in big $1B+ protocols.\nI am unconvinced that you need a big, complicated DAO with hundreds of voting members, delegates, governance bribes, etc. solely to maintain an asset that is roughly stable relative to USD. RAI/Reflexer are guided by an opposing philosophy: governance minimization. Aside from certain critical functions such as the ability to change oracles, Reflexer aims to minimize the power of governance.\nThis is a massive divergence from almost every other DeFi protocol. If it succeeds, RAI will be a public good and beyond the control of VCs, hedge funds, governments, regulators, even the Reflexer team. (Only the hand of the market will effect RAI price).\nIn conclusion, adding a RAI market will help Compound generate more fees. The initial on-chain proposal will set collateral factor to 0 so I don’t see much risk from Compound’s point-of-view in the short term. In the long term, having RAI as collateral will reduce risk because it is more resistant to the powers that be than any other stable asset and will provide collateral diversity.\nDisclosure: I hold a significant amount of Reflexer’s ungovernance token, FLX.\\nFirst of all, thanks for the detailed writeup - best set of facts I’ve seen so far on one of these, and clear you have done your homework.\nI know the current proposal isn’t for RAI being used as collateral. And it is still fun think through the collateral implications ahead of time.\nI think a big lesson from the recent C.R.E.A.M. total hack is that is fatal to a lending platform to list a wrapper token:\n\n…that can be created and destroyed from other assets\n…that has a low market cap\n…that can change value\n…that uses live, on-chain oracles for lending\n\nBecause you are proposing using Chainlink as a oracle, the last condition is not met. As a result, an attacker can’t really flash loan to manipulate the price.\nHowever, it’s still worth thinking about if it would be possible to exploit Compound using this coin since it meets the other criteria.\nThe majority of the RAI liquidity is in Uniswap v3. This liquidity is allocated in bands, and currently RAI price is on one side of the narrow liquidity band.\n\nimage1276×802 17.2 KB\n\nThis means that it only takes a comparatively small amount to shove the price arbitrarily further in this direction - the attacker just adds a liquidity band where they want to price to end up, and then buy up the small amount of liquidity in between. Maybe a million dollars might be enough to manipulate the price enough on v3, v2, and cex, which puts it into feasible to do without a flash loan. Pure arb bots wouldn’t respond to correct because all the DEX prices have changed. If an attacker can do this within a few blocks of when the chainlink oracles read the price, then the attacker can massively swing the oracle price on Compound. They could either swing the price down hard, and liquidate people (similar to the DAI event), or swing the price up hard, and roughly do a C.R.E.A.M. attack to empty Compound.\nAlthough the attacker can’t flashloan to manipulate the price, they can flashloan to exploit the protocol after the oracle price is manipulated.\nI’ve not proved out, or POC’d this attack - just back of the napkin thinking. Life in DeFi security has made me paranoid and jump at shadows.   I could be wrong about this…\n\\nUnderstand the concerns and happy to see the community here is thorough \nRAI isn’t a wrapper token so there’s no interest accruing when you hold it. Also as you mentioned, Compound would be using a Chainlink feed for RAI so manipulation isn’t that easy.\nAdditionally there’s a borrow cap for RAI and it will not start as a collateral asset.\\n\n\n\n stefan:\n\nRAI isn’t a wrapper token\n\n\nForgive my ignorance. \nIf I’m reading the RAI site correctly, you can instantly mint RAI from ETH, right? Is it possible to instantly redeem RAI back to ETH at the contract level?\\nIt works like DAI, you can mint RAI with ETH and then repay your position with the minted RAI to get back ETH.\\nAdding more details for RAI as advised in this post 1.\n\nGini Coefficient of EOAs: 0.972\nTop 10 EOAs own 4.2% of entire float\nAll CEX liquidity is currently on Coinbase.\n30/60/90 day moving average for CEX liquidity is approx $430K\n30/60/90 day moving average for DEX liquidity: $35.9M/$34.2M/$35.1M\n\nTop 10 EOAs:\naddr,rai\n0xc230ce5643a10702111572c938919c9f448c46d8,70789.966640855\n0x8ffab4fb8661484580a02d5408a49222c9e2f4ba,71399.8384428706\n0xc74199c865360812210745508d4efd12a18e907b,82518.0101538891\n0x9f03409298b9126254ff190012a12220fc73aba0,83150.9317642231\n0x7cd7a5a66d3cd2f13559d7f8a052fa6014e07d35,85627.3924139083\n0xe41a2b194f15aaae7500421ecb71e39c5be26bfd,110008.023617276\n0xeb2629a2734e272bcc07bda959863f316f4bd4cf,123760.433183171\n0xec61e3957739f01084d1b167012aaeeed367eec3,199767.59525634\n0x86f6ff8479c69e0cdea641796b0d3bb1d40761db,228852.899264981\n0x26da854f28f2181858ce4aad1cdb0c2027b6465c,294085.658265806\n\\nThe cToken has been deployed to 0x2220e2A723C5099aB0be7FBBc29D052938F82E0b 15. \\nSimulation: https://github.com/TylerEther/compound-protocol/blob/add-market-rai/spec/sim/1002-add-market-rai/hypothetical_proposal.sim 3\nWorks up until the Comptroller calls the price oracle for the price of RAI - we’re still waiting on Chainlink for a new oracle contract, which should be deployed tomorrow."
  },
  {
    "number_of_comments": 20,
    "postid": "bf3bf10c-8dda-4caf-ad55-900bfec6ad83",
    "posturl": "https://www.comp.xyz/t/yfi-listing-proposal/278",
    "combinedcontent": "yEarn Overview\nYearn.finance (“yEarn”) is a DeFi yield aggregator and optimizer.\nyEarn uses a suite of DeFi protocols and primitives including yTokens and delegated vaults to maximize yield.\nYFI is yEarn’s governance token. YFI is an ERC-20 token used to vote on yEarn improvement plans (“YIPs”) and new yield strategies for liquidity providers. Token holders can stake their YFI through the governance portal to earn a percentage of fees generated by the protocol.\nBackground\nyEarn was launched in early 2020 and initially optimized yield across Compound, dydx, Aave, and Fulcrum. yEarn has continued to add new DeFi products including yleverage.finance and ypool.finance.\nIn mid-July 2020, yEarn announced the release of the YFI token. Given the number of configurable parameters in yEarn, the YFI token is used by the community to govern the protocol. There was no pre-mine, sale, or allocation for investors/team. YFI was only earned by providing liquidity.\n30,000 YFI tokens were minted and have been fully earned/distributed among the community. The community is currently working with firms such as Delphi Digital and Gauntlet to construct an optimal token model and inflation schedule.\nA 6/9 Gnosis Safe multisig 3 has been granted full rights over the YFI governance contract. YIP 40 proposes replacing inactive multisig signers with four new signers.\nSince the launch of YFI, yEarn has grown from $8M to over $1 billion 1 in total value locked (“TVL”).\nYFI Liquidity Venues\nYFI trades on decentralized exchanges including Uniswap and Balancer, as well as centralized exchanges including Binance and FTX.\nYFI’s 24-hour volume on Binance currently stands at $17MM 2 across USDT and BTC pairs. On Uniswap, YFI’s 24-hour volume stands at $5.4MM.\nYFI Holder Base\n1600×679 170 KB\nSource: Etherscan\nRationale\n\nyEarn has one of the most engaged and active communities. This has translated into a YFI holder base looking for liquidity/leverage on their YFI. Compound could become the first DeFi platform to offer liquidity on YFI, further increasing borrow demand and utilization of assets on Compound.\nInitially, the community may see value in having a collateral factor of 0% to reduce risk to lenders. Additionally, if passed, the borrow limits proposal 3 may help mitigate risk to lenders.\n\nRisks\n\nYFI is a relatively new token with a limited track record of volatility and trading history.\nyEarn multisig and its potential exposure to lenders.\n\nWhile the community has not recently launched new markets, we see value in starting the discussion around new markets on Compound.\nWe would like to gauge the community for feedback.\\nI support this. YFI should definitely be added \\nWhat happened with this proposal? Can’t see any replies or discussion or anything.\\nI think the issue is we need a Coinbase price feed to handle liquidations\\nDoesn’t that become a limiting factor then? Only coinbase oracle supported tokens can be listed that means.\nThere are chainlink oracles btw for YFI price. And chainlink is quick to provide price feeds if there is a requirement for a specific pair.\\nOne month later, YFI is now listed on Coinbase Pro, any updates? There is lots of on-chain (and off-chain) liquidity for YFI, so it seems like an interesting candidate for Compound. It’s market has matured since this post was originally created.\nAre there people still working on this?\\n@paraficapital @mrhen unlike other governance processes, the community is able to develop changes to the Compound protocol, and list new assets without a gatekeeper.\nThis guide 17 lays out the process to add a new market. The steps to add YFI include:\n\nEnsuring YFI prices are being reported by Coinbase\nDeploy a new Open Price Feed view contract that includes YFI\nDeploy a cToken contract for YFI; this would likely be the generic upgradable ERC-20 cToken\nCreate a governance proposal; this is open to anybody with 100 COMP, using an Autonomous Proposal 5; this was recently completed by a community member 7 to increase the WBTC collateral factor 2\n\nGather public support for the change\n\nAll of this can be done right here on comp.xyz!\\n@rleshner it seems Coinbase does not support YFI in their open price feed (current list from their API as of a moment ago below). Do you have any idea what it takes to make that happen? I’d be unsure short of tagging them on twitter! \n\nBAT\nBTC\nCOMP\nDAI\nETH\nKNC\nLINK\nREP\nUNI\nXTZ\nZRX\n\\nI am here to report that Coinbase Pro has added YFI to its open price feed. So what is necessary to move this process forward. Keep in mind that I am not a coder.\\nI can’t confirm this. I just checked the coinbase pro oracle api and didn’t find YFI: https://api.pro.coinbase.com/oracle 10\\nI am referring to this: https://api.pro.coinbase.com/products/yfi-usd/ticker 5\\nOkay, I see. But this is not the API endpoint, which is used for the price oracle.\\nWhat needs to happen to get it added to the oracle?\\nWhy do we need Coinbase to add YFI to its open price feed ?\nIt is not a decentralized oracle \\nIn fact, the oracle is not really decentralized as the only source is currently Coinbase. This also means, that the Coinbase API is a single point of failure. Beside that, there is no guarantee, that price changes are posted to the blockchain at all, as this is only done by volunteers.\nUnder the bottom line, I think currently the open oracle is inferior compared to other solutions out there.\\nIt is a requirement for new asset listing.\nThe compound protocol supports Uniswap V2 as a decentralized fallback oracle in case of any funny business on the coinbase feed.\\nIs anyone in the community (with the ability to put up 100 COMP for the auto. proposal) interested in this? I think its a good value add for both the COMP and YFI communities.\nIt is clear there is lots of demand for YFI as collateral (see Maker quickly raising YFI-A debt ceiling from $7M to $20M, and both getting used almost instantly).\\nI don’t think, that UniswapV2 is used as a fallback oracle. It is just used to define a range for the Coinbase price to be valid.\nLike you wrote, it is a requirement, that the Coinbase API delivers the YFI price. As this is not the case, it wouldn’t make sense to create a proposal right now.\\nThank you for the correction.\nUnfortunate, but I agree. I wish I had any idea what Coinbase’s criteria are for this. UNI was added almost immediately to the price feed it seems, but YFI has been on their markets longer but is still not included.\\nhey @paraficapital , is there any interest to continue with this proposal? I don’t have any dev power that I can leverage but I do believe that expanding YFI’s market availability is important overall\nI have reached out Zach from Coinbase in order to request them to add YFI to their Coinbase Pro API and they told me they would look into it.\nDo let me know how can I be of help to move this forward. You can reach me out on Twitter as Luciano_vPEPO or on Telegram as vPEPO\\nI feel that the flash loan attack has revealed the vulnerability of YFI.\nWe still need to look at the addition of YFI more carefully."
  },
  {
    "number_of_comments": 21,
    "postid": "9d668ef0-1dcb-4b17-bf52-1b02bc738f47",
    "posturl": "https://www.comp.xyz/t/permissionless-listing-isolated-markets/1794",
    "combinedcontent": "I’ve had an idea in the back of my mind for the past month now, so let’s bring it to light!\nWouldn’t it be great to support the lending and borrowing of any sort of crypto asset? I have yet to see support for this in DeFi.\nThe main challenge to this idea is that every new market on Compound presents risks to the whole protocol. What if one token added as a market experiences a rug pull, price manipulation, or some other similar situation? Liquidations and losses can occur across all of the markets on Compound.\nSo how can we mitigate or contain these risks while simultaneously offering permissionless listings? Isolated markets!\nRight now when you deposit an asset as collateral on Compound (granted > 0% collateral factor), you can borrow any other asset supported by Compound.\nWith isolated markets, we can use pairs, where the collateralization of one asset allows for the borrowing of ONLY the other asset for the pair.\nLet’s consider an example: isolated GRT/USDC market. Are you a holder of USDC and you believe GRT is a safe asset? Lend out your USDC in this pool. What if all suppliers borrowing USDC against GRT get liquidated? The pool would be composed of mostly GRT. How comfortable are you as a USDC lender? If you’re not comfortable with this scenario, then simply don’t participate in this market. If you are comfortable with this, then you have an opportunity to make interest on your USDC at the rate of this specific isolated market.\nThe risk of permissionless listing is then contained to each and every isolated market.\nThe benefit of permissionless listing for, say, stablecoin lenders is that they have MANY more opportunities to make interest on their stablecoins. Theoretically, the more risky the non-stablecoin asset is, the higher the APR will be for lending stablecoins. The stablecoin lenders more skilled in risk assessment and management will then have much more opportunity to make higher returns.\nThere are most likely still risks to the overall protocol involved with this idea, as well as various challenges. Let’s discuss them here!\nGoing forward in this thread, let’s refer to the current Compound markets as cross markets and the new idea as isolated markets.\\nyes yes yes and yes. How can I help? Love the idea!\\nGreat idea… I had the same idea a while back:\n  \n\n      Medium – 24 Sep 20\n  \n\n  \n    \n\nThe BentoBox lending solution 14\n\n  Platforms like Compound have many limitations. Bentobox is the next generation of DeFi lending platform.\n\n  \n    Reading time: 6 min read\n  \n\n  \n\n  \n    \n    \n  \n\n  \n\n\nAnd it’s been live for a while:\nhttps://app.sushi.com/bento/kashi/lend/0x795feb1c35dc07991bfd23ab74378885ec86c233 13\nHappy to talk about licensing the code to Compound.\\nInteresting, maybe compound could be a Kashi strategy? if I am saying that right?\\nThanks for sharing this idea on the dev call today. I support the aspiration to offer permissionless listings for isolated markets. That said, I think the reputational risk to the protocol can’t be overstated. Kashi has been running for a while now, on multiple chains, but it still hosts a small, selective set of lend/borrow markets with Chainlink oracles.\nWe know from the experience of other protocols that permissionless listing on a protocol with decent network effects attracts projects seeking to take advantage of new user naivety. For example, if you quickly peruse recent activity in the Uniswap Discord, you’ll find a deluge of new Uniswap users clamoring for help after trading or providing liquidity to long-tail assets – dog tokens, “safe” this-or-that tokens, tokens with transfer taxes – with enormous slippage and finding themselves unable to trade out and/or frustrated that they have to pay for failed transactions. These are users completely new to DeFi buying a meme token on Uniswap, then failing to be able to sell it despite paying a huge gas fee, and their impression at the end of the day is that they have been robbed and that Uniswap is the thief.\nIt would be painful to see people having similar first-time experiences with the Compound protocol.\nAssuming crypto market caps take off again within the next several years, permissionless listing for isolated lending/borrowing markets will bring the next generation of meme tokens, projects with tax-on-transfer and/or Ponzi-like tokenomics. Those projects will permissionlessly list lending markets for their tokens and announce them as “partnerships” with Compound to lend credibility, bringing novice users to the protocol. With lower liquidity and easier-to-attack-oracles, these markets are likely to be very high-risk for novice users, leaving most of them with fewer assets and a sour taste in their mouth about Compound and DeFi in general.\nLong term, I love the idea of permissionless listing. How can Compound do it in a way that disincentivizes opportunistic money-grab projects and proactively educates users about the higher market and oracle risks of lending/borrowing long-tail assets? Or do folks feel like such efforts would amount to censorship?\\nGreat feedback, I really appreciate it!\nI am very supportive of the free market. One of the things that I love most about DeFi is how there are no governments or regulators here to “protect” users or to fix markets.\nLike here in Canada and the US, regulators “protect” investors by severely limiting access to investment opportunities because of risk. In doing so, these regulators severely limit the average person’s ability to learn how to significantly grow their wealth. In particular, accredited investors get access to MANY more and better deals, but one has to first make $1M to get that accredited status.\nProtecting users from anything which could hurt them generally hurts them more because they’re denied the opportunity to learn and grow.\nBut yes, Compound does have a reputation to maintain and we don’t want users to get scammed or lose a lot of money. It also goes without saying that protocol security is of utmost importance.\nTherefore, it’s our responsibility to educate users on the risks and to develop a UI/UX which aids the user in this regard as much as possible.\\n\nInfrastructure: Price Oracle\nA really basic overview of the price oracle to be used in these isolated markets are as follows.\n\nIf the price exists in Compound’s Chainlink powered UniswapAnchoredView, use that price. Otherwise,\nUse a TWAP of the price returned from a DEX aggregator.\n\nWe first try to use the price reported by Compound’s current price oracle as this feed contains the most accurate, secure, and reliable prices.\nSince Chainlink only supports a limited number of assets and we want to support any asset, we must get the price from a different source if our current price feed doesn’t support the asset in question.\nThe best on-chain source for prices are DEXs, but which DEX is best? Well, we can’t rely on any single DEX for prices because liquidity is fragmented across DeFi. The exchange with the most liquidity usually has the most reliable price. Hence the need to use a DEX aggregator.\n\n1inch as Price Oracle\n1inch is arguably the best 100% on-chain DEX aggregator, so we will use their contract/code.\nThe source code is available here: https://github.com/1inch/1inchProtocol 1.\nNote: 1inchProtocol was recently deprecated, so we’ll have to coordinate with the 1inch team to maintain it. We could also fork it and maintain it ourselves.\nHere’s an example of using the contract to get the price of GRT:\n\nimage576×586 31.9 KB\n\nReported is a price of 0.589060 USDC/GRT.\nIs it as easy as this to get the price of any asset? Sadly, no. It’s possible to manipulate this price through one of the supported DEXs which has low liquidity.\nSolutions to this problem?\n\nOkay solution: Alter the amount. Maybe 1% of the max supply of the token? 10%? The amount contained in the isolated market? Some combination of these approaches?\nBetter solution: fork the 1inch contracts to calculate a volume/liquidity weighted price. Impose minimum liquidity constraints to protect us from market manipulation. Remove the swap functionality of these contracts as we don’t need them - reducing overhead.\n\n\nConclusion\nThat’s the end of my basic overview of the proposed price oracle for isolated market supporting permissionless listing.\nPlease let me know your thoughts!\\n\nUpdate\nI’ve decided to build a new price oracle as there are no existing fully on-chain manipulation resistant reliable price oracles which aggregate multiple DEXs for the purpose of lending and borrowing.\nHaving a solid fully on-chain price oracle will be the hardest aspect of this proposal. It must be fully on-chain to support permissionless listing. The downside is that a lot of liquidity of various tokens are on CEXs, which makes on-chain prices easier to manipulate.\nTo combat price manipulation, I’m using the following strategies:\n\nAggregating DEXs\nWeighting the price from each DEX by the total liquidity of the base token (ETH or stablecoin) and possibly other factors\nUsing sliding window oracles\nUsing a large observation period (8 hours or more)\nUsing a somewhat small period granularity (15 minutes)\nChoosing the best algorithm(s) to calculate [stable] price and liquidity (for weighting)\n\nOne of the easiest ways to manipulate an aggregated DEX price oracle is to create a new liquidity pool on one of the smaller DEXs (assuming one doesn’t exist) and setting the token price really high (or low). Arbitrage bots and regular traders would probably be slow to realize this discrepancy making it less costly to pull off such an attack.\nTo combat such an attack:\n\nUse large observation windows\nUse algorithms that give more weighting to DEXs/pools which have had large amounts of liquidity earlier on\n\nThe oracle solution I’m building is/will be very abstract and flexible so as to support many different ways to calculate prices.\nMore discussion of algorithms and parameters will follow.\nCheers to DeFi Summer 2.0!\\nso what you want to do is create an entire protocol which would involve people actively picking and optimizing dex data sources, weighting, and methodology, continuously and individually for each coin?\ni think take a step back and look at what ur proposing.\nat that point just start a new oracle protocol.\nthe focus should be on making it easier to list tokens, not creating an imaginary obstacle course to play mental gymnastics with. adding 300 layers of voting doesn’t make the token listing feel any more permissionless.\\nStarting a new oracle protocol isn’t out of question (but this is off topic).\nThe process to get new tokens listed is complex because currently every new asset added adds risk to the entire protocol.\nWith isolated markets, the risks are a lot more contained. They have to be for permissionless listing so that anyone can list a new market without having to go through the governance process; without putting the entire Compound protocol at risk.\n300 layers of voting? No, no, no. Anyone will be able to create a new market at their own discretion. This is why we need a really strong price oracle that is resistant to manipulation.\nWe won’t need to optimize the parameters for each different coin. We just need one optimal set of parameters for the purpose of lending and borrowing at Compound. Then add DEX configurations here and there (or not). The more DEXs we aggregate, the more reliable and accurate the prices are.\\n\nOracle Update\nI’ve written the majority of the new oracle infrastructure code. Below are some results.\nToken = ETH\nBase = USDC\nUniswapV2 Price = 1785242344 , Token Liquidity = 66009898700084902688646 , Base Liquidity = 117843666286804\nUniswapV3 Price = 1785275963 , Token Liquidity = 84902713181993030425960 , Base Liquidity = 46104943013566\nSushiswap Price = 1785850053 , Token Liquidity = 104407714235399694110022 , Base Liquidity = 186456522071834\nAggregate Price = 1785570139 , Token Liquidity = 255320326117477627224628 , Base Liquidity = 350405131372204\n\nThe aggregated price is weighted by the liquidity of the base token as the base token will usually be the strongest asset and hardest to manipulate.\nAn aggregated oracle may seem trivial when used for large-cap tokens. but it’ll be very beneficial for small-cap tokens as they are less costly to manipulate. For example, on UniswapV2, $100k can cause ETH price to move by ~0.38% whereas for BIRD it’s ~12.9%. We can’t assume arbitrage bots/people are actively trading all tokens, so it’s possible for an attacker to manipulate the price on one DEX long enough to liquidate users and make a profit.\nLet’s consider the above results in terms of relative cost for manipulation.UniswapV2 has 117843666286804 in base (USDC) liquidity (117,843,666.28 USDC), whereas the aggregate totals 350405131372204 (350,405,131.37 USDC), making the aggregate price about 3x more costly to manipulate. The cost to manipulate the aggregate price goes up with the more liquidity pools (DEXs) we aggregate.\nAn aggregated oracle is essential for permissionless listing of lending/borrowing markets. The oracle must be very resistant to price manipulation to protect users’ funds which this new oracle system is designed to be.\nMore updates to follow as I get closer to completing the aggregated oracle.\nSide note: Compound’s price anchor to UniswapV2 could be upgraded (in time) to use an aggregate, further strengthening the existing protocol.\\ncheering you on here. Keep. It. Up.\\n\n  \n\n      github.com/compound-finance/compound-protocol\n  \n\n  \n    \n  \n    \n  \n\n  \n    \n      Add cDAIDelegate and DSR interest rate model\n    \n\n    \n      compound-finance:master ← compound-finance:compound/2.31-rc1a\n    \n\n    \n      \n        opened \n        \n          \n        \n        Dec 19, 2019\n      \n      \n\n      \n        \n          \n          jflatow\n        \n      \n\n      \n        \n          +11815\n          -5435\n        \n      \n    \n  \n\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nSo Kashi on Sushi has “strategies” but out own @jared worked on this bomb IRM and cDAI delegate contract that swept into the DSR, lets not toss this, rather we shall cherish the good times we have had and look into the future that we shall all seize, hand in hand.\\n@TylerEther,\nIs it possible to use data from 3 different protocols (uniswap, sushi, 1inch) for “t” time. Then, rank the protocols from most changed in price to least change in price. Give the protocol with the least change in price 60% weight, while the other two get 30% and 10% respectively. This would make it very difficult to manipulate price since it would have to be done on all 3 markets.\nOn tokens with very small liquidity, you could add the previous oracle price into the mix of the weighting system. If you gave it 50% all would be needed would be to figure out the above weighting 60/30/10 and then average that price to the previous oracle price.\nJust a couple of ideas… I think we should also attack this from a gas perspective as well, to minimize any unwanted gas fees.\\nI’m not sure how weighting it this way would play out as crypto assets are very volatile.\nI took a break from working on this to work on other Compound proposals… seeing as this idea has been getting more attention and support as well as being near completion of the other proposals, I’ll put more focus on this idea again. \\n\n\n\n Tyler Loewen:\n\nI’m not sure how weighting it this way would play out as crypto assets are very volatile.\n\n\nUsing 3 different protocols/markets to help with the risks involved with volatility/manipulation is a good way to combat the risk, especially as you mentioned:\n\n\n\n Tyler Loewen:\n\nTo combat such an attack:\n\nUse large observation windows\nUse algorithms that give more weighting to DEXs/pools which have had large amounts of liquidity earlier on\n\n\n\nHowever, if you could place emphasis – more weighting – on the protocol that has the least price change would make a malicious actor have to manipulate all 3 markets.\nExample:\nDuring the DAI Liquidation Event… If Compound was getting price data from 3 sources. And adjusted weighting system, where:\nPrice = (Latest Price * 0.25) + (((P0 * 0.6) + (P1 * 0.3) + (P2 * 0.1)) * 0.75)\nThe latest price of DAI = $1.0005.\nCoinbase:  DAI = $1.30\nUniswap: DAI = $1.005\n1inch : DAI = $1.05\nPrice = ($1.005 * 0.25) + ((($1.005 * 0.6) + ($1.05 * 0.3) + ($1.30 * 0.1) * 0.75)\nP = $0.25125 + ($0.603 + $0.315 + $0.13) * 0.75\nP = $0.25125 + ($1.048) * 0.75\nP = $0.25125 + $0.786\nP = $1.03725\nDoing the weighting like this would make it to where a malicious actor would have to move all 3 markets. You could even drop the algorithms (less gas) you were planning on using to weight DEXs by liquidity. Even if the bad actor was able to move the largest AMM pool’s price significantly, it would have less than a 10% weight factor.\nHope I explained it right, just want to simplify the oracle while maintaining the lowest risk possible and maybe even saving a lil gas.\\nInteresting idea, would be great for pegged/stable coins which are expected to not change much.\nAs for other tokens whose prices fluctuate a lot, I’m not sure if this would work well.\nHere’s an attack scenario for this idea. We aggregate Uniswap V2, V3, and Sushiswap. There’s some small-cap token, $XYZ, which is only listed on Uniswap V2. An attacker creates a pool on Uniswap V3 with only $1 worth of assets and initializes it at a very low price. Arbitrageurs won’t trade in this pool as gas costs are higher than the value extracted, keeping the low initial price. An aggregated oracle would then give the highest weighting to this data source - as the price doesn’t move - resulting in a price much lower than the market price. The attacker is then able to liquidate loans where $XYZ is the collateral.\nIf this attack scenario were to use the base token (ETH) liquidity strategy, the low amount of tokens used to create the pool would have almost zero effect on the aggregated price (assuming Uniswap V2 has much more liquidity).\nKeep the ideas coming! Every challenge to ideas proposed here will make the final product more resilient. \\nHere’s an overview of what I’ve designed so far.\n\nimage664×809 10.7 KB\n\nWe have 3 data sources: Uniswap V2, Uniswap V3, and Sushiswap. We create 3 different sliding window oracles - one for each data source - and they all use time-weighted average algorithms to calculate price and liquidity.\nThen we have our aggregated oracle which takes the market data (price and liquidity) from the 3 underlying oracles to compute the final price - weighing by the liquidity of the base token (ETH).\nHere’s a scenario - getting the price of XYZ, denominated by ETH (XYZ/ETH pools).\nUniswap V2: price(XYZ) = 1 ETH, liquidity(ETH) = 100\nUniswap V3: price(XYZ) = 0.01 ETH, liquidity(ETH) = 0.0001 ETH\nSushiswap: price(XYZ) = 0.95 ETH, liquidity(ETH) = 8\n\ntotalBaseLiquidity = 108.0001\naggregatedPrice(XYZ) = 1*(100/108.0001) + 0.01*(0.0001/108.0001) + 0.95*(8/108.0001) = 0.996295383\n\nWithout the innaccurate price from Uniswap V3:\naggregatedPrice(XYZ) = 1*(100/108) + 0.95*(8/108) = 0.996296296\n\nThe presence of the attacker’s Uniswap V3 pool had an impact of 0.000000913 ETH on the final price. \nI argue that if gas and protocol fees were 0, arbitrageurs would correct the price discrepancy between Uniswap V2 and Sushiswap to around the price reported by the aggregated oracle (all other things held constant) as the price impact of trades is determined by the liquidity of the pool. I’m too lazy to mathematically prove this point.\nGoing back to the design of this system, the underlying oracles have been designed to be immutable so that they can be used by other protocols without the concern of anything changing. The more other DeFi protocols use these immutable oracles, the less gas consumed by the ecosystem as a whole, benefiting everyone, and lessening the oracle maintainer responsibilities of individual protocols.\nI’ll publish the aggregated oracle system on Github tomorrow. \\n\n\n\n Tyler Loewen:\n\nThere’s some small-cap token, $XYZ, which is only listed on Uniswap V2. An attacker creates a pool on Uniswap V3 with only $1 worth of assets and initializes it at a very low price. Arbitrageurs won’t trade in this pool as gas costs are higher than the value extracted, keeping the low initial price. An aggregated oracle would then give the highest weighting to this data source - as the price doesn’t move - resulting in a price much lower than the market price.\n\n\nOh, I didn’t clarify. The more weighting would go to the protocol/pool that had the least price change from the previous oracle price, not the actual protocol’s change in price.\nPrevious oracle price in the above example was $1.0005 which gave the Uniswap price of $1.005 more weight since it represents the least change. This should work on any asset, really. If the asset is highly volatile across all pools then it would still be weighted 25% from the previous oracle price.\\nI’m a bit confused because the oracle’s price reflects the protocol’s price (with time delay).\nMy previous argument still holds. An attacker can create a pool with an invalid price and liquidity so small that users won’t swap using it, keeping the same invalid price that has the heaviest weighting as the price doesn’t change.\\nI’ve published the oracle code to Github: Pythia (Oracle of Ethereum) 2.\nNote: the contracts have not been optimized for gas usage yet.\\n\nUpdate on DEX aggregated oracle\n\nThe GitHub repository has been moved: https://github.com/pythia-oracle/pythia-library 3.\nI’ve identified a way to manipulate the oracle’s prices: adding a large amount of liquidity to a pool with an inaccurate price, updating the oracle, then removing the liquidity all in one transaction. To remedy this attack vector, we need resilient liquidity feeds. As UniswapV2 and Sushiswap don’t have liquidity accumulators/oracles, I’ve begun creating my own.\nThe aggregated oracle now uses the weighted harmonic mean rather than the weighted arithmetic mean as the harmonic mean is a better reflection of the asset’s true price.\nI’ve created a GitHub organization and an npm repository organization that will host this project’s code.\n\nOther than that, I’ve begun exploring research on the topic of asset pricing in markets with volatile liquidity. This topic is very important for people wanting to borrow against illiquid assets.\nI’ve also narrowed down the mission of Pythia: to provide the most reliable, accurate, and trustless fully on-chain market data feeds for crypto assets."
  },
  {
    "number_of_comments": 20,
    "postid": "75fc652a-492f-457d-93c5-e05d8f9181ac",
    "posturl": "https://www.comp.xyz/t/augur-migration-proposal/58",
    "combinedcontent": "Proposal for Augur v2 Upgrade Process for Compound:\nProposal Part 1:\nAround V2 launch:\n    - Prohibit new supply and borrow of rep v1\n\nProposal Part 2:\n10 weeks prior to earliest fork date:\n    - Set REPv1 collateral factor to 0% (going down 10% every 2 weeks until it’s at 0, e.g. 40-30-20-10-0). Encourage people with v1 to convert to v2.\n    - If any REPv1 loans are left, use a high rate to liquidate the position, e.g. 10% interest a day or something\n    - Assuming some exchanges have listed it and/or dexes have v2 with sufficient liquidity, add v2 to compound with same collateral rules as v1 with a new cREP contract\n\n2 weeks prior to potential fork date:\n    - Upgrade any REPv1 in the reserve to REPv2\n\nThe thinking behind this is we can keep it fairly simple (to just two proposals). When v2 comes out it no longer makes sense to be able to do new supplies or borrowing of rep, everyone else should be given time to migrate. There are four months until a fork can potentially happen after REPv2 launches (July 28), so there’s time. After the first proposal is enacted and it’s been a few weeks to see how v2 plays out, I then propose a part 2 which reduces how much people can borrow against rep over time (so as to not cause a bunch of cascading liquidations). Assuming there’s oracles with price feeds for v2 at this point and some of the major exchanges have migrated over, I think it then makes sense to in parallel add v2 with the same rules as v1 to encourage people to easily migrate over from v1 to v2 rep and then resupply or borrow from compound.\nSince REP has the ability to fork or split into two assets (rare, has never happened and should never happen but is good to prepare), we should go ahead and address that too. The cREPv2 contract could allow compound governance to vote to migrate REP in the event of a fork. But I don’t think it should for a couple reasons: when a fork happens there’s different REP tokens (different contract addresses too) for each universe of the fork, so to migrate, COMP governance would need to choose the winning fork (before it is known what the winning one is), migrate the cREP contract to it, and hope that exchanges list it immediately post fork (otherwise there’s no price feed to go off of for Compound).\nInstead what makes more sense is if a fork did happen in v2, there’s always at least 60 days of warning, so we could follow a similar process to the one above: prohibit new supply and borrow, gradually lower the collateral factor, liquidate existing positions if no one pays them back by the end of the 60 days, and upgrade any remaining REP in the reserve (or just convert it to dai or something to be even easier).\nNote: continuing the convo from the prior forum here, borrowed a couple ideas from there re collateral factor etc!\nEdit: forgot to add fork date info! After launch it takes 8 weeks for a market to potentially trigger a fork at the earliest, and then 60 days for a fork to actually occur.\\nmy concern is that maybe we should do this faster cause when v2 launches the liquidity of REP(v1) will decreases significantly as users start to migrate and liquidating or repaying a position might become difficult.\\nJoey, this makes a lot of sense - thanks for taking the time to organize the process.\n\nSince Compound Governance takes 5 days, a proposal to pause new supply/borrowing of REPv1 should occur soon\nDo you think that the community could deploy a “vanilla” REPv2 cToken?\nCould the community update the price feed to reference REPv2 once is begins/trading? Is there any clear difference in value between the different token addresses, or anything to pay attention to?\n\\nPoint above about liquidating v1 rep borrows makes sense. Going to edit the proposal slightly to mitigate that issue.\n\n\nSince Compound Governance takes 5 days, a proposal to pause new supply/borrowing of REPv1 should occur soon\n\n\nMakes sense\n\n\nDo you think that the community could deploy a “vanilla” REPv2 cToken?\n\n\nYes I think so\n\n\nCould the community update the price feed to reference REPv2 once is begins/trading? Is there any clear difference in value between the different token addresses, or anything to pay attention to?\n\n\nYep! The main thing to pay attention to is forking, which is basically the same as the process I outlined above so it’s not anything additional on top of that (basically, the community in theory may have to go through this process again).\\n++ on the proposal and comments, adding some dates:\nTwo relevant periods here, the slow dispute process (8 weeks) and the forking period (60 days). They work out to roughly equal timeframes, however the slow dispute process runs in weekly fee window intervals and the forking period is defined by a fixed number of days. Augur v2 introduces fast disputing where someone can post a bond to accelerate the dispute process up to 8 weeks before a fork, then Augur turns on the slow weekly dispute process for the remainder of the time until the forking period begins.\nAssuming a successful deployment of Augur v2 on July 28th, 2020, in theory someone could create a market with a near immediate resolution date, post a sufficient REP bond meeting the requirements of the slow dispute process and trigger it to begin in the upcoming weekly fee window.\nThe cadence of the weekly fee window is inherited from Augur v1, so the earliest date in which the slow 8-week dispute process can begin is July 30th, 2020 at 00:00 UTC. This would then result in the earliest possible forking period beginning on September 24th, 2020 (+8 weeks), concluding on November 23rd, 2020 (+60 days).\nEarliest possible event dates:\n\n\nSlow Dispute Start Date: July 30th, 2020\n\nFork Start Date: September 24th, 2020\n\nFork Conclusion Date: November 23rd, 2020\n\nSo to Joeys proposal:\n\n\n10 weeks prior to earliest fork date: September 14th, 2020\n\n2 weeks prior to earliest fork date: November 9th, 2020\n\\nI’d like to see borrows/supplies for cREP paused ASAP because of potential major liquidity changes that will occur when Augur 2 comes out. Additionally, I think we should deploy a cREPv2 as soon as market liquidity of the new asset is high enough. I don’t know if it can use vanilla ERC-20; haven’t looked into the code yet.\\nYeah it’s a (spec compatible Zeppelin lib) erc20 token\\nAdditionally when disabling supply/borrow, we could remove REP from the COMP rewards and set reserve factor to 100%\nWhat do you think ?\\nBTW, the relevant Comptroller functions to do  - Prohibit new supply and borrow of rep v1 are:\n_setMintPaused(cREP, true)\n\nand\n_setBorrowPaused(cREP, true)\n\\nThat sounds fine / makes sense to me re the reserve factor / COMP. Doesn’t let me edit OP anymore but would do it as part of proposal part 1 imo\\nSooner this process gets started the better. Theoretically, if a lot of REP is migrated to v2 soon after launch, it could lead to liquidity pressure for REPv1 which may make it tougher to deleverage (the migration is only 1 way as far as I know). Luckily there’s not much REP borrow outstanding.\\nthats just one side you see, cause there is like 6M$~ stablecoins that are borrowed with REP collateral,\nthat is also my only “fear” that the liquidity of REP (v1) will be significantly lower when users will start migrate.\\nFirst proposal is live : https://compound.finance/governance/proposals/17 23\\n@blck I believe it is time for a 10% collateral factor drop. DEX Rep liquidity is getting shallow.\\nJust wanted to bump this thread, REP migration has been going on for several months and we should consider reducing the collateral factor to encourage migration to REP v2.\\nDefinitely, good call @monet-supply\\nAugur is now safe to completely deprecate; cREP has shrunk to less than $150k, and is no longer an important collateral market.\nThere are no practical levers to force repayment of the remaining REPv1 borrows; but disabling the asset as collateral is a logical next step.\\n\n\n\n rleshner:\n\nThere are no practical levers to force repayment of the remaining REPv1 borrows; but disabling the asset as collateral is a logical next step.\n\n\nWe can always adjust the interest rate model if we want to force borrowers to close out \nEg. make the interest rate 100% APR regardless of utilization.\\nHi @joeykrug @rleshner, is there any plan to migrate existing $43K worth REP V1 legacy token to the V2 token given the V2 fork seems to be stable and successful right now?\nI noticed @joeykrug mentioned in his initial proposal\n\n\nUpgrade any REPv1 in the reserve to REPv2\n\n\nso I am curious about the timeline to execute that step.\nThe reason I am asking is personally I still have some REP V1 token in the pool and the ETH GAS fee is discounting me to withdraw and migrate on my own, which I believe some of the LPs in this market are keeping their small but still meaningful tokens in Compound.\nThanks ahead for your attention and response!\\n@leo there is no ability for the protocol to migrate V1 to V2 automatically; you’ll need to withdraw your tokens and migrate them on your own–ideally when gas prices are inexpensive.\\nThanks for your reply Robert! I was thinking the Compound contract owns the underlying REV V1 token so technically it might be possible to perform a migration function call within the Compound contract. Though I am not savvy on the Compound code implementation so I take your words for the feasibility.\nI’ll go ahead to plan that withdraw & migration on my own. Thanks again and stay safe!"
  },
  {
    "number_of_comments": 28,
    "postid": "924b1d7e-71be-4521-97ed-855dbaeafc33",
    "posturl": "https://www.comp.xyz/t/compound-proposal-022-systematically-reduce-emission-quantity/241",
    "combinedcontent": "Background\nAs discussed in @monet-supply’s post Adjusting COMP distributed model 87, it is clear that there is quite a bit of optimization that can be done to the COMP distribution model. We (Gauntlet) believe that the network should begin to increase the incentive for long-term holding of COMP relative to short-term holding and trading by yield farmers. While the introduction of COMP led to the ‘agricultural revolution’ and the usage of liquidity mining as a growth tool, it has become clear from on-chain and off-chain data that holding times, voter participation, and ‘real’ borrowing activity can be improved by distribution tweaks.\nBy distributing COMP to borrowers, the protocol encourages recursive leverage, which both brings little value and reduces security of the protocol. Based on the survey in that post, it is clear that at the very least, the COMP emission system needs to three control mechanisms:\n\nAdjusting COMP emission rate\nScalar multiplier to allow for more uneven distribution between borrowers and suppliers\nTime-locking mechanism\n\nProposal CP011 demonstrated that a reduction in the emission rate (e.g. via a call to _setCompRate) can be a useful tool for alleviating both sell pressure on COMP and as a way to encourage long-term COMP holdings.\nHowever, as DeFi has become increasingly interconnected via farming and COMP has been utilized as a farming asset in a number of protocols (e.g. YAM), we believe that these changes should be made in a cautious, security-forward, and incremental manner. For instance, if the COMP emission was reduced 90% and subsequently, a large yield farming incentive led to COMP holders putting large fractions of the supply in a third-party contract, then we can expect both a supply-side crisis and an increase in recursive leverage.\nTo handle the fact that any change to the COMP distribution will ripple through the ecosystem, we propose a making changes via series of proposals. Each proposal will make an incremental update to the protocol which will be monitored for two weeks before the next incremental state is taken. This way, we can observe the market reaction (both in asset price, volatility, and usage in farming) and use the three control mechanisms to adjust the protocol. This will allow us to utilize an online learning approach 23 to quantitatively choose the parameters for each of these incremental adjustments.\nThe sequence of incremental proposals that we will use is the following:\n\nReduce emissions rate (no technical risk, yields estimates of market demand for COMP)\nAdjust the distribution to suppliers and lenders (minimal technical risk)\nAdd in a vesting period / time-lock (larger technical risk, would be the proposal with the largest risk so far)\n\nParameter Selection and Justification\nWe have constructed a Monte Carlo model (partially described in a working paper for optimizing emissions that can be found here 25) that shows that the current market demand for COMP in both centralized and decentralized exchange implies that COMP is being overproduced by 35-40%. However, this can also be illustrated more simply via some descriptive statistics and illustrations. Using tagged exchange addresses provided by Flipside’s Will Price, we looked at the both the distributions of holding times — how long an address held COMP issued via the comptroller before they sent it to a tagged exchange address (CEX and DEX) — as well as the amount of COMP that was held over time.\nFirst, lets look at what fraction of COMP wealth was kept by addresses that didn’t sell all of their holdings (e.g. amount farmed - amount sent to exchanges > 0)\n\nThis distribution is unimodal [0] with a median of roughly 56%, implying that 50% of COMP holders kept 44% of their COMP earnings (e.g. used in farming, hodl’d). However, include COMP farmers who sold all COMP they earned we have a much more skewed distribution:\n\nThis is strong evidence that large fraction of COMP holders are selling all of their earning and not staying long-term holders. This data shows that roughly 17.2% of COMP issued is completely sold on exchanges and not held by long-term holders.\nIf we look at the length of time that COMP farmers hold COMP on their balance sheet, we find that most short-sighted COMP farmers sold their assets via DEXs such as Uniswap and Balancer:\n\n(Note that the x-axis is the base-10 logarithm of the holding time)\nGiven these descriptive statistics and combined with some cursory simulation results, we believe that emissions should be reduced by 20%. Currently, emissions are 0.44 COMP / block and we believe that it should be lowered to 0.352 COMP/block. Once this change is made, Gauntlet will observe the changes to on-chain behavior and market data to estimate whether further reductions are needed.\nProposal Description\nWe will call _setCompRate on the uint input 0.176e18 to execute this reduction.\nPost-Proposal Analysis\nIf the proposal passes, we will monitor the following data sources for changes after the proposal is executed:\n\nCOMP trading volumes, order book liquidity, DEX liquidity\nUsage of COMP in other farming protocols (e.g. YAM, YFI, etc.)\nChanges to borrower / supplier statistics (e.g. avg. loan size, avg. borrow)\n\nWe will combine these with our simulation model to estimate how much excess supply and/or demand there is in the market post-change and use this to guide our next emission-related proposal.\nProposal Timeline\n\nThree (3) days for community feedback and analysis on this post\nProposal submission on August 26, 2020, 3pm UTC / 11am EST / 8am PST (proposal will incorporate feedback)\n\nAcknowledgments\nThanks to Hsien-Tang Kao, Rei Chiang, John Morrow, and Victor Xu from the Gauntlet team for feedback and analysis.\nFurthermore, thanks to Flipside Crypto and Will Price for providing tagged exchange data that was used for descriptive analysis and well as for simulation stress testing.\nFinally, thanks go out to Peteris Erins of Auditless for feedback and analysis.\n[0] The blue line is a plot of the probability density function of the best fit Beta distribution 8, which is a one-parameter family of unimodal distributions on [0, 1]. There are a number of reasons for comparing this distribution to a Beta distribution, which have to do with the generative processes that one can construct to sample a Beta that can represent the behaviors of the COMP holders in the system.\\n\n\n\n tarun:\n\nWe (Gauntlet) believe that the network should begin to increase the incentive for long-term holding of COMP relative to short-term holding and trading by yield farmers.\n\n\nTo achieve the stated goal, I wish to present 3 alternative proposals:\na). Put a vesting period of x month on $COMP (similar to $SNX rewards’ 12 month vesting).\nb). Put a vesting period on $COMP AND increase current period emission, which is to be balanced out by lower/decreased future period emission.\nc). Put a x-month vesting period on $COMP on y% of token farmed, and the rest of the $COMP are immediately transferable.\nI believe my alternative proposals can:\n1). decrease market selling pressure.\n2). Change the return estimation for $COMP farmers (e.g. bring discount factor into the equation), discouraging $COMP farmers whose only objective is selling $COMP on the open market.\nand specifically for b).\n3). Incentivizing long-term $COMP supporter/holders by increase their current period $COMP yield (bring discount factor into the equation. More yield now and less yield in the future is better than having the same yield in all periods).\nEdit:\nAlso, it is worth considering the potential downsides of our proposals:\ni). Potentially decreasing the stablescoins and tokens supplied in Compound ecosystem.\nii). Potentially decreasing the sell side liquidity of $COMP on both CEX and DEX.\\nThank for your the feedback! We note that the third control mechanism (which we termed ‘time-locking’) is equivalent to individual cToken vesting. Currently, the current cToken design does not have a mechanism for vesting. The main complications with adding vesting is purely technical implementation — it would cover a lot of surface area and making sure that we have a minimal surface area implementation that doesn’t significantly increase gas costs for users will take a bit of time. We’ve been working on an implementation of this and once we have this tested and audited, we will submit a proposal to add in vesting functionality.\nWe think option b) is actually the most palatable in the long-run for all stakeholders, but requires vesting to be audited and implemented.\nRegarding your latter two points about downsides: Order book signals and analytics suggest that there sufficient COMP liquidity, especially compared to other DeFi assets, as it has the most professional market makers trading it (thanks to the Coinbase listing). Only option c) would have a direct impact on market makers’ ability to tightly quote $COMP, at least based on simulations and backtests.\\nI feel this is misguided - it seems to be in response to the fact that the price of COMP is under-performing competing protocols because farmers are selling it.\nCOMP’s value is entirely determined by the discounted value of future cash flows that go to COMP holders.  The best way to increase COMP’s value is to credibly show that value will come to those holders.  This comes from increasing TVL and fees given to protocol owners.\nThis proposal wants to decrease the amount of COMP distributed for the very simple reason that it means that people won’t be able to sell as much.  This is merely attacking a symptom and not the actual problem.  Furthermore, this slows the progressive decentralization of the Compound protocol, when there is a clear trend of liquidity providers preferring to use platforms where they share in more of the economics and have more influence on the protocol’s future.\nA long-term COMP investor should be happy that farmers are indiscriminately selling COMP today, depressing the price, and granting investors who believe the protocol will generate significant value in the future a better entry price.  In fact, this creates a better, more fair distribution, as miners are not hoarding their COMP tokens and consolidating control over the network.\\nI agree with @oooh_fa that extending the decentralization process of Compound is far from ideal.\nWhat I’d be more interested in seeing is vesting and alternative methods of COMP distributions. Some that come to mind are: incentives for voting or staking COMP in voting contracts, rewarding community members who submit successful proposals, and maybe tipping in discord. An additional advantage of these other distribution routes is that the COMP will be given to users who understand and value its use as a governance token.\\nWe certainly agree, but until we have a good generic method for adding vesting and lock-ups for issued COMP, we believe the network will bleed COMP and the majority of it will end up held by centralized market makers and exchanges. Given that COMP, unlike other liquidity mined assets, doesn’t incentivize on-chain pools (e.g. akin to Synthetix’s rewards contract 6), it also appears that a lot of the centralized exchange volume is held by market makers and exchanges rather than by new participants. Our first priority is making proposals is increasing the number of long-term active governance participants.\nWe have been working on code for vesting COMP rewards to cToken and have the following sequence of proposals in mind:\n\nAdd vesting (4-6 weeks): This is a large change and we want to get it audited and do a testnet trial for the community to analyze\nAdd pay-to-address (1-2 weeks): Allows the protocol to pay individual community members for successful proposals in a governance-transparent manner\nAdding payments to governance participants (2-3 weeks): This will utilize the vesting code\n\nOnce vesting is in-place, we believe that emissions should probably be higher as @odette has suggested and we will provide numerical evidence for how to choose how much higher it should be. We think that @oooh_fa’s point is true for COMP in a world where COMP is on the same footing as the other liquidity mined tokens. However, as COMP has much more exposure to centralized exchanges and market makers, it is becoming increasingly clear that tokens incentivizing on-chain liquidity are doing a better job of distributing their tokens to a large number of people. We believe that once vesting is in-place, the Compound protocol can similarly incentivize on-chain liquidity in a manner that yields more long-term governance participation.\\nIt sounds like there’s agreement that adding a vesting period is preferred vs. reducing per-block COMP distribution.\nI would argue that there is not a pressing / urgent need to reduce COMP distribution, and so I would encourage Gauntlet to skip this unnecessary intermediate step and focus on the long-term step of introducing a vesting period.\\nI’m not sure if the goal of reducing farming and recursive leverage is in the best interest of COMP holders. I think these are the current effects of farming:\n\nMassively inflated TVL\nGreat supply APY on DAI due to high utilization\nReserves filling at increased rates\n\nThe key driver behind all this (and the entire farming craze) is that we’re dealing with an irrational market. The key driver of the current market price is hype/good news and a limited supply. Reducing emission would reduce selling pressure by rational actors (as intended by this proposal) and therefore an increased price. The increased price will raise the farming APY, thereby possibly negating the effect of the reduced emission.\nI think those who hold COMP because they farmed it or paid for it have no interest in stopping farming as that is what makes Compound look more valuable with high TVL. Vesting COMP could get rid of farming, massively dropping TVL, causing the hype to falter, causing a large drop in COMP price.\nAs a side-note, I doubt the ‘open’ community feels the voting rights are useful considering all the real voting power is in the hands of a select group of early partners, etc. At the current rate it will be years before there is enough COMP out there to speak of community. So at the moment COMP is just another speculative asset with no current intrinsic value. So price and price expectation are the main motivators in the market.\\nIf the COMP Distribution rate (to users) is lowered, an increasing amount of COMP becomes available for other purposes; rewarding/incentivizing governance, community development, and new use-cases decided by the community.\nIn tandem with modifying the distribution of COMP, the community should actively discuss additional ways to distribute COMP.\\nI agree with @brendan_dharma that the community seems to prefer the introduction of vesting schedule vs. reduction of COMP distribution. Although modifying COMP distribution can also happen in tandem of additional ways for COMP distribution, including to third parties, which might be good for the future of compound. Some concrete examples:\n\nSetting aside x% of remaining COMP for a bug bounty program that will focus on the long-term sustainability of compound protocol\nIncentivize integration of compound to smart contract wallets or even financial institutions (i.e. CeFi actors)\n\nHowever, I acknowledge that such proposals might be contentious for the community if not done carefully1 3\\nYes, I probably should have been more clear about this. Our goals with the reduction are really two fold:\n\nTo allocate a portion of the COMP emissions for usage in the protocol later (e.g. bug bounties, audits, payment to developers)\nTo incentivize long-term holding of COMP\n\nIn order for the former to be viable, we need to add in vesting (see: Vesting for the Compound protocol 9). However, we believe that lower emissions now and then adding vesting is the correct ordering for the following reasons:\n\nBuilds a larger reserve for bug bounties, contribution/security, incentivized voting participation, and to protocols that build on top of Compound\nWill lead to a lessened concentration of COMP on centralized exchanges and increased holding times, especially as farming adds perverse incentives for long-term holders\n\\nReposting what I wrote in the Discord here:\nI’m pretty unconvinced on the proposal to reduce COMP emissions. If the goal is “building up a reserve of COMP that can be used for paying for other community needs” it seems like it’s a very roundabout way to get to it. Why not just create a proposal to re-allocate a portion of undistributed COMP? Or why not use the COMP already allocated to that purpose? Thinking of “775,000 COMP are reserved for the community to advance governance through other means — which will be announced at a future date”\nThe secondary goal is to stop people from farming + dumping COMP – I’m all for that but don’t see how reducing emissions will help with that.\nFinally, we’ve already reduced emissions since COMP launched. Reducing again seems to actually be going against the current evidence in the market that faster emission rates lead to both broader and deeper governance participation.\\nThe $COMP sold will be lower, by 20% and I don’t see how reducing COMP emissions would help stop from farming + dumping either.\n@tarun  With COMP emissions, you are trying to privatize system’s shares to users. A similar system of giving shares to workers happened back when Soviet Union collapsed and privatized government owned factories to workers who promptly sold it for a dinner and a bottle of vodka to a few folks hoarding them who are now known as oligarchs.\nWith COMP a basic question remains - why would anyone keep COMP ? Answer this and you will solve your problem.\nAs I see it now, the current answer is to govern the protocol by an elite few and the majority is on for a ride, for whom it wont matter anyway as we cannot get enough of it to voice our point of view.\nAnd this emission reduction is a prime example of that. Someone with a lot more shares will do as they please, always, imposing their will on the majority ( With all due respect to the Monte Carlo simulation from above).\nAnd that’s life.\\nGauntlet’s ill-considered proposal seems like nothing more than a thinly veiled attempt to cause a short term pump in COMP prices. Gauntlet fails to demonstrate how reducing yield benefits the users of the protocol in the long term, indeed, they fail to demonstrate why yield selling is actually a problem.\\nGauntlet’s proposal is not even worth consideration without a clarity on how that COMP would be spent and the benefits it would provide to the platform and its users. Further, such extreme volatility of COMP yields at the whims of preminers will destabilize trust in the platform.\\nTo preserve control in the hands of users, a system must restrict the commodification of that control. This is why most (if not all) democracies forbid the outright buying and selling of votes.\nA better system for true user control of Compound would be a hybrid. For example, say 50% of total voting weight is distributed among active users proportional to their current stake in the platform (the value lent/borrowed) and it is not tokenized (cannot be taken off platform, bought or sold). The remaining 50% is commodified as COMP governance tokens that can be bought and sold on the open markets.\\n@RacerX - an interesting point. Potentially $MKR / $DAI type of relationship would work better. Separating voting coin (shares) from common coin (shares).\nIn the current stock market the “owners” are the voting shareholders (of course much less hands on, however still with some voting power) And then the incentive is to either a) hold the coin (share) to leverage appreciating value or to receive the dividend or both, seems to work out okay …\\nAt first i’m largerly unconvinced that Community opinion actually matters, as Compound Governance is just pretend to be decentralised, while in reality it’s heavily centralised with like 20-30 entities can pass (or block any decision). So only their opinion actually do any difference and everything else can be just discarded as irrelevant noise. It’s worth noting, that pretty much none of controlling entities actually got their voting power from “user distribution”.\nThe argument that it’s in transition of transferring governance to users, while theoretically good quite contradicts with current proposal. Initial distribution time of COMP tokens is about 4 years to fully distribute. Which is a VERY LONG time for crypto.  And yet, less than 2 month from start here is talking that it is too fast. So, here we come to point\n1 : If COMP distribution a process of transferring Compound protocol to decentralised governance than we should abstract from current price valuation, which is just temporary noise. In that perspective, decreasing emission is just a way to preserve current status-quo, delay transition to community governance and hold voting power by initial voters, who probably should vote yes. (and i encourage community to pay close attention who will vote yes to that)\n\n\nProposal title suggest decreasing emission by 20%, while proposal itself suggest to decrease from current 0.44 COMP/block to 0.176 COMP/block, which is more like 60%, isnt it? How to understand that?\n\n\nI want to remind everybody, that current price valuation of COMP is fueled by yeild farming hype and big bubble. It’s hardly real and hardly sustainable long term, regardless of what would be done with emission. At the same time COMP and Compound itself is one of the few actual products with long-standing and proven value , sitting at the core of it. Extreme care should be taken, as any radical change can be the pin which blow that bubble.\n\n\nI don’t really getting argument about centralised exchanges having big portion of COMP. Centralised exchanges are not the owners, but custodians of the COMP, which is likely in hands of many small owners, who (for reason of point 1) can basically benefit from COMP holding only by price speculation, as votes of single-double-triple digit COMP owners are largely irrelevant in decision making process (and holding COMP isn’t incentivesed in any other way either) , not even talking about those who have less than 1 COMP. Besides, do we know that holdings of centralised exchanges are from COMP distribution and not from initial seeded Uniswap liquidity leaked to that exchanges overtime?\n\n\nTo resume, i don’t really see what decreasing emission is trying to acihieve aside of delay of decentralisation process. Maybe attempt of price manipulation? Or attempt of blowing down all that house of cards defi constructed around Compound’s idea of distribution of governce token to users.\nHowever, all of that being said, there is definitely some logic behind saying that might be there should be some improvement in distribution model.\\nNice write-up and insights.\nI don’t fully understand what the connection is between reducing emissions and increasing long-term hodling. I get that there will be funds for other things, but I don’t think the items mentioned will necessarily help.\nOne of the biggest reasons there are few long-term holders of COMP is because the (perceived) value of governance today is much less than the market value of the token. Why would I keep COMP knowing that governance is heavily centralized and my vote won’t count? Look at projects like yEarn, etc. No one is selling FYI (quite the opposite) because the expected value of governance (and potentially future cash flows) is much higher than today’s market price.\nIMO increasing emissions + vesting would have been a much better solution. This way only people willing to lock up their rewards long term would provide liquidity.\nAnother way to increase long term hodling is giving COMP utility. Yam.finance showed, unsurprisingly, that when the token is useful for something, farmers/traders will want to own it. Enabling COMP as collateral like CREAM does or partnering with other protocols to support COMP in their projects are two of the many ways this could be accomplished.\\nAs a new participant in this ecosystem, it is surprising to see that there are quite a few thoughtful responses and counter arguments to the proposal that are left completely unanswered. And governance poll is already setup without first attempting to at least provide more clarification to these counter arguments.\nAs many others have asked, I would also be curious to understand why is it considered a problem that most of the earned COMP is sold immediately?\nOne of the points of COMP distribution, as I understand, is to increase awareness of the platform and increase its user base, integrations and overall usage. Even though most of the new lenders are joining to “wash borrow” to maximize their yield, they are still becoming familiar with the platform and building a relationship with it. They will most likely remain as lenders even in future, as long as over time there will be natural demand for borrowing. (hopefully everyone understands that APRs of >40-50% can’t be sustainable)\nThe fact that most of the COMP is sold right away seems an extra positive too, because this is potentially bringing other new users too, who believe in compounds future and want to own piece of it without becoming liquidity providers right now.\nOf course most important part is to continue to build things that will increase actual borrowing demand, but changing COMP distribution either way wont make a difference for it. COMP distribution is already set over a long enough time period. If actual borrow demand doesn’t increase to much higher levels in next 2-4 years, slower COMP distribution wont really change much.\\nA bit late here, but just wanted to share Polychain’s view on this. We voted in favor of this proposal. We see this as doing 3 things:\n\n\nReducing some incentive to recursively farm COMP which distorts lending markets & crowds out real borrowing/lending activity. Recursive farming is an unintended consequence of the current levels of COMP distribution, and it’s likely subtracting more value from the protocol than it’s adding.\n\n\nEase some of the COMP bleed (i.e. farming COMP and dumping on exchanges). We are convinced the data shows that we aren’t effectively distributing COMP to long-term holders who are adding real value to the protocol (as opposed to ‘wash borrowers’ & farmers who immediately sell out). Lowering COMP distribution certainly doesn’t solve this problem, but it helps for the time being.\n\n\nActs as a temporary fix that buys us time to implement bigger, longer-term changes into the protocol like vesting and a more permanent fix to how COMP is distributed. This is the first step because it’s a pretty simple code change.\n\n\nWe find this proposal to be a net-beneficial step on the path to a longer-term solution. Looking forward to seeing vesting/lockups, updated COMP distribution logic that removes the incentive to recursively farm, and other ways to distribute COMP that continue the process of decentralization.\\n\nIf COMP distribution a process of transferring Compound protocol to decentralised governance than we should abstract from current price valuation, which is just temporary noise. In that perspective, decreasing emission is just a way to preserve current status-quo, delay transition to community governance and hold voting power by initial voters, who probably should vote yes.\n\nThis is not strictly about the price and/or valuation but rather about the distribution of the token, especially as there is no vesting period. Without a vesting period, as say Proof of Stake networks have with epoch and withdrawal windows, most of the assets accrete to a small number of market makers and exchanges who have no interest in voting. The Nansen data directly illustrates this. Moreover, spending the entire budget of the protocol on liquidity provision also does not make sense. The protocol has to spend on more than liquidity — it needs to spend on security and adding new features to stay competitive in the evolving DeFi market.\n\nProposal title suggest decreasing emission by 20%, while proposal itself suggest to decrease from current 0.44 COMP/block to 0.176 COMP/block, which is more like 60%, isnt it? How to understand that?\n\nCurrently, the implementation of COMP distribution stores a value equal to half of the COMP issued (e.g. compRate is equal to 2.2e17 in the deployed Comptroller contract). The reason for this is to save gas. Currently, 0.44 COMP / block is distributed with 0.22 COMP / block to suppliers and 0.22 COMP / block to borrowers. Note that 0.176 = 80% of 0.22. The reason for this is that the COMP issuance is computed via two operations, totalSupplied * compRate and totalBorrows * compRate. If the contract stored compRate as 0.44, then we would have to compute totalSupplier * compRate / 2, incurring an extra division step.\n\nI want to remind everybody, that current price valuation of COMP is fueled by yeild farming hype and big bubble. It’s hardly real and hardly sustainable long term, regardless of what would be done with emission. At the same time COMP and Compound itself is one of the few actual products with long-standing and proven value , sitting at the core of it. Extreme care should be taken, as any radical change can be the pin which blow that bubble.\n\nCOMP, in many ways, started this bubble as an extension and modification of the pioneering work done by Synthetix. The Compound governance contracts have been used in a large number of yield farming protocols and the safety and convenience that they provide has been assessed by the developer market as a Schelling Point for governance. However, just as the release of COMP caused this bubble, it must also protect itself in the future. If all of the token is spent on liquidity with no vesting and/or lock-up, then the protocol will be bankrupt in the future in terms of a development budget, a security budget, and an insurance budget. The current farming craze has led to most COMP accruing to high-time preference, non-governance participants. While the current distribution isn’t ideal and protocol parameters (e.g. minimum amount of COMP to make a proposal) can be improved, one needs to remember that COMP was the first DeFi token to launch fully automated governance after product-market fit. And mistakes will be made if you are the first! One of the mistakes is the lack of vesting, which leads to the concentration that we see today. And the goal of reduction is to reduce the treasury and short-term behavior now, add in vesting and/or lock-ups, and then increase issuance again, albeit with longer-term lock-ups that encourage governance participation.\n\nI don’t really getting argument about centralised exchanges having big portion of COMP. Centralised exchanges are not the owners, but custodians of the COMP, which is likely in hands of many small owners, who (for reason of point 1) can basically benefit from COMP holding only by price speculation, as votes of single-double-triple digit COMP owners are largely irrelevant in decision making process (and holding COMP isn’t incentivesed in any other way either) , not even talking about those who have less than 1 COMP. Besides, do we know that holdings of centralised exchanges are from COMP distribution and not from initial seeded Uniswap liquidity leaked to that exchanges overtime?\n\nSome of the analysis that Will Price of Flipside has done on COMP distribution at exchanges has shown that the transfers are not really from small holders. For instance, this effective data visualization 1 depicts that a small number of large holders (and presumably market makers) ended up providing most of the liquidity on Coinbase. Compare this distribution to that of the fair YFI launch 1, where holders are incentivized to hold the asset for cash flows. In the long run, COMP can be the vehicle for redistributing cash flows earned from the protocol (currently in the reserve), but only if there are enough long-term holders who participate in governance.\\nThat all makes sense to me. What is disappointing is the way Compound decided to handle this, which is much more analogous to how elections work in a totalitarian regime than how they should work within a decentralized democratic organization.\nEveryone understands that mistakes will be made. That’s fine. But, asking for feedback from the community and then completely disregarding that feedback is only going to alienate your users and destroy the already small community you had.\nThe better sequencing here would been to:\n\nPresent the analysis establishing the issues\nLet the community come up with ideas/ options to fix those issues\nAnalyze the impact of those options\nVote\n\nCompound rather decided to skip 2) and 3), come up with some band-aid solution and then pretend to vote on it.\\nTo be frank, problem with COMP being sold right away is not in it’s emmision rate but in the fact that it has no other value (utility) to a holder than speculative. That is the thing that should be fixed not distribution model\\n\nWhat is disappointing is the way Compound decided to handle this, which is much more analogous to how elections work in a totalitarian regime than how they should work within a decentralized democratic organization.\n\nI disagree with this characterization. This proposal is clearly initiated by a member of community and are being discussed freely in this forum. I agree that perhaps more input and feedback from the community would be better but this specific proposal does not seem to harm any long term COMP holder.\n\nTo be frank, problem with COMP being sold right away is not in it’s emmision rate but in the fact that it has no other value (utility) to a holder than speculative.\n\nAre you saying that COMP should have other utilities other than governance? I would love to see more proposals for this. You should suggest some @adamskrodzki\\n\n\n\n bolbo:\n\nAre you saying that COMP should have other utilities other than governance? I would love to see more proposals for this. You should suggest some @adamskrodzki\n\n\nGovernance is not a utility for a holder of a COMP token, it might be for a protocol, but holder need to the work (to make informed decisions) and pay tx fees and got nothing in return, there is more than obvious free rider problem here as well.\nOne of possible utility would be dividend (it was alredy described already in this forum) maybe others are discounts in borrowing fees for example.\\nI’ve thought about this proposal (though saw it a little late) and worry that it is misguided. The main issue with COMP distribution, to me, seems to be around the terms under which it is distributed, not how much is distributed. I suspect the primary effect of the current proposal will be to slow the decentralization process vs. correct the distribution mechanics.\nTwo points to consider here:\n\n\nHave you thought about the clear consequences for slowing decentralization? My impression was COMP emissions were intended to decentralize the protocol from initial stakeholders. Initial stakeholders still control most COMP, so in effect this proposal is initial stakeholders voting to slow down the decentralization of the protocol. If we like that rationale, perhaps that is fine; there could be good reasons for that. Or maybe the intent is really to open new routes for how COMP is distributed/decentralized, but not changing the net emission. I would suggest this be further clarified and ideally connected with the longer vision for maintaining the decentralization process (worth noting: there is some discussion of this in other comments).\n\n\nReduced emission does little to address the stated problem (or at least very much unclear, and may make it worse): holding COMP from rewards is not sticky. This is an interesting and critical question for governance, which we highlighted in our blog post/paper 7 back in June. There are different ways to try to address this that we might expect to have better effect. Some are already discussed in this thread, like vesting. I suggest the primary discussion should be around these vs. emission rate. And ideally, there should be some more formal work (some ideas may just put off the problem, and there are many other dimensions of possibilities–limits on transferability is one that isn’t often discussed).\n\n\n*Respect to Tarun and Gauntlet. Big fan of their work!\\nThere is discussion but the feedback is not impacting the proposal itself. Voting on this already started (https://compound.finance/governance/proposals/21?target_network=mainnet 10). Also, the issue here is not that this will harm long term holders, but rather that it doesn’t reflect with what the community wants.\nThis proposal is not about fixing the issue. As many others have pointed out this does little to encourage long term holding.\\nYes, traditional stock equity has worked passably - though crypto can be better.\nUsers will eventually move to the platform that best benefits them. The current consensus among preminers to act against the interests of users makes them vulnerable to disruption by another platform better incentivized to serve users."
  },
  {
    "number_of_comments": 30,
    "postid": "afc1f6ef-435f-4739-9db7-65e43f70624d",
    "posturl": "https://forum.makerdao.com/t/cloaky-ad-recognition-submission/21082",
    "combinedcontent": "AD Recognition Submission\n0x869b6d5d8FA7f4FFdaCA4D23FFE0735c5eD1F818\n[Ethereum Address of Delegation 1 Contract]: 0x475efaC48a0A18660A7A26eE6bd5FEBF466930f8 [Following Growth AVC]\n[Ethereum Address of Delegation 2 Contract]: 0xb335e8b70f95F28e79ACF58491751f83B0050888 [Following Sovereign Finance AVC]\n[Cryptographically signed AD Recognition Submission Message from Address Controlling Delegation Contract 1]  9\n[Cryptographically signed AD Recognition Submission Message from Address Controlling Delegation Contract 2]  5\n[Cryptographically signed AD Recognition Submission Message from Ecosystem Actor Ethereum Address]  7\\nThanks, I can confirm this is all fine. We’ll get you up on the portal soon.\nWelcome to Maker!\\nHi @cloaky !\nThank you very much for following Sovereign Finance AVC! We are looking forward to working together.\n Here is a link 5 with all the dates of our upcoming meetings.\nIn case you missed something, here is our Notion 3 with useful information…\nNext subcommittee meeting’s topic will be MIP 106: Support Scope.\nHope to see you there!\\n(Maker Governance - Proposal BlockTower Andromeda (RWA015-A) Onboarding, Risk Parameter Changes, DSR Increase, Spark Protocol Proxy Activation, MKR Vesting Transfers - June 14, 2023)\nGrowth AVC: No MKR\nSovFi AVC: Yes. It appears that there ain’t nothin’ unusual goin’ on 'round these parts.\n(Maker Governance - Proposal Out-of-Schedule Executive Vote - RWA014-A (Coinbase Custody) DAO Resolution - June 16, 2023)\nGrowth AVC: No MKR\nSovFi AVC: Yes. We believe in havin’ our ducks in a row and our biscuits buttered, just in case. It’s always better to be ready for whatever may come our way.\\n(Maker Governance - Ratification Poll for MIP Amendment Subproposal (MIP102c2-SP8) - June 12, 2023)\nGrowth AVC: No MKR\nSovFi AVC: Yes. I’m fixin’ to tell y’all that I’m supportive of them there various changes. I believe that these here changes bring us closer to that endgame state, yessiree!\n(Maker Governance - Ratification Poll for MIP Amendment Subproposal (MIP102c2-SP9) - June 12, 2023)\nGrowth AVC: No MKR\nSovFi AVC: Yes. It’s a fine idea to keep a bug bounty program alive for protocol security, I tell ya. It’s like keepin’ a watchful eye on them critters tryin’ to sneak into our henhouse. We gotta stay one step ahead, y’all!\\n(Maker Governance - Smart Burn Engine Launch Parameters - June 26, 2023)\nGrowth AVC: No MKR\nSovFi AVC: Yes. I do declare, it sure is mighty fine to lend my support to somethin’ that brings value back to them MKR holders who haven’t seen a lick of it in quite some time.\n(Maker Governance - Proposal BlockTower Andromeda Updates, GUSD PSM Parameter Changes, and Other Actions - June 28, 2023)\nGrowth AVC: No MKR\nSovFi AVC: Yes. It appears that there ain’t nothin’ unusual goin’ on 'round these parts.\\n(Maker Governance - DAO Resolution to Approve Legal Representation - July 3, 2023 1)\nGrowth AVC: No MKR\nSovFi AVC: Yes. I do declare, this here is a mighty necessity to fight this case, y’all. It’s as important as a hog in a mud puddle on a hot summer day.\\nI do declare, I am making a change to my empty neutral GSL. I reckon I’m switchin’ gears to follow that ReFi AVC.\nAD Recognition Submission\n[Ethereum Address of Delegation 1 Contract]: 0x475efaC48a0A18660A7A26eE6bd5FEBF466930f8 [Following ReFi AVC]\n[Cryptographically signed AD Recognition Submission Message from Address Controlling Delegation Contract 1]  2\n[Cryptographically signed AD Recognition Submission Message from Ecosystem Actor Ethereum Address]  1\ncc @Patrick_J @LongForWisdom\\n(Maker Governance - Proposal BlockTower Andromeda Upgrade, Smart Burn Engine Deployment, Keeper Job Updates, Scope Defined Parameter Changes, Delegate Compensation, Ecosystem Actor and Core Unit Funding Updates, Spark Protocol Proxy Spell Execution - July 14, 2023 1)\nReFi AVC: No MKR\nSovFi AVC: Yes. This Executive aligns with the Atlas and is in accordance with the outcomes of prior polls.\\n(Maker Governance - Decrease the Harbor Trade Credit (RWA004-A) Debt Ceiling - July 17, 2023)\nReFi AVC: No MKR\nSovFi AVC: Yes. It appears that this deal was structured in a peculiar manner that allows junior capital to redeem before senior capital. Moreover, the loans contained within are currently in default.\\n(Maker Governance - Stability Scope Bootstrapping Edits - July 24, 2023)\nReFi AVC: No MKR\nSovFi AVC: Yes. This could be viewed as a short-term investment in marketing aimed at generating interest in farming demand, with the potential for some participants to eventually engage in subDAO token farming.\n(Maker Governance - New Silver (RWA002-A) Restructuring and Parameter Changes - July 24, 2023)\nReFi AVC: No MKR\nSovFi AVC: Yes. This appears to be a fair and justifiable way to uphold a business partnership. The stability fee has been noticeably increased, which is advantageous for MakerDAO. It is crucial for MakerDAO to foster positive connections within this realm, as it enables subDAOs to seamlessly continue the work initiated by Maker Core.\n(Maker Governance - Adjust Spark Protocol D3M Debt Ceiling - July 24, 2023)\nReFi AVC: No MKR\nSovFi AVC: Yes. Given Spark’s sustainable growth, it is logical to support an increase in the debt ceiling.\n(Maker Governance - Spark Protocol WETH and DAI Market Parameter Changes - July 24, 2023)\nReFi AVC: No MKR\nSovFi AVC: Yes. I have confidence in PhoenixLabs’s recommendation in this matter. I find no reason to question or doubt their expertise and judgment in this particular matter.\\n(Maker Governance - Proposal Enhanced Dai Savings Rate Activation, Spark Protocol Debt Ceiling Increase, RWA Vault Updates, AVC Member Compensation for Q2 2023, DAO Resolution for Monetalis Clydesdale, Launch Project Funding, Spark Proxy Spell Execution - August 2, 2023)\nReFi AVC: No MKR\nSovFi AVC: Yes. This Executive aligns with the Atlas and is in accordance with the outcomes of prior polls.\\n(Maker Governance - Non-Scope Defined Parameter Changes - wstETH-B DC-IAM Changes - August 7, 2023)\nReFi AVC: No MKR\nSovFi AVC: Yes. The assessment from BA Labs is quite reasonable. While it’s expected that the heightened demand from the EDSR will diminish once rates are equalized, it doesn’t hurt to have additional throughput accessible for these vaults with higher collateralization requirements.\n(Maker Governance - Smart Burn Engine Parameter Update - August 7, 2023)\nReFi AVC: No MKR\nSovFi AVC: Yes. The impact of MEV on losses has been negligible, and it is anticipated to remain minimal even after implementing these modifications. Consequently, the savings in gas costs outweigh any potential drawbacks.\\n(Maker Governance - Stability Scope Bootstrapping Edits (EDSR Changes) - August 14, 2023 1)\nReFi AVC: No MKR\nSovFi AVC: Yes. These changes are crucial to effectively address the large scale borrowing arbitrage that is currently taking place.\\n(Maker Governance - Proposal EDSR Adjustment, Vault and Smart Burn Engine Parameter Updates, CRVV1ETHSTETH-A Offboarding, Delegate Compensation, and Other Changes - August 17, 2023)\nReFi AVC: No MKR\nSovFi AVC: Yes. This Executive aligns with the Atlas and is in accordance with the outcomes of prior polls.\\n(Maker Governance - Reserve Governance Facilitator Appointment - August 21, 2023)\nReFi AVC: No MKR\nSovFi AVC: Le Bateleur, VoteWizard, JanSky. Both Le Bateleur and VoteWizard were recommended by GovAlpha. In the near future, facilitators will need to maintain anonymity. It would be beneficial to distribute power among facilitators as much as possible for this reason. DAO Masons would no longer be able to contribute in this role when anonymity is required, so it would be preferable to onboard other entities who can fulfill this function. JanSky, being anonymous, would be a valuable addition to help distribute power among facilitators.\n(Maker Governance - Increase Spark Protocol wstETH Supply Cap - August 21, 2023)\nReFi AVC: No MKR\nSovFi AVC: Yes. I find both proposed changes to Spark Protocol markets to be logical, and I have confidence in the recommendations put forth by the Spark team.\n(Maker Governance - Spark Protocol ETH Market Parameter Changes - August 21, 2023)\nReFi AVC: No MKR\nSovFi AVC: Yes. I find both proposed changes to Spark Protocol markets to be logical, and I have confidence in the recommendations put forth by the Spark team.\\n(Maker Governance - Ratification Poll for MIP Amendment Subproposal (MIP102c2-SP11) - August 14, 2023)\nReFi AVC & SovFi AVC: No. A fantastic addition to the Atlas is on the horizon! In the coming weeks, a new bug bounty program will be introduced, making this MIP unnecessary and redundant.\n(Maker Governance - Ratification Poll for MIP Amendment Subproposals - Article 1 Edits (MIP102c2-SP12) - August 14, 2023)\nReFi AVC: Yes. This MIP102 aims to streamline the application process and outlines the desired competencies for Advisory Councils. This proposal represents an incremental enhancement compared to the existing content in the Scopes, and pursuing such gradual improvements resonates with ReFi AVC’s strategic perspective.\nSovFi AVC: Yes. This MIP102 draws significant inspiration from the SovFi AVC Aligned Scope Proposal and enjoys strong backing from the SovFi AVC. It aims to streamline the application process and outlines the desired competencies for Advisory Councils.\n(Maker Governance - Ratification Poll for MIP Amendment Subproposal (MIP102c2-SP13) - August 14, 2023 1)\nReFi AVC & SovFi AVC: Yes. The majority of the modifications in this MIP are relatively insignificant and logical. It will be fascinating to observe how unit bias influences the governance tokens. The notable alterations involve the implementation of Operational Security standards for ADs and the augmentation of the Launch Project budget, both of which are reasonable enough. Ideally, once additional details regarding the expenditure of the Launch Project can be disclosed, they will be. Moreover, this should mark the ultimate budget for the Launch Project, as any further expansions would be quite worrisome.\\n(Maker Governance - Approve DAO Resolution Pertaining to HV Bank (RWA009-A) - August 28, 2023)\nReFi AVC & SovFi AVC: Yes. This is a positive stride towards the offboarding of Legacy Legal Recourse Assets, in accordance with MIP104.\n(Maker Governance - Decrease Fortunafi (RWA005-A) Debt Ceiling - August 28, 2023)\nReFi AVC & SovFi AVC: Yes. This is a positive stride towards the offboarding of Legacy Legal Recourse Assets, in accordance with MIP104.\\n(Maker Governance - Proposal Management of ConsolFreight (RWA003-A) Default, ESM Authorization, Chainlog Updates, Budget Management, Spark Proxy Spell - August 30, 2023)\nReFi AVC & SovFi AVC: Yes. This Executive aligns with the Atlas and is in accordance with the outcomes of prior polls.\\n(Maker Governance - Adjust Spark Protocol D3M Parameters - September 4, 2023)\nReFi AVC & SovFi AVC: Yes. The justification provided by the Risk Core Unit Team to support this increase is acceptable.\n(Maker Governance - Adjust Spark Protocol DAI Interest Rate Strategy Borrow Spread - September 4, 2023)\nReFi AVC & SovFi AVC: Yes. Without implementing this change, users farming the airdrop would face no risk whatsoever.\n(Maker Governance - Adjust Spark Protocol Flash Loan Fee - September 4, 2023)\nReFi AVC & SovFi AVC: Yes. This is a straightforward reversal of a parameter to its previous state. There is no justification for having flash loan fees in place.\n(Maker Governance - JAT1 to JAT2 Asset Reallocation - September 4, 2023)\nReFi AVC & SovFi AVC: Yes. This change will enable enhanced simplicity and facilitate more organic liquidation events.\\n(Maker Governance - Proposal Stability Scope Parameter Changes, Spark Protocol D3M Parameter Changes, Set Fortunafi Debt Ceiling to Zero DAI, DAO Resolution for HV Bank, Delegate Compensation and Other Actions - September 13, 2023)\nReFi AVC & SovFi AVC: Yes. This Executive aligns with the Atlas and is in accordance with the outcomes of prior polls. The contents of the Spell underwent a comprehensive review using @votewizard’s guide, along with checks for any unusual or unexpected elements as outlined in @0xDefensor’s guide. Based on these assessments, everything appears to be in alignment with the intended objectives.\\n(Maker Governance - Activate Gnosis Chain Instance of Spark Lend - September 18, 2023 1)\nReFi AVC & SovFi AVC: Yes. Karpatkey is providing the initial liquidity, which significantly mitigates the risk associated with this cross-chain deployment for both Spark and MakerDAO.\n(Maker Governance - Ratification Poll for MIP Amendment Subproposals - Article 1 Edits (MIP102c2-SP15) - September 11, 2023 1)\nReFi AVC: Yes. This MIP102 makes the changes to the remaining two Scopes, Stability and Accessibility, that were already applied to the other three Scopes last governance cycle. It aims to streamline the application process and outlines the desired competencies for Advisory Councils. This proposal represents an incremental enhancement compared to the existing content in the Scopes, and pursuing such gradual improvements resonates with ReFi AVC’s strategic perspective.\nSovFi AVC: Yes. This MIP102 makes the changes to the remaining two Scopes, Stability and Accessibility, that were already applied to the other three Scopes last governance cycle. It draws significant inspiration from the SovFi AVC Aligned Scope Proposal and enjoys strong backing from the SovFi AVC. It aims to streamline the application process and outlines the desired competencies for Advisory Councils.\n(Maker Governance - Ratification Poll for MIP Amendment Subproposal (MIP102c2-SP16) - September 11, 2023 1)\nReFi AVC & SovFi AVC: Yes. This MIP brings forth significant changes, including the incorporation of operational security standards for facilitators and the inclusion of active facilitators for Governance. The current exclusion of LB is regrettable, but it could be a sensible option for them to join one of the active facilitator teams.\n(Maker Governance - Ratification Poll for MIP Amendment Subproposals (MIP102c2-SP4) - September 11, 2023)\nReFi AVC & SovFi AVC: Yes. This represents a clear enhancement to the reporting requirements for Arrangers.\\n(Maker Governance - Reconfiguring RWA Allocator Vaults - September 25, 2023)\nReFi AVC & SovFi AVC: Yes. This vote simply relaxes a technical constraint associated with these Allocator Vaults.\\n(Maker Governance - Proposal DAO Resolution for Monetalis Clydesdale, HV Bank On-chain RWA Agreement Update, Fortunafi Vault Change, Trigger Spark Protocol Proxy Spell - September 27, 2023)\nReFi AVC & SovFi AVC: Yes. This Executive aligns with the Atlas and is in accordance with the outcomes of prior polls. The contents of the Spell underwent a comprehensive review using votewizard’s guide, along with checks for any unusual or unexpected elements as outlined in 0xDefensor’s guide. Based on these assessments, everything appears to be in alignment with the intended objectives.\\n(Maker Governance - Non-Scope Defined Parameter Changes - WBTC DC-IAM Changes - October 2, 2023)\nReFi AVC & SovFi AVC: Yes. This modification mitigates risk for MakerDAO without sacrificing user experience.\n(Maker Governance - Increase rETH Supply Cap to 60k - October 2, 2023)\nReFi AVC & SovFi AVC: Yes. This expansion will enable the rETH vaults that will be offboarded to remain within the MakerDAO ecosystem.\n(Maker Governance - Onboard USDC to SparkLend Ethereum and Activate USD eMode for USDC and sDAI Markets - October 2, 2023)\nReFi AVC & SovFi AVC: Yes. This will enable sDAI depositors to increase their leverage without posing significant risk to Spark, as USDC will be added exclusively for borrowing purposes.\n(Maker Governance - Onboard USDT to SparkLend Ethereum and Activate USD eMode - October 2, 2023)\nReFi AVC & SovFi AVC: Yes. This will enable sDAI depositors to increase their leverage without posing significant risk to Spark, as USDT will be added exclusively for borrowing purposes.\\n(Maker Governance - Proposal USDP-PSM incentives, rETH initial offboarding, RWA vaults reconfiguration, various parameter changes, AVC and AD compensation, Facilitator and Ecosystem Actor compensation, Spark proxy-spell - October 11, 2023)\nReFi AVC & SovFi AVC: Yes. This Executive aligns with the Atlas and is in accordance with the outcomes of prior polls. The contents of the Spell underwent a comprehensive review using votewizard’s guide, along with checks for any unusual or unexpected elements as outlined in 0xDefensor’s guide. Based on these assessments, everything appears to be in alignment with the intended objectives, with the exception of a minor error.\nI noticed a minor error in the Spell while reviewing the amounts and addresses for MKR transfers. The intended amount to be sent to Vigilant was 34.5 MKR, but it was mistakenly written as 34.55 MKR. Fortunately, this should not pose a significant issue as the excess amount can be deducted from Vigilant’s AD buffer. Rest assured, all other amounts and addresses are correct. If this Spell is approved, kindly deduct 0.05 MKR from Vigilant’s AD buffer. @votewizard @JanSky\\nThanks for pointing that out, @cloaky. Well spotted. We’ll conduct a post-mortem to understand the oversight and communicate the steps we’ll take to address it\\n(Maker Governance - Proposal Spark Protocol-Aave Revenue Share Payment, DAO Resolution for HV Bank, Immunefi Security Core Unit MKR Vesting Transfer and Chainlog Housekeeping - November 1, 2023)\nReFi AVC & SovFi AVC: Yes. This Executive aligns with the Atlas and is in accordance with the outcomes of prior polls. The contents of the Spell underwent a comprehensive review using votewizard’s guide, along with checks for any unusual or unexpected elements as outlined in 0xDefensor’s guide. Based on these assessments, everything appears to be in alignment with the intended objectives.\\n(Maker Governance - Approve DAO Resolution to Onboard Mars Foundation - November 6, 2023)\nReFi AVC & SovFi AVC: Yes. This foundation is a crucial legal entity dedicated to supporting MakerDAO in the deployment and operation of frontends.\n(Maker Governance - SparkLend Gnosis Chain - Increase wstETH Supply Cap to 10,000 wstETH - November 6, 2023)\nReFi AVC & SovFi AVC: Yes. This approach will address market demand and facilitate ongoing growth, and the enhanced liquidity on Gnosis Chain enables this expansion in a carefully managed manner.\n(Maker Governance - SparkLend Ethereum - Set DAI Market Maximum Loan-to-Value to Zero Percent - November 6, 2023)\nReFi AVC & SovFi AVC: Yes. This modification effectively mitigates risk by removing the possibility of utilizing DAI as collateral, while maintaining the availability of sDAI as a collateral asset in the market.\n(Maker Governance - SparkLend Ethereum - Reactivate WBTC and Optimize Parameters for Current Market Conditions - November 6, 2023)\nReFi AVC & SovFi AVC: Yes. According to the BA Labs review, the advantages of incorporating WBTC into Spark surpass the associated costs, provided that risk can be adequately controlled. The interest rate model parameters are approximately derived from the well-established Aave v3 Ethereum parameters, which have demonstrated their safety over an extended period.\n(Maker Governance - SparkLend Ethereum - Adjust Spark Protocol D3M Maximum Debt Ceiling - November 6, 2023 1)\nReFi AVC & SovFi AVC: Yes. Although it is unlikely that the D3M maximum debt ceiling will be reached in the near term, Maker governance is expected to be relatively inactive through December. As a result, it would be wise to proactively make any necessary D3M adjustments to support user growth throughout the remainder of the year. BA Labs maintains confidence in the appropriateness of the LTV and liquidation threshold parameters for collateral assets, which are somewhat more conservative than those found in Aave v3. These parameters are designed to prevent the accumulation of bad debt, even in the face of increasing total debt exposure and significant market volatility. Therefore, the proposed increase in the debt ceiling is a reasonable decision.\n(Maker Governance - SparkLend Ethereum - Increase rETH & wstETH Supply Caps - November 6, 2023)\nReFi AVC & SovFi AVC: Yes. According to BA Labs, these supply caps are deemed adequate to meet demand until the end of the year, while also setting sensible limits on exposure growth.\n(Maker Governance - SparkLend Ethereum & Gnosis Chain - Adjust ETH Market Interest Rate Models - November 6, 2023)\nReFi AVC & SovFi AVC: Yes. To support ongoing ETH borrowing for leveraged staking and maximize expected ETH supplier revenue, it is appropriate to lower ETH borrowing costs. Specifically, reducing the base rate to 0% would be highly beneficial, as it would decrease borrowing costs during periods of low utilization while posing minimal risk to ETH market liquidity. However, it is important to note that reducing the optimal borrow rate carries a higher risk of allowing utilization to exceed healthy levels. This risk, however, is mitigated by implementing higher maximum borrowing rates.\\n(Maker Governance - Proposal Spark Proxy Spell, Increase Spark Lend Maximum Debt Ceiling, Launch Project Funding, Approve Updates to the HVBank Facility, Whistleblower Bounty, October 2023 Delegate Compensation - November 15, 2023)\nReFi AVC & SovFi AVC: Yes. This Executive aligns with the Atlas and is in accordance with the outcomes of prior polls. The contents of the Spell underwent a comprehensive review using votewizard’s guide, along with checks for any unusual or unexpected elements as outlined in 0xDefensor’s guide. Based on these assessments, everything appears to be in alignment with the intended objectives.\\n(Maker Governance - Ratification Poll for MIP Amendment Subproposals - Article 1 Edits (MIP102c2-SP17) - November 13, 2023)\nReFi AVC: Yes. The ReFi AVC holds the view that the tasks expected of Scope Advisors should be clearly defined and originate from within the organization. This proposal achieves that objective.\nSovFi AVC: Yes. This will aid in elucidating the responsibilities of Scope Advisors and mitigating any friction in their onboarding process by delineating specific tasks.\n(Maker Governance - Ratification Poll for MIP Amendment Subproposal (MIP102c2-SP18) - November 13, 2023)\nReFi AVC & SovFi AVC: No. This proposal conflicts with the Endgame proposal, which is currently up for vote in the same monthly cycle. Therefore, endorsing it would impede the advancement of the Maker Ecosystem towards the Endgame State.\n(Maker Governance - Ratification Poll for MIP Amendment Subproposal (MIP102c2-SP20) - November 13, 2023)\nReFi AVC & SovFi AVC: Yes. The stipulations for Alignment Conservers to prioritize the development of the Next Generation Atlas propel the Maker Ecosystem towards the Endgame State and will fortify the Alignment Artifacts. Additionally, this initiative reduces the Smart Burn Engine MKR Accumulation Rate to enhance uptime and introduces a much-needed bug bounty program into the Alignment Artifacts. Furthermore, it designates the Ecosystem-Team as a Facilitator, a necessary step in light of the voluntary offboarding of SES."
  },
  {
    "number_of_comments": 21,
    "postid": "ee67579c-c9d5-4c38-9aeb-15cde20a0200",
    "posturl": "https://www.comp.xyz/t/proposal-increase-usdt-maximum-interest-rate/1091",
    "combinedcontent": "Summary\nThe USDT Pool is in danger of running out of liquidity in the near future due to an interest rate curve that does not reflect the unique high borrow demand, high supply risk, and low pool liquidity currently associated with the USDT pool. We should raise the maximum interest rate on the pool’s interest curve to avoid this.\nUSDT is a unique pool\nLet’s compare the amount of supply in the USDT pool vs other pools:\nimage1790×168 24.1 KB\nUSDT has a much smaller total supply.\nBut that’s not even the full picture of the size differences. Let’s look at the proportions of actual liquidity available in these pools yesterday:\nimage1792×170 23.6 KB\nOnly 8-9 million of available liquity in USDT yesterday, vs 171 million for USDC.\nUSDT utilization was over 95% for parts of yesterday.\nWhy the difference?\nUSDT is the only pool that does not provide collateral when funds are placed into it. This disadvantages lending USDT vs lending some other stablecoin, and has historically meant that USDT supply has been much lower than USDT and USDC. Along with a lower supply has come higher rates, as the limited supply is allocated by market pricing to those willing to pay the most for it.\nIt’s a reasonable decision to not have USDT provide collateral. This isolates the damage from USDT loosing its peg down from all pools to only those lending USDT being at risk. This does mean though that this risk is concentrated on the lenders, and the Compound USDT pool becomes a betting market against the future of USDT. Suppose a whale believes that USDT might loose value on Feb 1st. The Whale can deposit 10 million into the USDC pool, take out a 7.5 million dollar loan from the USDT pool, and swap it into USDC (and maybe loop this a few times if desired). If USDT goes to $0.90, the whale can use USDC to repurchase this cheap USDT and pay off the loans, making a profit. If the peg falls to near zero, then the whale could almost 2.0x their initial money, all of this profit at the expense of lenders in the USDT pool.\nThe risk of this happening drives up the price at which lenders are willing to lend into the pool.\nAdditionally, there is a lot of demand for USDT for farming purposes right now. USDT only makes up 23.60% of Curve’s 3Pool, and is the most “expensive” coin there. It’s in high demand for borrowing.\nThe low liquidly in the USDT pool increases the risk getting stuck in the pool when lending, raising rates more.\nFinally, the low liquidly increases the size of “shocks” to the APR from liquidity moving in or out. Two million dollars moving out would not be noticeable on the DAI pool, but would make a big swing on the USDT pool.\nAll together, it is not surprising that USDT pool rates are the highest rates on Compound. It’s fundamentally different pool than the others.\nSo what?\nEven though the USDT pool is very different from other pools, and consistently runs higher interest rates, the interest rate curve for USDT is the same as the other stablecoins.\nMost importantly, the maximum interest rate for USDT is the same as other stablecoins. Why does this matter?\nWhat happens when a pool reaches 100% utilization?\nThe key to Compound’s design is the dynamic interest rates that constantly adjust prices to match supply and demand. If this interest rate were not dynamic, but fixed to a particular rate that did not match real world supply and demand, then a pool would no longer function. If the fixed rate was too high, then no one would borrow and there would not be money to pay the lenders. If the fixed rate was lower than reality, then things are even worse.\nWithout dynamic pricing a pool is dead. If a pool’s utilization reaches 100%, then the interest rate would be fixed at the highest rate set for that asset - which would be lower than the real world interest rate for that asset.\nAs long as the the real world interest rate for the asset was higher than the Compound interest rate, the liquidity would be gone and the pool would be stuck in a situation where:\n\nNo new borrowers can borrow\nNo old lenders can withdraw their funds\nNo sane new lenders would want to lend at below market rates as well as be unable to withdraw their funds afterwards.\n\nThese are all very bad things.\nThere is a self-reinforcing cycle here in that the more that running out of liquidity looks likely to happen in the near future, the higher interest rates lenders will require, reducing the size of liquidity on the pool and increasing the likelihood of it happening.\nProposal\nTo ensure that prices stay dynamic, the 100% utilization interest rate for USDT needs to be higher it is currently set at.\nI propose that we raise the 100% utilization USDT borrow rates to 39.9% APY. In dynamic sitatuions, this allows the price to go up further, continuing the range where supply and demand can interact with each other.\nThis should have the effect of reducing the risk of lending into the USDT pool, and with reduced risk, potentially increase the totally supply of the pool.\nNext Steps.\nI’ll leave this up over the weekend for comments, and will calculate the parameters needed and test on a fork next week.\\nIt is quite clear that having a max interest rate below market would be very bad for the reasons you described.\nLet’s take a step back and acknowledge what the IRMs are trying to do. We want to be generating as much interest as possible while maintaining sufficient market liquidity for normal usage. The new IRMs are kink based models, meaning the rate of change of interest rate increases after a kink. We set this kink to a utilization which allows for the ideal liquidity (currently 20% on cUSDT). The interest rate at the kink point is estimated to be a higher end normal market value for optimal efficiency. Beyond the kink, rates go up dramatically.\nWith the current environment in DeFi, we are seeing abnormal market activities and consistently high utilization across all the stable coin markets. I would be in favor of raising the rate at kink (across all stablecoins) due to this possibly new norm to encourage greater market liquidity.\nWhile we have seen times with high rates on cUSDT, I don’t believe that raising the max value on the IRM will independently fix this. The high rates are usually temporarily above market, resulting for a large withdrawal or borrow. The withdrawer does not care about the manipulation of rates, and it it takes some time for the market to fix the inefficiencies. What could fix this the liquidity issue, although I would argue against it, is lowering the kink utilization dramatically so that when large moves occur, there is still plenty of liquidity. The issue with this is that it would lower the generated interest a significant amount. If you believe that it is likely for the market USDT supply rate to be over 40% then we definitely should raise the IRM max, but I find this unlikely.\\nGreetings!\nI agree with you that there are more options than just raising the max rate.\nHowever, I do strongly see from the data that cUSDT is a very different beast than the other stablecoin pools, and as a result it does need a separate interest rate curve.\ncDAI and cUSDC are behaving how we hope to seem them act. In past months they’ve hung out around the kink interest rate. In the last month, with high demand, they have matched expectations well. Demand is indeed up on them, but liquidity is fine. At the moment cDAI has seen a couple days of sustained higher demand, but is steady at 85% utilization and 200 million in liquidity. Even in the last month of higher demand both cDai and cUSDC have spent at least a day down at their targeted kink utilization. Most spikes in cDAI/cUSD rates last only minutes long, as the active pool of borrowers and lenders move money to take advantage of, or to avoid the higher yield.\nIn contrast cUSDT reached near max utilization on January 19th stayed there for three hours before money moved to correct this. Then cUSDT ran around 90% - 95% utilization for two days at the end of last week, prompting this proposal. This morning rates on cUSDT went up 3x and stayed there for five hours with almost no changes, before correcting back down. These are just a couple examples from the last two weeks of both the high rates that cUSDT is reaching, and the delays in returning to normal. CUSDT has spent 40x more time in the last two weeks at over 90% utilization than the other two coins have averaged.\nTaleb says, “never cross a river that averages 4 ft deep”. The problem isn’t so much the average case (even though that’s a 40% higher relative APY’s than the other stablecoins last month), nor is the problem the average true market rate, it’s that cUSDT is spending hours or days in regions of the utilization curve that are unsafe to hang out in. It’s routinely getting near the limits of its emergency maneuvering room in normal usage. In case of a real crunch it has hardly anything extra it can do to return to a balance.\ncUSDT fundamentally is different than the other pools, and should have its own curves.\nTo answer your final question, the last few weeks have already shown that borrowers will spend hours at 25-29% interest as a “market rate”. Regardless of if this is the long term average market rate, it has been shown to be a defacto rate that market players will support for some length of time. It’s hard to know what the max one hour true market rate is because cUSDT can’t go high enough to test it.\nIt would be poor planning on my part to build a house a foot above the high tide line, because one day we might have a high tide plus a storm. Similarly although this is a high tide time, we haven’t had a storm, and yet the water is already lapping at the front steps some days.\nNow it’s true that if we hit 100% utilization and the market eventually corrected itself, that the pool would return to life. But it’s likely that those lenders ,who wanted to withdraw but could not do so during that time, would be burnt and might permanently leave the pool. Others would pull their liquidity faster in the face of future crunches, increasing the likelihood of future crunches.\nI think the most important parameter to adjust is the cUSDT max rate. By raising the maximum rate, we increase the incentives on both borrowers and lenders to return things to a more normal rates more quickly. This gives us more headroom for how much out of normal we can handle, and it also should make us spend less time in the danger zone.\nAs important as it is to be resilient to these unusual times and  not break during them, it is indeed likely that in the next year we’ll be back down to normal operation and hanging out near the kink utilization and interest rates. The current kink position and rate have seemed good for “normal times”, so I’d hate to mess with them. If we wanted to adjust them, we should plan on changing them back during quieter times. While raising the kink APY could indeed make for less of an effect on rates as liquidity moves in and out, that seems like it might be actually be counter-productive if we want to incentivize the pool to act faster. The gains in spare liquity would be small if we moved the kink rate to six percent, and yet doing more would incentivize a lot of borrowers to leave the pool when the rates go below that.\\nI was not aware of the actual durations of high utilization. Thank you for clarifying it in your most recent post. In this case, increasing the max rate would definitely be a valid place to start. Currently cUSDC, cUSDT, and cDAI point to the same IRM so we will need to deploy a new one and point cUSDT to the new IRM. If you’d like for me to help along the way or create an autonomous proposal, message me on discord.\\nThanks for your offer! I’ll reach out to you tomorrow!\\nI believe the correct new jumpMultiplierPerYear to use will be 1480000000000000000, changing up from 1090000000000000000. Testing on a forked mainnet, this gives a max borrow rate of 39.93% and the following chart comparing old and new values:\nimage1382×1024 74.6 KB\nSource for tests here:\n\n  \n      gist.github.com\n  \n  \n    https://gist.github.com/DanielVF/3f0d3279480a43a3b7256526ea7e5bb0 2\n_charts.ipynb\n{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 30,\n   \"id\": \"civilian-advocacy\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"%matplotlib inline\\n\",\nThis file has been truncated. show original\nafter.json\n{\"pct\": [100, 99, 98, 97, 96, 95, 94, 93, 92, 91, 90, 89, 88, 87, 86, 85, 84, 83, 82, 81, 80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0], \"supply\": [147831050226, 139906249997, 132111681885, 124447345887, 116913242007, 109509370240, 102235730591, 95092323057, 88079147638, 81196204336, 74443493148, 67821014077, 61328767121, 54966752281, 48734969556, 42633418947, 36662100454, 30821014077, 25110159815, 19529537669, 14079147639, 13729368814, 13383989724, 13043010367, 12706430744, 12374250854, 12046470698, 11723090276, 11404109587, 11089528632, 10779347410, 10473565923, 10172184169, 9875202148, 9582619861, 9294437308, 9010654488, 8731271402, 8456288050, 8185704432, 7919520547, 7657736395, 7400351977, 7147367293, 6898782343, 6654597126, 6414811642, 6179425893, 5948439877, 5721853595, 5499667046, 5281880231, 5068493149, 4859505802, 4654918188, 4454730307, 4258942160, 4067553747, 3880565067, 3697976121, 3519786909, 3345997430, 3176607686, 3011617674, 2851027396, 2694836852, 2543046042, 2395654965, 2252663622, 2114072012, 1979880136, 1850087994, 1724695585, 1603702910, 1487109969, 1374916761, 1267123287, 1163729547, 1064735540, 970141266, 879946727, 794151921, 712756849, 635761510, 563165905, 494970034, 431173896, 371777492, 316780821, 266183885, 219986681, 178189212, 140791476, 107793474, 79195205, 54996670, 35197869, 19798801, 8799467, 2199866, 0], \"borrow\": [159817351596, 152777777776, 145738203956, 138698630135, 131659056315, 124619482494, 117579908674, 110540334854, 103500761033, 96461187213, 89421613392, 82382039572, 75342465752, 68302891931, 61263318111, 54223744290, 47184170470, 40144596650, 33105022829, 26065449009, 19025875189, 18788051749, 18550228309, 18312404869, 18074581430, 17836757990, 17598934550, 17361111110, 17123287670, 16885464230, 16647640790, 16409817351, 16171993911, 15934170471, 15696347031, 15458523591, 15220700151, 14982876711, 14745053271, 14507229832, 14269406392, 14031582952, 13793759512, 13555936072, 13318112632, 13080289192, 12842465752, 12604642313, 12366818873, 12128995433, 11891171993, 11653348553, 11415525113, 11177701673, 10939878234, 10702054794, 10464231354, 10226407914, 9988584474, 9750761034, 9512937594, 9275114154, 9037290715, 8799467275, 8561643835, 8323820395, 8085996955, 7848173515, 7610350075, 7372526635, 7134703196, 6896879756, 6659056316, 6421232876, 6183409436, 5945585996, 5707762556, 5469939117, 5232115677, 4994292237, 4756468797, 4518645357, 4280821917, 4042998477, 3805175037, 3567351598, 3329528158, 3091704718, 2853881278, 2616057838, 2378234398, 2140410958, 1902587518, 1664764079, 1426940639, 1189117199, 951293759, 713470319, 475646879, 237823439, 0], \"borrow_apy\": [0.3993389871711319, 0.37878127570658227, 0.35852557776875726, 0.33856745647625885, 0.3189025401298937, 0.29952652125508084, 0.2804351556583293, 0.2616242614975768, 0.24308971836618443, 0.22482746639039042, 0.20683350534002232, 0.1891038937522731, 0.1716347480683511, 0.15442224178281205, 0.1374626046053895, 0.12075212163513815, 0.10428713254670985, 0.0880640307885836, 0.07207926229260075, 0.056329326704831484, 0.040810773593610294, 0.04029049856594602, 0.03977048312511622, 0.03925072811214503, 0.03873123242655141, 0.038211996908519064, 0.03769302045866696, 0.0371743039163388, 0.03665584618325224, 0.036137647614215895, 0.03561970856313468, 0.03510202793337269, 0.03458460656217821, 0.03406744335401091, 0.033550539145282166, 0.03303389284154701, 0.03251750527838082, 0.032001375362432416, 0.03148550344691703, 0.030969889884150836, 0.030454533582421783, 0.029939435375218704, 0.029424594171920226, 0.028910010805182518, 0.028395684185474446, 0.02788161514461973, 0.027367802594175927, 0.02685424688577287, 0.026340948370144668, 0.0258279059604809, 0.025315120486528064, 0.024802590862561757, 0.02429031791749936, 0.023778300566701827, 0.023266539638257866, 0.022755034048612455, 0.022243784147816026, 0.021732790285028614, 0.021222051378318785, 0.02071156825370779, 0.02020133983034622, 0.019691366933429766, 0.019181648483188818, 0.018672185303994615, 0.018162976317156687, 0.017654021871154058, 0.017145322313578903, 0.016636876567357683, 0.01612868545480306, 0.015620747899918186, 0.015113064724193714, 0.014605634852708382, 0.014098459106131722, 0.013591536410616945, 0.013084867113078102, 0.012578451559546622, 0.012072288677785314, 0.011566379286414241, 0.011060722314268023, 0.010555318579148931, 0.010050167010961841, 0.009545268426691456, 0.009040621757312461, 0.008536227348181713, 0.008032085543776324, 0.007528195276673921, 0.0070245573618188395, 0.006521170732854964, 0.0060180362039130575, 0.0055151527097021535, 0.0050125210635392214, 0.004510140201198443, 0.004008010466486134, 0.00350613220233309, 0.0030045043461088827, 0.0025031277090994397, 0.00200200122973615, 0.0015011257184944782, 0.0010005001148665205, 0.0005001252285175006, 0.0], \"supply_apy\": [0.36451625946601296, 0.34197026255336804, 0.3201582047551925, 0.2990563079703352, 0.2786417549148752, 0.25889265256890215, 0.23978798965830572, 0.2213076031553869, 0.2034321406078119, 0.18614303154352418, 0.1694224496651633, 0.1532532888809175, 0.13761913014455662, 0.12250421627846753, 0.10789342582765937, 0.0937722465999633, 0.08012675350726783, 0.06694358589897953, 0.054209923983025377, 0.04191347222882702, 0.030042434317611777, 0.02928524521146869, 0.028538126825370158, 0.02780105803239019, 0.027074017520157367, 0.026356985228939145, 0.025649940432124207, 0.024952863652472734, 0.024265735223458984, 0.023588535289974066, 0.022921245241553923, 0.02226384627670641, 0.021616319403587436, 0.020978646870479567, 0.02035081025665253, 0.019732792378605923, 0.01912457585909344, 0.01852614360322602, 0.01793747832247017, 0.017358563485073697, 0.016789383788579748, 0.016229922310038614, 0.01568016335536071, 0.015140092455972809, 0.014609693523740086, 0.014088951695633067, 0.013577852857392214, 0.013076382695814903, 0.012584527171980886, 0.012102272048122309, 0.011629603832594482, 0.011166510250162132, 0.010712977408298574, 0.010268993102161117, 0.009834544453779959, 0.009409619798385727, 0.00899420679776064, 0.008588294795802698, 0.008191871049961552, 0.0078049259104862045, 0.007427447641257912, 0.007059426184694351, 0.006700850808885717, 0.006351711987959918, 0.00601199999108859, 0.00568170488284947, 0.005360817931523654, 0.005049330199619728, 0.004747232544207058, 0.004454517023914306, 0.004171175022078977, 0.003897199590920497, 0.003632581701099147, 0.0033773153968643665, 0.0031313926409659576, 0.0028948070626400035, 0.002667551615255448, 0.0024496204489488083, 0.0022410075056971124, 0.0020417065195459383, 0.0018517124198564883, 0.0016710194597358008, 0.0014996230867430071, 0.0013375185395032574, 0.0011847013152666008, 0.0010411667023879634, 0.0009069111821933262, 0.0007819305594012338, 0.0006662218310860002, 0.0005597817847113884, 0.0004626069982529568, 0.00037469524127109466, 0.0002960436065162497, 0.00022665084484541076, 0.00016651362942221581, 0.00011563169199435563, 7.40026866312693e-05, 4.162592493428541e-05, 1.8500041559121883e-05, 4.624861596802532e-06, 0.0]}\n\nbefore.json\n{\"pct\": [100, 99, 98, 97, 96, 95, 94, 93, 92, 91, 90, 89, 88, 87, 86, 85, 84, 83, 82, 81, 80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0], \"supply\": [113513127851, 107630244005, 101843274351, 96152218890, 90557077623, 85057850549, 79654537669, 74347138982, 69135654488, 64020084187, 59000428079, 54076686167, 49248858445, 44516944918, 39880945584, 35340860442, 30896689496, 26548432742, 22296090180, 18139661813, 14079147639, 13729368814, 13383989724, 13043010367, 12706430744, 12374250854, 12046470698, 11723090276, 11404109587, 11089528632, 10779347410, 10473565923, 10172184169, 9875202148, 9582619861, 9294437308, 9010654488, 8731271402, 8456288050, 8185704432, 7919520547, 7657736395, 7400351977, 7147367293, 6898782343, 6654597126, 6414811642, 6179425893, 5948439877, 5721853595, 5499667046, 5281880231, 5068493149, 4859505802, 4654918188, 4454730307, 4258942160, 4067553747, 3880565067, 3697976121, 3519786909, 3345997430, 3176607686, 3011617674, 2851027396, 2694836852, 2543046042, 2395654965, 2252663622, 2114072012, 1979880136, 1850087994, 1724695585, 1603702910, 1487109969, 1374916761, 1267123287, 1163729547, 1064735540, 970141266, 879946727, 794151921, 712756849, 635761510, 563165905, 494970034, 431173896, 371777492, 316780821, 266183885, 219986681, 178189212, 140791476, 107793474, 79195205, 54996670, 35197869, 19798801, 8799467, 2199866, 0], \"borrow\": [122716894975, 117532343986, 112347792997, 107163242007, 101978691018, 96794140029, 91609589039, 86425038050, 81240487061, 76055936071, 70871385082, 65686834093, 60502283103, 55317732114, 50133181125, 44948630135, 39764079146, 34579528157, 29394977167, 24210426178, 19025875189, 18788051749, 18550228309, 18312404869, 18074581430, 17836757990, 17598934550, 17361111110, 17123287670, 16885464230, 16647640790, 16409817351, 16171993911, 15934170471, 15696347031, 15458523591, 15220700151, 14982876711, 14745053271, 14507229832, 14269406392, 14031582952, 13793759512, 13555936072, 13318112632, 13080289192, 12842465752, 12604642313, 12366818873, 12128995433, 11891171993, 11653348553, 11415525113, 11177701673, 10939878234, 10702054794, 10464231354, 10226407914, 9988584474, 9750761034, 9512937594, 9275114154, 9037290715, 8799467275, 8561643835, 8323820395, 8085996955, 7848173515, 7610350075, 7372526635, 7134703196, 6896879756, 6659056316, 6421232876, 6183409436, 5945585996, 5707762556, 5469939117, 5232115677, 4994292237, 4756468797, 4518645357, 4280821917, 4042998477, 3805175037, 3567351598, 3329528158, 3091704718, 2853881278, 2616057838, 2378234398, 2140410958, 1902587518, 1664764079, 1426940639, 1189117199, 951293759, 713470319, 475646879, 237823439, 0], \"borrow_apy\": [0.29433879783741124, 0.28030711874568137, 0.26642755373769345, 0.25269845437295224, 0.23911818948368468, 0.22568514558527086, 0.2123977266845456, 0.1992543540901759, 0.18625346622509653, 0.1733935184409774, 0.16067298283470266, 0.14809034860279913, 0.13564411971222556, 0.12333281795600404, 0.11115498061613538, 0.09910916083164545, 0.08719392742668264, 0.07540786474047834, 0.06374957245915125, 0.05221766544933515, 0.040810773593610294, 0.04029049856594602, 0.03977048312511622, 0.03925072811214503, 0.03873123242655141, 0.038211996908519064, 0.03769302045866696, 0.0371743039163388, 0.03665584618325224, 0.036137647614215895, 0.03561970856313468, 0.03510202793337269, 0.03458460656217821, 0.03406744335401091, 0.033550539145282166, 0.03303389284154701, 0.03251750527838082, 0.032001375362432416, 0.03148550344691703, 0.030969889884150836, 0.030454533582421783, 0.029939435375218704, 0.029424594171920226, 0.028910010805182518, 0.028395684185474446, 0.02788161514461973, 0.027367802594175927, 0.02685424688577287, 0.026340948370144668, 0.0258279059604809, 0.025315120486528064, 0.024802590862561757, 0.02429031791749936, 0.023778300566701827, 0.023266539638257866, 0.022755034048612455, 0.022243784147816026, 0.021732790285028614, 0.021222051378318785, 0.02071156825370779, 0.02020133983034622, 0.019691366933429766, 0.019181648483188818, 0.018672185303994615, 0.018162976317156687, 0.017654021871154058, 0.017145322313578903, 0.016636876567357683, 0.01612868545480306, 0.015620747899918186, 0.015113064724193714, 0.014605634852708382, 0.014098459106131722, 0.013591536410616945, 0.013084867113078102, 0.012578451559546622, 0.012072288677785314, 0.011566379286414241, 0.011060722314268023, 0.010555318579148931, 0.010050167010961841, 0.009545268426691456, 0.009040621757312461, 0.008536227348181713, 0.008032085543776324, 0.007528195276673921, 0.0070245573618188395, 0.006521170732854964, 0.0060180362039130575, 0.0055151527097021535, 0.0050125210635392214, 0.004510140201198443, 0.004008010466486134, 0.00350613220233309, 0.0030045043461088827, 0.0025031277090994397, 0.00200200122973615, 0.0015011257184944782, 0.0010005001148665205, 0.0005001252285175006, 0.0], \"supply_apy\": [0.2695341044969606, 0.25392898887328075, 0.23876546246578867, 0.22403210729864975, 0.209717899304124, 0.19581219711078557, 0.18230472845501966, 0.16918557769872677, 0.1564451743867168, 0.14407427957879815, 0.1320639785928075, 0.12040566663137997, 0.10909104076043352, 0.09811208960239282, 0.08746108396140362, 0.07713056782542838, 0.06711334973065863, 0.05740249496794991, 0.0479913156518581, 0.0388733665510097, 0.030042434317611777, 0.02928524521146869, 0.028538126825370158, 0.02780105803239019, 0.027074017520157367, 0.026356985228939145, 0.025649940432124207, 0.024952863652472734, 0.024265735223458984, 0.023588535289974066, 0.022921245241553923, 0.02226384627670641, 0.021616319403587436, 0.020978646870479567, 0.02035081025665253, 0.019732792378605923, 0.01912457585909344, 0.01852614360322602, 0.01793747832247017, 0.017358563485073697, 0.016789383788579748, 0.016229922310038614, 0.01568016335536071, 0.015140092455972809, 0.014609693523740086, 0.014088951695633067, 0.013577852857392214, 0.013076382695814903, 0.012584527171980886, 0.012102272048122309, 0.011629603832594482, 0.011166510250162132, 0.010712977408298574, 0.010268993102161117, 0.009834544453779959, 0.009409619798385727, 0.00899420679776064, 0.008588294795802698, 0.008191871049961552, 0.0078049259104862045, 0.007427447641257912, 0.007059426184694351, 0.006700850808885717, 0.006351711987959918, 0.00601199999108859, 0.00568170488284947, 0.005360817931523654, 0.005049330199619728, 0.004747232544207058, 0.004454517023914306, 0.004171175022078977, 0.003897199590920497, 0.003632581701099147, 0.0033773153968643665, 0.0031313926409659576, 0.0028948070626400035, 0.002667551615255448, 0.0024496204489488083, 0.0022410075056971124, 0.0020417065195459383, 0.0018517124198564883, 0.0016710194597358008, 0.0014996230867430071, 0.0013375185395032574, 0.0011847013152666008, 0.0010411667023879634, 0.0009069111821933262, 0.0007819305594012338, 0.0006662218310860002, 0.0005597817847113884, 0.0004626069982529568, 0.00037469524127109466, 0.0002960436065162497, 0.00022665084484541076, 0.00016651362942221581, 0.00011563169199435563, 7.40026866312693e-05, 4.162592493428541e-05, 1.8500041559121883e-05, 4.624861596802532e-06, 0.0]}\n\n\n\n  There are more than three files. show original\n\n\n  \n  \n    \n    \n  \n  \n\n\\nThanks to @arr00 who will be handling the contract deploy and creating a governance proposal for this.\\n\n\n\n dvf:\n\n\nNo old lenders can withdraw their funds\n\n\n\nA few points\n\nCompound has reserves for this exact reason, and USDT has a lot, meaning that at 100% UTIL, funds can still be withdrawn\nAt the moment, there are 305,381 USDT in reserves, not crazy, but not nothing\nCheck out idle finance and the old days of yUSD, they are yield aggregators for stablecoins, they optimize for highest yield\n\nidle 1\nAnother Approach could be to “trick” these aggregators to providing much needed liquidity and manage their rates, these whales are bots.\nCheck out this paper on IRM’s, very important to what your saying,\nVERY IMPORTANT PAPER ON IRM 5\nIf your concern is the last 5% of UTIL, check out dydx, their IRM is nonlinear at the end.\n1125×2436 314 KB\n1125×2436 402 KB\\nAlso, if you want more liquidity, increase the COMP speeds, simple as that\\n@massnomis, that is indeed a very interesting paper - from back when Compound had 1/20 the funds it does now. One section says:\n\nInvestigating the largest PLF, Compound, we find that the no arbitrage condition of Uncovered Interest Parity typically does not hold, suggesting that markets associated with these protocols may be relatively inefficient and agents may not be optimally reacting to interest rate incentives.\n\nThis is very much what I’m seeing. There’s certainly seems to be an opportunity to quickly react to liquidity / interest rate changes and earn a much higher overall yield (or have an overall lower cost of borrowing)\nWhile a non-linear function makes sense for the high side of utilization, I prefer to make gentle changes to system that are already working. \\n@arr00 has deployed the new interest rate model contract here:\n\n  \n      \n      Ethereum (ETH) Blockchain Explorer\n  \n  \n    \n\nContract Address 0x461c23c25942a1bb4b5f3d6b01c6d10f3d45862c | Etherscan 3\n\nThe Contract Address 0x461c23c25942a1bb4b5f3d6b01c6d10f3d45862c page allows users to view the source code, transactions, balances, and analytics for the contract address. Users can also interact and make transactions to the contract directly on...\n\n\n  \n  \n    \n    \n  \n  \n\n\nI’ve verified that the constructor parameters are correct, and that the new contract returns the expected new interest values.\nimage1402×1042 75 KB\n@arr00 has made two changes to the existing jump rate model v2 contract, which are in this PR 1:\n\nAdded a _setOwner function\nRenamed updateJumpRateModel to _updateJumpRateModel\n\n\\nI am very glad you read the paper, based on that…\nWhy do you think that there is a possibility and probability that those quickly react to instances of high utilization? If you are saying that this is the case, then who is doing it exactly?\nIf you prefer to make gentle changes, then how about changing just the borrow rate and not both?\nYou seem to be pointing out an issue, planning a solution, then fixing two things while only arguing for both.\nHow about tinkering borrow and not supply? then reserves grow at a faster pace and the issue is attacked more conservatively.\\nI think that people will react quicker to changes because there will be more of an incentive to do. Those that react quickly will make more money, and that tends to eventually sort itself out.\nOf course I could be wrong about the effects of this change, but in two months we’ll be able to look at the data and see. Importantly though, I don’t think this interest parameter change is likely to break anything.\nI think you are asking about raising the borrowing rate while keeping the lending rate the same? I guess this could somewhat be done by raising the borrowing rate while also increasing the reserve ratio?\nThere are a few reasons I don’t like that that option.\n\n\nReserves themselves, in the current quantity, only provide a tiny buffer to a situation where the market rate is higher than the max rate. Although it stops borrowers from borrowing, it doesn’t stop lenders from pulling their money. Right now the cash reserves on USDT are a sixth of a percent of the total amount lent into the pool.\n\n\nReserves are an inefficiency. They make lenders get less money than they otherwise would, thus incetivizing some percentage of lenders to take their money elsewhere where they can get higher rates for their money. For DAI and USDC, lending is wrapped up in providing collateral for borrowing, but for USDT, it’s considerably more about reward and risk on Compound vs risk and reward elsewhere. It’s much nicer for lenders, borrowers, and COMP holders if Compound has more supply in the USDT pool, rather than having less. (Some reserves are necessary, but increasing the  reserve ratio can be a good thing.)\n\n\nAdjusting both the borrow rates and the reserve amount is more complicated than just adjusting one parameter.\n\n\nUnless we write a new interest rate algorithm, adjusting the reserve ratio affects the entire curve, not part of it. Most of the curve seems to work okay. It’s just the 90-100% area that I think needs some tweaking.\n\n\nI might have misunderstood your question. If I didn’t answer you, let me know.\\n\n\n\n dvf:\n\nIf USDT goes to $0.90, the whale can use USDC to repurchase this cheap USDT and pay off the loans, making a profit\n\n\nUSDT will lose value compared to what? If USDT is pegged to dollar.\nYou think that USDT will lose value on global level or inside our price feed?\nChanging interest rates will fix issue in situation when dollar pegged stablecoin is no more pegged to dollar?\nMy opinion:\nIf USDT lose value, reason for that can only be underlying collateral problems (problem with depositing dollars for minting USDT).\nI dont see that changing interest rates fix that problem\\nI’m not trying to fix what happens if a dollar pegged stable coin looses its peg.\nHowever the added risk of a possible peg loss makes lending USDT less attractive (you could loose the value of what you have lent out) and the same risk makes borrowing USDT more attractive (you might get to pay back your debt with something cheaper). These two result in a decrease in supply and an increase in demand, which pushes the interest rate upwards to find a balance.\nI want to be sure USDT interest rates have enough room up there.\n(USDT price is hardcoded to $1 in the Compound price feed. It won’t change without a governance vote. I’m not trying to fix anything with the oracles.)\\nIf the utilization is 100% I see no reason with the pressure on interest rates, simply pause option for borrowing Tether.\nIf pressure is put on users who already have borrowed Tether they will replace the borrowing position with another stablecoin. With this move, it will only create additional costs for users, and we know what the gas fee is.\nA similar situation was with DAI, now it is with USDT and with this move it will be transferred to USDC.\nAre there other options for this problem?\\nI am not all about the Tether FUD, treat the intrest rate the same as the others, i also think collateral should be allowed for tether\\nThis proposal doesn’t force the interest rate to a particular amount - in fact it does the opposite. The market sets the interest rates, and this proposal expand the range that the market can freely set the rate in.\\nWell if you phrase it like that …\nYou have more support. Market deregulation per say.\\ncUSDT has been having a wild ride. Here’s a utilization chart of the last 24 hours.\n\\nhow are you viewing this data? I like the chart.\nCalculate the spread between supply and borrow interest on that one and we will see.\\nI dont think we should touch the USDT interestratemodel, the market was relative small and it was easy to push to 100% utilization, by a single borrower even, but that should not be a reason to increase its rate as it never stayd at 100% for more than few minutes, USDT grow in supply from 200m~ to 300m~ in 2 week and its liquidity is also double since 2 week, as it grows it will be harder to push it to 100% utilization, there is nothing to be worried about, but keeping an eye on market is always nice."
  },
  {
    "number_of_comments": 45,
    "postid": "5620d9fe-ce8f-43d0-8a02-b4da4b9725cb",
    "posturl": "https://forum.makerdao.com/t/cd-recognition-submission-vigilant/20457",
    "combinedcontent": "CD Recognition Submission\n0x2474937cB55500601BCCE9f4cb0A0A72Dc226F61\n0x363d3BdB8c992F0b18577FA9Ce7886B49dcAd5a3:Growth CVC\n0xC2DAea14891Fc47Ee76368Ce7c54C7b200FbA672:KISS CVC\nCryptograpically signed CD Recognition Submission Message from the Ethereum address controlling Delegation Contract 1 20\nCryptograpically signed CD Recognition Submission Message from the Ethereum address controlling Delegation Contract 2 5\nCryptograpically signed CD Recognition Submission Message from the Ecosystem Actor Ethereum address 8\\nThank you for following us and feel free to share your research and neutral suggestions on CVC forum and CVC calls.\\nConfirming submission for the Arbitration Scope, you can view yourself under 5.2.4:\n  \n      \n\n      MIPs Portal\n  \n\n  \n    \n\nMIPs Portal 3\n\n  Maker Improvement Proposals are the preferred mechanism for improving both Maker Governance and the Maker Protocol.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\\nSummary of Votes on April 20, 2023\n\nOnboard Coinbase Custody (RWA014) as a new RWA Vault Type\n\nGrowth AVC: Vote Yes. This is a stepping stone toward the Liquid Reserve Implementation detailed in the RWA Scope and will earn 2.6% yield on otherwise idle assets.\nKISS AVC: Vote Yes. This is a stepping stone toward the Liquid Reserve Implementation detailed in the RWA Scope and will earn 2.6% yield on otherwise idle assets.\n\nHow Should Coinbase Custody Hold Deposited USDC\n 1\nGrowth AVC: Vote Cold  Storage. Prioritizing security is very important, especially when the lag times from cold storage are unlikely to be very bad anyway.\nKISS AVC: Vote Cold Storage. Prioritizing security is very important, especially when the lag times from cold storage are unlikely to be very bad anyway.\n\nDecentralized Collateral Parameter Changes\n\nGrowth AVC: Vote Yes. The changes seem reasonable. Better risk management and more revenue.\nKISS AVC: Vote Yes. The changes seem reasonable. Better risk management and more revenue.\\nSummary of Votes on April 24, 2023\n\nRatification Poll for MIP Amendment Subproposals (MIP102c2-SP2) - April 10, 2023\n\nGrowth AVC: Vote Yes. This bundle of changes includes many simplifications, including simplified organization/structure throughout documents, simplified advisory council structure, more congruence within  documents, and many clarifications. It also adds provisions around the Core Artificial Intelligence System (CAIS) which is a crucial part of the endgame that has a lot of potential to help Maker Core and SubDAOs optimize business processes. These provisions include a budget for CAIS. It also adds a budget of up to $5M for a rebrand. While this is steep, it is a maximum and hopefully won’t all be used. There won’t be another opportunity to rebrand as good as in this transition the DAO is going through, so it makes sense to allocate efforts to do it right. This rebrand will also be simplifying- one of the components of the rebrand is unifying the brands of the governance token and the stablecoin. This also adds content to resilience scope, including a 5m legal budget/year for claims and 2m/year for legal research. These are steep but very useful budgets that likely won’t be fully spent every year. This also changes the minimum MKR to be a CVC member from 0.001 MKR to 1 MKR, which is useful as a basic form of spam prevention. Finally, it adds Reserve Facilitators to help mitigate the risk of primary Facilitators becoming unavailable or unresponsive.\nA downside is that this MIP adds provisions around Accessibility Rewards without detailing enough why they are useful. It might be missing details on what must be done for these rewards- the current wording suggests that in the pregame, Ecosystem Actors can sign up and simply hold DAI - inside or outside of the DSR -  to earn extra yield on it. It also alludes to Accessibility CVC Subcommittees, which may suggest increased complication. Another downside is that there are slight bugs that were left alone or even introduced in this MIP. The upsides vastly outweigh these slight downsides.\nKISS AVC: Vote Yes. This bundle of changes includes many simplifications, including simplified organization/structure throughout documents, simplified advisory council structure, more congruence within  documents, and many clarifications. It also adds provisions around the Core Artificial Intelligence System (CAIS) which is a crucial part of the endgame that has a lot of potential to help Maker Core and SubDAOs optimize business processes. These provisions include a budget for CAIS. It also adds a budget of up to $5M for a rebrand. While this is steep, it is a maximum and hopefully won’t all be used. There won’t be another opportunity to rebrand as good as in this transition the DAO is going through, so it makes sense to allocate efforts to do it right. This rebrand will also be simplifying- one of the components of the rebrand is unifying the brands of the governance token and the stablecoin. This also adds content to resilience scope, including a 5m legal budget/year for claims and 2m/year for legal research. These are steep but very useful budgets that likely won’t be fully spent every year. This also changes the minimum MKR to be a CVC member from 0.001 MKR to 1 MKR, which is useful as a basic form of spam prevention. Finally, it adds Reserve Facilitators to help mitigate the risk of primary Facilitators becoming unavailable or unresponsive.\nA downside is that this MIP adds provisions around Accessibility Rewards without detailing enough why they are useful. It might be missing details on what must be done for these rewards- the current wording suggests that in the pregame, Ecosystem Actors can sign up and simply hold DAI - inside or outside of the DSR -  to earn extra yield on it. It also alludes to Accessibility CVC Subcommittees, which may suggest increased complication. Another downside is that there are slight bugs that were left alone or even introduced in this MIP. The upsides vastly outweigh these slight downsides.\\n\nRaising GSM Pause Delay, Recognized Delegate Compensation, DAI and MKR Streams, ESM Interaction Changes - April 5, 2023\n\nGrowth AVC: Voted in support of this Executive Proposal. Nothing out of the ordinary is here.\nKISS AVC: Voted in support of this Executive Proposal. Nothing out of the ordinary is here.\\nSummary of Weekly Polls -  April 24, 2023\n\nArbitration Scope Clarification Edit - Determining the Minimum Verified MKR Holding for CVC Members - April 24, 2023\n\nGrowth AVC: Vote Increase to 1 MKR. Initially voted in favor of this as a basic form of spam prevention in the monthly MIP102. Confirming in this weekly poll.\nKISS AVC: Vote Increase to 1 MKR. Initially voted in favor of this as a basic form of spam prevention in the monthly MIP102. Confirming in this weekly poll.\n\nOnboard PullUp as an Incubating Ecosystem Actor - April 24, 2023\n\nGrowth AVC: Vote Yes. PullUp is going to do a lot of crucial innovation engineering development for the endgame. As a team of ex-PECU senior developers, they have the expertise to do so. Onboarding them also helps satisfy the Flexible Smart Contract Development DAO Level Objective.\nKISS AVC: Vote Yes. PullUp is going to do a lot of crucial innovation engineering development for the endgame. As a team of ex-PECU senior developers, they have the expertise to do so. Onboarding them also helps satisfy the Flexible Smart Contract Development DAO Level Objective.\n\nEcosystem Scope Clarification Edit - Amend PullUp Ecosystem Actor MKR Allocation - April 24, 2023\n 1\nGrowth AVC: Vote Yes. This was the confirmed  intended MKR allocation.\nKISS AVC: Vote Yes. This was the confirmed  intended MKR allocation.\\nExecutive Proposal - April 26, 2023\n\nStability Fee Changes, Collateral Offboarding Preparation, Coinbase Custody Legal Documentation, DAO Resolutions, PE MKR Stream Cleanup - April 26, 2023\n\nGrowth AVC: Voted in support of this Executive Proposal. Nothing out of the ordinary is here.\nKISS AVC: Voted in support of this Executive Proposal. Nothing out of the ordinary is here.\\nExecutive Proposal - May 02, 2023\n\nSpark Lend D3M Onboarding - May 02, 2023\n\nGrowth AVC: Voted in support of this Executive Proposal. In line with scopes and previous polls.\nKISS AVC: Voted in support of this Executive Proposal. In line with scopes and previous polls.\nSummary of Weekly Polls - May 1, 2023\n\nProtocol Engineering Scope Clarification Edits - May 1, 2023\n 1\nGrowth AVC: Vote Yes. These changes ensure budget continuity for protocol engineering facilitators to continue to provide critical governance security functions.\nKISS AVC: Vote Yes. These changes ensure budget continuity for protocol engineering facilitators to continue to provide critical governance security functions.\\nExecutive Proposal - May 10, 2023\n\nRisk Parameter Changes, Increase Starknet Bridge Limit, DAI Transfer, Vesting Stream Management, DAO Resolution - May 10, 2023\n\nGrowth AVC: Voted in support of this Executive Proposal. Nothing out of the ordinary is here.\nKISS AVC: Voted in support of this Executive Proposal. Nothing out of the ordinary is here.\\nSummary of Weekly Polls - May 15, 2023\n\nOnboard GNO to Spark Protocol - May 15, 2023\n\nGrowth AVC: Vote Yes. This would allow the Maker ecosystem to keep GNO as a strategic asset, and the risk parameters are deemed reasonable. Onboarding of GNO to Spark - #9 by rema 1\nKISS AVC: Vote Yes. This would allow the Maker ecosystem to keep GNO as a strategic asset, and the risk parameters are deemed reasonable. Onboarding of GNO to Spark - #9 by rema 1\n\nAdjust BlockTower Credit Debt Ceilings - May 15, 2023\n\nGrowth AVC: Vote Yes. Blocktower has requested to consolidate vaults. I see no issue here.\nKISS AVC: Vote Yes. Blocktower has requested to consolidate vaults. I see no issue here.\n\nApprove Credix Finance Assessment Work - May 15, 2023\n\nGrowth AVC: Vote Yes. It would be good to assess this proposal and make progress. We should strive to evaluate Arranger proposals in a timely manner to not discourage future proposals.\nKISS AVC: Vote Yes. It would be good to assess this proposal and make progress. We should strive to evaluate Arranger proposals in a timely manner to not discourage future proposals.\n\nCAIS Bootstrap Funding - May 15, 2023\n\nGrowth AVC: Vote Yes. Ecosystem scope facilitators are requesting time sensitive access to a portion of a budget laid out in their scope to engage in partnerships that will help CAIS development get underway. CAIS has so much potential to simplify and standardize the governance improvement process. I trust more details on the partnerships will come when it makes sense.\nKISS AVC: Vote Yes. Ecosystem scope facilitators are requesting time sensitive access to a portion of a budget laid out in their scope to engage in partnerships that will help CAIS development get underway. CAIS has so much potential to simplify and standardize the governance improvement process. I trust more details on the partnerships will come when it makes sense.\\nExecutive Proposal - May 17, 2023\n\nOffboarding Multiple Vault Types, Constitutional Delegate Compensation, Multiple DAI Budget Streams, MKR Vesting - May 17, 2023\n 2\nGrowth AVC: Voted in support of this Executive Proposal. Nothing out of the ordinary is here.\nKISS AVC: Voted in support of this Executive Proposal. Nothing out of the ordinary is here.\\nSummary of Monthly Poll - May 8, 2023\n\nRatification Poll for MIP Amendment Subproposals (MIP102c2-SP7) - May 8, 2023\n 1\nGrowth AVC: Vote Yes. This was a very difficult vote.\n12 scopes down were simplified to 5 scopes. A lot of endgame structure, such as allocatorDAOs and facilitatorDAOs, was introduced in the atlas and scopes, with provisions for how to run until those are in place. A new type of conserver, a facilitator, was also introduced. The roles of types of conservers, such as AVC members and ADs, were clarified, as were the interactions between these types. More details regarding the Resilience Fund have been added, including the definition of a Policyholder role. There is now more emphasis on the two-stage bridge and more details around its design. Rates were restructured and simplified, centered on the definition of a “base rate”. Junior capital requirements were added to some ALM tiers. The long-duration ALM tier maximum allocations were reduced.\nThe Public Goods Purpose fund got bigger and was repurposed to open source AI. Ideally, this focus encompasses the prior focus on climate change and the AI helps us more effectively fight climate change as well. Complex tokenomics were introduced with emissions slightly higher than previously expected.\nADs are now forced to be anonymous. This change seems a bit too rapid. This does make sense though, as it was always the plan to have fully anonymous ADs in the endgame, and if this change didn’t happen now, the doxed ADs would only get more and more entrenched in the new system, and they would eventually oppose the change even stronger. Having all delegates be anonymous also strengthens the security of delegates who are already anonymous.\nOverall, there are much more good changes than bad changes in this MIP. It is very unfortunate that many great doxed delegates will be abruptly cut off, and it might be a good idea to establish a paid knowledge transition program so that their knowledge is not lost. It would be great if they continued to participate in some form in the Maker ecosystem after this vote, but I would not be surprised if they did not.\nKISS AVC: Vote Yes. This was a very difficult vote.\n12 scopes down were simplified to 5 scopes. A lot of endgame structure, such as allocatorDAOs and facilitatorDAOs, was introduced in the atlas and scopes, with provisions for how to run until those are in place. A new type of conserver, a facilitator, was also introduced. The roles of types of conservers, such as AVC members and ADs, were clarified, as were the interactions between these types. More details regarding the Resilience Fund have been added, including the definition of a Policyholder role. There is now more emphasis on the two-stage bridge and more details around its design. Rates were restructured and simplified, centered on the definition of a “base rate”. Junior capital requirements were added to some ALM tiers. The long-duration ALM tier maximum allocations were reduced.\nThe Public Goods Purpose fund got bigger and was repurposed to open source AI. Ideally, this focus encompasses the prior focus on climate change and the AI helps us more effectively fight climate change as well. Complex tokenomics were introduced with emissions slightly higher than previously expected.\nADs are now forced to be anonymous. This change seems a bit too rapid. This does make sense though, as it was always the plan to have fully anonymous ADs in the endgame, and if this change didn’t happen now, the doxed ADs would only get more and more entrenched in the new system, and they would eventually oppose the change even stronger. Having all delegates be anonymous also strengthens the security of delegates who are already anonymous.\nOverall, there are much more good changes than bad changes in this MIP. It is very unfortunate that many great doxed delegates will be abruptly cut off, and it might be a good idea to establish a paid knowledge transition program so that their knowledge is not lost. It would be great if they continued to participate in some form in the Maker ecosystem after this vote, but I would not be surprised if they did not.\\nExecutive Proposal - May 25, 2023\n\nCoinbase Custody (RWA014) Onboarding, Keeper Network Amendments, Core Artificial Intelligence System (CAIS) Bootstrap Funding, Spark Lend GNO Onboarding and Associated Changes - May 25, 2023\n\nGrowth AVC: Voted in support of this Executive Proposal. Nothing out of the ordinary is included in the intent of the proposal. Nothing jumped out as improper upon a very light review of the spell, and the addresses within match the addresses in the Executive Vote Implementation Process intent document. Disclaimer: I am not familiar with coding, so this is not an endorsement of the security of the spell’s code.\nKISS AVC: Voted in support of this Executive Proposal. Nothing out of the ordinary is included in the intent of the proposal. Nothing jumped out as improper upon a very light review of the spell, and the addresses within match the addresses in the Executive Vote Implementation Process intent document. Disclaimer: I am not familiar with coding, so this is not an endorsement of the security of the spell’s code.\nThe above disclaimer goes for all Executive Proposals voted for previously as well.\\nSummary of Weekly Polls - May 29, 2023\n\nReduce PSM-USDP-A Debt Ceiling to 0 - May 29, 2023\n\nGrowth AVC: Vote Yes. USDP is not important for liquidity and does not earn revenue. In support of reducing the debt ceiling to 0, and of RWA-involved entities redeeming USDP instead of USDC in the future.\nKISS AVC: Vote Yes. USDP is not important for liquidity and does not earn revenue. In support of reducing the debt ceiling to 0, and of RWA-involved entities redeeming USDP instead of USDC in the future.\n\nAdjust Spark Protocol DAI Interest Rate Strategy to Match the DSR - May 29, 2023\n\nGrowth AVC: Vote Yes. This is in line with the Stability Scope.\nKISS AVC: Vote Yes. This is in line with the Stability Scope.\n\nOnboard rETH to Spark Protocol - May 29, 2023\n\nGrowth AVC: Vote Yes. BA Labs affirmed that risk parameters and liquidity are sufficient, and opined that rETH is an important asset for the Endgame Ecosystem.\nKISS AVC: Vote Yes. BA Labs affirmed that risk parameters and liquidity are sufficient, and opined that rETH is an important asset for the Endgame Ecosystem.\n\nDeploy and Use Spark Protocol Executive Proxy - May 29, 2023\n\nGrowth AVC: Vote Yes. This would horizontally scale the spell system, increasing efficiency.\nKISS AVC: Vote Yes. This would horizontally scale the spell system, increasing efficiency.\n\nAdjust Spark Protocol D3M Parameters - May 29, 2023\n\nGrowth AVC: Vote Yes. As BA Labs said, the “proposed increase is sustainable and appropriate given the time passed and test being done regarding liquidation system and participation.”\nKISS AVC: Vote Yes. As BA Labs said, the “proposed increase is sustainable and appropriate given the time passed and test being done regarding liquidation system and participation.”\n\nNon-Scope Defined Parameter Changes - May 29, 2023\n\nGrowth AVC: Vote Yes. These changes seem reasonable.\nKISS AVC: Vote Yes. These changes seem reasonable.\n\nAdd BlockTower Andromeda (RWA015) as a new RWA Vault Type - May 29, 2023\n\nGrowth AVC: Vote Yes. This is in line with the Atlas / Alignment Artifacts and will generate good yield for the protocol, strengthening its endgame position.\nKISS AVC: Vote Yes. This is in line with the Atlas / Alignment Artifacts and will generate good yield for the protocol, strengthening its endgame position.\\nSummary of Weekly Polls - June 12, 2023\n\n** GUSD PSM Parameter Adjustments - June 12, 2023**\n\nGrowth AVC: Vote Yes. GUSD is tier 1 collateral that pays yield. For this reason, I am hesitant to support any reductions. However, it does have worse liquidity than USDC, so it probably would not defend the peg as well.\nVoting in support to align this with the next likely iteration of MIP104 and also primarily because this is just a debt ceiling decrease, so it will not lead to large immediate outflows. I would hope that any RWA deals would fully drain the USDP PSM before touching this one.\nKISS AVC: Vote Yes. GUSD is tier 1 collateral that pays yield. For this reason, I am hesitant to support any reductions. However, it does have worse liquidity than USDC, so it probably would not defend the peg as well.\nVoting in support to align this with the next likely iteration of MIP104 and also primarily because this is just a debt ceiling decrease, so it will not lead to large immediate outflows. I would hope that any RWA deals would fully drain the USDP PSM before touching this one.\\nExecutive Proposal - June 14, 2023\n\nBlockTower Andromeda (RWA015-A) Onboarding, Risk Parameter Changes, DSR Increase, Spark Protocol Proxy Activation, MKR Vesting Transfers - June 14, 2023\n\nGrowth AVC: Voted in support of this Executive Proposal. Nothing out of the ordinary is included in the intent of the proposal. Nothing jumped out as improper upon a very light review of the spell. Looking forward to the DSR increase. Disclaimer: I am not familiar with coding, so this is not an endorsement of the security of the spell’s code.\nKISS AVC: Voted in support of this Executive Proposal. Nothing out of the ordinary is included in the intent of the proposal. Nothing jumped out as improper upon a very light review of the spell. Looking forward to the DSR increase. Disclaimer: I am not familiar with coding, so this is not an endorsement of the security of the spell’s code.\\nExecutive Proposal - June 16, 2023\n\nOut-of-Schedule Executive Vote - RWA014-A (Coinbase Custody) DAO Resolution - June 16, 2023\n\nGrowth AVC: Voted in support of this Executive Proposal. This is an important interim solution to maintain the peg by maintaining a well-stocked USDC PSM.\nKISS AVC: Voted in support of this Executive Proposal. This is an important interim solution to maintain the peg by maintaining a well-stocked USDC PSM.\\nSummary of Monthly Polls - June 12, 2023\n\nRatification Poll for MIP Amendment Subproposal (MIP102c2-SP8) - June 12, 2023\n\nGrowth AVC: Vote Yes. There are many changes within that make sense. Chronicle Labs was added with a budget. A stablecoin recovery phrase was added. Ethereum Core Protocol development was organized into stages. A clause that requires RETH to reach a defined level of success to remain collateral was added. The number of delegate slots was reduced, likely to reduce the chances of paying some delegates to do the bare minimum. This reduces the slots across the board and applies to AVC members as well, which is not ideal. I would likely be in support of future edits that unlink these numbers or change the parameters of the link. Some budgets were consolidated into a Launch Project budget, and the overall budget here was effectively expanded by adding a sizeable MKR portion. I urge caution in getting to a place where spend is excessive but recognize that this particular budget expansion may make sense.\nKISS AVC: Vote Yes. There are many changes within that make sense. Chronicle Labs was added with a budget. A stablecoin recovery phrase was added. Ethereum Core Protocol development was organized into stages. A clause that requires RETH to reach a defined level of success to remain collateral was added. The number of delegate slots was reduced, likely to reduce the chances of paying some delegates to do the bare minimum. This reduces the slots across the board and applies to AVC members as well, which is not ideal. I would likely be in support of future edits that unlink these numbers or change the parameters of the link. Some budgets were consolidated into a Launch Project budget, and the overall budget here was effectively expanded by adding a sizeable MKR portion. I urge caution in getting to a place where spend is excessive but recognize that this particular budget expansion may make sense.\n\nRatification Poll for MIP Amendment Subproposal (MIP102c2-SP9) - June 12, 2023\n 1\nGrowth AVC: Vote No. I agree with concerns expressed by others about creating reliance on one particular platform and its ruleset for a bug bounty program. Maker should have control over the ruleset of bounty programs it implements.\nKISS AVC: Vote No. I agree with concerns expressed by others about creating reliance on one particular platform and its ruleset for a bug bounty program. Maker should have control over the ruleset of bounty programs it implements.\\nSummary of Weekly Polls - June 26, 2023\n\nSmart Burn Engine Launch Parameters - June 26, 2023\n\nGrowth AVC: Vote Yes. Excited for the burn to begin and for on chain liquidity to grow! The initial parameters seem reasonable.\nKISS AVC: Vote Yes. Excited for the burn to begin and for on chain liquidity to grow! The initial parameters seem reasonable.\\nExecutive Proposal - June 28, 2023\n\nBlockTower Andromeda Updates, GUSD PSM Parameter Changes, and Other Actions - June 28, 2023\n\nGrowth AVC: Voted in support of this Executive Proposal. Nothing out of the ordinary is included in the intent of the proposal. Disclaimer: I am not familiar with coding, so this is not an endorsement of the security of the spell’s code.\nKISS AVC: Voted in support of this Executive Proposal. Nothing out of the ordinary is included in the intent of the proposal. Disclaimer: I am not familiar with coding, so this is not an endorsement of the security of the spell’s code.\\nSummary of Weekly Polls - July 3, 2023\nDAO Resolution to Approve Legal Representation - July 3, 2023\n\nGrowth AVC: Vote Yes. This seems to be the best option to fight this case given the necessity of a quick response.\nKISS AVC: Vote Yes. This seems to be the best option to fight this case given the necessity of a quick response.\\nJoining Resiliency AVC\nAD Recognition Submission\n0x51f3067cB6a1185d1e8316332921d9501FC4c006:Resiliency AVC\nCryptographically signed AD Recognition Submission Message from the Ethereum address controlling Delegation Contract 3  2\nSevering Neutral GSL\nAD Recognition Submission\n0x363d3BdB8c992F0b18577FA9Ce7886B49dcAd5a3:Severed\nCryptographically signed AD Recognition Submission Message from the Ethereum address controlling Delegation Contract 1  6\\nExecutive Proposal - July 14, 2023\n\nBlockTower Andromeda Upgrade, Smart Burn Engine Deployment, Keeper Job Updates, Scope Defined Parameter Changes, Delegate Compensation, Ecosystem Actor and Core Unit Funding Updates, Spark Protocol Proxy Spell Execution - July 14, 2023\n\nKISS AVC: Voted in support of this Executive Proposal. Nothing out of the ordinary is included in the intent of the proposal. Excited for the burn to begin and for on chain liquidity to grow! Disclaimer: I am not familiar with coding, so this is not an endorsement of the security of the spell’s code.\nResiliency AVC: Would have voted in support of this Executive Proposal if there was MKR in this GSL. Nothing out of the ordinary is included in the intent of the proposal. Excited for the burn to begin and for on chain liquidity to grow! Disclaimer: I am not familiar with coding, so this is not an endorsement of the security of the spell’s code.\\nSummary of Weekly Polls - July 17, 2023\nDecrease the Harbor Trade Credit (RWA004-A) Debt Ceiling - July 17, 2023\n\nKISS AVC: Vote Yes. Reducing debt ceiling to 0 seems like a reasonable risk mitigation move here.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. Reducing debt ceiling to 0 seems like a reasonable risk mitigation move here.\\nSummary of Weekly Polls - July 24, 2023\nStability Scope Bootstrapping Edits - July 24, 2023 1\nKISS AVC: Vote Yes. The Enhanced DSR can grow utilization in the DSR, gain momentum for Endgame, and give valuable learnings about DSR demand elasticity.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. The Enhanced DSR can grow utilization in the DSR, gain momentum for Endgame, and give valuable learnings about DSR demand elasticity.\nNew Silver (RWA002-A) Restructuring and Parameter Changes - July 24, 2023\nKISS AVC: Vote Yes. The increased Stability Fee is welcome and the new debt ceiling isn’t overly substantial.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. The increased Stability Fee is welcome and the new debt ceiling isn’t overly substantial.\nAdjust Spark Protocol D3M Debt Ceiling - July 24, 2023\nKISS AVC: Vote Yes. It makes sense to support the growth of Spark, the first subDAO product, at a sustainable pace, especially in time for the EDSR going live.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. It makes sense to support the growth of Spark, the first subDAO product, at a sustainable pace, especially in time for the EDSR going live.\nSpark Protocol WETH and DAI Market Parameter Changes - July 24, 2023\nKISS AVC: Vote Yes.  These adjustments will provide a better user experience on Spark in time for the EDSR going live.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. These adjustments will provide a better user experience on Spark in time for the EDSR going live.\\nExecutive Proposal - August 2, 2023\n\nEnhanced Dai Savings Rate Activation, Spark Protocol Debt Ceiling Increase, RWA Vault Updates, AVC Member Compensation for Q2 2023, DAO Resolution for Monetalis Clydesdale, Launch Project Funding, Spark Proxy Spell Execution - August 2, 2023\n\nKISS AVC: Voted in support of this Executive Proposal. Nothing out of the ordinary is included in the intent of the proposal. Disclaimer: I am not familiar with coding, so this is not an endorsement of the security of the spell’s code.\nResiliency AVC: Would have voted in support of this Executive Proposal if there was MKR in this GSL. Nothing out of the ordinary is included in the intent of the proposal. Disclaimer: I am not familiar with coding, so this is not an endorsement of the security of the spell’s code.\\nSummary of Weekly Polls - August 7, 2023\nNon-Scope Defined Parameter Changes - wstETH-B DC-IAM Changes - August 7, 2023  1\nKISS AVC: Vote Yes. wstETH-B vaults currently represent a low market drawdown risk, since the collateralization ratio is on the higher end and many of the debt holders who have DAI in the DSR can repay with ease. This change allows more minting in these vaults.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. wstETH-B vaults currently represent a low market drawdown risk, since the collateralization ratio is on the higher end and many of the debt holders who have DAI in the DSR can repay with ease. This change allows more minting in these vaults.\nSmart Burn Engine Parameter Update - August 7, 2023\nKISS AVC: Vote Yes. This will enable the protocol to save on gas costs.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. This will enable the protocol to save on gas costs.\\nSummary of Weekly Polls - August 14, 2023\nStability Scope Bootstrapping Edits (EDSR Changes) - August 14, 2023 \nKISS AVC: Vote Yes. In favor of capping the EDSR to a reasonable rate, removing the possibility of rate arbitrage from staked ETH collateral, and continuing to prop Spark Protocol up via a retroactive airdrop.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. In favor of capping the EDSR to a reasonable rate, removing the possibility of rate arbitrage from staked ETH collateral, and continuing to prop Spark Protocol up via a retroactive airdrop.\\nExecutive Proposal - August 17, 2023\n\nEDSR Adjustment, Vault and Smart Burn Engine Parameter Updates, CRVV1ETHSTETH-A Offboarding, Delegate Compensation, and Other Changes - August 17, 2023\n\nKISS AVC: Voted in support of this Executive Proposal. Nothing out of the ordinary is included in the intent of the proposal. Disclaimer: I am not familiar with coding, so this is not an endorsement of the security of the spell’s code.\nResiliency AVC: Would have voted in support of this Executive Proposal if there was MKR in this GSL. Nothing out of the ordinary is included in the intent of the proposal. Disclaimer: I am not familiar with coding, so this is not an endorsement of the security of the spell’s code.\\nSummary of Weekly Polls - August 21, 2023\nReserve Governance Facilitator Appointment - August 21, 2023\nKISS AVC: Vote VoteWizard and Le Bateleur. This is the recommendation from the current facilitators at GovAlpha. VoteWizard has also built a great reputation, and La Bateleur has recently been doing so as well. On the other hand, DAO Masons has not made a presence until this vote, and JanSky was very clearly trying to farm a reputation with ChatGPT a few months ago and cannot be trusted.\nResiliency AVC: Would have voted VoteWizard and Le Bateleur if there was MKR in this GSL. This is the recommendation from the current facilitators at GovAlpha. VoteWizard has also built a great reputation, and La Bateleur has recently been doing so as well. On the other hand, DAO Masons has not made a presence until this vote, and JanSky was very clearly trying to farm a reputation with ChatGPT a few months ago and cannot be trusted.\nIncrease Spark Protocol wstETH Supply Cap - August 21, 2023\nKISS AVC: Vote Yes. This adjustment is needed to allow more deposits in the wstETH pool as that has reached capacity.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. This adjustment is needed to allow more deposits in the wstETH pool as that has reached capacity.\nSpark Protocol ETH Market Parameter Changes - August 21, 2023\nKISS AVC: Vote Yes. These changes increase ETH market efficiency while reducing the possibility of the lending pool being at 100% utilization rate. These changes have been assessed positively by BA Labs.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. These changes increase ETH market efficiency while reducing the possibility of the lending pool being at 100% utilization rate. These changes have been assessed positively by BA Labs.\\n\n\n\n vigilant:\n\nJanSky was very clearly trying to farm a reputation with ChatGPT a few months ago and cannot be trusted.\n\n\nUnless you’re Officer’s Notes, BowTiedPickle.eth, or Banteg, it’s very difficult to trust anons or even pseudonym characters.  Gradual anonymity is the best path forward. Regardless if one uses crayons, smoke fire, or chatterboxgpt to communicate.\nTBF, it’s hard for me to trust any of you.\nRegardless.\nKeep pushing forward.\n-Synchrotron\\nSummary of Monthly Polls - August 14, 2023\n\nRatification Poll for MIP Amendment Subproposal (MIP102c2-SP11) - August 14, 2023\n\nKISS AVC: Vote No. An alternative is being worked on behind the scenes.\nResiliency AVC : Would have voted no if there was MKR in this GSL. An alternative is being worked on behind the scenes.\n\nRatification Poll for MIP Amendment Subproposals - Article 1 Edits (MIP102c2-SP12) - August 14, 2023\n\nKISS AVC: Vote Yes. The KISS AVC did not express an explicit voting preference, but these changes seem to be an improvement in the right direction and will simplify the work that KISS AVC has to do when creating its own MIP102.\nResiliency AVC : Would have voted yes if there was MKR in this GSL. Resiliency AVC took the stance that AC requirements right now are too vague, and generally supports fleshing them out. This MIP102 accomplishes that.\n\nRatification Poll for MIP Amendment Subproposal (MIP102c2-SP13) - August 14, 2023\n\nKISS AVC: Vote Yes. The expansion of the Launch Project budget is massive but it may be necessary to see Endgame executed well. It will be interesting to see how the DSR spread differing from the decentralized collateral rebate plays out. In full support of the remaining changes.\nSummary of changes-\n\nRedenominates from 1:1200 to 1:12000 for NewGovToken. Redoniminates subDAO and miniDAO tokens by a factor of 10 similarly.\nChanges privacy requirements for ADs such that they are derecognized if there is evidence that they have poor Operational Security, rather than if their identity becomes known.\nReduces DSR spread from base rate from 0.6% to 0.5%\nDecreases ALM Tier 2 maximum slippage from 5% within 2 weeks to 1% within 2 weeks\nIncreases allowance of ALM Tier 2 or higher collateral from 75% to 78%\nRemoves ALM Tier 3 collateral, renames Tier 4 to Tier 3 and Tier 5 to Tier 4\nAdds USDP to the list of Cash Stablecoins- “USDP - Exposure to USDP is capped at 120 million USDP and exposure to USDP requires that a marketing reward of at least 2% is available.”\nReduces decentralized collateral rebate from 0.6% per year to 0.4%\nMassively expands the Launch Project budget- from 6.4M DAI to 20M DAI and from 3647 MKR to 5K MKR. Some of the launch project budget has been spent already also and this resets that, so the expansion is a bit bigger even.\n\nResiliency AVC : Would have voted yes if there was MKR in this GSL. The expansion of the Launch Project budget is massive but it may be necessary to see Endgame executed well. It will be interesting to see how the DSR spread differing from the decentralized collateral rebate plays out. In full support of the remaining changes.\nSummary of changes-\n\nRedenominates from 1:1200 to 1:12000 for NewGovToken. Redoniminates subDAO and miniDAO tokens by a factor of 10 similarly.\nChanges privacy requirements for ADs such that they are derecognized if there is evidence that they have poor Operational Security, rather than if their identity becomes known.\nReduces DSR spread from base rate from 0.6% to 0.5%\nDecreases ALM Tier 2 maximum slippage from 5% within 2 weeks to 1% within 2 weeks\nIncreases allowance of ALM Tier 2 or higher collateral from 75% to 78%\nRemoves ALM Tier 3 collateral, renames Tier 4 to Tier 3 and Tier 5 to Tier 4\nAdds USDP to the list of Cash Stablecoins- “USDP - Exposure to USDP is capped at 120 million USDP and exposure to USDP requires that a marketing reward of at least 2% is available.”\nReduces decentralized collateral rebate from 0.6% per year to 0.4%\nMassively expands the Launch Project budget- from 6.4M DAI to 20M DAI and from 3647 MKR to 5K MKR. Some of the launch project budget has been spent already also and this resets that, so the expansion is a bit bigger even.\n\\nSummary of Weekly Polls - August 28, 2023\n\nApprove DAO Resolution Pertaining to HV Bank (RWA009-A) - August 28, 2023\n\nKISS AVC: Vote Yes. This aligns with the Stability Scope which states that Legacy Legal Recourse Assets should be offboarded prior to the launch of NewChain.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. This aligns with the Stability Scope which states that Legacy Legal Recourse Assets should be offboarded prior to the launch of NewChain.\n\nDecrease Fortunafi (RWA005-A) Debt Ceiling - August 28, 2023\n\nKISS AVC: Vote Yes. This aligns with the Stability Scope which states that Legacy Legal Recourse Assets should be offboarded prior to the launch of NewChain.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. This aligns with the Stability Scope which states that Legacy Legal Recourse Assets should be offboarded prior to the launch of NewChain.\\nExecutive Proposal - August 30, 2023\n\nManagement of ConsolFreight (RWA003-A) Default, ESM Authorization, Chainlog Updates, Budget Management, Spark Proxy Spell - August 30, 2023\n\nKISS AVC: Voted in support of this Executive Proposal. Nothing out of the ordinary is included in the intent of the proposal. Disclaimer: I am not familiar with coding, so this is not an endorsement of the security of the spell’s code.\nResiliency AVC: Would have voted in support of this Executive Proposal if there was MKR in this GSL. Nothing out of the ordinary is included in the intent of the proposal. Disclaimer: I am not familiar with coding, so this is not an endorsement of the security of the spell’s code.\\nSummary of Weekly Polls - September 4, 2023\n\nAdjust Spark Protocol D3M Parameters - September 4, 2023\n\nKISS AVC: Vote Yes. The debt ceiling is maxed out and at capacity, so this is needed to support the growth of Spark. The increase in cooldown helps make this change sustainable.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. The debt ceiling is maxed out and at capacity, so this is needed to support the growth of Spark. The increase in cooldown helps make this change sustainable.\n\nAdjust Spark Protocol DAI Interest Rate Strategy Borrow Spread - September 4, 2023\n\nKISS AVC: Vote Yes. This increases revenue and adds needed risk to farming the airdrop.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. This increases revenue and adds needed risk to farming the airdrop.\n\nAdjust Spark Protocol Flash Loan Fee - September 4, 2023\n\nKISS AVC: Vote Yes. Flash loans are unlikely to be used if there are non-zero fees due to other free options, and this increased capability does not carry additional risk.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. Flash loans are unlikely to be used if there are non-zero fees due to other free options, and this increased capability does not carry additional risk.\n\nJAT1 to JAT2 Asset Reallocation - September 4, 2023\n\nKISS AVC: Vote Yes. This adds predictability to cash flows and is simpler. Hopefully this is a profitable move as well.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. This adds predictability to cash flows and is simpler. Hopefully this is a profitable move as well.\\nExecutive Proposal - September 13, 2023\n\nStability Scope Parameter Changes, Spark Protocol D3M Parameter Changes, Set Fortunafi Debt Ceiling to Zero DAI, DAO Resolution for HV Bank, Delegate Compensation and Other Actions - September 13, 2023\n\nKISS AVC: Voted in support of this Executive Proposal. Nothing out of the ordinary is included in the intent of the proposal. Disclaimer: I am not familiar with coding, so this is not an endorsement of the security of the spell’s code.\nResiliency AVC: Would have voted in support of this Executive Proposal if there was MKR in this GSL. Nothing out of the ordinary is included in the intent of the proposal. Disclaimer: I am not familiar with coding, so this is not an endorsement of the security of the spell’s code.\\nSummary of Weekly Polls - September 18, 2023\nActivate Gnosis Chain Instance of Spark Lend - September 18, 2023\nKISS AVC: Vote Yes. It is exciting to see the Spark multichain strategy in action. The risk parameters for this launch were deemed sufficient by BA labs.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. It is exciting to see the Spark multichain strategy in action. The risk parameters for this launch were deemed sufficient by BA labs.\\nSummary of Monthly Polls - September 11, 2023\n\nRatification Poll for MIP Amendment Subproposals - Article 1 Edits (MIP102c2-SP15) - September 11, 2023\n\nKISS AVC: Vote Yes. These changes, besides some grammatical fixes, were already implemented in other scopes.\nResiliency AVC : Would have voted yes if there was MKR in this GSL. These changes, besides some grammatical fixes, were already implemented in other scopes.\n\nRatification Poll for MIP Amendment Subproposal (MIP102c2-SP16) - September 11, 2023\n 1\nKISS AVC: Vote Yes. The biggest changes are that this adds operational security standards for facilitators, adds large facilitator budgets to set the standard that they should operate as teams, reduces the DSR spread, adds a borrow spread to Spark, adds a Spark revenue share ecosystem agreement with Aave.\nResiliency AVC : Would have voted yes if there was MKR in this GSL. The biggest changes are that this adds operational security standards for facilitators, adds large facilitator budgets to set the standard that they should operate as teams, reduces the DSR spread, adds a borrow spread to Spark, adds a Spark revenue share ecosystem agreement with Aave.\n\nRatification Poll for MIP Amendment Subproposals (MIP102c2-SP4) - September 11, 2023\n\nKISS AVC: Vote Yes. These changes seem reasonable and seem like an improvement. There were no significant, reasonable objections by Arrangers. The author has also already committed to further improving this in a future governance cycle.\nResiliency AVC : Would have voted yes if there was MKR in this GSL. These changes seem reasonable and seem like an improvement. There were no significant, reasonable objections by Arrangers. The author has also already committed to further improving this in a future governance cycle.\\nSummary of Weekly Polls - September 25, 2023\nReconfiguring RWA Allocator Vaults - September 25, 2023\nKISS AVC: Vote Yes. These line increases make sense, given that the Allocator Vaults remain bound and subject to the Stability Scope.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. These line increases make sense, given that the Allocator Vaults remain bound and subject to the Stability Scope.\\nExecutive Proposal - September 27, 2023\n\nDAO Resolution for Monetalis Clydesdale, HV Bank On-chain RWA Agreement Update, Fortunafi Vault Change, Trigger Spark Protocol Proxy Spell - September 27, 2023\n\nKISS AVC: Voted in support of this Executive Proposal. Nothing out of the ordinary is included in the intent of the proposal. A light review of the spell’s code shows that the ipfs hashes and spark spell address are as intended. Disclaimer: I am not familiar with coding, so this is not an endorsement of the security of the spell’s code.\nResiliency AVC: Would have voted in support of this Executive Proposal if there was MKR in this GSL. Nothing out of the ordinary is included in the intent of the proposal. A light review of the spell’s code shows that the ipfs hashes and spark spell address are as intended. Disclaimer: I am not familiar with coding, so this is not an endorsement of the security of the spell’s code.\\nSummary of Weekly Polls - October 2, 2023\nNon-Scope Defined Parameter Changes - WBTC DC-IAM Changes - October 2, 2023\nKISS AVC: Vote Yes. As BA Labs mentioned, recent WBTC minting activity has been low compared to what is currently available, so Gap should be lowered to minimize unnecessary risks for the protocol.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. As BA Labs mentioned, recent WBTC minting activity has been low compared to what is currently available, so Gap should be lowered to minimize unnecessary risks for the protocol.\nIncrease rETH Supply Cap to 60k - October 2, 2023\nKISS AVC: Vote Yes. rETH supply utilization on Spark is at 63%, so it makes sense to increase the supply cap there to enable more growth, especially since rETH is being offboarded in Maker Core.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. rETH supply utilization on Spark is at 63%, so it makes sense to increase the supply cap there to enable more growth, especially since rETH is being offboarded in Maker Core.\nOnboard USDC to SparkLend Ethereum and Activate USD eMode for USDC and sDAI Markets - October 2, 2023\nKISS AVC: Vote Yes. This and the USDT change below will enable more growth for Spark and for the DSR.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. This and the USDT change below will enable more growth for Spark and for the DSR.\nOnboard USDT to SparkLend Ethereum and Activate USD eMode - October 2, 2023\nKISS AVC: Vote Yes. This and the USDC change above will enable more growth for Spark and for the DSR.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. This and the USDC change above will enable more growth for Spark and for the DSR.\\nExecutive Proposal - October 11, 2023\n\nUSDP-PSM incentives, rETH initial offboarding, RWA vaults reconfiguration, various parameter changes, AVC and AD compensation, Facilitator and Ecosystem Actor compensation, Spark proxy-spell - October 11, 2023\n\nKISS AVC: Voted in support of this Executive Proposal. Nothing out of the ordinary is included in the intent of the proposal. A light review of the spell’s code shows that the resolution ipfs hash and spark spell address are as intended. Besides a small error surfaced by @cloaky, the addresses and amounts for payments and vesting streams are correct. Disclaimer: I am not familiar with coding, so this is not an endorsement of the security of the spell’s code.\nResiliency AVC: Would have voted in support of this Executive Proposal if there was MKR in this GSL. Nothing out of the ordinary is included in the intent of the proposal. A light review of the spell’s code shows that the resolution ipfs hash and spark spell address are as intended. Besides a small error surfaced by @cloaky, the addresses and amounts for payments and vesting streams are correct. Disclaimer: I am not familiar with coding, so this is not an endorsement of the security of the spell’s code.\\nExecutive Proposal - November 1, 2023\n\nSpark Protocol-Aave Revenue Share Payment, DAO Resolution for HV Bank, Immunefi Security Core Unit MKR Vesting Transfer and Chainlog Housekeeping - November 1, 2023\n\nKISS AVC: Voted in support of this Executive Proposal. Nothing out of the ordinary is included in the intent of the proposal. A light review of the spell’s code shows that the resolution ipfs hash, contract addresses, payment addresses, and payment amounts are as intended. Disclaimer: I am not familiar with coding, so this is not an endorsement of the security of the spell’s code.\nResiliency AVC: Would have voted in support of this Executive Proposal if there was MKR in this GSL. Nothing out of the ordinary is included in the intent of the proposal. A light review of the spell’s code shows that the resolution ipfs hash, contract addresses, payment addresses, and payment amounts are as intended. Disclaimer: I am not familiar with coding, so this is not an endorsement of the security of the spell’s code.\nSummary of Weekly Polls - November 6, 2023\nApprove DAO Resolution to Onboard Mars Foundation - November 6, 2023\nKISS AVC: Vote Yes. This onboards Mars Foundation as a legal entity to own and operate frontends controlled by Maker.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. This onboards Mars Foundation as a legal entity to own and operate frontends controlled by Maker.\nSparkLend Gnosis Chain - Increase wstETH Supply Cap to 10,000 wstETH - November 6, 2023\nKISS AVC: Vote Yes. The wstETH supply is near its cap and this would allow for more growth.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. The wstETH supply is near its cap and this would allow for more growth.\nSparkLend Ethereum - Set DAI Market Maximum Loan-to-Value to Zero Percent - November 6, 2023\nKISS AVC: Vote Yes. This is basically a housekeeping change that slightly reduces risk. DAI has an LTV of 0.01% already.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. This is basically a housekeeping change that slightly reduces risk. DAI has an LTV of 0.01% already.\nSparkLend Ethereum - Reactivate WBTC and Optimize Parameters for Current Market Conditions - November 6, 2023\nKISS AVC: Vote Yes. As long as exposure is kept limited to manage risk, WBTC is good collateral that can drive growth.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. As long as exposure is kept limited to manage risk, WBTC is good collateral that can drive growth.\nSparkLend Ethereum - Adjust Spark Protocol D3M Maximum Debt Ceiling - November 6, 2023\nKISS AVC: Vote Yes. This will help Spark grow and won’t limit growth when governance is slow-moving in December.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. This will help Spark grow and won’t limit growth when governance is slow-moving in December.\nSparkLend Ethereum - Increase rETH & wstETH Supply Caps - November 6, 2023\nKISS AVC: Vote Yes. This will help Spark grow and won’t limit growth when governance is slow-moving in December.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. This will help Spark grow and won’t limit growth when governance is slow-moving in December.\nSparkLend Ethereum & Gnosis Chain - Adjust ETH Market Interest Rate Models - November 6, 2023\nKISS AVC: Vote Yes. I trust the Spark team’s recommendations to adjust the interest rate models to fit market conditions.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. I trust the Spark team’s recommendations to adjust the interest rate models to fit market conditions.\\nExecutive Proposal - November 15, 2023\n\nSpark Proxy Spell, Increase Spark Lend Maximum Debt Ceiling, Launch Project Funding, Approve Updates to the HVBank Facility, Whistleblower Bounty, October 2023 Delegate Compensation - November 15, 2023\n 1\nKISS AVC: Voted in support of this Executive Proposal. Nothing out of the ordinary is included in the intent of the proposal. A light review of the spell’s code shows that the resolution ipfs hashes, spark spell address, payment addresses, and payment amounts are as intended. Disclaimer: I am not familiar with coding, so this is not an endorsement of the security of the spell’s code.\nResiliency AVC: Would have voted in support of this Executive Proposal if there was MKR in this GSL. Nothing out of the ordinary is included in the intent of the proposal. A light review of the spell’s code shows that the resolution ipfs hash, spark spell address, payment addresses, and payment amounts are as intended. Disclaimer: I am not familiar with coding, so this is not an endorsement of the security of the spell’s code.\\nSummary of Monthly Polls - November 13, 2023\nRatification Poll for MIP Amendment Subproposals - Article 1 Edits (MIP102c2-SP17) - November 13, 2023\nKISS AVC: Vote Yes. This MIP102 is derived from KISS’s Position Document. It adds specific desired advisory work.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. Increased specificity provides better guidance for people looking to do advisory work.\nRatification Poll for MIP Amendment Subproposal (MIP102c2-SP18) - November 13, 2023\nKISS AVC: Vote No. This on its own makes very little changes. However, if passed, it negates changes proposed MIP102c2-SP20 in overlapping sections. I think the changes proposed in MIP102c2-SP20 are stronger and open up levers we may use to increase the liquidity that effectively maintains the peg, which we may need, as the USDC PSM has been depleting.\nResiliency AVC: Would have voted no if there was MKR in this GSL. This on its own makes very little changes. However, if passed, it negates changes proposed MIP102c2-SP20 in overlapping sections. I think the changes proposed in MIP102c2-SP20 are stronger and open up levers we may use to increase the liquidity that effectively maintains the peg, which we may need, as the USDC PSM has been depleting.\nRatification Poll for MIP Amendment Subproposal (MIP102c2-SP20) - November 13, 2023\nKISS AVC: Vote Yes. This adds to the Atlas a section that requires ACs to focus on developing the Next Generation Atlas, which is crucial for the Endgame. It must happen before subDAOs can launch. It also adds a bug bounty program to the Atlas. The changes to the Stability Scope open up levers we may use to increase the liquidity that effectively maintains the peg, which we may need, as the USDC PSM has been depleting.\nResiliency AVC: Would have voted yes if there was MKR in this GSL. This adds to the Atlas a section that requires ACs to focus on developing the Next Generation Atlas, which is crucial for the Endgame. It must happen before subDAOs can launch. It also adds a bug bounty program to the Atlas. The changes to the Stability Scope open up levers we may use to increase the liquidity that effectively maintains the peg, which we may need, as the USDC PSM has been depleting."
  },
  {
    "number_of_comments": 12,
    "postid": "d8222aae-3deb-4a92-90aa-963067338734",
    "posturl": "https://www.comp.xyz/t/trueusd-proposal-update-proposal/1902",
    "combinedcontent": "Hello, Compound community，\nThank you! With the support of the community, TrueUSD has been added as a new market in Compound after gaining 100% support in the recent vote.\nIt has been a while since TUSD was added to Compound Finance and performed well as a new market. The circulation and active address number have increased dramatically in the last two months with billions of dollars in monthly transaction volume. Recently, we integrated Signet by Signature Bank as a new banking partner, providing additional real-time settlement options for TUSD users and holders.\nNow is an excellent time to update TUSD market parameters and make TUSD a more desirable vehicle for financial opportunity on Compound which also makes Compound an attractive Defi option for the growing TUSD community. Diversifying the portfolios and market shares of stable coins on Compound will provide the Compound community more options to park and secure their assets. As the first independently verified and live attested digital asset redeemable 1-for-1 for US Dollars, TUSD represents the best fit candidate for a diversified stable coin portfolio.\nTrueUSD would like to propose the following update:\n1） Improve the TUSD collateral rate to 75%\n2） Distribute COMP rewards to TUSD market participants.\nWe are planning the following parameters. Please make a comment and share your opinion with us.\nUpdate COMP speed for TUSD, USDC, USDT, DAI markets\nFrom\nComptroller._setCompSpeed(“cUSDC”, 67000000000000000)\nComptroller._setCompSpeed(“cUSDT”, 9650000000000000)\nComptroller._setCompSpeed(“cDAI”, 67000000000000000)\nTo\nComptroller._setCompSpeed(“cUSDC”, 63000000000000000)\nComptroller._setCompSpeed(“cUSDT”, 8825000000000000)\nComptroller._setCompSpeed(“cDAI”, 63000000000000000)\nComptroller._setCompSpeed(“cTUSD”, 8825000000000000)\nCalculated by the COMP distributed per day, it is 115 for TUSD, USDT market and 827 for USDC, DAI market.\nWe hope to hear from you and would love to provide any additional support or resource that could facilitate the execution of the proposal.\nIf you like to know more about TUSD, you can find us here \nTrueUSD.Website: https://trueusd.com/  2\nTwitter: https://twitter.com/tusd_official  1\nMedium: https://trueusd.medium.com/ 1\nTelegram EN: Telegram: Contact @TUSDofficial_EN  1\nTelegram CN: [Telegram: Contact @TUSDofficial_CN\nYours,\nTrueUSD\\n\n\nAgree 100% we need to distribute COMP reward to TUSD just like every other token in compound. Frankly i am not clear why it wasn’t included in the initial proposal for golive.\n\n\nFor Joyce/TUSD, you guys need to make the token much more accessible, liquid and convertible from popular wallets like coinbase.  As it stands the spread to convert to/from TUSD is insane for any volume, i can only swap batches of $25k ether per trade and even then, still losing thousands $ in slippage.  (I used binance’s wallet ).\n\n\nIt involves:  converting my tokens (ie usdc) to ether first (because there is only ether/tusd market that’s anywhere close to being liquid) → sending ether to binance wallet / swap (because most wallet include coinbase do not support TUSD swap) → convert from ether to TUSD in $25k chunks in multiple transactions while losing crap load $ in spread/slippage + fees →  send TUSD back to my original wallet…to be able to finally use it in compound\nvs something like USDC, which i can convert most tokens to/from USDC directly in my primary wallet and done in 1 step with very little slippage.\n\nDoes anyone know what happened to TUSD in the last few days, looks like the market liquidity went nuts resulting in the borrow api shooting up from <1% to 4% \n\n\\nI see that one address is currently supplying over 90% of the TUSD present in the cTUSD market:\n\n  \n      \n\n      DeBank\n  \n\n  \n    \n\nDeBank ｜ DeFi Wallet for Ethereum Users 19\n\n  A DeFi wallet for managaging and tracking your DeFi portfolio, with data and analytics for decentralized lending, stablecoins, margin trading and DEX projects.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nThey have also borrowed over 30k COMP in order to farm COMP rewards, with the COMP tokens held in the wallet and used to vote for the TUSD addition proposal 9.\nThe rest of the wallet’s behavior is consistent with farm and dump defi participation (Circular supply/borrow of the same assets on Aave, etc).\nIn light of these facts, I have a few questions:\n\nPlease confirm the owner of the above mentioned wallet. Is it Justin Sun or a related entity?\nDoes the wallet owner have any financial ties with TrustToken? Are they a beneficial owner of or under common control with TrustToken?\n\nIn the current state, I don’t see how offering COMP rewards for TUSD would be beneficial for Compound. It seems likely to increase rather than reduce ownership concentration in the protocol. And the behavior of the TrustToken side has been completely lacking in transparency.\\nIn December 2020, TrustToken announced:\n\nToday, we’re proud to announce TUSD will enter the next stage of its growth. Ownership of TUSD will be moving over to an Asia-based consortium that will be working with Tron to support TUSD across Ethereum, Tron and other blockchain networks.\n\nThis has been discussed on several forums including MakerDAO 5 and Aave 5. I also commented regarding the topic on Twitter in response to some misinformation.\n\n  \n\n      twitter.com\n  \n\n  \n    \nRyan Rodenbaugh 8\n@RyanRodenbaugh\n\n\n  @FrankResearcher @nomos_paradox @justinsuntron Just to clarify @nomos_paradox ‘s tweet.. TrustToken was not sold. The TUSD asset was \n\nMore detailed post https://t.co/MZUBnnTclo\n    \n      \n        Justin Sun \uD83C\uDD63 \uD83C\uDF1E @justinsuntron\n      \n\n      $TUSD APR on @CurveFinance is 11520.17% to 28800.42% @tusd_official #TUSD pic.twitter.com/1Ek1p9OKXL\n    \n\n\n\n  4:10 PM - 14 Mar 2021\n\n    \n      \n        \n      \n      1\n    \n\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nI believe @monet-supply 's questions requesting additional transparency are fair and warranted when deciding whether or not to incentivize the TUSD market.\\nWell that’s kind lame. So basically TUSD just took bunch of their own tokens and flooded compound making up 90%+ of its TUSD market to cast vote so they can earn compound….isn’t that precisely what you not suppose to do as a stable coin owner…and goes against what compound was intended…not to mention artificially gapped up the borrow rate for the rest of the folks who are actually trying to borrow TUSD to support it.\nThink I am just going to exit the whole position if a single player can manipulate a token like that.\\nThanks for your suggestions and interest in TUSD! We are actively working on partnering with more wallets, exchanges, and OTC platforms to support TUSD. We are proud of the progress we made recently and will include the details in our next reply. We also update everything on our Twitter twitter.com/tusd_official.\\nThanks for your due diligence. As for your questions:\n\nWho the wallet you speak of belongs to, we do not know. There is also no way for us to find out, as wallet owners are anonymous which honors the value of blockchain technology. How wallet owners wish to use their assets is also entirely up to them and to their own discretion.\nIf you mean to ask whether the funds in this wallet are TrustToken or TrueUSD’s, we can confirm NO. All TUSD are held in escrow accounts with 24/7 live on-chain attestations by Armanino, which you can see here: TrustExplorer - real-time audit 2. TUSD holders mint and redempt their TUSD directly with our financial institution partners, such as Silvergate, PrimeTrust, Signet, etc. TrueUSD or TrustToken have no access to their funds.\nAbout this address, thanks for sharing. We looked into more details, there are 37K COMP, 16512 AAVE and 11K WBTC and more. More importantly, he/she holds COMP rewards. We don’t see the farm and dump you mentioned. From what we do see, the wallet owner has borrowed about $1 billion and supplied $1.5 billion on AAVE, and surely Compound would like to see the same support for COMP.\nWe don’t want to pick sides and would want everyone to enjoy the self governing community Compound founders created. However I’m very curious, as a diehard Compound supporter yourself who is very active on the forum and cares about the health of the entire ecosystem, how much COMP have you held since COMP launched? and how much money you put into Compound? Let’s build the community together and welcome more whales and holders of any size.\n\nIn light of your concerns regarding a wallet address, yes, we can all see this wallet belongs to a whale on Compound, AAVE, and other defi platforms. Whales in any ecosystem expands the platform’s strength and growth, so we are happy to see a whale using COMP to obtain TUSD, USDC, DAI or any form of stablecoin he/she would like. A higher utilization rate will incentivize more assets to join Compound. Isn’t this what we all want to see?\\nFrom a risk management point of view, having a diverse stablecoin options is beneficial for funds safety on a decentralized lending platform.Relying on USDC as the only regulated stable coin backed by the US dollar on the platform is not sustainable. It’s reasonable to support TUSD, a well-performing asset market, considering its total supply and total borrow rates.\nFrom the Compound growth point of view, the current TVL on Compound is lower than AAVE. Our collective goal is to boost the TVL and make COMP holders proud.\nThis is also why diversification of multiple stablecoins and additional rewards activities on Compound should be encouraged to attract more users to participate on Compound. If you are a Compound lover, and wish to see the community flourish, you want to see holders of any size find their spot in Compound. The amazing thing about Defi is that it is built on liquidity and cash flow, funds moving around means the platform is active and energetic.\nFrom TUSD point of view, we would like to see more whales come over to where TUSD is integrated, as we want to see our partners flourish as well.\nMoreover, TUSD strives to expand into more quality platforms and applications. TUSD market cap and holders have seen rapid growth in the past quarter, and the momentum is expected to be maintained if not expedited. To name a few, other than the Compound market, TUSD has opened a metapool on Curve.fi with over 4M in votes from veCRV holders. TUSD is also exploring partnerships with various projects on BSC, including listing on Ellipsis and adding TUSD in the Syrup pool on PancakeSwap. TUSD also launched on TRON and joined SUN.io 1 and the JUST DeFi ecosystem. To facilitate the use of TUSD and make TUSD more accessible for users, TUSD works with leading mainstream exchanges with trading pairs and rewarding activities. A recent example would with Binance on their first NFT auction campaign, where rewards are given in TUSD. We will share all the latest TUSD progress whenever one is finalized.\\nMost importantly, TrueUSD’s commitment to transparency for our community remains strong and will not change. Under Techteryx ownership led by Ms Jennifer Jiang, a business executive with extensive experience in traditional finance, currently worth over US$3.0 billion and is deeply connected with traditional industries.\nTrueUSD still prioritizes trust and transparency with four key points:\n\nThe same TrustToken team will continue to be deeply involved in running TUSD and for compliance and underlying reserve management.\nThe underlying reserves will continue to be audited and attested to on a live basis by Armanino LLP, a top 25 accounting and audit firm based in the US (https://www.armaninollp.com/) who, in addition to live attestations, also provide “proof of reserves” audits for exchanges (Proof of Reserves | Armanino).\nAll TUSD funds are held at fully licensed and regulated banks and trust companies in the United States and Hong Kong, including PrimeTrust (https://www.primetrust.com/ ), Silvergate Bank (https://www.silvergatebank.com/), First Digital (https://1stdigital.com/ ), and Signature Bank (www.signatureny.com). All funds are held in FBO accounts for the benefit of TUSD token holders and cannot, under any circumstances, be withdrawn by its owners for any proprietary use. All minting and redemptions of TUSD will continue to be verified by these custodians and banks regulated by U.S. and Hong Kong regulators before any TUSD can be created or burnt.\nTrueUSD private keys are managed in the same way as before, by the new TUSD management team with multisignature, a Gnosis SAFE (gnosis-safe.io ) solution.\n\nI again emphasize TUSD’s priority on transparency and security, and having multiple third-parties for fund attestations and escrow is evident to that. Here on Compound, we aim to help each other mutually grow and serve our communities!\\nUnderstand your concerns regarding a single player manipulating a market. As you can see from my message above, we want to encourage more whales to come over to help our partner ecosystems grow, and TUSD funds are fully in escrow and attested.\\nI would just like to confirm a couple of the points @monet-supply brings up here which seem most important:\n\nThe account is supplying close to 116m TUSD out of 121m total supply in Compound\nThey have borrowed over 30k COMP, used for voting in the TUSD proposal.\n\nAttaching some charts that show the account’s activity, which anyone can verify here: https://mariorz.github.io/compcharts/?0x3ddfa8ec3052539b6c9549f12cea2c295cff5296 6\n\nvisualization - 2021-06-29T110826.722820×783 43 KB\n\\nthey borrowed comp to vote, not farm\\nTechnically they’re doing both  - Compound | Profile - 0x3ddfa8ec3052539b6c9549f12cea2c295cff5296 12\n\nimage749×194 13.3 KB\n"
  },
  {
    "number_of_comments": 17,
    "postid": "e6fd5f50-25fb-4db2-9122-85393139993c",
    "posturl": "https://www.comp.xyz/t/gauntlet-weekly-market-updates-arbitrum-usdc/4552",
    "combinedcontent": "[Gauntlet] Weekly Market Update: Arbitrum USDC (7/21/23 - 7/28/23)\nGauntlet would like to provide the community with an update on metrics from the Arbitrum USDC comet over the past week and will include any relevant recommendations.\nSimple Summary\n\nGauntlet recommends decreasing the daily COMP supply rewards from 34.73 to 10 (~$614k annual savings). Compound Labs is working on launching a native USDC comet on Arbitrum, to migrate liquidity from this comet to be deprecated to the new comet.\nUSDC borrows are up 33.6%.\nUSDC supply is down 6.4%.\nThe comet lost $663 in USDC reserves over the past week, with an average reserve growth of -56.4%.\nThe comet distributed $19.17k COMP rewards over the past week, for a Net Protocol Profit of -$19.83k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week.\nMarket Growth\nScreen Shot 2023-07-28 at 9.23.55 PM1794×498 63.8 KB\nTotal Collateral (USD) is up 38.0%, from $6.08M to $8.39M.\nScreen Shot 2023-07-28 at 9.24.21 PM1788×492 68.3 KB\nUSDC Borrows are up 33.6%, from $2.32M to $3.10M.\nScreen Shot 2023-07-28 at 9.24.43 PM1798×506 67.1 KB\nUSDC Supply is down 6.4%, from $16.28M to $15.24M.\nSupply Caps\nScreen Shot 2023-07-28 at 9.08.12 PM3536×618 96.8 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-07-28 at 4.48.05 PM3544×844 160 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-07-28 at 9.25.53 PM1698×432 29.3 KB\nThe minimum USDC utilization was 14.1%, and the maximum was 21.2%.\nThe minimum USDC reserve growth was -62.9%, and the maximum was -44.9%. The average USDC reserve growth was -56.4%.\nScreen Shot 2023-07-28 at 9.27.25 PM1678×480 37.8 KB\nThe comet lost $663 in USDC reserves while distributing $19.17k COMP rewards, for a weekly Net Protocol Profit of -$19.83k.\nRecommendations\nGauntlet recommends decreasing the daily COMP supply rewards from 34.73 to 10 (~$614k annual savings). Compound Labs is working on launching a native USDC comet on Arbitrum, to migrate liquidity from this comet to be deprecated to the new comet.\nCurrently, the Earn Distribution is 6.09%, with $14.05M USDC supply and 31.27% utilization. The USDC supply is overly incentivized, losing $19.17k daily COMP rewards. Given the current $14.05M USDC supply, decreasing the daily COMP supply rewards to 10 will dilute the Earn Distribution to 1.77%, resulting in a Net Earn APR of 2.78%. If the USDC supply decreases to $10M, the resulting Earn Distribution will increase to 2.48%, resulting in a Net Earn APR of 3.49%, utilization of 43.9%, and Borrow APR of 3.04%.\\nGauntlet Weekly Market Update: Arbitrum USDC (7/28/23 - 8/3/23)\nGauntlet would like to provide the community with an update on metrics from the Arbitrum USDC comet over the past week.\nSimple Summary\n\nIn our previous update, we recommended decreasing the daily COMP supply rewards from 34.73 to 10 (~$614k annual savings), due to low utilization and the upcoming launch of a native USDC comet on Arbitrum. We will create an on-chain proposal next week.\nUSDC Borrows are up 61.6%, from $3.10M to $5.01M.\nUSDC Supply is down 9.4%, from $15.24M to $13.80M.\nUSDC utilization increased from 20.2% to 36.3%.\nThe comet lost $551 USDC reserves, with an average reserve growth of -31.8%.\nThe comet distributed $16.30k COMP rewards for a weekly Net Protocol Profit of -$16.85k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week.\nMarket Growth\nScreen Shot 2023-08-04 at 9.28.52 AM1798×484 61.8 KB\nTotal Collateral (USD) is up 48.2%, from $7.97M to $11.81M.\nScreen Shot 2023-08-04 at 9.29.12 AM1800×492 68.4 KB\nUSDC Borrows are up 61.6%, from $3.10M to $5.01M.\nScreen Shot 2023-08-04 at 9.29.30 AM1796×498 64.5 KB\nUSDC Supply is down 9.4%, from $15.24M to $13.80M.\nScreen Shot 2023-08-04 at 9.30.18 AM1674×454 33.3 KB\nUSDC utilization increased from 20.2% to 36.3% due to increased USDC borrows and decreased USDC supply.\nSupply Caps\nScreen Shot 2023-08-04 at 9.31.04 AM3518×612 98.5 KB\nAs seen above, ARB (100%) is the only asset with a supply cap utilization > 75%.\nScreen Shot 2023-08-04 at 9.31.24 AM3546×838 163 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-08-04 at 9.32.56 AM1670×436 30.1 KB\nThe minimum USDC utilization was 20.2%, and the maximum was 36.3%.\nThe minimum USDC reserve growth was -47.3%, and the maximum was -17.3%. The average USDC reserve growth was -31.8%.\nScreen Shot 2023-08-04 at 9.33.14 AM1668×476 36.2 KB\nThe comet lost $551 USDC reserves while distributing $16.30k COMP rewards for a weekly Net Protocol Profit of -$16.85k.\\nWe put up the on-chain proposal here  2. Voting begins in 1 day and lasts for 3 days.\\n[Gauntlet] Weekly Market Update: Arbitrum USDC (8/4/23 - 8/10/23)\nGauntlet would like to provide the community with an update on metrics from the Arbitrum USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are unchanged, staying constant at $5.01M.\nUSDC Supply is down 4.9%, from $13.80M to $13.12M.\nUSDC utilization increased from 35.8% to 37.9%.\nThe comet lost $438 USDC reserves, with an average reserve growth of -16.4%.\nThe comet distributed $13.67k COMP rewards for a weekly Net Protocol Profit of -$14.11k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week.\nMarket Growth\nScreen Shot 2023-08-11 at 10.17.10 PM1798×484 61.7 KB\nTotal Collateral (USD) is up 3.0%, from $12.04M to $12.40M.\nScreen Shot 2023-08-11 at 10.17.33 PM1798×486 68.3 KB\nUSDC Borrows are unchanged, staying constant at $5.01M.\nScreen Shot 2023-08-11 at 10.17.52 PM1804×490 67.3 KB\nUSDC Supply is down 4.9%, from $13.80M to $13.12M.\nScreen Shot 2023-08-11 at 10.18.19 PM1646×462 38.1 KB\nUSDC utilization increased from 35.8% to 37.9%.\nSupply Caps\nScreen Shot 2023-08-11 at 10.25.20 PM3542×622 100 KB\nAs seen above, ARB (100%) is the only asset with a supply cap utilization > 75%.\nScreen Shot 2023-08-11 at 10.24.56 PM3550×838 157 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-08-11 at 10.12.56 PM1672×432 37.6 KB\nThe minimum USDC utilization was 34.8%, and the maximum was 39.2%.\nThe minimum USDC reserve growth was -19.7%, and the maximum was -13.2%. The average USDC reserve growth was -16.4%.\nScreen Shot 2023-08-11 at 10.13.22 PM1672×464 32.3 KB\nThe comet lost $438 USDC reserves while distributing $13.67k COMP rewards for a weekly Net Protocol Profit of -$14.11k.\\n[Gauntlet] Arbitrum v3 USDC Update (08/11/2023 - 08/17/2023)\nGauntlet would like to provide the community with an update on metrics from the Arbitrum v3 USDC comet over the past week.\nSimple Summary\n\n\nUSDC Borrows are down 20.26%, from $5.01M to $4.0M.\n\n\nUSDC Supply is down 18.63%, from $13.12M to $10.68M.\n\n\nUSDC utilization increased 7.11%, from 38.2% to 40.9%.\n\n\nThe minimum USDC reserve growth was -14.1%, and the maximum was 2.7%. The average USDC reserve growth was -7.1%.\n\n\nThe comet lost $0.2k USDC reserves while distributing $12.38k COMP rewards for a weekly Net Protocol Profit of -$12.57k.\n\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-08-18 at 8.10.06 AM1802×478 62.9 KB\nTotal Collateral (USD) is down 7.77%, from $11.4M to $10.51M.\nScreen Shot 2023-08-18 at 8.10.29 AM1800×490 70.5 KB\nUSDC Supply is down 18.63%, from $13.12M to $10.68M.\nScreen Shot 2023-08-18 at 8.10.52 AM1798×472 69.7 KB\nUSDC Borrows are down 20.26%, from $5.01M to $4.0M.\nScreen Shot 2023-08-18 at 8.11.15 AM1654×454 37.5 KB\nUSDC utilization increased 7.11%, from 38.2% to 40.9%.\nScreen Shot 2023-08-18 at 8.31.21 AM3546×614 97.5 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-08-18 at 8.30.56 AM3536×840 170 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-08-18 at 8.12.49 AM1674×422 33.1 KB\nThe minimum USDC utilization was 38.2%, and the maximum was 52.5%.\nThe minimum USDC reserve growth was -14.1%, and the maximum was 2.7%. The average USDC reserve growth was -8.1%.\nScreen Shot 2023-08-18 at 1.22.43 PM1656×448 31.1 KB\nThe comet lost $0.2k USDC reserves while distributing $12.38k COMP rewards for a weekly Net Protocol Profit of -$12.57k.\\n[Gauntlet] Arbitrum v3 USDC Update (08/18/2023 - 08/24/2023)\nGauntlet would like to provide the community with an update on metrics from the Arbitrum v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are down 8.26%, from $4.0M to $3.67M.\nUSDC Supply is down 27.37%, from $10.68M to $7.76M.\nUSDC utilization increased 26.32%, from 37.4% to 47.3%.\nThe minimum USDC reserve growth was -19.3%, and the maximum was -0.3%. The average USDC reserve growth was -7.1%.\nThe comet lost $0.15k USDC reserves while distributing $2.96k COMP rewards for a weekly Net Protocol Profit of -$3.11k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-08-25 at 4.14.02 PM1786×462 62 KB\nTotal Collateral (USD) is down 1.06%, from $9.88M to $9.78M.\nScreen Shot 2023-08-25 at 4.14.45 PM1788×464 68.3 KB\nUSDC Supply is down 27.37%, from $10.68M to $7.76M.\nScreen Shot 2023-08-25 at 4.15.07 PM1786×484 68.7 KB\nUSDC Borrows are down 8.26%, from $4.0M to $3.67M.\nScreen Shot 2023-08-25 at 4.15.34 PM1644×452 37.3 KB\nUSDC utilization increased 26.32%, from 37.4% to 47.3%.\nSupply Caps\nScreen Shot 2023-08-25 at 4.10.35 PM3540×620 97.9 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-08-25 at 4.10.51 PM3534×840 161 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-08-25 at 4.16.06 PM1660×424 35.6 KB\nThe minimum USDC utilization was 35.0%, and the maximum was 49.7%.\nThe minimum USDC reserve growth was -19.3%, and the maximum was -0.3%. The average USDC reserve growth was -7.1%.\nScreen Shot 2023-08-25 at 4.16.32 PM1668×460 33.9 KB\nThe comet lost $0.15k USDC reserves while distributing $2.96k COMP rewards for a weekly Net Protocol Profit of -$3.11k.\\n[Gauntlet] Arbitrum v3 USDC Update (08/25/2023 - 08/31/2023)\nGauntlet would like to provide the community with an update on metrics from the Arbitrum v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are up 9.80%, from $3.67M to $4.03M.\nUSDC Supply is down 17.59%, from $7.76M to $6.39M.\nUSDC utilization increased 33.24%, from 47.3% to 63.0%.\nThe minimum USDC reserve growth was -3.0%, and the maximum was 14.8%. The average USDC reserve growth was 4.4%.\nThe comet accumulated $0.13k USDC reserves while distributing $2.95k COMP rewards for a weekly Net Protocol Profit of $-2.82k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-09-01 at 6.56.13 AM1798×490 62.2 KB\nTotal Collateral (USD) is up 7.84%, from $9.66M to $10.41M.\nScreen Shot 2023-09-01 at 6.56.37 AM1790×500 68.4 KB\nUSDC Supply is down 17.59%, from $7.76M to $6.39M.\nScreen Shot 2023-09-01 at 6.57.01 AM1788×480 68.5 KB\nUSDC Borrows are up 9.80%, from $3.67M to $4.03M.\nScreen Shot 2023-09-01 at 6.57.42 AM1660×456 35 KB\nUSDC utilization increased 33.24%, from 47.3% to 63.0%.\nSupply Caps\nScreen Shot 2023-09-01 at 7.01.00 AM3560×618 98.8 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-09-01 at 7.02.29 AM3546×844 151 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-09-01 at 6.58.03 AM1660×424 32.3 KB\nThe minimum USDC utilization was 47.3%, and the maximum was 66.1%.\nThe minimum USDC reserve growth was -3.0%, and the maximum was 14.8%. The average USDC reserve growth was 4.4%.\nScreen Shot 2023-09-01 at 6.58.28 AM1672×452 32.2 KB\nThe comet accumulated $0.13k USDC reserves while distributing $2.95k COMP rewards for a weekly Net Protocol Profit of $-2.82k.\\n[Gauntlet] Arbitrum v3 USDC Update (09/01/2023 - 09/07/2023)\nGauntlet would like to provide the community with an update on metrics from the Arbitrum v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are down 18.62%, from $4.03M to $3.28M.\nUSDC Supply is down 36.86%, from $6.39M to $4.04M.\nUSDC utilization increased 28.93%, from 63.0% to 81.2%.\nThe minimum USDC reserve growth was 2.7%, and the maximum was 23.0%. The average USDC reserve growth was 14.9%.\nThe comet accumulated $0.43k USDC reserves while distributing $0.0k COMP rewards for a weekly Net Protocol Profit of $0.43k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-09-08 at 8.09.41 AM1798×494 63.3 KB\nTotal Collateral (USD) is down 18.22%, from $9.86M to $8.06M.\nScreen Shot 2023-09-08 at 8.10.16 AM1794×472 66.4 KB\nUSDC Supply is down 36.86%, from $6.39M to $4.04M.\nScreen Shot 2023-09-08 at 8.10.41 AM1792×498 68.7 KB\nUSDC Borrows are down 18.62%, from $4.03M to $3.28M.\nScreen Shot 2023-09-08 at 8.11.02 AM1652×458 36.5 KB\nUSDC utilization increased 28.93%, from 63.0% to 81.2%.\nSupply Caps\nScreen Shot 2023-09-08 at 8.12.48 AM3544×632 98.3 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-09-08 at 8.13.23 AM3532×842 156 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-09-08 at 8.14.28 AM1660×420 38.6 KB\nThe minimum USDC utilization was 63.0%, and the maximum was 84.9%.\nThe minimum USDC reserve growth was 2.7%, and the maximum was 23.0%. The average USDC reserve growth was 14.9%.\nScreen Shot 2023-09-08 at 8.14.53 AM1644×452 34.1 KB\nThe comet accumulated $0.43k USDC reserves while distributing $0.0k COMP rewards for a weekly Net Protocol Profit of $0.43k.\\n[Gauntlet] Arbitrum v3 USDC.e Update (09/08/2023 - 09/14/2023)\nGauntlet would like to provide the community with an update on metrics from the Arbitrum v3 USDC.e comet over the past week.\nSimple Summary\n\nUSDC Borrows are down 29.52%, from $3.28M to $2.31M.\nUSDC Supply is down 26.27%, from $4.04M to $2.98M.\nUSDC utilization decreased 4.40%, from 81.2% to 77.6%.\nThe minimum USDC reserve growth was -11.1%, and the maximum was 24.3%. The average USDC reserve growth was 17.3%.\nThe comet accumulated $0.36k USDC reserves while distributing $0.0k COMP rewards for a weekly Net Protocol Profit of $0.36k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-09-15 at 12.21.46 PM1782×484 63.6 KB\nTotal Collateral (USD) is down 23.69%, from $7.57M to $5.78M.\nScreen Shot 2023-09-15 at 12.22.20 PM1790×472 67.6 KB\nUSDC Supply is down 26.27%, from $4.04M to $2.98M.\nScreen Shot 2023-09-15 at 12.22.40 PM1790×488 68.3 KB\nUSDC Borrows are down 29.52%, from $3.28M to $2.31M.\nScreen Shot 2023-09-15 at 12.22.59 PM1664×458 37.9 KB\nUSDC utilization decreased 4.40%, from 81.2% to 77.6%.\nSupply Caps\nScreen Shot 2023-09-15 at 12.26.17 PM3792×612 98.1 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-09-15 at 12.26.40 PM3796×842 154 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-09-15 at 12.23.19 PM1676×426 35.5 KB\nThe minimum USDC utilization was 74.0%, and the maximum was 93.0%.\nThe minimum USDC reserve growth was -11.1%, and the maximum was 24.3%. The average USDC reserve growth was 17.3%.\nScreen Shot 2023-09-15 at 12.23.39 PM1656×468 33.2 KB\nThe comet accumulated $0.36k USDC reserves while distributing $0.0k COMP rewards for a weekly Net Protocol Profit of $0.36k.\\n[Gauntlet] Arbitrum v3 USDC Update (09/15/2023 - 09/21/2023)\nGauntlet would like to provide the community with an update on metrics from the Arbitrum v3 USDC comet over the past week.\nSimple Summary\n\nUSDC Borrows are down 1.55%, from $1.99M to $1.95M.\nUSDC Supply is down 6.98%, from $2.88M to $2.68M.\nUSDC utilization increased 5.73%, from 77.6% to 82.1%.\nThe minimum USDC reserve growth was 2.8%, and the maximum was 23.1%. The average USDC reserve growth was 14.7%.\nThe comet accumulated $0.29k USDC reserves while distributing $0.0k COMP rewards for a weekly Net Protocol Profit of $0.29k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-09-22 at 10.19.03 AM1790×498 65.2 KB\nTotal Collateral (USD) is down 0.52%, from $5.71M to $5.68M.\nScreen Shot 2023-09-22 at 10.19.21 AM1792×486 66.1 KB\nUSDC Supply is down 6.98%, from $2.88M to $2.68M.\nScreen Shot 2023-09-22 at 10.19.38 AM1796×494 65.2 KB\nUSDC Borrows are down 1.55%, from $1.99M to $1.95M.\nScreen Shot 2023-09-22 at 10.19.59 AM1674×452 35.2 KB\nUSDC utilization increased 5.73%, from 77.6% to 82.1%.\nSupply Caps\nScreen Shot 2023-09-22 at 10.22.53 AM3794×612 97.8 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-09-22 at 10.23.26 AM3796×846 182 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-09-22 at 10.20.20 AM1658×428 33.7 KB\nThe minimum USDC utilization was 77.2%, and the maximum was 84.9%.\nThe minimum USDC reserve growth was 2.8%, and the maximum was 23.1%. The average USDC reserve growth was 14.7%.\nScreen Shot 2023-09-22 at 10.20.42 AM1650×452 33.5 KB\nThe comet accumulated $0.29k USDC reserves while distributing $0.0k COMP rewards for a weekly Net Protocol Profit of $0.29k.\\n[Gauntlet] Arbitrum v3 USDC.e Update (09/22/2023 - 09/28/2023)\nGauntlet would like to provide the community with an update on metrics from the Arbitrum v3 USDC.e comet over the past week.\nSimple Summary\n\nUSDC Borrows are down 4.27%, from $1.95M to $1.87M.\nUSDC Supply is down 3.64%, from $2.68M to $2.58M.\nUSDC utilization decreased 0.36%, from 82.1% to 81.8%.\nThe minimum USDC reserve growth was 6.7%, and the maximum was 23.9%. The average USDC reserve growth was 13.5%.\nThe comet accumulated $0.27k USDC reserves while distributing $0.0k COMP rewards for a weekly Net Protocol Profit of $0.27k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-09-29 at 12.10.28 PM1790×482 64.3 KB\nTotal Collateral (USD) is down 0.84%, from $5.47M to $5.43M.\nScreen Shot 2023-09-29 at 12.10.47 PM1794×482 65.5 KB\nUSDC Supply is down 3.64%, from $2.68M to $2.58M.\nScreen Shot 2023-09-29 at 12.11.05 PM1798×486 66.1 KB\nUSDC Borrows are down 4.27%, from $1.95M to $1.87M.\nScreen Shot 2023-09-29 at 12.11.20 PM1668×450 35 KB\nUSDC utilization decreased 0.36%, from 82.1% to 81.8%.\nSupply Caps\nScreen Shot 2023-09-29 at 12.13.52 PM3538×626 96.6 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-09-29 at 12.14.29 PM3546×850 171 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-09-29 at 12.11.39 PM1646×428 34.6 KB\nThe minimum USDC utilization was 79.2%, and the maximum was 83.7%.\nThe minimum USDC reserve growth was 6.7%, and the maximum was 23.9%. The average USDC reserve growth was 13.5%.\nScreen Shot 2023-09-29 at 12.12.02 PM1646×452 32.6 KB\nThe comet accumulated $0.27k USDC reserves while distributing $0.0k COMP rewards for a weekly Net Protocol Profit of $0.27k.\\n[Gauntlet] Arbitrum v3 USDC.e Update (09/29/2023 - 10/05/2023)\nGauntlet would like to provide the community with an update on metrics from the Arbitrum v3 USDC.e comet over the past week.\nSimple Summary\n\nUSDC Borrows are down 6.01%, from $2.14M to $2.01M.\nUSDC Supply is down 4.08%, from $2.66M to $2.55M.\nUSDC utilization decreased 2.00%, from 81.8% to 80.1%.\nThe minimum USDC reserve growth was 2.2%, and the maximum was 24.0%. The average USDC reserve growth was 14.4%.\nThe comet accumulated $0.27k USDC reserves while distributing $0.0k COMP rewards for a weekly Net Protocol Profit of $0.27k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-10-06 at 5.36.32 PM1794×492 64.2 KB\nTotal Collateral (USD) is down 11.49%, from $6.28M to $5.56M.\nScreen Shot 2023-10-06 at 5.37.04 PM1786×490 67.5 KB\nUSDC Supply is down 4.08%, from $2.66M to $2.55M.\nScreen Shot 2023-10-06 at 5.37.20 PM1800×482 66.5 KB\nUSDC Borrows are down 6.01%, from $2.14M to $2.01M.\nScreen Shot 2023-10-06 at 5.37.36 PM1658×462 38.6 KB\nUSDC utilization decreased 2.00%, from 81.8% to 80.1%.\nSupply Caps\nScreen Shot 2023-10-06 at 5.40.26 PM3540×608 93.1 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-10-06 at 5.40.45 PM3540×850 184 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-10-06 at 5.38.11 PM1658×430 42.2 KB\nThe minimum USDC utilization was 78.2%, and the maximum was 85.1%.\nThe minimum USDC reserve growth was 2.2%, and the maximum was 24.0%. The average USDC reserve growth was 14.4%.\nScreen Shot 2023-10-06 at 5.38.31 PM1648×460 32.8 KB\nThe comet accumulated $0.27k USDC reserves while distributing $0.0k COMP rewards for a weekly Net Protocol Profit of $0.27k.\\n[Gauntlet] Arbitrum v3 USDC.e Update (10/06/2023 - 10/12/2023)\nGauntlet would like to provide the community with an update on metrics from the Arbitrum v3 USDC.e comet over the past week.\nSimple Summary\n\nUSDC Borrows are down 22.51%, from $2.01M to $1.56M.\nUSDC Supply is down 9.49%, from $2.55M to $2.31M.\nUSDC utilization decreased 14.00%, from 80.1% to 68.9%.\nThe minimum USDC reserve growth was 3.2%, and the maximum was 24.4%. The average USDC reserve growth was 14.0%.\nThe comet accumulated $0.2k USDC reserves while distributing $0.0k COMP rewards for a weekly Net Protocol Profit of $0.2k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-10-13 at 1.12.36 PM1796×478 59.7 KB\nTotal Collateral (USD) is down 25.11%, from $5.21M to $3.9M.\nScreen Shot 2023-10-13 at 1.13.00 PM1792×472 61.3 KB\nUSDC Supply is down 9.49%, from $2.55M to $2.31M.\nScreen Shot 2023-10-13 at 1.13.55 PM1798×486 63.8 KB\nUSDC Borrows are down 22.51%, from $2.01M to $1.56M.\nScreen Shot 2023-10-13 at 1.14.14 PM1652×464 31.1 KB\nUSDC utilization decreased 14.00%, from 80.1% to 68.9%.\nSupply Caps\nScreen Shot 2023-10-13 at 1.15.58 PM3782×604 92.7 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-10-13 at 1.16.26 PM3776×740 161 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-10-13 at 1.14.34 PM1660×424 33.6 KB\nThe minimum USDC utilization was 67.2%, and the maximum was 84.8%.\nThe minimum USDC reserve growth was 3.2%, and the maximum was 24.4%. The average USDC reserve growth was 14.0%.\nScreen Shot 2023-10-13 at 1.14.53 PM1652×448 33.2 KB\nThe comet accumulated $0.2k USDC reserves while distributing $0.0k COMP rewards for a weekly Net Protocol Profit of $0.2k.\\n[Gauntlet] Arbitrum v3 USDC.e Update (10/13/2023 - 10/19/2023)\nGauntlet would like to provide the community with an update on metrics from the Arbitrum v3 USDC.e comet over the past week.\nSimple Summary\n\nUSDC Borrows are up 2.75%, from $1.56M to $1.6M.\nUSDC Supply is down 16.06%, from $2.31M to $1.94M.\nUSDC utilization increased 22.21%, from 68.9% to 84.2%.\nThe minimum USDC reserve growth was 5.0%, and the maximum was 24.4%. The average USDC reserve growth was 15.9%.\nThe comet accumulated $0.21k USDC reserves while distributing $0.0k COMP rewards for a weekly Net Protocol Profit of $0.21k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-10-20 at 12.31.00 PM1796×488 62.4 KB\nTotal Collateral (USD) is up 2.72%, from $3.86M to $3.97M.\nScreen Shot 2023-10-20 at 12.31.22 PM1800×496 66.1 KB\nUSDC Supply is down 16.06%, from $2.31M to $1.94M.\nScreen Shot 2023-10-20 at 12.31.46 PM1804×498 65.1 KB\nUSDC Borrows are up 2.75%, from $1.56M to $1.6M.\nScreen Shot 2023-10-20 at 12.32.05 PM1664×458 30.8 KB\nUSDC utilization increased 22.21%, from 68.9% to 84.2%.\nSupply Caps\nScreen Shot 2023-10-20 at 12.34.17 PM3790×606 92.2 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-10-20 at 12.34.45 PM3790×794 161 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-10-20 at 12.32.23 PM1668×426 30.9 KB\nThe minimum USDC utilization was 68.9%, and the maximum was 84.2%.\nThe minimum USDC reserve growth was 5.0%, and the maximum was 24.4%. The average USDC reserve growth was 15.9%.\nScreen Shot 2023-10-20 at 12.32.40 PM1654×456 33.6 KB\nThe comet accumulated $0.21k USDC reserves while distributing $0.0k COMP rewards for a weekly Net Protocol Profit of $0.21k.\\n[Gauntlet] Arbitrum v3 USDC.e Update (10/20/2023 - 10/26/2023)\nGauntlet would like to provide the community with an update on metrics from the Arbitrum v3 USDC.e comet over the past week.\nSimple Summary\n\nUSDC Borrows are down 17.61%, from $1.6M to $1.32M.\nUSDC Supply is down 13.78%, from $1.94M to $1.67M.\nUSDC utilization decreased 4.09%, from 84.2% to 80.8%.\nThe minimum USDC reserve growth was -10.6%, and the maximum was 23.8%. The average USDC reserve growth was 9.4%.\nThe comet accumulated $0.12k USDC reserves while distributing $0.0k COMP rewards for a weekly Net Protocol Profit of $0.12k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-10-27 at 4.25.04 PM1784×484 64.1 KB\nTotal Collateral (USD) is down 15.91%, from $4.7M to $3.95M.\nScreen Shot 2023-10-27 at 4.25.28 PM1808×484 67.3 KB\nUSDC Supply is down 13.78%, from $1.94M to $1.67M.\nScreen Shot 2023-10-27 at 4.25.52 PM1800×484 67.9 KB\nUSDC Borrows are down 17.61%, from $1.6M to $1.32M.\nScreen Shot 2023-10-27 at 4.26.11 PM1660×460 45.5 KB\nUSDC utilization decreased 4.09%, from 84.2% to 80.8%.\nSupply Caps\nScreen Shot 2023-10-27 at 4.28.44 PM3798×616 92.4 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-10-27 at 4.29.11 PM3792×842 188 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-10-27 at 4.26.34 PM1666×432 45.8 KB\nThe minimum USDC utilization was 76.5%, and the maximum was 92.4%.\nThe minimum USDC reserve growth was -10.6%, and the maximum was 23.8%. The average USDC reserve growth was 9.4%.\nScreen Shot 2023-10-27 at 4.26.57 PM1652×462 32.7 KB\nThe comet accumulated $0.12k USDC reserves while distributing $0.0k COMP rewards for a weekly Net Protocol Profit of $0.12k.\\n[Gauntlet] Arbitrum v3 USDC.e Update (10/27/2023 - 11/02/2023)\nGauntlet would like to provide the community with an update on metrics from the Arbitrum v3 USDC.e comet over the past week.\nSimple Summary\n\nUSDC Borrows are up 6.39%, from $1.32M to $1.4M.\nUSDC Supply is down 0.10%, from $1.67M to $1.67M.\nUSDC utilization increased 6.29%, from 80.8% to 85.9%.\nThe minimum USDC reserve growth was -11.2%, and the maximum was 19.9%. The average USDC reserve growth was -4.2%.\nThe comet accumulated $-0.1k USDC reserves while distributing $0.0k COMP rewards for a weekly Net Protocol Profit of $-0.1k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-11-03 at 10.04.51 AM1798×510 61.4 KB\nTotal Collateral (USD) is down 9.51%, from $4.3M to $3.89M.\nScreen Shot 2023-11-03 at 10.05.19 AM1800×490 63.7 KB\nUSDC Supply is down 0.10%, from $1.67M to $1.67M.\nScreen Shot 2023-11-03 at 10.05.37 AM1804×504 63.6 KB\nUSDC Borrows are up 6.39%, from $1.32M to $1.4M.\nScreen Shot 2023-11-03 at 10.05.54 AM1672×454 36.3 KB\nUSDC utilization increased 6.29%, from 80.8% to 85.9%.\nSupply Caps\nScreen Shot 2023-11-03 at 10.08.08 AM3534×628 91.2 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-11-03 at 10.08.30 AM3534×846 178 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-11-03 at 10.06.10 AM1666×418 32.8 KB\nThe minimum USDC utilization was 80.8%, and the maximum was 93.2%.\nThe minimum USDC reserve growth was -11.2%, and the maximum was 19.9%. The average USDC reserve growth was -4.2%.\nScreen Shot 2023-11-03 at 10.06.28 AM1668×468 35.3 KB\nThe comet accumulated $-0.1k USDC reserves while distributing $0.0k COMP rewards for a weekly Net Protocol Profit of $-0.1k.\\n[Gauntlet] Arbitrum v3 USDC.e Update (11/03/2023 - 11/09/2023)\nGauntlet would like to provide the community with an update on metrics from the Arbitrum v3 USDC.e comet over the past week.\nSimple Summary\n\nUSDC Borrows are up 11.60%, from $1.4M to $1.57M.\nUSDC Supply is up 15.52%, from $1.67M to $1.93M.\nUSDC utilization decreased 3.66%, from 85.9% to 82.7%.\nThe minimum USDC reserve growth was -13.0%, and the maximum was 24.4%. The average USDC reserve growth was 1.6%.\nThe comet accumulated $-0.01k USDC reserves while distributing $0.0k COMP rewards for a weekly Net Protocol Profit of $-0.01k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-11-10 at 1.13.00 PM1790×474 60.9 KB\nTotal Collateral (USD) is up 8.84%, from $4.03M to $4.39M.\nScreen Shot 2023-11-10 at 1.13.22 PM1798×472 65.2 KB\nUSDC Supply is up 15.52%, from $1.67M to $1.93M.\nScreen Shot 2023-11-10 at 1.13.45 PM1794×486 63.6 KB\nUSDC Borrows are up 11.60%, from $1.4M to $1.57M.\nScreen Shot 2023-11-10 at 1.14.07 PM1664×466 42.8 KB\nUSDC utilization decreased 3.66%, from 85.9% to 82.7%.\nSupply Caps\nScreen Shot 2023-11-10 at 1.16.32 PM3792×616 91.7 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-11-10 at 1.16.58 PM3790×842 187 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-11-10 at 1.14.29 PM1672×436 39.7 KB\nThe minimum USDC utilization was 72.8%, and the maximum was 96.1%.\nThe minimum USDC reserve growth was -13.0%, and the maximum was 24.4%. The average USDC reserve growth was 1.6%.\nScreen Shot 2023-11-10 at 1.14.51 PM1654×466 37.7 KB\nThe comet accumulated $-0.01k USDC reserves while distributing $0.0k COMP rewards for a weekly Net Protocol Profit of $-0.01k.\\n[Gauntlet] Arbitrum v3 USDC.e Update: (11/10/23 - 11/16/23)\nGauntlet would like to provide the community with an update on the USDC.e comet over the past week.\nSimple Summary\n\nUSDC.e Borrows increased 35.0%, from $1.6M to $2.16M.\nUSDC.e Supply increased 10.61%, from $1.93M to $2.14M.\nUSDC.e utilization increased 22.04%, from 82.71% to 100.94%.\nThe minimum USDC.e reserve growth was -14.05%, and the maximum was 11.12%. The average USDC.e reserve growth was -7.96%.\nThe comet accumulated $-0.27K USDC.e reserves while distributing $0.0K COMP rewards for a weekly Net Protocol Profit of $-0.27K.\n\nCollateral Asset Supply\nThis graph shows the time series of total supply of all collateral assets.\nSupply1920×1080 109 KB\nTo see updated statistics, please see the live version of this graph here.\nUSDC.e Borrows\nThis graph shows the time series of USDC.e borrows.\nBorrows1920×1080 63.3 KB\nTo see updated statistics, please see the live version of this graph here.\nUtilization\nThis graph shows the utilization (borrow / supply) of USDC.e over the past week.\nUtilization1920×1200 208 KB\nSupply Cap Usage\nThis graph shows the supply cap usage (supply / supply cap) of all collateral assets over the past week.\nSupply Cap Usage1920×1200 153 KB"
  },
  {
    "number_of_comments": 21,
    "postid": "7e4cbf41-d646-48e3-8aac-39ee3a6a3c4e",
    "posturl": "https://www.comp.xyz/t/gauntlet-weekly-market-updates-ethereum-weth/4541",
    "combinedcontent": "[Gauntlet] Weekly Market Updates: Ethereum WETH (7/21/23 - 7/28/23)\nGauntlet would like to provide the community with an update on metrics from the Ethereum WETH comet over the past week and will include any relevant recommendations.\nSimple Summary\n\nDue to the low utilization, Gauntlet recommends allocating rewards to the borrowers. Additionally, to increase the appeal of the protocol, we can decrease the Liquidation Bonus while decreasing the Storefront Price Factor. We will provide a formal proposal next week.\nWETH borrows are down 1.2%.\nWETH supply is up 0.2%.\nThe comet accumulated $4.13k WETH reserves over the past week, with an average reserve growth of 13.4%.\nThe comet distributed $21.35k COMP rewards over the past week, for a Net Protocol Profit of -$17.22k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week.\nMarket Growth\nScreen Shot 2023-07-28 at 3.49.20 PM1476×484 60.4 KB\nTotal Collateral (USD) is down 5.1%, from $59.04M to $56.00M.\nScreen Shot 2023-07-28 at 3.49.40 PM1466×494 60.4 KB\nWETH borrows are down 1.2%, from $44.20M to $43.67M.\nScreen Shot 2023-07-28 at 3.50.06 PM1474×484 59.5 KB\nWETH supply is up 0.2%, from $95.97M to $96.17M.\nSupply Caps\nScreen Shot 2023-07-28 at 3.54.44 PM3540×618 82.1 KB\nAs seen above, wstETH (39.6%) and cbETH (2.1%) both have supply cap utilizations < 75%. We do not currently recommend changing the supply caps.\nScreen Shot 2023-07-28 at 3.56.14 PM3542×832 128 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-07-28 at 4.00.57 PM1642×538 37.5 KB\nThe minimum WETH utilization was 43.5%, and the maximum was 46.9%.\nThe minimum WETH reserve growth was 12.5%, and the maximum was 17.0%. The average WETH reserve growth was 13.4%.\nScreen Shot 2023-07-28 at 4.01.44 PM1656×586 74.2 KB\nThe comet steadily accumulated $4.13k WETH reserves, while distributing $21.35k COMP rewards, for a weekly Net Protocol Profit of -$17.22k.\nRecommendations\nDue to the low utilization, Gauntlet recommends allocating rewards to the borrowers. Additionally, to increase the appeal of the protocol, we can decrease the Liquidation Bonus while decreasing the Storefront Price Factor. We will provide a formal proposal next week.\\nHello @Gauntlet, the stETH APR is at around 4% 2 and the cbETH APR is at around 3.5% 2. Thus it is not profitable for users to borrow above 60% utilization, which explains the current low utilization (and big spread). Don’t you think that this market needs an IRM update also ?\\nEchoing @MathisGD’s sentiment here. The WETH market has shown to function effectively by only incentivizing suppliers. This is because borrowers are naturally incentivized by the leveraged staking yield trade whenever the borrow rate is below the ETH staking rate. An IRM update seems like the better approach here compared to giving COMP incentives to a market that doesn’t necessarily need it.\nFurthermore, if COMP incentives were to be increased in the WETH market, I’d argue that they should be allocated to the supply side because that would attract more suppliers, bring down the utilization, and ultimately lead to lower borrow rates that would attract new borrowers into the system.\\nThank you @MathisGD and @kevin for your feedback. Here 4 is our proposal for risk parameters, IR curve and incentive optimization changes on the WETH comet. Let us know if this answers your question.\\nGauntlet Weekly Market Update: Ethereum WETH (7/28/23 - 8/3/23)\nGauntlet would like to provide the community with an update on metrics from the Ethereum WETH comet over the past week.\nSimple Summary\n\nGauntlet recently recommended the following changes in this forum post 2:\n\nIncrease storefront price factor from 50% to 100%.\nDecrease liquidation penalty from 5% to 2.5%.\nDecrease the borrow interest rate curve lower slope from 0.51 to 0.37.\nIncrease COMP supply distribution from 38 to 70.\n\n\nWETH borrows are up 7.3%, from $43.13M to $46.28M.\nWETH supply is up 4.0%, from $94.98M to $98.77M.\nWETH utilization has increased from 43.6% to 46.9%.\nThe comet accumulated $6.83k WETH reserves over the past week, with an average reserve growth of 18.2%.\nThe comet distributed $20.87k COMP rewards over the past week, for a Net Protocol Profit of -$14.04k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week.\nMarket Growth\nCollateral Time Series\nScreen Shot 2023-08-04 at 10.07.28 AM1814×422 56.8 KB\nTotal Collateral (USD) is up 10.5%, from $55.06M to $60.82M.\nWETH Borrows Time Series\nScreen Shot 2023-08-04 at 10.07.41 AM1798×426 56.7 KB\nWETH borrows are up 7.3%, from $43.13M to $46.28M.\nWETH Supply Time Series\nScreen Shot 2023-08-04 at 10.07.53 AM1810×430 59 KB\nWETH supply is up 4.0%, from $94.98M to $98.77M.\nScreen Shot 2023-08-04 at 10.08.33 AM1680×440 33.9 KB\nWETH utilization has increased from 43.6% to 46.9%.\nSupply Caps\nScreen Shot 2023-08-04 at 1.18.42 PM3536×652 84.5 KB\nAs seen above, wstETH (44.7%) and cbETH (0.8%) both have supply cap utilizations < 75%.\nScreen Shot 2023-08-04 at 1.18.58 PM3538×830 133 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-08-04 at 1.07.01 PM1666×432 27.8 KB\nThe minimum WETH utilization was 43.6%, and the maximum was 57.1%.\nThe minimum WETH reserve growth was 12.4%, and the maximum was 28%. The average WETH reserve growth was 18.2%.\nScreen Shot 2023-08-04 at 1.10.22 PM2286×644 81.5 KB\nThe comet steadily accumulated $6.83k WETH reserves, while distributing $20.87k COMP rewards, for a weekly Net Protocol Profit of -$14.04k.\\n[Gauntlet] Weekly Market Update: Ethereum WETH (8/4/23 - 8/10/23)\nGauntlet would like to provide the community with an update on metrics from the Ethereum WETH comet over the past week.\nSimple Summary\n\nWETH borrows are down 5.3%, from $46.66M to $44.19M.\nWETH supply is down 7.3%, from $99.57M to $92.27M.\nWETH utilization has decreased from 53.0% to 47.9%.\nThe comet accumulated $4.62k WETH reserves over the past week, with an average reserve growth of 15.9%.\nThe comet distributed $15.23k COMP rewards over the past week for a Net Protocol Profit of -$10.61k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week.\nMarket Growth\nCollateral Time Series\nScreen Shot 2023-08-11 at 11.19.48 PM1800×434 60.1 KB\nTotal Collateral (USD) is up 0.3%, from $61.33M to $61.52M.\nWETH Borrows Time Series\nScreen Shot 2023-08-11 at 11.20.14 PM1798×430 61.2 KB\nWETH borrows are down 5.3%, from $46.66M to $44.19M.\nWETH Supply Time Series\nScreen Shot 2023-08-11 at 11.21.37 PM1806×432 61.3 KB\nWETH supply is down 7.3%, from $99.57M to $92.27M.\nScreen Shot 2023-08-11 at 11.23.32 PM1662×456 34 KB\nWETH utilization has decreased from 53.0% to 47.9%.\nSupply Caps\nScreen Shot 2023-08-11 at 11.28.01 PM3538×616 82.2 KB\nAs seen above, wstETH (47.7%) and cbETH (0.6%) both have supply cap utilizations < 75%.\nScreen Shot 2023-08-11 at 11.27.39 PM3540×842 132 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-08-11 at 11.31.55 PM1656×428 27.7 KB\nThe minimum WETH utilization was 41.5%, and the maximum was 51.0%.\nThe minimum WETH reserve growth was 9.7%, and the maximum was 21.8%. The average WETH reserve growth was 15.9%.\nScreen Shot 2023-08-11 at 11.32.22 PM1664×444 33 KB\nThe comet steadily accumulated $4.62k WETH reserves while distributing $15.23k COMP rewards for a weekly Net Protocol Profit of -$10.61k.\\nHello @Gauntlet, what do you think about having the balances values quoted in ETH too in these weekly updates ?\\nHi @MathisGD,\nAbsolutely, we were planning on doing something similar. For the next updates, we will show the balances normalized to the end date USD value of ETH, so as to show the changes in user behavior as opposed to changes in underlying ETH price. Let us know if you have any other thoughts.\\n[Gauntlet] Weekly Market Update: Ethereum WETH (8/11/23 - 8/17/23)\nGauntlet would like to provide the community with an update on metrics from the Ethereum WETH comet over the past week.\nSimple Summary\n\nWETH borrows are down 17.1%, from $40.24M to $33.34M.\nWETH supply is down 29.5%, from $84.02M to $59.25M.\nWETH utilization has decreased from 47.9% to 56.3%.\nThe minimum WETH reserve growth was 17.3%, and the maximum was 35%. The average WETH reserve growth was 26.7%.\nThe comet steadily accumulated $8.13k WETH reserves while distributing $13.88k COMP rewards for a weekly Net Protocol Profit of -$5.75k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end-date token prices to get an idea of intentional user behavior instead of fluctuations in underlying token prices.\nMarket Growth\nCollateral Time Series\nScreen Shot 2023-08-18 at 5.42.01 PM1790×422 61.5 KB\nTotal Collateral (USD) is down 20.7%, from $59.97M to $47.58M.\nWETH Borrows Time Series\nScreen Shot 2023-08-18 at 5.42.53 PM1798×430 61.8 KB\nWETH borrows are down 17.1%, from $40.24M to $33.34M.\nWETH Supply Time Series\nScreen Shot 2023-08-18 at 5.43.23 PM1804×424 62.9 KB\nWETH supply is down 29.5%, from $84.02M to $59.25M.\nScreen Shot 2023-08-18 at 5.45.27 PM1648×448 36.6 KB\nWETH utilization has decreased from 47.9% to 56.3%.\nSupply Caps\nScreen Shot 2023-08-18 at 5.48.47 PM3538×620 82.1 KB\nAs seen above, wstETH (35.7%) and cbETH (0.6%) both have supply cap utilizations < 75%.\nScreen Shot 2023-08-18 at 5.49.49 PM3542×840 136 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-08-18 at 5.50.32 PM1644×426 31.5 KB\nThe minimum WETH utilization was 47.2%, and the maximum was 65.3%.\nThe minimum WETH reserve growth was 17.3%, and the maximum was 35%. The average WETH reserve growth was 26.7%.\nScreen Shot 2023-08-18 at 5.51.53 PM1648×462 38 KB\nThe comet steadily accumulated $8.13k WETH reserves while distributing $13.88k COMP rewards for a weekly Net Protocol Profit of -$5.75k.\\n[Gauntlet] Weekly Market Update: Ethereum WETH (8/18/23 - 8/24/23)\nGauntlet would like to provide the community with an update on metrics from the Ethereum WETH comet over the past week.\nSimple Summary\n\nWETH borrows are down 11.8%, from $32.83M to $28.96M.\nWETH supply is up 0.3%, from $58.34M to $58.54M.\nWETH utilization has decreased from 56.3% to 49.5%.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end-date token prices to get an idea of intentional user behavior instead of fluctuations in underlying token prices.\nMarket Growth\nCollateral Time Series\nScreen Shot 2023-08-25 at 4.58.18 PM1790×420 60.1 KB\nTotal Collateral (USD) is down 11.2%, from $44.23M to $39.26M.\nWETH Borrows Time Series\nScreen Shot 2023-08-25 at 4.59.29 PM1794×420 60.8 KB\nWETH borrows are down 11.8%, from $32.83M to $28.96M.\nWETH Supply Time Series\nScreen Shot 2023-08-25 at 5.00.44 PM1802×418 61.7 KB\nWETH supply is up 0.3%, from $58.34M to $58.54M.\nScreen Shot 2023-08-25 at 5.03.52 PM1650×452 37.7 KB\nWETH utilization has decreased from 56.3% to 49.5%.\nSupply Caps\nScreen Shot 2023-08-25 at 5.08.36 PM3536×620 81.5 KB\nAs seen above, wstETH (31.1%) and cbETH (1.4%) both have supply cap utilizations < 75%.\nScreen Shot 2023-08-25 at 5.09.04 PM3526×844 133 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\\n[Gauntlet] Ethereum v3 WETH Update (08/25/2023 - 08/31/2023)\nGauntlet would like to provide the community with an update on metrics from the Ethereum v3 WETH comet over the past week.\nSimple Summary\n\nWETH Borrows are up 14.53%, from 17.45k ($28.71M) to 19.99k ($32.88M).\nWETH Supply is up 8.21%, from 35.21k ($58.04M) to 38.1k ($62.8M).\nWETH utilization increased 5.85%, from 49.5% to 52.4%.\nThe minimum WETH reserve growth was -4.1%, and the maximum was 5.4%. The average WETH reserve growth was -0.5%.\nThe comet accumulated $-0.06k WETH reserves while distributing $20.63k COMP rewards for a weekly Net Protocol Profit of $-20.69k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-09-01 at 10.20.26 AM1794×488 64.2 KB\nTotal Collateral (USD) is up 12.01%, from $39.92M to $44.71M.\nScreen Shot 2023-09-01 at 10.21.02 AM1798×470 65.7 KB\nWETH Supply is up 8.21%, from 35.21k ($58.04M) to 38.1k ($62.8M).\nScreen Shot 2023-09-01 at 10.21.26 AM1800×494 65.1 KB\nWETH Borrows are up 14.53%, from 17.45k ($28.71M) to 19.99k ($32.88M).\nScreen Shot 2023-09-01 at 10.21.43 AM1654×456 37.1 KB\nUSDC utilization increased 5.85%, from 49.5% to 52.4%.\nSupply Caps\nScreen Shot 2023-09-01 at 10.23.40 AM3540×616 82.2 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-09-01 at 10.23.59 AM3546×846 133 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-09-01 at 10.22.00 AM1654×424 31.1 KB\nThe minimum USDC utilization was 46.8%, and the maximum was 54.2%.\nThe minimum USDC reserve growth was -4.1%, and the maximum was 5.4%. The average USDC reserve growth was -0.5%.\nScreen Shot 2023-09-01 at 10.22.23 AM1662×448 33.8 KB\nThe comet accumulated $-0.06k USDC reserves while distributing $20.63k COMP rewards for a weekly Net Protocol Profit of $-20.69k.\\n[Gauntlet] Ethereum v3 WETH Update (09/01/2023 - 09/07/2023)\nGauntlet would like to provide the community with an update on metrics from the Ethereum v3 WETH comet over the past week.\nSimple Summary\n\nWETH Borrows are up 20.81%, from 19.99k ($32.92M) to 24.14k ($39.77M).\nWETH Supply is up 0.09%, from 38.1k ($62.87M) to 38.13k ($62.93M).\nUSDC utilization increased 20.69%, from 52.4% to 63.2%.\nThe minimum USDC reserve growth was -2.9%, and the maximum was 18.2%. The average USDC reserve growth was 7.4%.\nThe comet accumulated $1.72k USDC reserves while distributing $19.56k COMP rewards for a weekly Net Protocol Profit of $-17.83k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-09-08 at 10.44.15 AM1796×476 66 KB\nTotal Collateral (USD) is up 18.95%, from $42.82M to $50.93M.\nScreen Shot 2023-09-08 at 10.44.34 AM1798×462 67.6 KB\nWETH Supply is up 0.09%, from 38.1k ($62.87M) to 38.13k ($62.93M).\nScreen Shot 2023-09-08 at 10.44.51 AM1792×488 66.4 KB\nWETH Borrows are up 20.81%, from 19.99k ($32.92M) to 24.14k ($39.77M).\nScreen Shot 2023-09-08 at 10.45.09 AM1652×450 36.8 KB\nUSDC utilization increased 20.69%, from 52.4% to 63.2%.\nSupply Caps\nScreen Shot 2023-09-08 at 10.47.00 AM3536×616 82 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-09-08 at 10.47.22 AM3544×844 137 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-09-08 at 10.45.29 AM1656×430 32.7 KB\nThe minimum USDC utilization was 47.6%, and the maximum was 66.9%.\nThe minimum USDC reserve growth was -2.9%, and the maximum was 18.2%. The average USDC reserve growth was 7.4%.\nScreen Shot 2023-09-08 at 10.45.54 AM1652×454 33.5 KB\nThe comet accumulated $1.72k USDC reserves while distributing $19.56k COMP rewards for a weekly Net Protocol Profit of $-17.83k.\\n[Gauntlet] Ethereum v3 WETH Update (09/08/2023 - 09/14/2023)\nGauntlet would like to provide the community with an update on metrics from the Ethereum v3 WETH comet over the past week.\nSimple Summary\n\nWETH Borrows are down 8.81%, from 24.14k ($39.29M) to 22.02k ($35.83M).\nWETH Supply is up 3.60%, from 38.13k ($62.17M) to 39.5k ($64.41M).\nUSDC utilization decreased 11.98%, from 63.2% to 55.6%.\nThe minimum USDC reserve growth was -0.5%, and the maximum was 26.4%. The average USDC reserve growth was 14.2%.\nThe comet accumulated $3.93k USDC reserves while distributing $18.61k COMP rewards for a weekly Net Protocol Profit of $-14.68k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-09-15 at 1.03.25 PM1794×474 67.1 KB\nTotal Collateral (USD) is down 6.85%, from $50.21M to $46.77M.\nScreen Shot 2023-09-15 at 1.04.21 PM1798×484 65.4 KB\nWETH Supply is up 3.60%, from 38.13k ($62.17M) to 39.5k ($64.41M).\nScreen Shot 2023-09-15 at 1.04.46 PM1788×496 65.8 KB\nWETH Borrows are down 8.81%, from 24.14k ($39.29M) to 22.02k ($35.83M).\nScreen Shot 2023-09-15 at 1.05.01 PM1680×456 37.5 KB\nUSDC utilization decreased 11.98%, from 63.2% to 55.6%.\nSupply Caps\nScreen Shot 2023-09-15 at 1.07.32 PM3796×624 85.9 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-09-15 at 1.07.56 PM3794×840 140 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-09-15 at 1.05.18 PM1666×444 30.3 KB\nThe minimum USDC utilization was 49.4%, and the maximum was 77.3%.\nThe minimum USDC reserve growth was -0.5%, and the maximum was 26.4%. The average USDC reserve growth was 14.2%.\nScreen Shot 2023-09-15 at 1.05.31 PM1660×462 32.5 KB\nThe comet accumulated $3.93k USDC reserves while distributing $18.61k COMP rewards for a weekly Net Protocol Profit of $-14.68k.\\n[Gauntlet] Ethereum v3 WETH Update (09/15/2023 - 09/21/2023)\nGauntlet would like to provide the community with an update on metrics from the Ethereum v3 WETH comet over the past week.\nSimple Summary\n\nWETH Borrows are up 14.67%, from 22.02k ($34.9M) to 25.25k ($40.02M).\nWETH Supply is down 2.41%, from 39.5k ($62.74M) to 38.55k ($61.23M).\nUSDC utilization increased 17.51%, from 55.6% to 65.4%.\nThe minimum USDC reserve growth was 1.2%, and the maximum was 16.8%. The average USDC reserve growth was 8.3%.\nThe comet accumulated $1.93k USDC reserves while distributing $19.3k COMP rewards for a weekly Net Protocol Profit of $-17.37k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-09-22 at 10.54.59 AM1790×480 65.1 KB\nTotal Collateral (USD) is up 19.32%, from $47.26M to $56.39M.\nScreen Shot 2023-09-22 at 10.55.17 AM1796×464 65.5 KB\nWETH Supply is down 2.41%, from 39.5k ($62.74M) to 38.55k ($61.23M).\nScreen Shot 2023-09-22 at 10.55.32 AM1796×480 66.8 KB\nWETH Borrows are up 14.67%, from 22.02k ($34.9M) to 25.25k ($40.02M).\nScreen Shot 2023-09-22 at 10.55.48 AM1658×446 34.4 KB\nUSDC utilization increased 17.51%, from 55.6% to 65.4%.\nSupply Caps\nScreen Shot 2023-09-22 at 10.57.47 AM3786×608 85.1 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-09-22 at 10.58.09 AM3794×844 144 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-09-22 at 10.56.03 AM1658×426 29.6 KB\nThe minimum USDC utilization was 50.7%, and the maximum was 65.4%.\nThe minimum USDC reserve growth was 1.2%, and the maximum was 16.8%. The average USDC reserve growth was 8.3%.\nScreen Shot 2023-09-22 at 10.56.22 AM1652×450 33 KB\nThe comet accumulated $1.93k USDC reserves while distributing $19.3k COMP rewards for a weekly Net Protocol Profit of $-17.37k.\\n[Gauntlet] Ethereum v3 WETH Update (09/22/2023 - 09/28/2023)\nGauntlet would like to provide the community with an update on metrics from the Ethereum v3 WETH comet over the past week.\nSimple Summary\n\nWETH Borrows are up 7.56%, from 25.25k ($41.72M) to 27.16k ($44.87M).\nWETH Supply is up 4.07%, from 38.55k ($63.83M) to 40.12k ($66.43M).\nWETH utilization increased 3.37%, from 65.4% to 67.6%.\nThe minimum WETH reserve growth was 10.8%, and the maximum was 19.2%. The average WETH reserve growth was 15.1%.\nThe comet accumulated $3.88k WETH reserves while distributing $20.1k COMP rewards for a weekly Net Protocol Profit of $-16.22k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-09-29 at 12.40.11 PM1792×480 65.7 KB\nTotal Collateral (USD) is up 8.53%, from $55.54M to $60.28M.\nScreen Shot 2023-09-29 at 12.40.31 PM1796×468 64.8 KB\nWETH Supply is up 4.07%, from 38.55k ($63.83M) to 40.12k ($66.43M).\nScreen Shot 2023-09-29 at 12.40.49 PM1794×474 66.8 KB\nWETH Borrows are up 7.56%, from 25.25k ($41.72M) to 27.16k ($44.87M).\nScreen Shot 2023-09-29 at 12.41.12 PM1642×452 38.9 KB\nWETH utilization increased 3.37%, from 65.4% to 67.6%.\nSupply Caps\nScreen Shot 2023-09-29 at 12.44.00 PM3534×620 82 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-09-29 at 12.44.26 PM3540×852 141 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-09-29 at 12.41.55 PM1656×426 36 KB\nThe minimum WETH utilization was 59.1%, and the maximum was 68.1%.\nThe minimum WETH reserve growth was 10.8%, and the maximum was 19.2%. The average WETH reserve growth was 15.1%.\nScreen Shot 2023-09-29 at 12.42.36 PM1648×452 34.2 KB\nThe comet accumulated $3.88k WETH reserves while distributing $20.1k COMP rewards for a weekly Net Protocol Profit of $-16.22k.\\n[Gauntlet] Ethereum v3 WETH Update (09/29/2023 - 10/05/2023)\nGauntlet would like to provide the community with an update on metrics from the Ethereum v3 WETH comet over the past week.\nSimple Summary\n\nWETH Borrows are down 6.38%, from 27.16k ($43.8M) to 25.42k ($41.01M).\nWETH Supply is up 0.41%, from 40.12k ($64.85M) to 40.27k ($65.12M).\nWETH utilization decreased 6.77%, from 67.5% to 63.0%.\nThe minimum WETH reserve growth was 14.6%, and the maximum was 26.5%. The average WETH reserve growth was 18.7%.\nThe comet accumulated $5.83k WETH reserves while distributing $22.33k COMP rewards for a weekly Net Protocol Profit of $-16.49k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-10-06 at 6.07.29 PM1786×470 65.4 KB\nTotal Collateral (USD) is down 6.19%, from $62.18M to $58.33M.\nScreen Shot 2023-10-06 at 6.07.59 PM1796×472 65.4 KB\nWETH Supply is up 0.41%, from 40.12k ($64.85M) to 40.27k ($65.12M).\nScreen Shot 2023-10-06 at 6.08.21 PM1792×460 64.8 KB\nWETH Borrows are down 6.38%, from 27.16k ($43.8M) to 25.42k ($41.01M).\nScreen Shot 2023-10-06 at 6.08.38 PM1648×452 33.4 KB\nWETH utilization decreased 6.77%, from 67.5% to 63.0%.\nSupply Caps\nScreen Shot 2023-10-06 at 6.10.15 PM3534×620 82.1 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-10-06 at 6.10.38 PM3540×846 139 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-10-06 at 6.08.57 PM1658×428 26.3 KB\nThe minimum WETH utilization was 63.0%, and the maximum was 77.4%.\nThe minimum WETH reserve growth was 14.6%, and the maximum was 26.5%. The average WETH reserve growth was 18.7%.\nScreen Shot 2023-10-06 at 6.09.17 PM1658×448 34.4 KB\nThe comet accumulated $5.83k WETH reserves while distributing $22.33k COMP rewards for a weekly Net Protocol Profit of $-16.49k.\\n[Gauntlet] Ethereum v3 WETH Update (10/06/2023 - 10/12/2023)\nGauntlet would like to provide the community with an update on metrics from the Ethereum v3 WETH comet over the past week.\nSimple Summary\n\nWETH Borrows are up 1.39%, from 25.42k ($39.09M) to 25.78k ($39.64M).\nWETH Supply is up 8.82%, from 40.27k ($62.08M) to 43.83k ($67.55M).\nWETH utilization decreased 6.82%, from 63.0% to 58.7%.\nThe minimum WETH reserve growth was 4.4%, and the maximum was 24.8%. The average WETH reserve growth was 14.6%.\nThe comet accumulated $4.06k WETH reserves while distributing $20.63k COMP rewards for a weekly Net Protocol Profit of $-16.57k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-10-13 at 2.53.13 PM1792×486 62.3 KB\nTotal Collateral (USD) is down 0.35%, from $55.49M to $55.29M.\nScreen Shot 2023-10-13 at 2.53.33 PM1796×484 63.7 KB\nWETH Supply is up 8.82%, from 40.27k ($62.08M) to 43.83k ($67.55M).\nScreen Shot 2023-10-13 at 2.53.52 PM1796×490 62.8 KB\nWETH Borrows are up 1.39%, from 25.42k ($39.09M) to 25.78k ($39.64M).\nScreen Shot 2023-10-13 at 2.54.09 PM1666×450 35.2 KB\nWETH utilization decreased 6.82%, from 63.0% to 58.7%.\nSupply Caps\nScreen Shot 2023-10-13 at 2.55.48 PM3786×610 84.6 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-10-13 at 2.56.07 PM3782×796 133 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-10-13 at 2.54.26 PM1656×422 30.2 KB\nThe minimum WETH utilization was 53.4%, and the maximum was 75.2%.\nThe minimum WETH reserve growth was 4.4%, and the maximum was 24.8%. The average WETH reserve growth was 14.6%.\nScreen Shot 2023-10-13 at 2.54.47 PM1670×448 33.7 KB\nThe comet accumulated $4.06k WETH reserves while distributing $20.63k COMP rewards for a weekly Net Protocol Profit of $-16.57k.\\n[Gauntlet] Ethereum v3 WETH Update (10/13/2023 - 10/19/2023)\nGauntlet would like to provide the community with an update on metrics from the Ethereum v3 WETH comet over the past week.\nSimple Summary\n\nWETH Borrows are up 5.56%, from 25.78k ($40.4M) to 27.21k ($42.65M).\nWETH Supply is down 1.09%, from 43.83k ($68.85M) to 43.34k ($68.1M).\nWETH utilization increased 6.72%, from 58.7% to 62.6%.\nThe minimum WETH reserve growth was 8.4%, and the maximum was 22.0%. The average WETH reserve growth was 13.7%.\nThe comet accumulated $3.77k WETH reserves while distributing $19.77k COMP rewards for a weekly Net Protocol Profit of $-16.0k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-10-20 at 2.05.54 PM1798×488 64.4 KB\nTotal Collateral (USD) is down 4.87%, from $55.23M to $52.54M.\nScreen Shot 2023-10-20 at 2.06.19 PM1796×484 65 KB\nWETH Supply is down 1.09%, from 43.83k ($68.85M) to 43.34k ($68.1M).\nScreen Shot 2023-10-20 at 2.06.41 PM1788×480 65.6 KB\nWETH Borrows are up 5.56%, from 25.78k ($40.4M) to 27.21k ($42.65M).\nScreen Shot 2023-10-20 at 2.07.01 PM1664×440 41.8 KB\nWETH utilization increased 6.72%, from 58.7% to 62.6%.\nSupply Caps\nScreen Shot 2023-10-20 at 2.09.17 PM3802×612 84.4 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-10-20 at 2.09.40 PM3792×834 144 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-10-20 at 2.07.17 PM1660×426 34.7 KB\nThe minimum WETH utilization was 56.9%, and the maximum was 71.4%.\nThe minimum WETH reserve growth was 8.4%, and the maximum was 22.0%. The average WETH reserve growth was 13.7%.\nScreen Shot 2023-10-20 at 2.07.41 PM1670×468 31.5 KB\nThe comet accumulated $3.77k WETH reserves while distributing $19.77k COMP rewards for a weekly Net Protocol Profit of $-16.0k.\\n[Gauntlet] Ethereum v3 WETH Update (10/20/2023 - 10/26/2023)\nGauntlet would like to provide the community with an update on metrics from the Ethereum v3 WETH comet over the past week.\nSimple Summary\n\nWETH Borrows are down 8.85%, from 27.21k ($49.09M) to 24.8k ($44.74M).\nWETH Supply is up 2.10%, from 43.35k ($78.38M) to 44.27k ($80.02M).\nWETH utilization decreased 12.05%, from 62.6% to 55.1%.\nThe minimum WETH reserve growth was 5.1%, and the maximum was 14.7%. The average WETH reserve growth was 10.4%.\nThe comet accumulated $2.82k WETH reserves while distributing $21.57k COMP rewards for a weekly Net Protocol Profit of $-18.75k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-10-27 at 5.22.39 PM1800×482 65.2 KB\nTotal Collateral (USD) is down 8.46%, from $60.09M to $55.01M.\nScreen Shot 2023-10-27 at 5.22.57 PM1794×478 65.1 KB\nWETH Supply is up 2.10%, from 43.35k ($78.38M) to 44.27k ($80.02M).\nScreen Shot 2023-10-27 at 5.23.16 PM1796×466 62 KB\nWETH Borrows are down 8.85%, from 27.21k ($49.09M) to 24.8k ($44.74M).\nScreen Shot 2023-10-27 at 5.23.33 PM1660×468 29.8 KB\nWETH utilization decreased 12.05%, from 62.6% to 55.1%.\nSupply Caps\nScreen Shot 2023-10-27 at 6.09.05 PM3786×612 84.5 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-10-27 at 6.09.22 PM3782×844 141 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-10-27 at 5.23.50 PM1672×434 23.6 KB\nThe minimum WETH utilization was 53.9%, and the maximum was 63.1%.\nThe minimum WETH reserve growth was 5.1%, and the maximum was 14.7%. The average WETH reserve growth was 10.4%.\nScreen Shot 2023-10-27 at 5.24.07 PM1666×458 34 KB\nThe comet accumulated $2.82k WETH reserves while distributing $21.57k COMP rewards for a weekly Net Protocol Profit of $-18.75k.\\n[Gauntlet] Ethereum v3 WETH Update (10/27/2023 - 11/02/2023)\nGauntlet would like to provide the community with an update on metrics from the Ethereum v3 WETH comet over the past week.\nSimple Summary\n\nWETH Borrows are down 12.20%, from 24.99k ($44.97M) to 21.94k ($39.48M).\nWETH Supply is down 0.02%, from 45.27k ($81.64M) to 45.26k ($81.62M).\nWETH utilization decreased 12.18%, from 55.1% to 48.4%.\nThe minimum WETH reserve growth was -6.5%, and the maximum was 17.9%. The average WETH reserve growth was 3.0%.\nThe comet accumulated $1.23k WETH reserves while distributing $22.67k COMP rewards for a weekly Net Protocol Profit of $-21.44k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-11-03 at 11.06.27 AM1794×482 66.3 KB\nTotal Collateral (USD) is down 10.31%, from $57.26M to $51.36M.\nScreen Shot 2023-11-03 at 11.06.44 AM1790×480 63.6 KB\nWETH Supply is down 0.02%, from 45.27k ($81.64M) to 45.26k ($81.62M).\nScreen Shot 2023-11-03 at 11.07.03 AM1788×498 66.6 KB\nWETH Borrows are down 12.20%, from 24.99k ($44.97M) to 21.94k ($39.48M).\nScreen Shot 2023-11-03 at 11.07.18 AM1666×468 33.2 KB\nWETH utilization decreased 12.18%, from 55.1% to 48.4%.\nSupply Caps\nScreen Shot 2023-11-03 at 11.09.24 AM3540×610 79.8 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-11-03 at 11.09.47 AM3542×852 146 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-11-03 at 11.07.34 AM1654×424 26.4 KB\nThe minimum WETH utilization was 45.1%, and the maximum was 66.5%.\nThe minimum WETH reserve growth was -6.5%, and the maximum was 17.9%. The average WETH reserve growth was 3.0%.\nScreen Shot 2023-11-03 at 11.07.53 AM1666×456 36.4 KB\nThe comet accumulated $1.23k WETH reserves while distributing $22.67k COMP rewards for a weekly Net Protocol Profit of $-21.44k.\\n[Gauntlet] Ethereum v3 WETH Update (11/03/2023 - 11/09/2023)\nGauntlet would like to provide the community with an update on metrics from the Ethereum v3 WETH comet over the past week.\nSimple Summary\n\nWETH Borrows are down 29.72%, from 21.94k ($46.55M) to 15.42k ($32.72M).\nWETH Supply is down 1.38%, from 45.27k ($96.24M) to 44.65k ($94.91M).\nWETH utilization decreased 28.74%, from 48.4% to 34.5%.\nThe minimum WETH reserve growth was -43.4%, and the maximum was -1.9%. The average WETH reserve growth was -17.4%.\nThe comet accumulated $-2.3k WETH reserves while distributing $24.7k COMP rewards for a weekly Net Protocol Profit of $-27.0k.\n\nAnalysis\nBelow are metrics of the market and parameters over the past week. Note that collateral supply values are normalized to end date token prices, in order to get an idea of intentional user behavior as opposed to fluctuations in underlying token prices.\nMarket Growth\nScreen Shot 2023-11-10 at 1.49.00 PM1794×472 64.3 KB\nTotal Collateral (USD) is down 32.66%, from $52.65M to $35.45M.\nScreen Shot 2023-11-10 at 1.49.23 PM1796×484 62.6 KB\nWETH Supply is down 1.38%, from 45.27k ($96.24M) to 44.65k ($94.91M).\nScreen Shot 2023-11-10 at 1.49.42 PM1794×490 66.3 KB\nWETH Borrows are down 29.72%, from 21.94k ($46.55M) to 15.42k ($32.72M).\nScreen Shot 2023-11-10 at 1.50.12 PM1662×458 34.3 KB\nWETH utilization decreased 28.74%, from 48.4% to 34.5%.\nSupply Caps\nScreen Shot 2023-11-10 at 1.51.35 PM3784×604 83.3 KB\nAbove are the current supply cap utilizations for each collateral asset.\nScreen Shot 2023-11-10 at 1.51.52 PM3792×848 139 KB\nAbove is a time series of supply cap utilization for each asset over the past week.\nUtilization and Reserves\nScreen Shot 2023-11-10 at 1.50.37 PM1646×424 28.4 KB\nThe minimum WETH utilization was 26.6%, and the maximum was 48.4%.\nThe minimum WETH reserve growth was -43.4%, and the maximum was -1.9%. The average WETH reserve growth was -17.4%.\nScreen Shot 2023-11-10 at 1.50.53 PM1664×466 31.7 KB\nThe comet accumulated $-2.3k WETH reserves while distributing $24.7k COMP rewards for a weekly Net Protocol Profit of $-27.0k.\\n[Gauntlet] Ethereum v3 WETH Update: (11/10/23 - 11/16/23)\nGauntlet would like to provide the community with an update on the Ethereum v3 WETH comet over the past week.\nSimple Summary\n\nWETH Borrows increased 16.42%, from $32.06M to $37.33M.\nWETH Supply decreased 10.4%, from $93.01M to $83.34M.\nWETH utilization increased 29.92%, from 34.47% to 44.79%.\nThe minimum WETH reserve growth was -25.04%, and the maximum was 3.88%. The average WETH reserve growth was -8.02%.\nThe comet accumulated $-1.36K WETH reserves while distributing $26.54K COMP rewards for a weekly Net Protocol Profit of $-27.9K.\n\nCollateral Asset Supply\nThis graph shows the time series of total supply of all collateral assets.\nSupply1920×1080 90.3 KB\nTo see updated statistics, please see the live version of this graph here.\nWETH Borrows\nThis graph shows the time series of WETH borrows.\nBorrows1920×1080 77.5 KB\nTo see updated statistics, please see the live version of this graph here.\nUtilization\nThis graph shows the utilization (borrow / supply) of WETH over the past week.\nUtilization1920×1200 177 KB\nSupply Cap Usage\nThis graph shows the supply cap usage (supply / supply cap) of all collateral assets over the past week.\nSupply Cap Usage1920×1200 136 KB"
  },
  {
    "number_of_comments": 75,
    "postid": "4fbc2b8a-19a4-4aa5-b1ee-4a1d90c3779a",
    "posturl": "https://www.comp.xyz/t/oracle-infrastructure-chainlink-proposal/1272",
    "combinedcontent": "In addition to the medianizer, I have another proposal for the community to consider for improving the oracle infrastructure. While researching various solutions, I talked with Johann at Chainlink to learn more about their system because Aave and other notable protocols utilize them. I thought it would be prudent to get their opinion as well.\nThe Chainlink team has a proposal for the community to consider. As many of you may know, Chainlink has an existing oracle system that anyone can use. While Chainlink has been considered in the past, I mostly dismissed it because their existing system contains an admin key (multisig), and I did not think it would pass a vote. However, their team is willing to launch oracle contracts, similar to this CRV/ETH 32 oracle 19, without an admin key or with an admin key that the Compound community controls. As well, their team is willing to launch any oracle the protocol would like and built to the parameters we want as a community.\nThis was a game-changer to me, so after the call, I started doing due diligence. Here is a quick summary I have gathered from speaking with the Chainlink team on why the community should consider using the Chainlink solution.\n\n\nThey are the most experienced team in the space with building secure oracle solutions. They have a 100+ person team, with engineers, researchers, and DevOps solely focused on oracles full time.\n\n\nTheir systems have been online for years and are securing significant amounts of onchain value. Aave, Synthetix, dYdX, and a bunch of other DeFi dApps use them.\n\n\nThey already have a proven system of reporters, posters, and aggregators in place.\n\n\nWill require minimal technical effort for us to switch over and launch new oracles in the future.\n\n\nThey have high standards for data quality. Each oracle node takes a median from multiple data aggregators, and numerous oracle responses are further aggregated. This will give us strong market coverage and redundancy.\n\n\nTheir systems are reliable and consistently post price updates during extreme network congestion, such as the recent gas price spike of over 1,400 Gwei, and provide extensive real-time monitoring of all networks.\n\n\nThey have built a node network with reputation 33 systems and visualization 18 tools for the community to monitor their onchain performance. We can even select which nodes we want.\n\n\nShared costs for oracle networks with other ecosystem projects, meaning it’s cheaper than publishing the data ourselves.\n\n\nAfter thinking on these points and how they compare to a community built medianizer, I do have some concerns about the medianizer that I would like to share:\n\n\nUsing centralized exchange APIs only provides raw, unrefined data. That leaves us vulnerable to price wicks, exchange downtime, AWS/Cloudflare concerns, etc.\n\n\nUsing DEX data as a primary source is unideal due to the rapidly changing liquidity landscape.\n\n\nGetting exchanges to join the Open Oracle seems to be harder than anticipated. All exchanges are currently low on resources due to trading volume and general usage being at an all-time high.\n\n\nThe economics of posters are incomplete and unrefined.\n\n\nAfter weighing the options, I feel Chainlink is worth considering. However, before I and the Chainlink team explore their approach any further, I want to get a temperature check from the community.\nIf the community is interested in exploring this, here are the rough steps we would take:\n\n\nOnce enough interest is expressed, Chainlink would launch the 10 oracles Compound requires.\n\n\nSetup a new oracle contract for the comptroller to point to.\n\n\nLaunch everything on a testnet and have the contracts audited.\n\n\nCosts: Chainlink would like Compound to pay $150 per oracle per month. With 10 markets (pegging USDC to 1), that would be $1500 a month to stream to Chainlink via the set _setContributorCompSpeed.\nTo learn more about Chainlink, check out these links:\n\n\nChainlink Decentralised Data Model 26\n\n\nmarket.link 12\n\n\nhttps://reputation.link/ 33\n\n\nDecentralized Price Reference Data | Chainlink 32\n\n\\nGreat idea.  I am 100% for this.\\nThe cost of $1,500 for chainlink is very small, but we can effectively improve the quality of oracle.\\nI’m in full support of this, delegating the operational burden of oracles to a decentralized node network that has proven its resiliency is a strong move for Compound. Let Compound focus on creating the best protocol for open financial markets and let Chainlink do what they do best - provide reliable data.\\nI support this, at this point it is the best possible solution. I think that the protocol should move over time from any centralized solutions in all segments of functioning.\\nThank you @getty for this information and doing the research. Some questions:\n\nYou mention a price of $150 per oracle, will this stay the same if the gas fees increase?\nSome of the Chainlink medianizers that are live right now use 21 nodes, some 15 and some 9. Do you have any insight on what amount would be sufficient decentralization to ensure both up-time and security against manipulation?\nIs it possible to choose which nodes are added to these price feeds? As in, can we choose based on what the reputation.link website says are the best nodes?\n\\nI am fully in support of utilizing Chainlink. They have the best track record in the space, and leveraging their specialized team to address our oracle concerns will allow us to better focus on our core competencies as a lending protocol.\nI agree, removing or giving our community control of the the admin keys is a game-changer. I imagine it will alleviate many people’s concerns with utilizing a 3rd-party solution within our governance framework.\nWhile I admire the initiative to build our own medianizer, I do not think it will match the level of security, data quality, or reliability we will get with Chainlink. Again, this would allow us to focus more of our efforts toward improving the Compound protocol itself, which is invaluable.\nI look forward to hearing others’ opinions.\\n\n\nYes\n\n\nWe can choose how many we want. The Chainlink guys think 16 would be the standard, but I’ll see if I can find some more info on this.\n\n\nYes, the community will have the option to chose which nodes are added to the price feed. I think Chainlink will make a recommendation because they know them best, but the community can obsoletely give feedback.\n\n\\nIt goes without saying. Thank you for the work you have done.\nWhile I love the idea of set comp contributor speeds to them, I would love to add in the chainlink market as well, and have part of the costs inside the reserve factor.\nObviously this would be after the fact, but maybe if it’s together it could save some time and be more efficient.\nIf this is not the case, then by all means I support adding this, but maybe down the road we could add link. If you use the api, chainlink is in the prices from the API, so that’s something. https://api.compound.finance/api/v2/prices 6\\n\n\n\n getty:\n\nThey have built a node network with reputation  systems and visualization  tools for the community to monitor their onchain performance. We can even select which nodes we want.\n\n\nHey, I’m just chimming in to say something. While data.chain.link is -indeed- a tool created and updated by ChainlinkLabs, resources like reputation.link and market.link are the initiative of third parties. Personally I think this brings some added value and enrichment in terms of decentralization, which is also extremely positive.\n\n\n\n Manta_Ray:\n\nIs it possible to choose which nodes are added to these price feeds? As in, can we choose based on what the reputation.link website says are the best nodes?\n\n\nBoth the Reputation site and the Marketplace serve as a Hub / Explorer / and meeting place for developers, node operators and users who want to use the Chainlink protocol without the need to interact with ChainlinkLabs, so they allow any developer or community governed effort to pick whatever node they see fit. However it’s worth noting that their approach of using security reviewed nodes operated by independent teams with strong DevOps background has been proven efficient so far.\\nBumping this thread and tagging a few members who have weighed in on oracle improvements in the past. I’d love to get your thoughts on this option now that we know we can remove Chainlink admin keys (or have control via governance).\nAgain, with so many new initiatives being launched I think taking this oracle development/maintenance off our plate will be hugely beneficial.\n@cryptix, @tonyotonio, @wario, @Boxcar, @blck, @arr00, @kybx86\\nAside from the admin key issue, Chainlink has different security assumptions than the Open Price Feed oracle. Most notably the existence of an additional layer of signing/reporting nodes, which imply an increased attack surface. I think moving to the originally proposed medianizer is a much safer transition, that would not require Compound to follow the security trade-offs of using Chainlink. Incorporating Chainlink price feeds as part of the medianizer, along with Coinbase, Uniswap, and Okex, seems much preferable.\\nThanks for weighing in. However, I don’t agree with this line of thinking, especially given we already fell victim to a manipulation attack on single exchange sources ala Coinbase, meanwhile Chainlink nodes performed.\nFull market coverage (data from quality aggregators, secured by a network of independent nodes) is the key to reliability/security. Using Chainlink as just another part the medianizer would diminish this value proposition.\\nThat’s the way to go. Compound should focus on building a great lending market and just use a ready and working oracle solution.\\nI think this proposal is a great idea for Compound protocol. Chainlink is a proven oracle solution and has been securing other money markets like Aave since launch without any pricing issues or false liquidations as far as I am aware. Using Chainlink sovles the main issue I had with the medianzier in regards to inconsistent and insufficent market coverage. Chainlink oracles use multiple data aggregators who generate volume weighted average prices from across the crypto market. Most people here seem to be in favor of this proposal so when can the testnet deployment and contract auditing begin?\\nWhy deal with Coinbase? They did not show the users the data on how the “DAI manipulation” took place. Why are we dealing with something aimed at Wall Street? Why are we on a decentralized protocol?\\nHey everyone, Johann here, I’d like to weigh in here and provide my insights. I work as the head of integrations over at Chainlink Labs and I’m quite happy to see this discussion shaping up.\nI see Compound as one of the leading protocols within the DeFi space so I’m excited to work with the community and figure out the ways Chainlink oracles can be used to solve some of the oracle related issues the protocol is currently running into.\nWe want to support this proposal however we can and will begin by taking steps towards what could lead to a vote for a mainnet implementation in the coming weeks. Our price feeds currently cover all the live Compound markets on the Kovan testnet (Ethereum Price Feeds 7). In the coming days, we will launch a version of the Comptroller which utilises Chainlink price feeds deployed on testnet and get some testing going on there. The community can observe and monitor this process and provide any feedback you have.\nOnce this is live and operational on testnet, I think we should see the conversation going on by discussing the kind of format the Compound specific feeds we’ll be launching should take. For example, should the Compound community have control over the feed’s Multisig (ability to add and remove oracle nodes), should the multisig be a combination of Compound community members and Chainlink node operators, should it be set to the Compound governance module for COMP holders to vote on, or should we just avoid any Multisig and set it to a burn address (which in my view would be a more risky approach long term as it’s always useful to be able to do changes in the set of node operators such as adding more nodes when more value is at stake etc…).\nThis is entirely a decision to be determined by the Compound community though and we will follow whatever the community thinks is best and we’ll work at your rhythm. Our main motivation here is to solve problems for you guys, letting you focus on managing Compound markets, as well as put the experience our node operators and network has had providing price feeds for the DeFi ecosystem in the last 2 years at the service of the Compound community and protocol. Lastly, I want to say thank you to Getty for creating this proposal and the Compound community for your support. I will provide an update in this thread as we move closer to something we can showcase on testnet!\\n@Johann_Eid\nChainlink is a great product!\nThank you guys for your help!\\nI appreciate the update @Johann_Eid and look forward to seeing the testnet deployment.\\nExcellent! I’m very much looking forward to seeing this live on the testnet so we can review.\\nChainlink is fundamentally a trust based system as far as I understand it. To be clear, I believe that using VWAP of various exchanges is good but if those exchanges report prices directly using their exchange reporter key there is no need for the chainlink to be an intermediary. For several tokens DEX liquidity may be even greater than CEX liquidity and no chainlink nodes are needed there either.\nThere are fixes that we should make to the way the open price feed reports data potentially leading to an “Open Price Feed V2”. The primary current shortfall of the open price feed is the failure to incorporate volume data such that the computation of a VWAP with signed and verified data is possible.\\nExchanges signing their data is an off-chain process, but you still need an oracle mechanism to deliver this data reliably on chain. In the current open price feed design, there are no direct financal incentives to perform this as the relayers aren’t paid, while Chainlink nodes are directly incentivized to post data on-chain, even during network congestion and high gas fees.\nGenerating VWAP on-chain isn’t currently possible because you need to know each exchange’s volume, which would require exchanges to host another signed API endpoint. Consider the lack of exchanges who sign their price data in the first place (we’ve seem to hit wall on this front), so this doesn’t seem to be feasible in the short or medium term.\nTo me, it all boils down to the fact that Chainlink can provide the full market coverage we need today with minimal developmental considerations or costs. For us to develop the same level of market coverage based on upon the current design while ensuring reliable delivery is expensive, time consuming, and full of unknowns, especially given the lack of signed exchange APIs\\nHi everyone, I wanted to provide an update on the progress that has been made regarding the implementation of this proposal. We have created a pull request on the Compound github\nhere 34 which includes a new contract, ChainlinkPriceOracle that implements PriceOracle so that it is compatible with Comptroller's current implementation.\nThe pull request also provides unit tests and the steps Compound governance would need to take should this proposal be accepted by the community. This initial pull request is intended to gather feedback from the community, so we encourage anyone who is interested to take a look through the code and provide any suggestions you may have.\nWe have also deployed all the necessary contracts onto the Kovan testnet to test the deployment process and showcase how this integration would work in practice. We’ve included all the contract addresses in the pull request. If there is anything else the community wants to see, let us know as we are more than happy to help.\nFor the next steps once we get enough feedback, we’ll be creating additional integration test scenarios and simulate changes with a patch to test all the edge cases. We’ll then aim to get an independent audit of the integration and begin testing on the Ethereum mainnet for further security assurance. After all of this testing is done and reviewed by the community, we can move towards generating an on-chain proposal for the Compound community to vote upon and create test simulations of the proposal finalizing on a forked testnet.\nI believe the community should also start discussing what kind of governance method they would like to have over the price feeds, as discussed before, Chainlink’s price feeds can be extremely flexible in the way they’re governed/coordinated and we will leave these discussions in the hands of the community.\nThank you to all of you for your support so far and we look forward to any feedback you have so we can move forward on all this.\\nThank you, Johann, and the Chainlink team for publishing the contracts on Kovan for the community to review.\nThe Compound community needs to decide what to do with the admin key for each price feed. We have four options:\n\nWe could set the admin to the burn address. This means we wouldn’t be able to update the contracts in any way. If we wanted to change something, it would require launching a new contract and using governance to remove the old market and add the new one.\nWe could set the admin key to the regular Compound governance key. This would allow the community to update the contract if needed. Any change the community might want to implement would take the full length of a regular governance proposal.\nWe could set the admin to the Compound Community Multisig 7 that is currently used for borrow caps. If something arose and we needed to act fast, this would be much faster than the normal governance process.\nWe could use the existing Chainlink multisig, and that exists in the current Chainlink markets.\n\nPersonally, I think using the Compound Community Multisig that is currently being used for borrow caps is a good middle solution. I haven’t brought this up with the multsig holders, so we would need to confirm they are okay with that responsibility. Otherwise, I think we should set it to the Compound governance key.\nAdmin Key Vote58%Compound Community Mutlisig42%Compound Governance0%Burn address0%Chainlink Multisig19voters\n                    \n                    Closed Mar '21\n                   \n                \n                Results will be shown on vote.\n               \\nas a member of the community multisig i have to say that giving this to the community multisig is extremely too much power, if it would give the ability to change price by upgrading the contract etc.\\nI, also as a member of the community multisig think that this is giving too much power. However, if that is what the community wants, we can assure to be very transparent about what is happening\\nHey guys, I’m writing this post which I initially sent on the Discord governance channel as a hope that it will help answer the thoughtful questions which have been asked by the community as well as some of the possible doubts which some of you might have. We’re discussing a very key and important aspect for the protocol security here, hence I want to take the time to give as many details and information for the Compound community to be able to reach an informed decision on this topic. It’s great to be having these conversations which in my view are always a positive sum game for the DeFi space and its security.\nThe specification of our oracle networks is described within our documentation (e.g. Chainlink Decentralised Data Model 8), covering when updates occur, the contracts used, and how the network is kept decentralized. Keep in mind, the documentation describes how our mainnet price feeds currently work. This architecture was made to be fully modular for any community or DAO who would want to start running/coordinating their own Chainlink data feeds. Hence, things such as the proxy contract which allow for the network to be upgraded to the latest release of the feeds could be removed from this architecture if the Compound community doesn’t want the multisig to own this kind of power. Having an admin key on the oracle network is not strictly required, but an important consideration is that while new network contracts can be deployed, this takes time to pass governance so having this capability of upgradability can be a large advantage. For instance, if the Compound community wanted to scale up the number of nodes on a feed for a market which grew in value, they would have to redeploy a new contract with more node operators and then pass an on-chain governance vote to point to this newly deployed contract. The process could create delays which could result in unintended consequences.\nThe role of the multisig admin key is essentially the one of a group of coordinators. Oracle networks are inherently complex as they combine multiple non-deterministic off-chain data points. Unplanned events can and do happen (huge gas spikes, issues with data sources, etc…) In these situations, the coordinators can efficiently prevent these events from adversely affecting the network through the necessary changes. These coordinators would also be able to add/remove nodes, for instance as the value on Compound grows, it can add more and more reputable entities with a proven on-chain performance history to the set of oracles powering the price feed, scaling up the network’s security. Again, this is not a requirement, price feeds can and will work without a multisig, however it’s just about assessing the risks you’re willing to take here. Both approaches do bring about their own set of challenges. This is all about incentives in the end, right now, the assumption is that the multisig owners for Compound would have way too much at stake in the system (COMP token exposure) for them to be malicious here. As Wayne mentioned on Discord, an attack by them would likely not result in any net benefits for them and would destroy both the implicit stake they have in the system as well as their reputation. This is an important point to consider in regards to the likelihood of such adversarial admin key usages. However, as mentioned before, the multisig is not a requirement and the Compound community can go with whichever approach that is preferred based on the security assumptions and trust model that is desired.\nThe requirement for nodes to post price data on-chain is based upon the economic incentives of being a Chainlink node in both the short term and the long term. In the short term, nodes who fail to post prices would not get paid and would eventually get removed from the price feed for their non-activity (either by the multisig coordinators or the deployment of a new price feed to replace the previous). Medium to long term the nodes would harm their reputation as a service provider in both the Chainlink network and any other external services they provide (nodes on mainnet include traditional enterprises like Deutsche Telekom’s T-Systems and premium data providers like Kaiko). I think it’s important to emphasize this point can also be seen in the current Open Oracle design as it’s a trust based assumption regarding Coinbase: e.g Coinbase has an incentive to not act maliciously or carelessly as this would affect their main business. This trust dynamic based on reputation and revenue is the same for the Chainlink nodes, e.g. why would Deutsche Telekom’s T Systems act maliciously in a way that could adversely impact their company which generates $100B+ billion in revenue a year? However with the system we’re proposing, we’re counting on many different independent parties in different industries, distributed geographically across many different jurisdictions and with a plethora of different business models. Hence, the risk here is very distributed which makes an overall takedown of the network far less likely than relying on one single category of providers.\nAdditionally, the loss of reputation would result in the opportunity cost of losing all future revenue in the Chainlink Network (nobody would choose to include or keep an unreliable node in their feeds), representing a strong financial incentive to continue publishing prices to capture this growing revenue. Historically, looking at the performance of Chainlink node operators over the past two years, the financial incentive structure has worked as designed with nodes publishing on-chain updates even during extreme network congestion (1,500 gwei) and downtime events like when Infura went offline. This historical on-chain data can be reviewed on independent analytic services like Chainlink Market and https://reputation.link 2, which can be used to determine which nodes have the strongest economic incentive to continue their services into the future as well as monitor the current performance of selected nodes in all price feed networks they participate in.\nChainlink price feeds also support historical circuit breakers to prevent the consumption of any potential outlier data points by comparing a new price update to previous updates, without the downsides of DEX based TWAPs which can become inaccurate during market volatility due to the time delay. A circuit breaker isn’t a requirement but it can provide an additional safety net if it is a desired feature. Our main worry right now around DEX based circuit breakers which we would like to make clear is that in the case of a flash crash, a TWAP would be an extremely lagging indicator, meaning it would give an outdated stale price point, preventing people from liquidating collateral on time. Hence, if a token falls 50% in 30 minutes, liquidators won’t be able to liquidate positions in time as the TWAP will show a huge deviation with the primary price feed, causing a false positive to occur. This could result in the whole platform becoming under collateralized. So, I would advise to be very careful about circuit breakers and if you decide to use one here. I am happy to discuss this further as I understand there’s nuances at play here when upgrading a critical component of Compound’s infrastructure, but I wanted to provide some additional clarity on where we are coming from.\nUltimately, I want to make it clear that our attention is and always has been to offer these Chainlink price feeds and oracle networks as a neutral technology that communities such as Compound can shape and build around your needs however you feel like it. Compound could go with a multisig or without, similarly you could go with a circuit breaker or without, this is entirely up to you and the community, depending what risk assumptions are willing to be taken.\nOur goal here is to provide our experience and expertise as a project that has been working exclusively on oracles with developers and researchers for 4 years, to create the most modular, redundantly secured technology for DeFi to grow and succeed. Hence, we want to advise and share our experience and what we think is the best approach for Compound. However, this is all in the hands of the community and we will assist you in whichever path you deem is best for your protocol. Everyone on this forum has a stake in making sure Compound grows to a trillion dollar protocol and beyond, not only for the sake of the community but for the whole of the DeFi space and the creation of a new more inclusive and economically fair financial system. We’re here to assist you in this path and adventure in making this a reality and hope our contribution can be the establishment of a fully fledged and secure price feed system which will allow the Compound community to achieve its ultimate vision as a money market protocol.\\n\n\n\n getty:\n\nThe Compound community needs to decide what to do with the admin key for each price feed. We have four options:\n\n\nI think giving admin key powers to the community multisig makes the most sense here. It provides the most amount of flexibility. After a Chainlink integration is complete, we can monitor if the selected nodes are performing as expected and change them out if needed. At a later point in time, we can reconvene to see if this setup should change and switch admin key powers over to Compound Governance, which is slower to react but still has the ability to change oracle network attributes. I don’t think we should remove the admin key altogether as I believe it is better to have it and not need it, rather than to need it and not have it. Open to what others have to think as well.\n\n\n\n Johann_Eid:\n\nHey guys, I’m writing this post which I initially sent on the Discord governance channel as a hope that it will help answer the thoughtful questions which have been asked by the community as well as some of the possible doubts which some of you might have.\n\n\nI appreciate you posting this info Johann, provides a lot of insight into Chainlink’s current architecture and design rationale. I think an integration with Chainlink would greatly increase the security of the Compound protocol all around. I simply see a lot of flaws with the current oracle used by Compound as it relies on just Coinbase for data as well as the other proposed Medianizer design as it cannot provide market coverage and dilutes high quality data with low quality data. I would like to see Compound to grow into a multi trillion dollar protocol and to do so, it needs a good oracle solution.\\nHow about allowing the community multisig the ability to invalidate the reporter, and that’s it.\n  \n      \n      compound.finance\n  \n  \n    \n\nCompound | Docs - Open Price Feed 14\n\nCompound is an algorithmic, autonomous interest rate protocol built for developers, to unlock a universe of open financial applications.\n\n\n  \n  \n    \n    \n  \n  \n\n\\n@TennisBowling Would love your input\\nWould it also also be possible to have the admin keys default to Compound Governance to vote on regular operation (selection of feeds/scale up number of nodes)  and have backup keys held by by the Community Multisig incase emergency intervention is needed?\nAlso, I think the community via governance vote would be needed to set ground rules for the use of the admin key in any case it’s given to the Compound Community Multisig members.\\nThis wouldn’t help because, in an emergency, the mutlisig would also need to deploy a fix.\\nSo revert back to coinbase pro. A multisig could then steal the system. It’s too much power.\\nReverting to Coinbase won’t work if the coin isn’t on there.\nMaybe we could add an emergency function that the multisig could call that would revert to using Chainlink’s main oracles that Aave and others use. That way the multisig can’t update the contracts to something unexpected.\\nI’m glad to see others voicing their support in adding Chainlink to the Compound protocol, I think it’s the right long term move. In regards to multisig, it looks like most people are leaning towards either the community multisig or the governance contract. If the former gets chosen by the community as the desired solution, I’d like to voice my interest in becoming one of the signers to help improve the multisig’s decentralization and therefore price feed security. For context, I was a pre-seed investor in Compound in November 2017. If I am added to the Compound community multisig, I will faithfully relay the community’s sentiments in my contributing transactions within the multisig process.\\n\n\n\n getty:\n\nReverting to Coinbase won’t work if the coin isn’t on there.\n\n\nThat’s a great point. When we add more assets that will be a problem. Maybe our internal oracle infrastructure will have some competition.\\nHi all,\nKatherine here from reputation.link 17, an independent analytical service to Chainlink. We’re thrilled by the proposed integration, and we can assist by providing insights into the security and reliability offerings of the Chainlink network. We’re here to provide that additional context to the Compound community about Chainlink as a solution - to help you understand the value of secure feeds.\nWe understand that there is so much value being secured by some of these data feeds, so it is important for you to do due diligence and really assess the price feeds and individual oracles that are supporting those feeds to ensure that you’re comfortable and willing to proceed with the integration.\nAs Johann mentioned, Chainlink has deployed all the necessary contracts onto the Kovan testnet to test the deployment process. Once the feeds are up on Ethereum mainnet, you will be able to visualise them on reputation.link. For example, check out the ETH/USD feed 4 that is utilised by Synthetix and several other protocols. You can visualise how many (and which) oracles are securing that individual feed, and if there have been any deviations or failures on that feed. You can view round specific information, such as the price returned by each oracle on the feed, and if the returned prices deviated from the aggregated price. This provides the Compound community insights to understand the security guarantees of the data provided by the Chainlink network.\nJohann touched upon the performance of Chainlink nodes during downtime events like when Infura went offline. During this outage, the Chainlink network and its node operators showed incredible resilience. For those interested, we provided an autopsy of the event here 6.\nDepending on what your community decides to do with the admin key for each price feed, you’re absolutely right in assuming you can utilise reputation.link 17, to conduct due diligence on all of the verified Chainlink nodes, and hand pick specific nodes that meet your use case. Our homepage 17 provides a list of the top ten best performing oracles. Explore the individual profiles of each Chainlink oracle and evaluate their reliability through visualising their on-chain performance (such as their average latency when responding to requests).\nOur goal is to provide a premium experience for you to do your research on the quality of node operators for your integration. We can provide any insight you’d like to derive from the Chainlink network. We’re happy to assist in any way, to help ease the integration.\\nUpdate: 3/31/2021\nAfter a lot of community feedback and debate, a number of changes have been made to the proposal since I first posted it.\n\n\nCompound Governance is going to be the admin on the contracts. That gives the community the ability to add/remove reporters for each market. Governance will also be the admin on the main oracle contract that controls which markets to support.\n\n\nAn emergency function will exist for each market that the Compound Community Multisig controls. In the unlikely event that something happens with one of the oracle contracts that requires immediate intervention, the emergency function will switch the price feed to the Chainlink v2 contracts that are widely used in DeFi.\n\n\nAfter a lot of debate and we’re going to continue using Uniswap v2 as an anchor. Once v3 launches, we’ll upgrade the contracts for v3 and send it to governance for approval.\n\n\nThe plan is to publish the revised contracts on testnet in the coming weeks for the community to review. Once feedback has been implemented, we’ll send the contracts for audit. After the audit, we’ll move towards governance.\nHigh level: The oracles that Aave, Sythentix, and others use are Chainlink v2. Those have an admin multisig that Chainlink and other stakeholders control. The system we’re developing is purpose-built for Compound. The community controls admin permissions, emergency functionality, anchor specifications, reporters per-market, and who the reporters are. We are essentially swapping Open Oracle/Coinbase for a community-designed version of Chainlink and maintaining the Uniswap anchor.\\nThanks for the update @getty. Excited to see all of these efforts coming to fruition!\\nI think this is a good way to do it, nice progress.\\nI love seeing all this progress being made! This is looking really good for the Compound protocol.\\nThank you for update and transparency on this proposal’s progress\\nI hope the proposal can be realized soon\\nI really love this approach. Great progress and excited to see this come to fruition.\\nAwesome effort getty! Will definitely get behind this once it’s put to vote\\nThanks for all of your hard work on this @getty!\nI am convinced that an upgrade is necessary and I haven’t seen any preferable alternative.\nSignaling my support and I will encourage the rest of StanfordCrypto to support it as well.\\nPolychain is largely supportive of this proposal as a short-term solution.\nWe definitely echo some concerns about Chainlink: It’s effectively a semi-trusted system that has been very slow at taking steps to enhance cryptoeconomic security (still no staking, random shuffling of nodes, few integrations of primary data sources). Generally, we don’t find this model to be aligned with COMPs values… However, a semi-trusted oracle that works is better than a decentralized oracle with insufficient guarantees. Additionally, the fact that the COMP community will be able to select reporters and will have control over the admin multisig is a reasonable tradeoff. We should treat these reporters as trusted parties, and, of course, should prioritize selecting reporters who are aligned with the Compound Protocol.\nOverall, we still think the medianizer route is the optimal solution, but building out the Compound community’s version of Chainlink is a step in the right direction, as this will then enable Chainlink to be safely used in a medianizer built by the COMP community (along with centralized exchange data (signed off-chain) & eventually price data from liquid DEXs).\nBig thanks to @getty for taking the lead on this.\\nFirst, want to thank @getty for the hard work he’s put in to research.\nHistorically, Chainlink was rejected by the Compound Labs team for the following reasons:\n\nSmart contract risk surface\nManual administration (risk of error)\nReliance on third-parties to post prices (with no guarantees that they would be posted accurately, or at all)\n\nIn its current form, the suggested approach is a security risk to the protocol, and I will be voting against it.\nSimply put, the “core” Chainlink is safer than an offshoot.\n\nCreating an offshoot introduces new contracts / risks\nThe manual administration is shifted from a dedicated organization, to a decentralized one. Compound governance moves slowly, and is poorly equipped to be rotating reporters, whether through a 7-day governance process, or a multi-sig. This idea is madness.\n\nThe risks of Chainlink can be offset in other ways; including it as part of a median, or anchoring it to Uniswap or the current system. Carefully designed, this can abate the risk of prices not being posted, or posted incorrectly.\\nIf you read both of the LINK whitepapers you will know that every argument you mentioned above has been debunked. Instead you pretend. Compound is deliberately undercutting their systems to extract rent of it using flashbots among other means. They don’t want to integrate chainlink because it means honesty and they can’t profit from that.\nThis is the critical moment in time to decide whether trust wins or not, and people like Leshner are a roadblock.\\nAfter discussing the proposal with Robert, we have come up with an adjusted plan that he, Polychain, and blck will support.\nThe high-level idea is:\n\n\nWe are going to use the main Chainlink oracles instead of building our own.\n\n\nThe Uniswap anchor will be tightened to 15%. (I am going to do some research, but 15% is what we’re going with as of now)\n\n\nUSDC and USDT are going to be continued to be pegged at 1. If someone wants to use USDT as collateral in the future, they will need to add an oracle.\n\n\nIf a Chainlink price is outside of the anchor, it will not update. We will add an additional feature that will allow the community multisig to choose to use the Uniswap price.\n\n\nThe Chainlink team will get started on this change, and I’ll follow up with more details and docs as we have them.\nOverall I think this is good news, and I am excited about the progress.\\nThanks for the update @getty. I’m pleased to hear we are honing in on an oracle implementation that has broad community support. FWIW I tend to agree with Robert in that Compound governance’s 7-day process is not ideally suited to manage reporters, and going with a “core” solution is the safer bet.\nLooking forward to the next update!\\n@getty I’m very excited to hear this! Thanks for the hard work everyone. \nPlease let me know how I can help with this.\\nHey everyone, this post is a follow up on the recent developer call we had describing the proposal to switch the current oracle mechanism to Chainlink price feeds combined with a Uniswap anchor acting as a safety net.\nThis proposal is based on the feedback of Compound community members. Today, I want to describe the implementation we put in place and ensure everyone understands the workflow this new oracle system will have if voted in. The last part of this post will also contain links to testing contracts we’ve deployed on mainnet.\n\nCurrent Mechanism\nFirst off, I would like to give a quick description of the current system so we all have shared context. The UniswapAnchoredView (UAV) is currently configured with multiple markets, each containing the name, cToken, underlying token, Uniswap V2 pool data and conversion data. Prices are posted to it via a configured reporter. If it is believed that the reporter account has been compromised (private keys stolen, etc.), there is a function called invalidateReporter(), which can be called by the reporter which permanently switches over to the Uniswap V2 TWAP prices.\nSigned prices are posted to the UniswapAnchoredView regularly, by the reporter. During this transaction, the following steps occur:\n\nSignatures are verified\nFor each posted price:\n\nUpdate the Uniswap V2 TWAP window if due\nUpdate the anchor price\n\nIf posted price is within threshold of anchor price: store the posted price\n\nElse, if posted price is outside of the threshold: store the anchor price\n\n\n\n\n\n\n\nNew Community Proposal\n\nUniswapAnchoredView Modifications\nThe community indicated their desire to switch over to Chainlink production price feeds in place of the current reporter. They also indicated a desire to keep a Uniswap Anchor as a safety net to remain in place. Hence, our current proposal modifies the existing architecture whilst maintaining the same anchor mechanism. This architecture is very much optimized to give compound users low gas costs.\nThe key differences are:\n\nPrices will now be pushed from production aggregators, each responsible for a single asset. They call the validate() function, which replaces the postPrices() function. This change is so that the UAV conforms to the AggregatorValidatorInterface , which production aggregators are compatible with.\nThe Compound community multisig will be able to failover single markets, on a per-market basis. In this scenario (as in the current architecture) the Uniswap V2 TWAP price is used as the failover. Function: activateFailover(bytes32 symbolHash)\n\nThe community can also reverse this failover. Function: deactivateFailover(bytes32 symbolHash)\n\n\n\nPrice Feeds\nUsually, contracts that consume price feeds read the data from an AggregatorProxy contract, which itself reads the price from an underlying OffchainAggregator contract. This proxy pattern is used so that the underlying aggregator can be upgraded without disruption of service for contracts that consume prices from the proxy. More information on this model in the docs: https://docs.chain.link/docs/architecture-decentralized-model/#contracts-overview 9\nUniswapAnchoredView's anchor mechanism relies on prices being pushed to it, not read, so reading from aggregator contracts (as shown in our initial proposal: https://github.com/compound-finance/compound-protocol/pull/105 6) is not feasible. However, OffchainAggregator contracts enable a validator to be set. Every price transmission that the aggregator receives from nodes is passed to the validator via the validate() function. This is the entry point for the modified UAV. Upon deployment, production aggregators will have their validator set to point to the UAV.\n\nAdding new markets\nGiven how UniswapAnchoredView is written, any changes (like adding a new market) requires a fresh deployment with a new configuration, which then goes through governance to be set as the oracle in the Comptroller.\nAn example: If the community deploys a new UAV, the OffchainAggregator of each asset would need to have its validator changed to push prices to the new UAV. Changing the validator would mean that the old UAV no longer receives prices from the aggregator. A transition period exists where both UAVs need to have prices pushed to them so that:\n\nService is not disrupted on the existing UAV.\nThe new UAV can be tested and have its prices verified on-chain before being accepted by governance.\n\nTo solve this challenge, we have created the ValidatorProxy contract to sit in between each OffchainAggregator and the UniswapAnchoredView . For the most part, this contract just forwards validate() calls from the aggregator to the UAV. However, It also supports a “proposed” validator (a second UAV), which it also pushes prices to when it is set.\n\nSimple Architecture\nThis image describes 4 asset prices being pushed to the UniswapAnchoredView. It does not include the anchor mechanism that is performed during the validate() call on the UAV.\n\nScreenshot 2021-05-07 at 12.03.52860×1144 145 KB\n\n\nUniswap Anchor Flow\nThis anchor mechanism has not changed in our proposal. Here is a flowchart (might need to zoom in on this one) representation of the BAT/USD price being posted to it via the validate() function:\n\nUAV_Validate2x_(2)2488×5536 738 KB\n\n\nMainnet Tests\nTo test the modifications to the UAV and the overall architecture, we deployed the contracts to mainnet and tested them using a node network providing prices for the UNI/USD market over the course of roughly 2 days. Here are the addresses:\n\n\nUNI / USD Aggregator: 7 0x6235b643251401F2c1bf8cE901F09aC84fbC0FCF\n\n\nValidatorProxy: 3 0xE2093Baf575b3f71eA2a4f15aC70E0bbEeca0b36\n\n\nUniswapAnchoredView: 4 0x52DF1FA8C48efeD68395602EEfAc213C20b02301\n\n\nProposed UniswapAnchoredView: 6 0xdd7294BBF8cB758B132AA3855BA1A16bdc64Acf9\n\n\n\nCode\n\nUniswapAnchoredView\nChainlink has forked the Compound open-oracle repo to make the proposed changes to the UniswapAnchoredView. This PR is being used for internal reviews and can be seen here: https://github.com/smartcontractkit/open-oracle/pull/1 13.\nOnce this has been audited in the coming few days, Chainlink will open a pull request from our fork to the Compound open-oracle repo.\n\nValidatorProxy\nInitial PR: https://github.com/smartcontractkit/chainlink/pull/4265 7\nSecond PR: https://github.com/smartcontractkit/chainlink/pull/4301 10\n\nAggregators\nChainlink aggregator code can be found in the libocr repo: https://github.com/smartcontractkit/libocr 6\n\nConclusion\nThis should be it with all the details for this proposal, if you have any questions please reach out here, I’ll be happy to answer it. We’re looking forward for a long term collaboration with the Compound community and making sure we can help build out the best oracle system available for Compound to keep being a leader in the money market space through the use of secure and accurate oracles. As Uniswap v3 gains momentum we will be looking to work with the community in upgrading to its oracles as a new safety net mechanism and have already started coordinating this process with their team.\\nThanks to @Johann_Eid for writing this up, and to everyone for the hard work that’s gone into the proposal thus far. The amount of effort that’s gone into it is impressive, and makes me very optimistic about this collaboration.\nThe price oracle is hugely important to the protocol, which makes it one of the most difficult pieces to change, especially in terms of convincing governance that its safe and correct. Personally, I see the value that switching to Chainlink could provide to the protocol, but its important we uphold a level of scrutiny for any changes being made. After discussing with some colleagues, I would like to voice a few remaining questions/concerns from Compound Labs about the current architecture.\nThe first question is about the set of aggregators and the redundancy of the price posting. How will the set of aggregators be managed? It seems we would need to deploy a new UniswapAnchoredView each time, and have some knowledge of the validator set of Chainlink. What would be the process for choosing these validators, getting their addresses, and keeping the list updated? This could be a huge burden on governance, which it isn’t really designed to be able to handle, and probably couldn’t do well. We think a solution here could be to use the shared Chainlink on-chain price feed, and the UniswapAnchoredView contract could be poked to read updates from it.\nThe second question is around adding new markets. The main concern is that the proposed process for adding assets might also be too difficult to manage. Governance would now need to get all the individual validators to add the new asset first, and a new view is deployed with a transition period? The proxy does avoid the need to update the oracle in the Comptroller, but adds gas costs to every transaction utilizing a price. It’s worth nothing that the protocol actually used to use a proxy, prior to Open Oracle. If the protocol were to use the existing Chainlink feed however, this might also be less of a problem?\nIf we don’t have a clear understanding and agreement about how these things will be managed, I would worry that governance would likely reject the current proposal. Since we don’t want to see any wasted effort, it would be great to make sure this is architected around processes that governance can manage and will accept immediately.\nIt’s exciting to see the progress here, and I have faith in our ability to resolve these remaining open questions effectively .\\nThank you for that feedback.\n\n\n\n jared:\n\nThe first question is about the set of aggregators and the redundancy of the price posting. How will the set of aggregators be managed?\n\n\nGreat question, this was part of the talks we had between the community. At first, the idea was to have governance or the community multisig manage this directly and select specific validators. As you’re pointing out here, this process could have been quite cumbersome and time consuming for the governance process. We found that the best way forward would be to utilize the existing Chainlink price feeds 4 that a lot of protocols are already relying on like Aave, Synthetix, and others. These feeds already have a process in place to manage the selection of oracle nodes and data providers for each price feed. Since it’s the core focus of Chainlink Labs and they’ve been doing it for years, the process is very robust and thorough so I believe it will be good to lean on their experience here.\n\n\n\n jared:\n\nThe second question is around adding new markets.\n\n\nWhenever a new market needs to be added, it will be up to the proposer to create a new Uniswap Anchor View contract. The UAV would just need to point to the ValidatorProxy address for the price feed we need. The protocol already needs to launch a new Uniswap Anchor View in the current system anytime a new asset is added so it would be quite similar to what we have today.\nTo add a new asset to the system I believe we will have to follow the following steps\n\nCommunity Proposal: the proposer should start a forum post where they outline why a coin should be added to the protocol, define the protocol parameters of the asset, and gather sufficient interest in the proposal.\nPrice Feed: once it is clear the community wants to list the asset, a price feed can be launched. Based on the assets Chainlink currently supports (https://data.chain.link/ 2) I believe most assets we vote in will already have a corresponding price feed already live. Otherwise, the Chainlink network can easily deploy a new one like it’s been doing with its current users before us.\nLaunch: The proposer will be responsible for launching a new Uniswap Anchor View. Adding an asset to the protocol will be a one-vote process where the new ctoken is added, and the Uniswap Anchor View contract is updated.\n\nThis process will need some fine tuning and like everything new will require some learning and adaptation, but the protocol will have the ability to list more assets instead of waiting on Coinbase for support and hoping that the volume there is good enough for us to get it listed.\nBased on the community discussions I have had, this proposal has enough support for it to pass. I think we should see this as a first step towards improving the oracle system which I aim to improve and make the oracle system more reliable and easy to use. With the launch of Uniswap v3 the Chainlink guys have already discussed their willingness to work with us in adding it as a circuit breaker and proposing some new code there as the v3 volume grows.\\nI have some questions/concerns about the suggested architecture, and to echo @jared think there needs a higher level of scrutiny before making such a large transition. Basic information and descriptions (in addition to code) would be extremely useful here.\nIn the status quo, updating prices is a non-privileged function; in this suggested approach, there appears to be a validator role; who or what controls this function? If there is already a “fresh” aggregator price, will this validator slow down the process of “pushing” a price?\nWhy does this contract need to exist at all? It adds a friction to switching UAVs, and a dependency on a single address. Does it also duplicate the “AggregatorProxy” pattern? Who is responsible for rotating validators?\nDoes this proposed system use the existing offchain-aggregators, that are already in use by other projects, or create duplicates?\nWere other approaches considered? The “push” model only existed in order to get signed price data on-chain in the first place; it adds inefficiencies here, and there should be no reason or constraint to try to use the existing UAV pattern. As an alternative, consider:\nThe UAV has a public function to update anchors; getUnderlyingPrice reads the current price from an AggregatorProxy, and returns it if it is within the anchor.\\n\nWe found that the best way forward would be to utilize the existing Chainlink price feeds 2 that a lot of protocols are already relying on like Aave, Synthetix, and others.\n\nMaybe I am missing something then in the materials that were shared above, but it looks to me (looking at the actual contracts), that there is a list of reporters statically included in the view who are allowed to call validate, and this is going to be difficult to manage. I would be glad to be shown otherwise.\n\nLaunch: The proposer will be responsible for launching a new Uniswap Anchor View. Adding an asset to the protocol will be a one-vote process where the new ctoken is added, and the Uniswap Anchor View contract is updated.\n\nAgain, I don’t see this looking at what’s actually being proposed (and happy to know if I am missing something). It looks to me like the list of included reporters will then need to actively post separately to this view contract, which means those reporters need to start reporting the new asset, which I believe would need to be communicated to them as well somehow.\\nGood questions\n\n\n\n rleshner:\n\nthere appears to be a validator role; who or what controls this function?\n\n\nIn the current open oracle design used in production today, the postPrices function is not explicitly privileged, only prices given by the reporter can update the contract. In this proposal, the validator is deemed to be the UAV contract itself, which validates the data is being sent from the correct Chainlink Price Feed.\n\n\n\n rleshner:\n\nIf there is already a “fresh” aggregator price, will this validator slow down the process of “pushing” a price?\n\n\nEvery time a price feed updates its on-chain aggregator contract with the freshest value, it also updates the UAV contract with this same data, thereby providing the Compound protocol data as soon as it arrives on-chain. With this design, there is no need for a relayer as data being pushed to the UAV happens in the same transaction as the Chainlink Price Feed update. Additionally, the Uniswap TWAP value is updated if it has become stale during this transaction.\n\n\n\n rleshner:\n\nWhy does this contract need to exist at all?\n\n\nI believe you are referring to the ValidatorProxy, that contract routes price data from the relevant aggregator contracts to the UAV contract (occurs in a single transaction). The addition of this proxy contract is key to ensuring not only the current UAV contract used by the Comptroller is updated but also any newly deployed UAV contract that governance wants the Comptroller to reference going forward. This proxy contract design makes the process of upgrading to a new UAV contract (such as when adding a new asset) much simpler, with significantly less risk of downtime when the switch occurs. Therefore, Compound governance can operate on its own time frame as the UAV proxy contracts ensure both the current and future UAV contracts are updated.\nThe reason why the UAV contract is being used is that the community wanted to use a TWAP value from Uniswap v2 as an anchor to check for devitations, just as the system does today for additional protection. Using the existing contracts will make the transition easier as the community is comfortable with code they already know and have had live with for a long time.\n\n\n\n rleshner:\n\nDoes this proposed system use the existing offchain-aggregators, that are already in use by other projects, or create duplicates?\n\n\nThe AggregatorProxy contract makes prices readable by consumers using a pull model. The ValidatorProxy contract pushes price data directly to the UAV, where it can then be pulled by the Comptroller at any time. The Chainlink Labs team will be responsible for managing the oracle node-set in the underlying aggregator contract, but the validator address would always be the current UAV contract defined in the Comptroller (determined by Compound governance). Please keep in mind that the additional costs associated with pushing the data are handled by the node operators and won’t affect the Compound users.\n\n\n\n rleshner:\n\nDoes this proposed system use the existing offchain-aggregators, that are already in use by other projects, or create duplicates?\n\n\nThis is correct; it would be leveraging the existing Chainlink Price Feed networks used by other protocols such as Sytnetix and Aave (as shown on https://data.chain.link 1). There is no duplication of feeds.\n\n\n\n rleshner:\n\nWere other approaches considered?\n\n\nA pull approach was considered for this proposal, but the community preferred a solution that was much closer to the existing UAV contract used today. We do agree with this approach as it will be much easier to have a transition with a model that the community already knows and is familiar with.\n\n\n\n jared:\n\nMaybe I am missing something then in the materials that were shared above, but it looks to me (looking at the actual contracts), that there is a list of reporters statically included in the view who are allowed to call validate , and this is going to be difficult to manage. I would be glad to be shown otherwise.\n\n\nIn the existing solution, there is a single reporter which is the only entity that affects what prices are stored in the view. This proposal uses Chainlink Price Feeds, where each market has a reporter that can push prices. These reporters are the Chainlink price feed aggregator contracts (updated by a decentralized network of nodes), which each push through a ValidatorProxy (seen in the whiteboard diagram above).\n\n\n\n jared:\n\nAgain, I don’t see this looking at what’s actually being proposed (and happy to know if I am missing something). It looks to me like the list of included reporters will then need to actively post separately to this view contract, which means those reporters need to start reporting the new asset, which I believe would need to be communicated to them as well somehow.\n\n\nThis is where the ValidatorProxy contracts come into play. They enable a second UAV to be deployed, where the price of a single price feed can be pushed to the existing UAV and the new UAV without disruption of service. The ValidatorProxy is a reporter on both UAVs for a specific market.\nHere is a good resource to see how the price is updated: https://ethtx.info/mainnet/0x06dad1a4215d510aa18ee21999216877c4ca5ea37b042088b8565eca6c210571 8\\nThanks for the explanations @getty . So the reporters are expected to be a set of contract addresses - I didn’t appreciate that distinction initially, glad you called it out. In practice though still, how does governance choose this set and under what conditions do you think we might change it? I think this is the part we need to be really clear on and walk through how it would work\nEDIT: Now I see, each market is a reporter here https://data.chain.link 18, so modifying assets is changing reporters IIUC \\n\nThe contracts have been deployed!\nContract Address/UniswapAnchorView config:\n\nNew UniswapAnchoredView: 0x34aB75D077c148F1eCAa435e7125c16842A0AeA3 11\n\nCurrent UniswapAnchoredView: 0x4007B71e01424b2314c020fB0344b03A7C499E1A 3\n\n\nReported Prices\nThe proposed UniswapAnchoredView has prices pushed to it by production Chainlink Price Feeds, each through a ValidatorProxy (a reporter). The frequency of the updates depends on a number of triggers, namely:\n\nDeviation threshold - when a price is witnessed to have moved more than x% of the previously reported price.\nHeartbeat threshold - If x minutes have passed without an update.\n\nMore information can be found on data.chain.link 6. Example: ETH / USD 2.\nPrice update events can be witnessed on the Etherscan events tab 3.\nMarket Configurations\nThe configuration of each market in the new UAV (which is mostly similar to the current one).\n\ncToken - address\nunderlying - address\nsymbolHash - bytes32\nbaseUnit - unit256\nPriceSource - priceSource\nfixedPrice - unit256\nuniswapMarket - address\nreporter - address (new)\nreporterMultiplier - unit256 (new)\nisUniswapReversed - bool\n\nreporter and reporterMultiplier are new.\n\nreporter is the address that submits prices for a particular cToken. This is always a ValidatorProxy contract that is called by a price feed aggregator contract for each relevant price update posted by Chainlink oracle nodes. In this case, 0xeBa6F33730B9751a8BA0b18d9C256093E82f6bC2 is the reporter of the price of BAT.\nreporterMultiplier is used to transform the price reported by the reporter to the correct base unit that the UniswapAnchoredView expects. This is required because Chainlink Price Feeds report prices with different decimal placement than the UniswapAnchoredView.\n\nHere is a breakdown of all of the configs\n\n\nETH\n\ncToken: 0x4Ddc2D193948926D02f9B1fE9e1daa0718270ED5\nunderlying: 0x0000000000000000000000000000000000000000\nsymbolHash: 0xaaaebeba3810b1e6b70781f14b2d72c1cb89c0b2b320c43bb67ff79f562f5ff4\nbaseUnit: 1000000000000000000\npriceSource: 2\nfixedPrice: 0\nuniswapMarket: 0xB4e16d0168e52d35CaCD2c6185b44281Ec28C9Dc\nreporter: 0x264BDDFD9D93D48d759FBDB0670bE1C6fDd50236\nreporterMultiplier: 10000000000000000\nisUniswapReversed: true\n\n\n\n\nUSDC\n\ncToken: 0x39AA39c021dfbaE8faC545936693aC917d5E7563\nunderlying: 0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48\nsymbolHash: 0xd6aca1be9729c13d677335161321649cccae6a591554772516700f986f942eaa\nbaseUnit: 1000000\npriceSource: 1\nfixedPrice: 1000000\nuniswapMarket: 0x0000000000000000000000000000000000000000\nreporter: 0x0000000000000000000000000000000000000000\nreporterMultiplier: 1\nisUniswapReversed: false\n\n\n\n\nDAI\n\ncToken: 0x5d3a536E4D6DbD6114cc1Ead35777bAB948E3643\nunderlying: 0x6b175474e89094c44da98b954eedeac495271d0f\nsymbolHash: 0xa5e92f3efb6826155f1f728e162af9d7cda33a574a1153b58f03ea01cc37e568\nbaseUnit: 1000000000000000000\npriceSource: 2\nfixedPrice: 0\nuniswapMarket: 0xA478c2975Ab1Ea89e8196811F51A7B7Ade33eB11\nreporter: 0xb2419f587f497CDd64437f1B367E2e80889631ea\nreporterMultiplier: 10000000000000000\nisUniswapReversed: false\n\n\n\n\nWBTC\n\ncToken: 0xC11b1268C1A384e55C48c2391d8d480264A3A7F4\nunderlying: 0xC11b1268C1A384e55C48c2391d8d480264A3A7F4\nsymbolHash: 0xe98e2830be1a7e4156d656a7505e65d08c67660dc618072422e9c78053c261e9\nbaseUnit: 100000000\npriceSource: 2\nfixedPrice: 0\nuniswapMarket: 0xBb2b8038a1640196FbE3e38816F3e67Cba72D940\nreporter: 0x4846efc15CC725456597044e6267ad0b3B51353E\nreporterMultiplier: 1000000\nisUniswapReversed: false\n\n\n\n\nUSDT\n\ncToken: 0xf650C3d88D12dB855b8bf7D11Be6C55A4e07dCC9\nunderlying: 0xdac17f958d2ee523a2206206994597c13d831ec7\nsymbolHash: 0x8b1a1d9c2b109e527c9134b25b1a1833b16b6594f92daa9f6d9b7a6024bce9d0\nbaseUnit: 1000000\npriceSource: 1\nfixedPrice: 1000000\nuniswapMarket: 0x0000000000000000000000000000000000000000\nreporter: 0x0000000000000000000000000000000000000000\nreporterMultiplier: 1\nisUniswapReversed: false\n\n\n\n\nUNI\n\ncToken: 0x35A18000230DA775CAc24873d00Ff85BccdeD550\nunderlying: 0x1f9840a85d5af5bf1d1762f925bdaddc4201f984\nsymbolHash: 0xfba01d52a7cd84480d0573725899486a0b5e55c20ff45d6628874349375d1650\nbaseUnit: 1000000000000000000\npriceSource: 2\nfixedPrice: 0\nuniswapMarket: 0xd3d2E2692501A5c9Ca623199D38826e513033a17\nreporter: 0x70f4D236FD678c9DB41a52d28f90E299676d9D90\nreporterMultiplier: 10000000000000000\nisUniswapReversed: false\n\n\n\n\nCOMP\n\ncToken: 0x70e36f6BF80a52b3B46b3aF8e106CC0ed743E8e4\nunderlying: 0xc00e94cb662c3520282e6f5717214004a7f26888\nsymbolHash: 0xb6dbcaeee318e11fe1e87d4af04bdd7b4d6a3f13307225dc7ee72f7c085ab454\nbaseUnit: 1000000000000000000\npriceSource: 2\nfixedPrice: 0\nuniswapMarket: 0xCFfDdeD873554F362Ac02f8Fb1f02E5ada10516f\nreporter: 0xE270B8E9d7a7d2A7eE35a45E43d17D56b3e272b1\nreporterMultiplier: 10000000000000000\nisUniswapReversed: false\n\n\n\n\nZRX\n\ncToken: 0xB3319f5D18Bc0D84dD1b4825Dcde5d5f7266d407\nunderlying: 0xe41d2489571d322189246dafa5ebde1f4699f498\nsymbolHash: 0xb8612e326dd19fc983e73ae3bc23fa1c78a3e01478574fa7ceb5b57e589dcebd\nbaseUnit: 1000000000000000000\npriceSource: 2\nfixedPrice: 0\nuniswapMarket: 0xc6F348dd3B91a56D117ec0071C1e9b83C0996De4\nreporter: 0x5c5db112c98dbe5977A4c37AD33F8a4c9ebd5575\nreporterMultiplier: 10000000000000000\nisUniswapReversed: true\n\n\n\n\nBAT\n\ncToken: 0x6C8c6b02E7b2BE14d4fA6022Dfd6d75921D90E4E\nunderlying: 0x0d8775f648430679a709e98d2b0cb6250d2887ef\nsymbolHash: 0x3ec6762bdf44eb044276fec7d12c1bb640cb139cfd533f93eeebba5414f5db55\nbaseUnit: 1000000000000000000\npriceSource: 2\nfixedPrice: 0\nuniswapMarket: 0xB6909B960DbbE7392D405429eB2b3649752b4838\nreporter: 0xeBa6F33730B9751a8BA0b18d9C256093E82f6bC2\nreporterMultiplier: 10000000000000000\nisUniswapReversed: false\n\n\n\n\nTUSD\n\ncToken: 0x12392F67bdf24faE0AF363c24aC620a2f67DAd86\nunderlying: 0x0000000000085d4780B73119b644AE5ecd22b376\nsymbolHash: 0xa1b8d8f7e538bb573797c963eeeed40d0bcb9f28c56104417d0da1b372ae3051\nbaseUnit: 1000000000000000000\npriceSource: 1\nfixedPrice: 1000000\nuniswapMarket: 0x0000000000000000000000000000000000000000\nreporter: 0x0000000000000000000000000000000000000000\nreporterMultiplier: 1\nisUniswapReversed: false\n\n\n\n\nLINK\n\ncToken: 0xFAce851a4921ce59e912d19329929CE6da6EB0c7\nunderlying: 0x514910771af9ca656af840dff83e8264ecf986ca\nsymbolHash: 0x921a3539bcb764c889432630877414523e7fbca00c211bc787aeae69e2e3a779\nbaseUnit: 1000000000000000000\npriceSource: 2\nfixedPrice: 0\nuniswapMarket: 0xa2107FA5B38d9bbd2C461D6EDf11B11A50F6b974\nreporter: 0xBcFd9b1a97cCD0a3942f0408350cdc281cDCa1B1\nreporterMultiplier: 10000000000000000\nisUniswapReversed: false\n\n\n\n\nSAI\n\ncToken: 0xF5DCe57282A584D2746FaF1593d3121Fcac444dC\nunderlying: 0x89d24a6b4ccb1b6faa2625fe562bdd9a23260359\nsymbolHash: 0x4dcbfd8d7239a822743634e138b90febafc5720cec2dbdc6a0e5a2118ba2c532\nbaseUnit: 1000000000000000000\npriceSource: 0\nfixedPrice: 5285000000000000\nuniswapMarket: 0x0000000000000000000000000000000000000000\nreporter: 0x0000000000000000000000000000000000000000\nreporterMultiplier: 1\nisUniswapReversed: false\n\n\n\n\nREP\n\ncToken: 0x158079Ee67Fce2f58472A96584A73C7Ab9AC95c1\nunderlying: 0x1985365e9f78359a9B6AD760e32412f4a445E862\nsymbolHash: 0x91a08135082b0a28b4ad8ecc7749a009e0408743a9d1cdf34dd6a58d60ee9504\nbaseUnit: 1000000000000000000\npriceSource: 2\nfixedPrice: 0\nuniswapMarket: 0x8979A3Ef9D540480342AC0F56e9D4c88807b1CBa\nreporter: 0x90655316479383795416B615B61282C72D8382C1\nreporterMultiplier: 10000000000000000\nisUniswapReversed: false\n\n\nCompound Community Multisig Ownership\nThe owner of the proposed UniswapAnchoredView has the power to activate and deactivate the failover mechanism for each market. This was implemented based on community feedback and in case any low liquidity market on Uniswap v2 needs to be deactivated.\nCurrently, the community multisig is not the owner. However, the current owner has proposed a transfer of ownership in this transaction 3. To obtain ownership, the multisig must call acceptOwnership() at the proposed UniswapAnchoredView address: 0x34aB75D077c148F1eCAa435e7125c16842A0AeA3\nTrail of Bits Audit\nTrail of Bits was commissioned for audit of the contracts. Here is a link to it: link 17\nNext Steps\nNow that everything is deployed and ready for a vote the community can review the changes. I am aiming to create a governance proposal Tuesday the 1st or Wednesday the 2nd. Feel free to shoot me a message on Discord or post your questions/comments here.\\nVery exciting! Thank you for all the hard work on this initiative @getty. I’m looking forward to seeing what the community thinks getting this put to a vote.\\nThe Compound Community Multisig is voting to accept the ownership of the new UniswapAnchoredView at 0x34aB75D077c148F1eCAa435e7125c16842A0AeA3.\nCheck how the vote is going at\nGnosis Safe (gnosis-safe.io) 50\\nI found a small/cosmetic issue with the UniswapAnchorView deployment. The cWBTC v2 token configuration had the cWBTC v1 address instead of the v2 address. While this wouldn’t affect anything within the Compound protocol and has zero impact on the price reporting, we thought it would be best to correct it and redeploy.\nI have coordinated with the Chainlink team, and they have redeployed the UAV and pointed the validator proxy contract to the new UAV. The community multisig will need to accept ownership of the UAV at 0x841616a5CBA946CF415Efe8a326A621A794D0f97 15.\\nHey everyone, to provide some additional insight to some of the questions raised on the dev call today in the Discord, I wanted to provide some data and analytics info to show that the integration work with Chainlink price feeds and Compound is working as expected without issue.\nFirst to note, the UniswapAnchoredView (UAV) contract used here is the same UAV contract that is currently being used in-production by the Compound protocol. It uses the same logic of ingesting data from a reporter, checking it against the Uniswap V2 TWAP, and storing the price if it is within the deviation threshold. The only change has been switching the reporter from Coinbase to Chainlink price feeds and a conversion of the decimal spacing to align to what is already being used. Additionally, the deviation threshold between the reporter and Uniswap has been lowered from 20% to 15%, providing additional protection against any potential outlier data.\nThe deployed UAV contract address is 0x841616a5CBA946CF415Efe8a326A621A794D0f97 and the recent updates can be seen on the Etherscan events page 4. PriceUpdated() is an event emitted when the stored price is updated and PriceGuarded() is an event when new prices are posted but the stored price is not updated due to the anchor deviation. The contract is also open-source and has been verified on Etherscan. Additionally, the UAV contract has been audited by Trail of Bits and the ValidatorProxy contracts has been audited by SigmaPrime.\nThe Chainlink Price Feeds being used by the UAV are the same data feeds that are already being used by other DeFi projects like Syntheix and Aave, and have operated for many months without issue. This includes during the recent market crash and network congestion when gas prices on Ethereum mainnet spiked to 2,500 Gwei. A visualization for these feeds is available from data.chain.link 3, which displays data that is stored on-chain and can be cross-referenced with the contracts on Etherscan. Below are the contract addresses and Etherscan links to the events page for each price feed which delivers data to the UAV through the ValidatorProxy contracts.\nETH/USD:\n\nAggregator: 0x37bC7498f4FF12C19678ee8fE19d713b87F6a9e6 6\n\nValidatorProxy: 0x264BDDFD9D93D48d759FBDB0670bE1C6fDd50236 4\n\nChainlink Visual: data.chain.link/eth-usd 8\n\n\nUSDC/USD:\n\nHardcoded at $1\n\nDAI/USD:\n\nAggregator: 0xDEc0a100eaD1fAa37407f0Edc76033426CF90b82 2\n\nValidatorProxy: 0xb2419f587f497CDd64437f1B367E2e80889631ea 1\n\nChainlink Visual: data.chain.link/dai-usd 3\n\n\nWBTC/USD:\n\nAggregator: 0xAe74faA92cB67A95ebCAB07358bC222e33A34dA7 3\n\nValidatorProxy: 0x4846efc15CC725456597044e6267ad0b3B51353E 1\n\nChainlink Visual: data.chain.link/btc-usd 1\n\n\nUSDT/USD:\n\nHardcoded at $1\n\nUNI/USD:\n\nAggregator: 0x68577f915131087199Fe48913d8b416b3984fd38 2\n\nValidatorProxy: 0x70f4D236FD678c9DB41a52d28f90E299676d9D90\n\nChainlink Visual: data.chain.link/uni-usd\n\n\nCOMP/USD:\n\nAggregator: 0x6eaC850f531d0588c0114f1E93F843B78669E6d2 2\n\nValidatorProxy: 0xE270B8E9d7a7d2A7eE35a45E43d17D56b3e272b1 1\n\nChainlink Visual: data.chain.link/comp-usd 3\n\n\nZRX/USD:\n\nAggregator: 0x3d47eF9690Bd00C77c568b73140dC20F34453766 1\n\nValidatorProxy: 0x5c5db112c98dbe5977A4c37AD33F8a4c9ebd5575\n\nChainlink Visual: data.chain.link/zrx-usd\n\n\nBAT/USD:\n\nAggregator: 0xd90CA9ac986e453CF51d958071D68B82d17a47E6 2\n\nValidatorProxy: 0xeBa6F33730B9751a8BA0b18d9C256093E82f6bC2 1\n\nChainlink Visual: data.chain.link/bat-usd\n\n\nTUSD/USD:\n\nHardcoded at $1\n\nLINK/USD:\n\nAggregator: 0xDfd03BfC3465107Ce570a0397b247F546a42D0fA 2\n\nValidatorProxy: 0xBcFd9b1a97cCD0a3942f0408350cdc281cDCa1B1\n\nChainlink Visual: data.chain.link/link-usd\n\n\nSAI/USD:\n\nHardcoded to 0.005285 ETH\n\nREP/USD:\n\nAggregator: 0x9AdF01321833A5Cba51B9f8A4C420C7e62481Ae5 1\n\nValidatorProxy: 0x90655316479383795416B615B61282C72D8382C1\n\nChainlink Visual: data.chain.link/rep-usd 2\n\n\n\nAuditing the Chainlink Visualization Page\nThe values displayed on the Chainlink visualization page can be audited by using the Etherscan events tab on the Aggregator contract of each market (hyperlinks above). Each price feed update generates an AnswerUpdated() event, which has a field [topic1] containing the new value encoded in hexadecimal. This value can be converted into decimals to see the price data and divided by 10^8 to get the correct price with decimals to then compare against the data displayed on data.chain.link.\nFor example, on the BAT/USD feed, this update transaction has an index_topic_1 int256 current value of 0x0000000000000000000000000000000000000000000000000000000004ca1b37. When converting into decimal, this results in a value of 80354103. When divided by 10^8, this results in a value of 0.80354103. When looking at the BAT/USD visualization page, this same value is shown in USD form $0.80354103.\nEthtx.info 2 does a nice job showing the execution trace and emitted events.\n\n1506×250 20.7 KB\n\n\n654×514 20.2 KB\n\n\n553×587 26.5 KB\n\nTo showcase that the decimal conversion in the UAV contract is occurring as expected, here is output of a script showing a comparison of pricing data from the production open oracle system with Coinbase, the proposed UAV with Chainlink price feeds, and the market-wide price from CoinGecko.\nValues retrieved at: 2021-06-02 18:15:10.727044\nbat\nproduction: 835071000000000000\nproposed: 811253000000000000\ncoingecko: 806510000000000000\ncomp\nproduction: 449050000000000000000\nproposed: 447514145000000000000\ncoingecko: 448400000000000000000\ndai\nproduction: 1001271000000000000\nproposed: 1001143000000000000\ncoingecko: 1000000000000000000\neth\nproduction: 2772600000000000000000\nproposed: 2768319267000000000000\ncoingecko: 2760569999999999934464\nlink\nproduction: 30696265000000000000\nproposed: 31272512000000000000\ncoingecko: 31260000000000000000\nrep\nproduction: 29540000000000000000\nproposed: 26186473000000000000\ncoingecko: 26620000000000000000\nsai\nproduction: 14653191000000000000\nproposed: 14630567000000000000\ncoingecko: 14070000000000000000\ntusd\nproduction: 1000000000000000000\nproposed: 1000000000000000000\ncoingecko: 999673000000000000\nuni\nproduction: 27319550000000000000\nproposed: 28170000000000000000\ncoingecko: 28170000000000000000\nusdc\nproduction: 1000000000000000000000000000000\nproposed: 1000000000000000000000000000000\ncoingecko: 999315000000000000000000000000\nusdt\nproduction: 1000000000000000000000000000000\nproposed: 1000000000000000000000000000000\ncoingecko: 1000000000000000000000000000000\nbtc\nproduction: 380981900000000000000000000000000\nproposed: 378318378280000000000000000000000\ncoingecko: 378790000000000002621440000000000\nbtc2\nproduction: 380981900000000000000000000000000\nproposed: 378318378280000000000000000000000\ncoingecko: 378790000000000002621440000000000\nzrx\nproduction: 1147138000000000000\nproposed: 1149519000000000000\ncoingecko: 1149999999999999872\nAs shown above, the decimal placement is correct and the proposed price feeds closely tracks the market-wide CoinGecko price.\nTo check each asset’s token configuration in the UniswapAnchorView contract go to the current UAV and the proposed 2 UAV to compare them. Each contract is verified on Etherscan. To verify the token configuration use Etherscan’s read contract functionality and one/all of the getTokenConfigBy functions.\nThe proposed UAV also has an “owner” who has the ability to engage the failover function. Chainlink transferred ownership of the proposed UAV to the Compound 1 community 1 multisig  4\nIf there is any additional information the community is interested in, let me know.\\nAre the mentioned simulation scripts available for the community to see? Also, is a network fork simulation which simulates the proposal execution and sanity checks everything available?\\nSure, you can run this script Compare UAVs · GitHub 11\nFor each supported market, this script calls getUnderlyingPrice() on both the current production UAV (being used by the protocol right now), and the proposed UAV. It prints the results as returned.\nRegarding your second question, we thought about doing a fork, but it is pointless without the needed Uniswap pools and would be pretty complicated to set up and test; we do have something better to check that everything is working. All the contracts are deployed on mainnet and functioning as expected in production. We (anyone) can prove the proposed UAV doesn’t change the output of prices compared to the current UAV.\\nI ran the script and got sensible results.\nBut I think arr00 is also concerned about the proposal execution itself. Is it just a single call to Comptroller._setPriceOracle(0x8416...)?\\nUsing the Compound protocol tools built into Github, forking simulations are actually quite easy to make. I’ve run them for every proposal I’ve created. Just take a look at this folder 22.\\nThanks, @arr00, I didn’t know about that. I’ll dig in.\nBy the way, I threw together a site for comparing the current 7 oracle’s prices, the proposed 4 oracle’s prices, and Chainlink’s 1 price. defialerts.io/oracle 30, it shows the block number from when it pulled the data, and I have it updating every 10 minutes right now. Here is the Github repo for it as well: GitHub - gettty/ChainlinkCompound 25.\\nthanks @getty, appreciate the work.  One question: does it not matter that the number of significant digits between current and proposed are different?\\nWhat do you mean by different? The precision of the prices doesn’t matter, just that the price is being expressed correctly. If you’re referring to the website I put together, you can see in the repo that no manual adjustments are being made and that I am reading the contracts directly.\\nThe proposed change is now live and in review. Voting will begin in 48 hours.\n  \n      \n\n      compound.finance\n  \n\n  \n    \n\nCompound 28\n\n  Compound is an algorithmic, autonomous interest rate protocol built for developers, to unlock a universe of open financial applications.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\nhttps://www.withtally.com/governance/compound/proposal/47 57\nI would like to thank Polychain for delegating their votes to the CAP and set the precedent that large voters will work with community members.\\nExcellent work Getty. Very happy to see large shareholders like Polychain signalling support for the oracle improvement. The added security and ability to support a wider range of assets it enables will be a boon to Compound’s continued growth. Looking forward to voting going live tomorrow!\\nGreat work @getty and everyone else involved!\nThis change will greatly improve the reliability of the protocol as well as thrust us forward in our ability to expand our markets!\\nA note to any offchain event watchers: the price updated event signature is a bit different than the existing oracle. It changed from PriceUpdated (string symbol, uint256 price) to PriceUpdated (index_topic_1 bytes32 symbolHash, uint256 price)\\nGood point; here is the reasoning for that. Chainlink OCR aggregators as reporters \uD83D\uDE80 by alexroan · Pull Request #1 · smartcontractkit/open-oracle · GitHub 39"
  },
  {
    "number_of_comments": 12,
    "postid": "52d0d41b-0b43-4dbb-9932-7a8e6ea2b1a6",
    "posturl": "https://www.comp.xyz/t/gauntlet-polygon-usdc-reward-recommendations/4562",
    "combinedcontent": "Simple Summary\nIf the community believes that demand for the Polygon USDC market has stagnated, Gauntlet recommends decreasing rewards for the comet to mitigate COMP losses. We will create a poll below to gauge the community’s sentiment.\nOption 1 - Decrease Rewards\n\nGauntlet recommends decreasing daily COMP supply rewards from 34.73 (Earn Distribution 4.19%) to 15 (Earn Distribution 1.82%)\nGauntlet recommends decreasing daily COMP borrow rewards from 34.73 (Borrow Distribution 4.57%) to 15 (Borrow Distribution 1.98%)\n\nOption 2 - Keep Rewards\nDo not change rewards in hopes of incentivizing more protocol growth.\nAnalysis\nScreen Shot 2023-08-03 at 5.04.38 PM1810×494 71.2 KB\nScreen Shot 2023-08-03 at 5.03.51 PM1800×486 71.5 KB\nScreen Shot 2023-08-03 at 5.03.33 PM1800×492 73 KB\nAs mentioned in our most recent Weekly Polygon USDC Market Update 4, and as seen in the above charts, demand for the Polygon market has stagnated recently.\nScreen Shot 2023-08-03 at 8.10.31 PM1680×424 39.3 KB\nNote that utilization of late has been ≥ 70%, indicating there may be borrowing demand given enough USDC supply. However, Earn Distribution is currently 4.12%, which should appeal to suppliers.\nScreen Shot 2023-08-03 at 8.02.05 PM1656×458 31.8 KB\nThe comet has accumulated $9k reserves in the past month while distributing ~$148k COMP rewards. This rewards cut would decrease the COMP rewards to ~$64k.\nNext Steps\n\nCreate a poll to gauge community preferences.\n\\nTo gauge community sentiment, starting a poll here\n50%Option 1: Decrease Rewards      50%Option 2: Keep Rewards      0%Abstain12voters\n                    \n                    Closed Aug 10\n                   \\nThe move to introduce Supply rewards worked  better than i expected and lead to noticeble increase in both supply and borrow, which was stagnated for a long time before that. That was quite a successful adjustment and i don’t see it very reasonable to adjust it as soom as in 2 month after. So i am leaning more to keep it for now. Cryptomarkets aren’t very volatile lately, so that probably contributes to a growth stagnation a lot. Cutting distributions at that point might save a little bit of tokens, but might jeopardize future growth. As initiatives actually work best at a quiet times, where earnings everywhere drop a lot, rather than when everything is booming. I believe Polygon is perspective market and it’s worthwile to continie with distributions to grow marketshare, which is still very small.\nBut if we still have to consider cutting distributions, than looking at current utilisations in my opinion it makes a lot more sense, to start from halving Supply rewards while keeping the Borrow intact. Rates will still look attractive on both sides. Protocol actually gets nothing from Suppliers, those rewards can really work as a temporary short-term boost. And good example of it is Arbitrum market, where Supply rewards were prioritised over Borrow rewards and it  really suck comparing to Polygon, with just a mere 5M of borrowing in a month, where almost half of it just arrived this week. Protocol wants borrowers, that is where money are coming from. So if we are to learn anything from observing Polygon and Arbitrum deployments, than it would be rather that it should start with generous Borrow rewards and small or even zero Supply distributions. Supply distributions might be increased on introduced on temporary basis in case of lack of Suppliers and high utilizations, but eventually should be lowered or even eventually zerowed. As Suppliers earn enough from market naturally. Of course it is somewhat chicken and egg problem, as you can’t get Borrowers without existing Supply. But there is no reason to attract Supply if there is nobody to take it either. And yet again we can see at Arbitrum deployment as an example. There is a Supply, but why utilisation so low? Naturally because Borrow APR isn’t attractive. It’s good enough for existing Borrower, but it doesn’t really incentivise new ones to come to Compound as similar rates could be achieved at other places. And you don’t really move your loan between protocols for small fluctuation of interest.\\nHello, Kila here from the team behind Stabl.fi and Retro.finance 6.\nThe recent integration of Stabl.fi’s interest-bearing stablecoin CASHv2 has injected ~800k USDC into Compound v3 (Polygon) since the beginning of August.\nScreenshot 2023-08-08 at 4.36.02 PM1592×176 27.2 KB\nThis integration has not only spurred an increase in $CASH Total Value Locked (TVL) but has also catalyzed the growth of Retro, a fast growing ve(3,3) dex on Polygon using the CASH yields (in part coming from Compound) as bribes to drive liquidity in the ecosystem. $CASH is one of the main quote token for trading pairs. In light of these developments, it is imperative to advocate for the preservation of Compound’s current incentive setup on Polygon. This argument outlines the critical factors that emphasize the significance of maintaining this setup, emphasizing the benefits for Stabl.fi, Compound, and the wider DeFi ecosystem.\nSynergistic Growth:\nThe collaboration between Compound and Stabl.fi has initiated a symbiotic relationship that is fueling the growth of both platforms. Users deposits into stabl.fi to get interest-bearing CASH, fueling deposits into the Compound’s USDC vaults, gaining a surge in liquidity, further solidifying its role as a foundational DeFi platform. Simultaneously, Stabl.fi leverages Compound’s established user base and reputation to enhance the adoption of its own products, fostering a mutually beneficial cycle of growth.\nSticky Liquidity:\nThe current incentive setup on Compound’s Polygon protocol provides CASH holders with a lucrative opportunity to earn sustainable yields and ensures that there is always USDC on compound v3. The sustainable yield generation through Compound bolsters users’ confidence in the CASH stablecoin, fortifying its position in the broader DeFi landscape. The retention of this incentive structure on Compound’s Polygon protocol propels the development of stable yield and encourages more teams to build on top of compound\\nHello , Gim here from the Tetu team . I do not support reducing rewards as most of our community members use our USDC splitter strategy which is a major supplier of USDC in compound market . Reducing rewards affects this and also the projects that have build strategies on top of compound due to this sudden change\\n\n\n\n Kila:\n\nStabl.fi,\n\n\nStabl.fi, CASH Will Shape the new era.\\nThanks for the community’s feedback. Given that the poll was 50/50, we will move forward with an on-chain proposal that will be the source of truth. We aim to publish the on-chain proposal on Monday, August 14.\\nBe it as it may, it is still feels not well thought-out and rushed proposed change, especially at times when the protocol is in process of adding staked matic to exact same market. Which would no doubt change the input data in some way.\nI’d say we would be much better to pospone it and observe for a month or so, to at least avoid being put in situation where multiply changes to market are introduced at the same time and we wouldn’t really be able to tell what cause what.\\nGiven community feedback, we will wait one month and revisit the Polygon USDC comet after the addition of MaticX.\\nIs it time to revisit this?\nDoesn’t seem that Polygon USDC market dynamics have changed much since MaticX was added.\\nHi @tylerw ,\nHere’s an update on the current COMP rewards structure in Polygon v3 USDC:\n\nDaily COMP Supply Rewards: 34.73 COMP\nDaily COMP Borrow Rewards: 34.73 COMP\n\nAssuming maxed-out supply caps and current liquidation factors, the total borrowing power would be $82.29M.\nHere’s a breakdown based on our assumptions:\n\nBorrow Usage (60%): This leads to a borrowing volume of $49.37M USDC.\nUtilization (80%): Corresponding to a supply volume of $61.72M USDC.\n\nWith the above utilization and the present Interest Rate curve:\n\nSupply APR: 2.6%\nBorrow APR: 4.3%\n\nGiven the current COMP price of $46:\n\nSupply Distribution APR: 0.9%\nBorrow Distribution APR: 1.2%\n\nThis results in the following Net APRs:\n\nNet Supply APR: 3.5%\nNet Borrow APR: 3.1%\n\nThe current projected net APRs are within the reasonable ranges. If the community believes there is still growth to be had in the comet, we do not currently recommend changes to the COMP rewards on Polygon v3 USDC comet.\\nI agree with Gauntlet view that rewards distributions on polygon usdc are within reasonable range and don’t really need adjusting for now.\nI believe that this market actually performing quite well in recent months, and addition of staked matic brought useful contribution to growth of usage.\nPersonally i would say that further growth is more likely tied to addition of more collateral, of which the most useful to growth would likely be wsteth, rather than any sort of  reward manipulations. At least for now. Theoretically slight increase of distributions for supply side might be on a table eventually, but considering that current very attractive total apr for supply of usdc do not attract more supply already, i don’t see the need to advocate for increasing it at the moment. APRs currently might be a bit distorted due to the recent surge of btc, which led to similar price movement in a broad crypto market. Such distorted numbers are bad guidance for adjusting longterm and midterm  policies, which reward distributions are.\\nLend and borrow have been pretty much stagnant since July. Yield on this market has been consistently >2% higher than Aave v3 USDC on Polygon, and capital still hasn’t migrated here.\nimage1140×930 56.4 KB\nIt feels like it’s worth experimenting with incentives on this particular market. This market is still small and also mature enough to use as a model for Base and Arbitrum growth, as well as future launches.\nCOMP incentives as is don’t seem to be showing good ROI.\n\n\n\n[Gauntlet] Weekly Market Updates: Polygon USDC\n\nThe comet accumulated $2.8k USDC reserves while distributing $19.62k COMP rewards for a weekly Net Protocol Profit of $-16.82k.\n\n"
  },
  {
    "number_of_comments": 30,
    "postid": "25390716-7974-4a1c-8fd0-75b167177670",
    "posturl": "https://www.comp.xyz/t/dai-market-risk/688",
    "combinedcontent": "This is a dedicated thread for the community to discuss potential changes to the Compound DAI market. See the main thread  26 for information and analysis of the liquidation event.\nBackground\nThe DAI market on Compound has been growing for months as a result of the COMP Distribution; it currently accounts for 309,178 of 427,880 COMP distributed to users.\nWith 1.56 billion DAI supplied to the protocol, and 1.20 billion DAI borrowed, the Compound Dai market eclipses both the underlying DAI market, the liquidity on exchanges, and the global trading volume of DAI by a vast margin.\nThe Gauntlet Market Risk Assessment  31 analyzes markets as a function of total outstanding debt relative to daily trading volume–Compound’s DAI market is literally off the charts of any scope considered in the report, which likely contributed to the DAI liquidation event.\nThe community may want to consider changes to limit the market risk of DAI. Here is a summary of the ideas and levers that have been discussed in Discord so far:\n1. Disable DAI’s COMP Distribution\nBy disabling the distribution of COMP to the DAI market, the incentive to use DAI over another stablecoin like USDC would be reduced, and market size may shift between markets quickly. The COMP Distribution could be re-activated once the market risk has been reduced, or other changes are implemented.\n2. Increase DAI Reserve Factor\nIncreasing the Reserve Factor of DAI would decrease the attractiveness of DAI relative to other stablecoins, but in a significantly less material fashion than disabling the COMP Distribution. Usage of DAI may shift much more slowly.\nReserves would also build more quickly, benefitting the protocol & user-base.\n3. Set a DAI Borrowing Cap\nSetting a Borrowing Cap would limit the market size of DAI; this would prevent new usage, while preserving the economics of existing users.\nA Borrowing Cap could be set relative to the amount of DAI outstanding (1/3 would be 300 million), or as a function of daily trading volume (300%, the high end of the risk assessment, would be 300 million).\nThis can be implemented with the Community Multi-Sig, which sets Borrowing Caps, without requiring a governance proposal (fast solution).\n4. Lower DAI Collateral Factor\nLowering the Collateral Factor would decrease the leverage of users supplying DAI, and the attractiveness to “yield farm” with the asset, but would likely not change borrowing demand for DAI. Alternatively, the Collateral Factor of another stablecoin (e.g. USDC) could be increased 4, lowering DAI’s Collateral Factor on a relative basis.\n5. Supply Cap\nThe cDAI contract could be upgraded to implement a Supply Cap, which is an alternate approach to limit market size. This would require development resources (slow solution).\nOther Ideas\nAll other ideas are encouraged – and the community may decide / believe that the market doesn’t need to be modified. To expedite the conversation, a quick poll:\nWhich solutions (if any) do you support? Disable COMP Distribution Raise Reserve Factor Set Borrowing Cap Lower Collateral Factor Create Supply Cap Other Ideas (Please comment below) No Changes38voters\n                      \n                      Choose up to 5 options.\n                     \n                \n                Results will be shown on vote.\n               Vote now!\\nWhile it’s true that DAI market size is a direct result of COMP distribution and early imbalances, which led to it being Dai and not something else. While DAI market looks huge, it’s just accounting, and there is not even 500M of actual DAI inside the protocol, as Supplied Dai and Borrowed Dai are in majority very same Dai often supplied and borrowed at same time by same accounts. Risks are generally concentrated in that accounts and for that accounts, rather than for protocol in general.\nI also believe that current COMP distribution model isn’t really that great, and certanly should be adjusted, but i think disabling it completely for Dai market is somewhat unfair and too radical. Though i do think it’s also bad design to be able to capture majority of distribution in one particular market, bleeding other ones off initiative. What i would prefer to see is more complex model, where every single market couldn’t get less than certain percent of daily distribution. Like not less than 5% of daily COMP distribution, which will stimulate usage of other markets and somewhat balance out distribution.\nAs for Reserve Factor, that is certanly too low for DAI market size, and i believe it should be at least 10%, if not 20% until Reserves for Dai market reach at least 1% of Supplied. Currently they are not even at 0.1%\nI don’t like ideas of borrowing or supply  caps, as it’s somewhat temporary patch rather than solution. If it works, it should work on any scale, if not, than it should be fixed, caps don’t fix anything but just create a false feeling of safety. And it’s also a competetive disadvantage, as if users can’t execute their strategy because of caps, they might go to other platform without such restrictions, and possibly even stay there.\nIf we look at Collateral factor on relative basis, like what is suggested with USDC, it’s probably possible as well, but i don’t see much margin there unfortunately. USDC collateral factor could be increaset to like 80%, but the higher we go, the more risk could potentially be generated for market, which in combination with generally low reserves everywhere aside of BAT market could be a recipe fo disaster in future. Reserves should be created before and far ahead of any potential accident and i strongly believe Reserve Factor should be increased overall, not only for Dai market. Actually it should be done “yesterday”.\nFor our current dai market situation i would suggest:\nincrease Reserve Factor for Dai to 20%\nincrease Collateral Factor for usdc to 80%\npossibly decrease Collateral factor for Dai to 70%\nObserve the results. In meantime think about improvement to the Comp distribution model.\\nOverall, I don’t think we should make adjustments to the Dai market unless it is a temporary change before an oracle update can be put in place.\nMy opinion is that the primary issue here is not risk parameters of the Dai market but a rather brittle oracle design. The oracle did perform as intended which is great but unfortunately we now understand it’s possible for the oracle to perform as intended and for it also not to accurately reflect the fair market price of Dai (I’m basing this assertion on my understanding that Coinbase Dai price was a major outlier).\\nScreenshot from 2020-11-28 23-31-45730×200 33.5 KB\nThese are the current outstanding total debts per assets, normalized with today’s Messari’s 24 hour real volume.\nAs Robert notes, the Gauntlet risk assessment simulations find the “safe” limits somewhere around 300%, so DAI has clearly blown way passed the point of concern. It may be true that the collateral used for most of the DAI debt is DAI itself, instead of a more volatile asset like ETH, which would reduce the risk of the system becoming undercollateralized, as simulated by Gauntlet, but as we’ve seen with this liquidation incident, debt exceeding liquidity by such vast amounts carries other risks.\\nAfter analyzing the situation more in depth and reading some of Rob’s comments. It does seem most sensible to disable COMP distribution for Dai.\nThe core issue here is that the Dai market on Compound is so large in relationship to total Dai market cap that the market for it on Coinbase can be manipulated and therefore Coinbase is not acting reliably as a price oracle.\nThe reason the Dai market is so large is because the COMP disbursal skews borrowing / lending incentives. So I would be in favor of removing the COMP disbursal temporarily and putting it back in place when the oracle issue has been more deeply analyzed and addressed.\\nRaising DAI reserve factor seems like a win-win: reduces incentives to borrow and resupply DAI for COMP farming, while building up reserves to use in future drawdowns.\nDisabling COMP rewards for DAI is a pretty drastic step, it will likely result in leveraged farming moving to other assets. Would Compound be better off with a massive amount of recursive USDC or ETH leverage for farming? These assets arguably have much worse supply elasticity (at least in the short term - minutes or hours), and could be more vulnerable to liquidity crises.\nIt would be slightly problematic if an issue that was (apparently?) caused by Coinbase deposit/withdrawal problems resulted in Compound increasing reliance on Coinbase’s USDC mint.\\nI’m sorry if this is a naive question, but why not just cap the amount of COMP going to any asset so that proportionality doesn’t continually shift from one to another?\\n\nDisabling COMP rewards for DAI is a pretty drastic step, it will likely result in leveraged farming moving to other assets. Would Compound be better off with a massive amount of recursive USDC or ETH leverage for farming?\n\nEither of those seems preferable as they both have much more liquidity in markets than DAI. Though the current amount of DAI debt would probably be too much even for USDC markets. A drastic step is urgently required since DAI borrowers are at the moment at risk of the exact same market manipulation attack being repeated.\\nI think it’s not expedient to discuss specific changes to the DAI market, as the problem is not a specific DAI market problem.\nThe root cause is, that the open oracle, which Compound uses, has got only one source, which is Coinbase. This means, exactly the same problem can happen to any other Compound market (beside the the markets with a fixed USD value). Imo for that reason the solution can only be to fix the Compound oracle problem 9.\\nNo, that is a misunderstanding, exacerbated by some obvious shills. This was not an oracle issue, it was a market liquidity issue. As you can see from the table above, other assets are not at risk of the same market manipulation attack as it would not be profitable given the amount of debt and liquidity available for each specific asset.\\nAgain, it doesn’t really matter if this was an attack or a liquidity crunch, the point is that changing the oracle by adding more markets does not address the vulnerability of DAI debt in the system exceeding all global DAI market liquidity by a fantastic amount, as you can see in the attached table. There is no magical price feed that can solve that fundamental problem, as long as the possible profits from off-price liquidations exceed the cost of moving the price, there will be a risk of market manipulation. The only realistic way to address this issue is by reducing DAI outstanding debt. Screenshot from 2020-11-28 23-31-45730×200 33.5 KB\\nReducing/capping Dai also allows other tokens to earn more comp. Some views would probably lean that way because the incentive is there to increase comp earnings of non-dai based depositors/borrowers.\\nOther assets have vastly more liquidity than DAI, as you can see from the attached table previously on this thread. So debt moving into other assets and away from DAI should be indeed the intention.\\n\n  \n    \n    \n    DAI Liquidation Event \n  \n  \n    Coinbase looked at the data from many different angles and concluded that not only were there no price manipulation alerts in our trade surveillance software, but we did not find any evidence of collusion or single actors that pushed the price higher. We believe our books properly operated according to the availably liquidity at the time which, in stablecoin markets, may become thin as price moves away due to the collateralized nature of these markets. And, Coinbase Price Oracle accurately repor…\n  \n\n\n@wario  What information do you have that shows it was an attack contrary to what oracle’s CTO says? Is this just conjecture?\\nExpanding a bit on why addressing the DAI market risk is of urgent necessity.\nThere are, at least, two important risks when we have an asset in the system in vast excess of what liquidity is available in the markets.\n1) Risk of the system becoming under-collateralized:  This is the issue addressed by the Gauntlet Risk analysis linked by @rleshner in the OP. The gist of this risk is that if we have collateral for an asset in excess of market liquidity available, in a situation of a sharp price drop for the asset, liquidators would not be able to sell the seizable assets quickly, and will likely not want to risk holding an asset that is falling in price either, so they would not perform liquidations and the system becomes under-collateralized\n2) Risk of market manipulation:  When outstanding debt of an asset exceeds market liquidity by a sufficient amount, it becomes profitable to artificially move the price of said asset with the intention of unfairly liquidating borrowers for a huge profit. This might or might not have been what happened with this event, but regardless, the cost of buying DAI up to $1.3 was in the order of 21k USD, and required under 400k USDC to perform. To put these numbers in context, the largest single liquidation during the event netted the liquidator in the order of 3M DAI.\nDAI in the system is over 4000% global daily volume, according to Messaris’ data, so as mentioned several times, neither of these risks can be resolved by doing any kind of oracle change, though IMO discussions of oracle revamping are also worth having after we deal with this more pressing issue. The only realistic way to resolve these risks is by  reducing the amount of DAI in the system by a great amount. I don’t really know which of the options laid out in the OP are the best ones to take, but I think it should be clear that the goal should be to select the ones that result in a swifter reduction of Compound’s  present DAI market risk.\nSome have commented, here and on the Discord, that these risks are mitigated because a lot of the largest DAI borrwers/depositors are “loopers”, that is to say, they recursively borrow and deposit DAI as a COMP farming strategy. This is definitely true, and for the ones that are “perfectly looped”, meaning they only borrow and supply one asset, these risks are nullified as their collateral and debts grow and shrink concurrently, not having an effect on their account’s health. However, it is definitely incorrect to assume that all the DAI in the system is “perfectly looped” as has been shown by the current event liquidations.\\nWhy other protocols did not have a “DAI event”?\nObviously, this is a flaw in the price discovery mechanism (oracle) in the Compound protocol.\nAnd please maybe I missed something in Gauntlet researsch paper, but in simulation they write: “assuming that the price of DAI is stable” and use constant gas fee.\nIs possible that liquidation event on compound had frontrunning factor? Because gas fee was 500gwei in moment.\nMy opinion is that COMP farming should be reduced only for stablecoin pairs.\nIs there an advantage for the protocol in farming with stablecoin pairs (besides pumping volume)?\nOnly DAI market risk is biggest volume opposit to other assets because it is more suitable for manipulation.\\nJust want to highlight that DAI liquidity is qualitatively different than other assets. There is no possibility of a sell side liquidity crisis in the defi/dex context, Compound liquidators can always source DAI from MakerDAO at a $1 mark price. I don’t think it makes sense to compare trading volume on a like for like basis in this case.\\nDoes being able to source DAI from maker DAO resolve the lack of market liquidity that liquidators require to sell seized assets, or the possibility of market manipulation to unfairly liquidate borrowers?\\nFYI: @kybx86 has created an Autonomous Proposal 6 to increase the DAI Reserve Factor to 15%. You can delegate votes to 0x5576a4db81a44cb7158fc8d5ae752cb44f57be76 to support this change; if the proposal receives 100k votes, it will enter the formal voting process to upgrade the protocol.\n\nDiscussion 4\n\\nI agree that this was primarily a flaw in the price discovery mechanism. Namely its clear lack of market coverage, only pulling from CB Pro with Uniswap TWAP as a backstop. Aave, for example, didn’t experience anything similar because they are using Chainlink which pulls data from a range of aggregators, which is why several here have proposed 2 that Compound integrate their price feeds to mitigate this risk in the near term.\\n\n\n\n wario:\n\nthe cost of buying DAI up to $1.3 was in the order of 21k USD, and required under 400k USDC to perform .\n\n\nThe reason it only cost $21k to bid DAI up to $1.30 is because Compound’s oracle currently only tracks a single exchange Coinbase, accounting for only 2-7% of DAI’s daily volume. Had the oracle been tracking all trading environments (CEXs plus DEXs) and taken the volume weighted price (while removing fake volume), the cost of bidding the global price of DAI up to $1.30 would have been significantly more expensive than $21k.\nUniswap accounts for the vast majority of DAI’s liquidity (35-40% daily volume), but is only being used as a backstop and not as as source of price data (a source that can’t cryptographically sign data like the OOS requires). This is where data aggregators come into play, because taking the median across preselect centralized exchanges doesn’t take into account volume shifts or volume consolidations.\nThe global price of DAI wasn’t manipulated, a single lower liquidity exchange was, which is what makes this an oracle issue. It only becomes a liquidity issue once the global market wide price is manipulated, but that’s not what happened in this situation. Market coverage raises the costs of price oracle manipulation to highest possible degree, which is why Aave experienced no issues. Not just because of its lower DAI debt market size, but because it consumes the market wide price which was never above $1.04 per DAI during this event.\nAlso like you state, much of the DAI debt on Compound is from self-loopers meaning that 4,000% debt to volume ratio is artificially inflated by what I believe is quite a bit, though I would be interested to see this quantified to see by how much. The DAI parameters definitely need to be adjusted to reduce the amount of debt, reducing market risks, but I think it’s equal priority to ensure the Compound oracle tracks the market-wide price, and not just 3-7% of the DAI market as it does currently.\\n\nHad the oracle been tracking all trading environments (CEXs plus DEXs) and taken the volume weighted price (while removing fake volume),\n\nThe way to remove fake volume is by carefully selecting the set of markets to take any form of average or median from, and not include them in your price feed.\n\nUniswap accounts for the vast majority of DAI’s liquidity (35-40% daily volume), but is only being used as a backstop\n\nTrue, this is probably a good opportunity to revise that. But it is not enough to resolve the DAI market risk issue. There simply isn’t enough liquidity globally, which is why there is also no oracle modification that would fix this issue.\n\nwhich is why Aave experienced no issues\n\nAave has at the moment around 23M DAI borrows, vs 1.23B DAI borrows on Compound. So they do not face the same DAI market risk Compound is currently facing. It would be as dangerous for any money market to let it’s borrows exceed global market liquidity by such an enormous amount.\\nIt’s sounds like we all agree there is too much DAI debt and too little market coverage. However, there seems to be a lack of attention to fixing the latter. Seriously, why haven’t we seen any proposals for updating the oracle alongside these DAI market proposals? I feel like we are just talking in circles about it.\\n\n\n\n wario:\n\nThe way to remove fake volume is by carefully selecting the set of markets to take any form of average or median from, and not include them in your price feed.\n\n\nIf you have a full time monitoring team watching for data feed market coverage and various shifts across exchanges, this can be true, but for the Compound governance community, closely monitoring  market coverage across each and every asset supported without external help from an external oracle project simply isn’t scalable when new collateral gets added to Compound to stay competitive in the money market space. Ensuring market coverage will continue to be a frequent pain point here, even when DAI’s market parameters are adjusted.\n\n\n\n wario:\n\nThere simply isn’t enough liquidity globally, which is why there is also no oracle modification that would fix this issue.\n\n\nWe’re talking about two different issues here. The first is the DAI debt market size and the second is oracle market coverage. Both the former and the latter need to be fixed to sufficiently raise the cost of protocol manipulation. The market wide price of DAI was simply never $1.30, using a single exchange lowered the cost of manipulation.\n\n\n\n wario:\n\nAave has at the moment around 23M DAI borrows, vs 1.23B DAI borrows on Compound. So they do not face the same DAI market risk Compound is currently facing. It would be as dangerous for any money market to let it’s borrows exceed global market liquidity by such an enormous amount.\n\n\nA protocol lacking market coverage is cheaper to manipulate than one without. If Compound and Aave both had equal sizes of outstanding DAI debt, the cost of manipulating and causing false liquidations on Aave would be higher than that of Compound because of the differences in the oracle’s market coverage. This is why I believe we need to solve both the DAI debt size issues and the market coverage issue in tandem for this issue to be resolved.\\n\nIf Compound and Aave both had equal sizes of outstanding DAI debt, the cost of manipulating and causing false liquidations on Aave would be higher than that of Compound because of the differences in the oracle’s market coverage\n\nWe don’t know that this statement is true, since Chainlink’s oracle system includes middle layers (aggregators, and node operators) that are also possible points of attack without requiring manipulating market price directly, and it is not easy to determinate what the cost of performing such an attack would be. Regardless, the point is not which one is more expensive to attack, it’s that no money market should let their collateral or debts grow to a point where it becomes profitable to manipulate market price.\\nI honestly don’t understand why we are talking in circles here. Chainlink is clearly a better solution than using Coinbase Pro w/ Uniswap TWAP as a backstop. To use an unlikely hypothetical situation (that Chainlink’s extensive list of trusted data providers and node operators somehow engage in a large scale coordinated attack) to argue against fixing something we KNOW is an issue makes no sense to me. Even cursory research shows Chainlink goes to insane lengths 1 to ensure data integrity, and they have proven performance during extremely volatile market situations.\nRight now we are driving down the highway with a busted tire (faulty oracle) and broken brake pads (the DAI market parameters). Instead of sitting here arguing which is more dangerous, why don’t we fix both so we are as certain as possible we don’t crash again?\\n\n\n\n MasterofNonce:\n\nI honestly don’t understand why we are talking in circles here\n\n\nI think this is because a few accounts are incessantly shilling a non-solution to the problem at hand.\\nNo. It’s because the so called “community” doesn’t take any action. Why don’t you just create a proposal to change some DAI market parameters? And let others, who think this isn’t enough, discuss about oracle changes?\\nAn autonomous proposal to modify DAI market parameters has already been submitted https://compound.finance/governance/address/0x5576a4db81a44cb7158fc8d5ae752cb44f57be76 4\nSome accounts are proposing oracle changes in a thread specifically created to discuss the DAI market risk, perhaps proposed oracle changes would be better addressed in a separate thread, there are already a few created by those same accounts to discuss that topic.\\nIt looks like Yearn has started using a COMP farming strategy that should result in further increasing leveraged DAI already in the system.\n\n  \n      twitter.com\n  \n  \n    \n\nbanteg (bantg) 5\n\n First harvest spotted on @arbingsam's COMP farming strategy.\n\nFlash loans allow it to lever up and unwind cheaply.\n\nhttps://t.co/9PkUS3KVY3\n\n\n  11:39 AM - 6 Dec 2020\n    \n      \n        \n       59\n    \n    \n      \n        \n       7\n    \n\n\n  \n  \n    \n    \n  \n  \n\n\\n@wario is correct @cryptix, we should keep the oracle discussion to a separate thread just so things are more siloed (don’t worry I’m guilty too haha).\nHere is the thread on Integrating Chainlink Oracles 15, so we can continue conversing there."
  },
  {
    "number_of_comments": 28,
    "postid": "02685f49-16be-4f30-a446-719a95eb2655",
    "posturl": "https://www.comp.xyz/t/proposal-distribute-dai-to-users-affected-by-dai-liquidations/2110",
    "combinedcontent": "\nFormal Proposal: Distribute DAI to Users Affected by DAI Liquidations\n\nOBJECTIVES\nThis post will outline an on-chain proposal that will compensate users affected by the liquidation events of November 26th, 2020 in the DAI market. After feedback and discussions, this proposal’s code will be published on-chain in collaboration with those that support this methodology.\nThank you to the Compound community and investors for the weeks of feedback, iteration and discussions that have helped craft this proposal.\nBACKGROUND\nOn November 26th, 2020 an unexpected increase 9 in the DAI price to $1.30 on Coinbase Pro led to 85.2 million in DAI being liquidated. An initial compensation proposal for that event did not pass an executive vote 16, with 680k COMP voting against and 212k COMP voting for.\nSince the original proposal, there have been positive changes in Compound’s oracle system 2 and improvements in the state of the DAI market’s reserves. The oracle fix and the increased DAI reserves address three key issues voiced by the community with the previous compensation proposal:\n\nReimbursement to users before clarity on when/how the underlying issue would be fixed.\nReimbursement denominated in COMP to affected users may not necessarily align with the objectives of COMP usage or COMP holders.\nSetting a precedent that tail-risk events should be subsidized with COMP.\n\nToday, the 15.1 million DAI in reserves 5 is sufficient to cover the November 26th losses as originally calculated in full. The total expected compensation amount is approximately 6.8 million DAI based on the protocol’s 8% liquidation penalty. Compensation is well within the capabilities of Compound governance today and will help give closure to a topic that’s still a point of ongoing discussion in the community and allow the protocol to move forward on stronger footing.\nCODE MECHANICS\nThis proposal utilizes a slightly modified version of the merkle distributor 3 used for the airdrop of Uniswap’s UNI token. The merkle tree contains the addresses of those affected by the liquidation event, and was reconciled and analyzed against on-chain data from around the time of the liquidation event. A relayer will claim the merkle drop for each wallet according to the distributor and distribute the DAI to user’s wallets. Users do not need to interact with any contract to claim this DAI.\nThe script to generate the affected addresses list 11 (and distribute DAI to them) fetches the liquidation transactions which repaid DAI within the given block bounds (11332733 to 11335286). The repaid DAI is multiplied by 0.08 to attain the liquidation fee 1 incurred by the affected address. If a wallet had 100 DAI repaid by a liquidator, then that wallet would receive 8 DAI as part of this compensation proposal.\nFull coverage is ensured through unit testing 1, forking simulation 1 and a testrun of the whole proposal process on testnet 2. @arr00 arr00 1 is the core contributor to the code.\nDISTRIBUTION METHODOLOGY\nReferencing the DAI liquidations spreadsheet 16 published by @rleshner, a total of 85,220,406.43 DAI was repaid on 11/26/20. Applying the 8% liquidation penalty, this proposal would pay out a total of 6,817,632.51 DAI.\nTo preview the DAI distribution per address, see here 22.\nAction Items\nIf you support this proposal, you can delegate COMP to this autonomous proposal (CAP), which will be deployed after allowing sufficient time for the community to review the code and proposal.\nAgain, thank you to the Compound community and investors for the weeks of feedback, iteration and discussions that have helped craft this proposal.\nDisclosure\nFor full transparency, I was one of the Compound users affected in the DAI liquidation event and believe this governance owes it to the community to consider this proposal. For context, in 2020, I worked with the community to pass a reserve factor change to the DAI market and improve the overall risk positioning of the Compound protocol.\nResources\n\nOriginal context on liquidation event 9\nDiscussion on compensation plans 1\nProposal #32 details 6\nProposal #32 on-chain vote 16\nDAI market 5\n\\nI see there are two accounts making up the vast majority of compensation amounts:\n“0x909b443761bbD7fbB876Ecde71a37E1433f6af6f”: “3691394.287603915”\n“0xB1AdceddB2941033a090dD166a462fe1c2029484”: “1401662.9011659885”\nWould it make sense to curtail the compensation amounts for these two users? Eg compensating only 80% of amounts over 1 million DAI would yield the following compensation amounts:\n“0x909b443761bbD7fbB876Ecde71a37E1433f6af6f”: “3153115.430083132”\n“0xB1AdceddB2941033a090dD166a462fe1c2029484”: “1321330.3209327908”\nCurtailing compensation for just these two accounts would save 618611.4377539807 DAI, or ~9% of the total compensation amount. Based on the amount of COMP these users have farmed, they would still have made a large profit overall.\nThe specifics of compensation caps/reductions could be adjusted to other thresholds. But in general, it seems unfair to take reserves from the Compound community and current/future DAI depositors for the benefit of industrial scale farming operations.\nAlso, sorry to bring this feedback only now, I’ll admin I have not been following the discussions closely and should have commented earlier. Thanks for your work on the compensation scheme!\\n@monet-supply it could make sense but curious to hear what other people think?\\nIf anyone wants to simulate options for curtailing compensation to the top end of account values, you can make a copy of this sheet: DAI Liquidations - Compensation Reduction - Google Sheets 8\nIndependent variables are “reduction threshold” and “reduction ratio”.\\nI think that we as a community should make a final decision on this issue now. Almost all the issues addressed last time around are no longer relevant or have been addressed. At this point, I think the only things to consider is the following: was $1.30 the correct market price for Dai, and if not, should the Compound community compensate those effected.\nI was strongly against the last proposal as it was trying to compensate users affected by a non-mitigated issue. Now that the oracle was changed and this event cannot happen again, I do think we should consider the proposal. While I don’t think there was a hack, or a mishap in the system, users experienced an unexpected loss. I know of accounts that were safely borrowing at ~70% utilization which were liquidated. These users should be compensated at this point. On the other hand, we have many accounts that were dangerously farming COMP who are probably less deserving of full compensation; however, I think it is a bad idea to start differentiating between different users in this situation. In my mind, its an all or nothing scenario which the community should decide on.\\nThank you, @monet-supply, for making a Google Sheets with the information.\nI think it makes sense for the protocol to acknowledge that users were harmed and that the protocol’s design was flawed at the time. Now that a fix has been implemented, I think it makes sense to reimburse users partially. However, I think larger users should bear some responsibility for not understanding the risks of the protocol. The vast majority of Compound users were unaffected by the incident, and they won’t be receiving any compensation for acting responsibly.\nI propose a Reduction Threshold of $100k and a Reduction Ratio of 50%. With those parameters, the protocol would distribute ~$4m.\nOn a separate note, I would prefer the compensation is paid in COMP rather than DAI. We need to continue to distribute the protocol’s large treasury.\\n@getty I disagree with this logic pretty much in full.\nWhat is the justification to reduce the amount of compensation? In order to save Compound treasury money? That really isn’t appropriate given that the liquidations occurred here as a result of Compound’s inability to use a proper price oracle. Not to mention the fact that this was a very long time ago, and the price of pretty much every single liquidated asset (other than stablecoins) has increased immensely since then.\nAt a bare minimum the 8% liquidation fee should be repaid to affected users of the protocol. If you want to cap that at some threshold (like what @monet_supply proposed) that seems fair, since those huge wallets are not the average user of the protocol, but other than that this should be a full reimbursement in my opinion.\nIf it’s being paid in COMP, it should be based on the value of COMP at the time of liquidation (which someone had proposed on the prior thread). I would be in favor of that, as it appropriately reimburses folks for the damages they experienced and the opportunity cost of not having those assets invested during the price run up in early 2021.\nAnd yes in full transparency, I was affected by it. But I was not participating in any risky behavior and was just borrowing DAI against other stables because I didn’t want to pay to swap them at that time. I was NOT levered farming, and there’s zero reason I should have been liquidated. It’s way past the time that this should have been resolved\\nThanks @kybx86 for the continued effort on this.\nWe can go back and forth on whether or not code is law, but it’s clear Compound didn’t work as intended. Given the nuance, for this isolated situation, I think it’s reasonable for COMP to compensate users involved in this event.\nOn the specifics: excited to see compensation coming from the reserves and paid in DAI instead of COMP (this was a key reason PC voted against the original prop). I’m onboard with the logic behind capping the repayment for large-scale farmers and would support this. Though overall, it’s important to remember the total $ amount being distributed here is not super significant in the grand scheme of things.\nOne nice addition to this would be for us to define (or add more structure around) what situations in the future (if any) would merit reimbursement, and how this reimbursement will occur.\\n@JacobPPhillips @tob @arr00, before moving forward with a formal CAP, I’d like to hear from the community, particularly those that have played a role in this proposal, any concrete next steps that we need to take to close the gap and get the proposal deployed.\nIt seems like we are in agreement to proceed with:\n\nDAI reserves\n8% liquidation penalty\n\nIt seems like the outstanding items revolve around whether or not there should be manual adjustments to cap compensation for certain wallets.\nIn my view, governance should be scientific and systematic, not emotional. While I agree with others that there’s a power curve toward the top wallets, I don’t believe we should manually adjust or interfere with the uniformity of the proposal (as @arr00 wrote) or what each of us believes is fair. “Justice is blind”.\nThat said, I’m open to hearing if those that have contributed to the development of this proposal have very strong views against moving forward as is.\\nWhat @kybx86 is proposing seems fair to me. One thing to consider is the idea that @tob suggested of basing the compensation on an amount of COMP computed at the time of liquidation is an interesting one as many of the victims lost assets which would have significantly appreciated in the time it has taken to resolve this issue. This idea is worth consideration, though no need for this to be a blocking issue.\\n\n\n\n getty:\n\nacting responsibly\n\n\nWhat that mean “acting responsibly”? Lower LTV ratio?\nDamaged users did not determine CR, so “acting responsibly” is a completely subjective assessment.\\nThank you to those that provided feedback. The CAP has been deployed here 28.\nThis CAP needs 65k COMP delegate votes until it becomes a formal proposal.\nIf you support this proposal, please consider delegating to it.\nVIEW CAP & DELEGATE 28\\nI ask the community delegate votes. Your support is important to us!\\n@Dmitry Thank you.\nThe CAP is live and it needs votes to get to 65K so it can be made into a formal proposal. The CAP already has 3 votes and ~12K COMP, please delegate to it to get it across the line:\n\n  \n      \n\n      compound.finance\n  \n\n  \n    \n\nCompound 19\n\n  Compound is an algorithmic, autonomous interest rate protocol built for developers, to unlock a universe of open financial applications.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\\n65,000 votes delegated!  I just want to say a massive thanks to @kybx86 and the community.\\nGauntlet is in agreement that using DAI over COMP sets a good precedent and acts as a strong signal to the broader community. We also intend to model reserve growth to help the community reason better about scenarios such as this.\n\n\n\nDynamic Risk Parameters\n\n\nProposal 49 routes a fraction of liquidation incentive from liquidators to the reserves. This change effectively allocates 5.2% of liquidation to liquidators and 2.8% to the reserves, which increases the protocol’s ability to recover from insolvency by growing the backstop liquidity, but reduces the incentive for liquidators. We will update the effective liquidator incentive to 5.2% in the simulation to accommodate the change.\nOur current simulation is mainly focused on modeling insolvency risk in one day. Considering the average liquidation size relative to the sizes of reserves, the 2.8% of liquidations added to the reserves in a day will likely not have an immediate impact in such a short time frame. However, tracking the amount of reserves over time and forecasting the growth rate of the reserves due to parameter changes can definitely help community members to better understand the protocol’s liquidity backstop. Forecasting the reserve growth rate is not in our initial scope, but we will evaluate how to support this in Q1 2022.\n\n\nWhile a threshold or reduction ratio intuitively make sense the risk of subjectivity is too high.\nFor those reasons, Gauntlet will be voting FOR CP059.\\nThank you for the support @inkymaze. I look forward to seeing Gauntlet’s future risk modeling work as Compound market reserves continue to grow.\\n\n\n\n inkymaze:\n\nWhile a threshold or reduction ratio intuitively make sense the risk of subjectivity is too high.\n\n\nPretty much agree with this sentiment. I’d like to reduce payouts to some of the top addresses, but there’s no easy way to decide where the cutoff should be / how much to reduce compensation - this would likely bog down the discussion and compensation process. I’m also planning to vote in favor.\\nThanks again to everyone that has supported this proposal and provided thoughtful commentary.\nVoting is now live.\nProp 059:\n  \n      \n\n      compound.finance\n  \n\n  \n    \n\nCompound 22\n\n  Compound is an algorithmic, autonomous interest rate protocol built for developers, to unlock a universe of open financial applications.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\\nI voted against this proposal because 74% of the payments go to two accounts, and 54% goes to a single account who was farming COMP by recursively borrowing and resupplying DAI.\nAccount 0x909b443761bbD7fbB876Ecde71a37E1433f6af6f\n\nsupplied $18M in USDC\nrecursively supplied $108M in DAI and borrowed $93M DAI\nwas liquidated with a repayment of $46M in DAI here: Ethereum Transaction Hash (Txhash) Details | Etherscan 2\n\ncollected 17,733 COMP from farming, worth about $2.2M at the time of liquidation and worth about $7.1M today\n\nThe protocol should not encourage behavior that exploited a flaw in the design of COMP rewards. This was an industrial-scale farmer who realized a profit of between $2M and $7M in COMP value without contributing net liquidity to the protocol. They bypassed two different user protections built into the protocol interface by (a) recursively borrowing DAI and (b) borrowing well beyond the safe limit. Had they not also supplied some USDC to add to their yield they would have been safe in spite of everything. They do not deserve to be rewarded with an additional $3.69M.\\nI abstained from voting on Proposal 32 2, and voted FOR Proposal 59. This proposal isn’t perfect, but I believe that the advantages outweigh the disadvantages.\nWhile there are strong arguments on both sides of the debate as to whether the November DAI price spike warrants compensation, it is the only event in Compound history that comes close to a “reserve worthy event”, and @kybx86 / @arr00 have done a remarkable job in the technical implementation of a solution (following the work by @getty and Chainlink to add preventative measures against the root cause).\nThis proposal demonstrates how reserves can be utilized by the community to protect users of the protocol, and I firmly believe that its passage sends a message to current and prospective users that the community is capable and willing to tackle adverse events (rare as they should be).\n@monet-supply, @pyggie and others have pointed out that the allocation of reserves is inefficient; that it rewards farmers, and that the community should pro-actively select which users deserve compensation; I completely agree, but don’t think we should seek to punish (or not-compensate) users retroactively for activity we wish wasn’t occurring–instead, we should use this as a catalyst to calibrate COMP distribution and incentives going-forward, to disincentivize farming or non-useful activity.\\nI’ve been supplying dai cdai cuni cusd ceth what am I to expect from this.\\nYou are likely not impacted in any way, unless you were liquidated while borrowing DAI, November 2020.\\nSo don’t I get my compensation for them borrowing from my supply of dai.on compound dai\\nThank you @kybx86  for putting together this proposal and thank you to @arr00 for your contribution in the technical implementation of this proposal.\na16z voted yes on this proposal, for the following reasons:\n\n\nThe protocol behaved in a manner that significantly diverged from users’ expectations: The price provided by the Compound oracle was significantly divergent from the rest of the global market at the time. While the specific oracle design was publicly known, users generally expected the oracle (and the protocol more generally) to reflect general market prices for Dai, and this expectation was not met. Given that the losses stemmed from the protocol behaving in this manner, it’s reasonable that the community should step in and make users whole.\n\nPrevious concerns have been addressed: @kybx86  has taken the time to directly address the criticisms and concerns levied against the previous reimbursement proposal.\n\nThere has been successful testing for the technical implementation: If this proposal were to pass, the mechanism for executing the reimbursements have already been created + successfully tested on the kovan testnet 1. The code is a fork of the Uniswap merkle code used for their airdrop with a few small modifications, so there is a high degree of confidence that the implementation will work as expected.\n\nFinally, while we supported this proposal, we do not think it should establish a broad precedent for backstopping any manner of user losses going forward. The losses in this case arose from the protocol behaving in a manner that materially defied user expectations (e.g. the oracle mispricing Dai by ~30%). While it’s reasonable for the community to cover losses of this sort, the community should generally oppose attempts to stretch this logic to cover other (less valid) claims for reimbursement going forward (e.g those arising from general market volatility, or minor deviations in oracle pricing, etc).\\nThe most important thing in this situation is that the community agreed that the users are not guilty for that liquidations (there was talk of poor asset management, which is a ridiculous argument).\nI agree with the members who claim that compensation is the minimum possible and that it barely covers the gas fee cost in an attempt to defend the position.\nI think that users who were not involved in stablecoin farming should have been compensated more, because for example in my case (as with a large part of the damaged users) Ethereum positions were liquidated (at that time about 350$/ETH). It’s been almost a year and ETH has gone 10x, so the losses are huge.\nIt is understandable that the protocol simply cannot cover amounts based on market changes, but it must be understood that these losses would not have occurred if users had used another protocol (i.e. Aave). Losses were incurred solely through the use of the Compound protocol and I think all big COMP holders should have been more committed to this case in early stage after that event.\nThe positive thing is also that their comments about this case are now appearing - 9 months later, but better late than never.\\nProposal 059 passed.\nBig thanks to everyone that cast their votes and participated in the discourse over the past 9 months.\nFor those that say Compound Governance is disorganized, yes, it may not be perfect, but this is a clear example of governance and community in action.\\nCompound Governance is not disorganized, it just suffers from too many political games. Realistically, this situation would be resolved by the state administration within 9-10 months.\n@kybx86 all praise for the engagement in this case\\nThe proposal has been executed and all Dai has been distributed to the recipients. See the merkle distributor here MerkleDistributor | 0x0634ca1d878a050eb525ed08852cc3bad7f5a1dc 14."
  },
  {
    "number_of_comments": 22,
    "postid": "03d0810b-f7e6-4d96-a808-ea82dc1ade03",
    "posturl": "https://www.comp.xyz/t/proposal-should-compound-governance-contribute-funding-to-the-nomic-foundation/3032",
    "combinedcontent": "\nSummary\n\n\nNomic Labs, the team behind Hardhat, has become the Nomic Foundation, a non-profit organization dedicated to Ethereum. Our mission is to empower developers to decentralize the world.\nThe Nomic Foundation’s work will be focused on Ethereum’s developer platform with the objective of achieving a world-class developer experience, and generally improving Ethereum’s public goods support structures.\n\nHardhat 5 is the de facto standard developer tool used to build Ethereum software, with more than 23000 Github repositories using it and tens of thousands of active users. Prominent teams relying on it include ENS, Uniswap, Optimism, OpenZeppelin, Aave, Balancer, Chainlink, Synthetix, and many more leading teams.\nThe new foundation will expand the Hardhat suite of tools and, most importantly, build long-term infrastructure to catalyze organic growth in the Ethereum tooling ecosystem, decreasing Ethereum’s dependence on any one organization to build and maintain core development platform components.\nSeeking $30m in total funding from the ecosystem. Donations of $15M already secured by the Ethereum Foundation, Vitalik Buterin, Coinbase, a16z, The Graph, Polygon, Chainlink, a16z, and Kaszek Ventures.\nWe’re proposing to Compound Governance to make a contribution of $5m to the Nomic Foundation to support its mission.\n\n\nWhat is developer experience?\n\nIf you are already familiar with this concept feel free to skip this section, if you’d like to learn more, keep on reading.\nUnderstanding developer experience starts with realizing that software development platforms are multi-layered technical products and that software engineers are their users. Developer experience is analogous to UX for software developers, particularly UX in professional tools that are essential for abstract tasks and have direct impacts on productivity, effectiveness, happiness, frustration, and therefore cost.\nThe practice of software development is a complex discipline, and developer experience is significantly different from the UX of a note-taking app. When a professional is working on an intricate problem that requires deep thinking and accessing information via a tool, or when one’s creative output is directly limited by the capabilities of their tool, then the professional will be directly restricted—or empowered—by the tools being used.\nAn additional aspect that may not be obvious and emphasizes the impact tooling has, is that developers frequently don’t understand what they’ve created. Writing code that doesn’t behave as expected is routine, and the process of making it work as expected (debugging) is one of the core activities of software development, which is heavily aided by development tools.\nDeveloper experience is about the effectiveness of interactions with platforms and their tools, as well as the feelings that arise when trying to meet a specific software development objective with them.\nEmpowering users to create something without knowing what they’re going to be creating is a difficult challenge. The author of a tool or platform cannot optimize for a specific use case, only for the abstract needs of software development in general. In addition, the development platform that software is built on ultimately becomes part of the software itself, and very often, the software directly interacts in some way with the tools.\nSince the created software is dynamic and has a limitless life of its own, it’s easy for tools and platforms to end up constraining what is created or how. Software development, in general, is a moving target that keeps evolving. Tools and platforms need to accommodate a future that is partially unknown, making achieving a solid developer experience a challenge.\nSome aspects that contribute to a good developer experience include:\n\nAbility to get up and running quickly as a beginner\nClear, thorough, and detailed documentation that’s easy to explore and facilitates learning\nAccessible developer community as a support resource\nFreedom and flexibility to take the preferred path towards a solution without rigid obstacles\nAbility to easily gather relevant technical information related to code failure\nClear and explicit error messages that explain issues and suggest potential solutions\nAutomatic resolution of tedious automatable tasks\nReusable infrastructure to easily build ad-hoc solutions\nAvailability of specialized tools for domain-specific problems (local environments, programming editors, debuggers, etc)\nFast performance\n\nDevelopers are empowered to meet their objectives with a high degree of comfort and efficiency when there are high-quality resources available, well-designed architectures with APIs that get out of the way, and insightful tooling functionalities to address domain-specific problems.\nWhen the basic tenets of developer experience aren’t met, developers feel frustrated and suffer significantly lower productivity.\n\nEthereum developer experience\n\nWhen it comes to Ethereum, which is primarily a software development platform to build decentralized systems, developer experience is a key strategic aspect for success. Ecosystem growth requires more developers to build more software on top of Ethereum. Developer adoption and learning speed, core contributors to this growth, are critically affected by developer experience.\nThe rate at which the ecosystem innovates, coming up with new creations and solving difficult problems, both at the dapp layer and EVM/Solidity/Vyper layer, is also directly affected by developer productivity.\nSoftware development platforms aren’t new, and playbooks established by the great developer experience success stories (Rust, .Net, TypeScript, etc) prove that achieving a quality developer experience requires a specialized approach paired with a long-term, big-picture strategy. The potential impact in executing a dedicated effort for Ethereum would increase the ecosystem’s pace of innovation and growth, building a powerful compounding effect over the long term for the entire industry.\nThe inspiration for our vision came from our experience building Hardhat, which allowed us to see how deeply challenging it is to build sophisticated Ethereum tooling. These challenges must be alleviated to bring about organic ecosystem-led improvement of developer experience that achieves world-class quality.\n\nNomic Foundation\n\nNomic Labs has been fully dedicated to Ethereum developer experience since 2019, and we’re now pivoting 1 to a non-profit foundation formally dedicated to Ethereum. We’re aiming to build a long-lasting organization that makes Ethereum’s public goods support structures stronger by contributing to the Ethereum Foundation’s existing efforts, and reducing the ecosystem’s reliance on any one organization for development platform components.\n\nRoadmap\n\nGiven the size and innovation pace of the ecosystem, there’s no way to foresee exactly what needs developers are going to have as things scale. However, we do know what engineering foundations the ecosystem will need in order to build its own solutions.\nOur overarching engineering strategy is to empower the ecosystem to build its own specialized tools. This plan is based on four strategic pillars of the stack, each of which offers an opportunity to leverage a platform to empower the ecosystem to keep building open-source infrastructure.\nFor each of these pillars, we will build a platform. The four ecosystem pillars and platform opportunities we’ve identified are:\n\nSolidity\nEVM tooling\nLocal development environment\nEthereum connector library\n\n\nThe projects\n\n\nSlang & Rethnet\n\nOver the long term, these are our most important projects. Targeting the Solidity and EVM tooling pillars, Slang and Rethnet will serve as core infrastructure for the ecosystem to build new tools faster, cheaper, and better. We’re essentially building the tools that would have let us build Hardhat a lot faster. We previously published a Medium post with high-level descriptions of how both projects will complement each other.\nSlang\nA Solidity compiler designed as a platform for tooling development, an approach also known as compiler as a service. Its top priority will be servicing tools through domain-specific APIs. Much like .Net’s Roslyn, it will feature a compilation pipeline made of distinct reusable components with standalone APIs. A completely modular design guarantees that others can build on top of it by replacing the part of functionality they need to, and reusing everything else:\n\nParser that is only concerned with producing trees from code. Usable on its own, for example, to create third-party formatters like Prettier plugins.\nSemantic analysis (binding) is concerned only with building a type system and validating the produced trees. Usable on its own, to implement third-party type checking, security/threat models, and more advanced third-party linting.\nCode generation. By replacing just this isolated part, the compiler can compile for different targets (e.g. non-EVM L2s).\nLanguage services. These will receive an immutable representation of the above (syntax trees, bound trees, codegen settings), and will only be tasked with answering questions. Usable on its own to expose in different IDEs (same service for VSCode, IntelliJ, Vim, etc). Reusable to extend the functionality of other editor features (task runners, testing, deployment, CI, debugging).\nRuntime observation APIs to support Rethnet.\n\nAll of this will be reusable to create entirely new EVM programming languages, since by replacing the parser and type system, one can get an entire high-quality toolchain working from the get-go.\nRethnet\nTo provide a simulated environment where developers can build and test their Ethereum software, tools need to replicate many of the components that make up a full Ethereum node implementation. This is a significant engineering effort, which given the complexity of Ethereum, represents a barrier to entry to tooling development given the depth of knowledge that is required.\nRethnet aims to make this easier by offering a native, flexible, extensible, fast, and language-agnostic EVM local development network, distributed as a Rust library, that is designed to be the underlying core in tools that provide debugging information to developers (like Hardhat, Foundry, Remix, Truffle, DappTools, etc). It will be a Rust library made to be consumed from other languages like TypeScript, Go, Python, etc as a native dependency. It will implement the baseline of essential functionality every tool should have like Solidity console.log, stack traces, and descriptive error messages, as well as implement code coverage, gas profiling, and a step debugger. At its core, it’s an implementation of an Ethereum node with a layer of EVM runtime observation to provide development features.\nBuilding a new Hardhat, Truffle, Remix, or DappTools using Rethnet will be a much more manageable project, and Rethnet will be completely reusable for any EVM language through adapters.\n\nHardhat\n\nOur flagship project targets the local development environment pillar, and it’s currently at an advanced level of progress and adoption. While Slang and Rethnet mature and catalyze organic growth in the tooling space, developers still have needs to be met, positioning Hardhat as our immediate-term solution to empower developers to keep decentralizing the world.\nHardhat is an Ethereum development environment that developers use to compile, deploy, test, and debug Ethereum software. Most importantly, it’s highly flexible, extensible, and designed to empower the community to build their own solutions. This strategy has been successful, and there’s already a valuable ecosystem of reusable plugins.\nHardhat’s roadmap is focused on becoming an extensible development environment with deep integrations across components in key areas of the tooling stack:\n\n\nHardhat VSCode 1 — programming editor\n\nHardhat Ignition — contract deployment\n\nHardhat Network 1 — local development network\n\nHardhat Runner — build/testing workflow\n\nThis roadmap leads to developers being well equipped to build powerful extensions to their workflow that increase their productivity according to their exact needs, and to then share them with the ecosystem in the form of plugins.\nHardhat will also eventually migrate to using Rethnet and Slang, increasing its feature richness, speed, and stability while enabling dogfooding at scale for our brand new building blocks.\n\nWeb3.js as a frontend platform\n\nThe OG Ethereum connector library, Web3.js, is being revitalized into a high-value project. By focusing on community and ecosystem growth, supported by an extensible architecture, it can become a great source of value, much like React represents in the front-end world, but for dapps. A website hub connecting community spaces, support spaces, educational resources, extensions, and related projects, combined with an active ecodev effort (workshops, talks, contests, and incentives), will create a source of leverage for the ecosystem. This will provide better troubleshooting, faster developer training, more reusable code, and, most importantly, the possibility of extending the library. This effort is currently spearheaded by the ChainSafe team.\n\nFunding\n\nThe Nomic Foundation aims to benefit the entire Ethereum ecosystem, which is why we’re fundraising across multiple organizations and individuals within it.\nThe Ethereum Foundation is leading this round of contributions with $8M, alongside contributions from Vitalik Buterin, Coinbase, Consensys, The Graph, Polygon, Chainlink, Gnosis, a16z, a_capital, and Kaszek Ventures. These donors make up $15M, and we’re aiming to raise $15M more.\n\nWhy Compound?\n\nGenerally, we think that allocating capital to the Nomic Foundation makes strategic sense for any protocol treasury that is aligned long term with the growth of Ethereum, and we’ve approached and will continue approaching several protocols.\nThe projects that the Nomic Foundation will deliver will create value for the entire ecosystem, including Compound. We’ll provide services to the Ethereum community that will:\n\nContinue the maintenance of critical infrastructure used to build most protocols (Hardhat).\nIncrease developer productivity for every team in the ecosystem.\nAccelerate developer onboarding to Ethereum, increasing the size of the experienced engineering hiring pool and making time-to-productivity shorter for new hires.\nAccelerate the pace of innovation and the number of products being built.\nIncrease market volume driven by new users and new products.\n\nWe believe this grows the market for everyone, including Compound, and we’d love to have the Compound DAO contribute $5m in funding to this community effort.\n\nAn ecosystem-wide effort\n\nWe’re currently seeking funding from multiple DAOs. We’ll update with the corresponding links below as we create each forum thread.\n\nUniswap forum thread 3\nENS forum thread 5\nYearn forum thread 3\nSushiSwap forum thread 2\n\n\nLinks\n\nNomic Foundation Announcement 2\\nThanks for the detailed proposal and for Nomic’s open-source tools. Our community and the others on your ecosystem outreach list have certainly benefitted from access to these tools. Hopefully Compound governance will ultimately be part of the new funding initiative.\nIt’s also great to see from the requests that Nomic considers us one of the most deep-pocketed DAOs, though that perspective may frankly be a bit dated. This request strikes me as at least a 2-3x larger ask, relative to DAO size, than the requests to other protocols.\nTo compare with ENS as an example: ENS has a fully diluted valuation of $1.5B compared to COMP’s fully diluted valuation of $1.2B; by that metric ENS is, financially speaking, the larger DAO. ENS also enjoys a revenue stream (from registrations) that is fully divorced from protocol security; in contrast, Compound’s “revenue” is in the form of token reserves which serve double-duty as a sort of insurance against bad debt within the protocol. One more example, Uniswap: we are only about 1/10 the size of Uniswap in treasury valuation, so a Uniswap-sized contribution (also $5M request) seems incorrectly weighted.\nCurrently a significant portion of our reserves are being considered to fund core community developer teams, leaving COMP emissions as the primary route for funding public goods.\nI will state support for up to a $2M contribution in COMP (approx 17000 COMP).\nBut I am one of the stingier community members on the DAO spending front. Let’s hear what others think.\\nI had a great chat with @FrancoNomic a number of weeks ago where he discussed all of what Nomic is planning, and I have to say, I’m impressed.\nThey’ve already built a great platform for development, Hardhat, which has greatly improved the developer experience. I’ve been using it for my own projects, I used it to build tools to aid in the recovery of the proposal 62 bug, and work is underway to adapt Compound protocol to use it.\nHardhat addresses many pain points of smart contract development; improving speed, quality, and security. The further development plans of Nomic will greatly improve upon these points. This benefits the whole ecosystem, allowing everyone to ship at a faster rate with fewer headaches and higher quality and security. Such tools will enhance the development process of Compound developers as well as those integrating Compound.\nCompound was founded in a time of very limited tooling. The work Compound Labs did to bring about Compound protocol was marvelous - creating Saddle and various other tools. This was years ago, and thanks to Hardhat, we’re able to retire the responsibility of maintaining and expanding on these tools as we shift to Hardhat. Once the shift is complete, we’ll be able to dedicate more of our resources to what we’re best at - providing the best lending and borrowing protocol.\nI’m very supportive of this proposal and can’t wait to use the tools to come.\\nHi all, I’m representing Avantgarde Finance. We’re supportive of this proposal.\nBuilding on Ethereum is complex. Even today, there’s no getting around it. Having spent a decent portion of February using a Hardhat mainnet fork combined with locally deployed subgraphs to build and test the front end for Enzyme’s integration with Compound, I know this challenge intimately. Conservatively 50% of my month was spent messing with deployment config rather than building the app. With that in mind, I listened in for 45 minutes several weeks ago while our CTO grilled @FrancoNomic on the foundation’s plans (both for Hardhat and more generally) moving forward. As they lay them out, it’s easy to see a world where they’ve created a bedrock layer of modular, unopinionated tools and they’ve also showcased one opinionated combination of those tools in Hardhat. The potential for additional devops products to be built off of the base-level tooling is enormous.\nThere will inevitably be pushback on the price tag. Bryan from Otherinternet said it best in his post 5 on this topic in the Uniswap forum:\n\nAs an initiative framed as growing/sustaining ethereum public goods, we need to view this proposal and others like it as an easy opportunity to mobilize stagnant capital, not a budgeting exercise.\n\nContributing to these public goods will empower present and future Compound contributors to focus more exclusively on building features and functionality that bring value to the protocol rather than devops configuration and tooling. In our opinion, this makes every future proposal easier to implement, every future request for funding easier to scope out and the teams behind those requests more likely to succeed.\\n\n\n\n 0x7751:\n\nThere will inevitably be pushback on the price tag. Bryan from Otherinternet said it best in his post on this topic in the Uniswap forum:\n\nAs an initiative framed as growing/sustaining ethereum public goods, we need to view this proposal and others like it as an easy opportunity to mobilize stagnant capital, not a budgeting exercise.\n\n\n\nThe Nomic foundation has requested funding at a specific amount; as strongly as we may share the ideal of supporting public goods with a ‘yes’ to requests from top-quality tooling contributors like Nomic, there is no getting around the fact that the specific amount requested, together with the existence of the protocol’s other priorities (including internal funding for the protocol’s development needs and other public goods funding) unavoidably makes this is a budgeting exercise.\nAs I mentioned earlier, Nomic has requested the same magnitude of funding from Compound as it has from Uniswap, perhaps not realizing that Compound is only 1/10 the size (by fully diluted market cap of its governance token). Since no rationale was offered for the request size, I am inferring Nomic was unaware of the significant difference in valuation between the protocols.\nIt is perfectly reasonable for the community to support this at a funding level that is more generous than Uniswap if it wants; 10x just strikes me as excessively generous relative to the other organizations participating. I hope this will be funded at a level that is still generous but also more closely proportional to Compound’s market presence.\\nSomething we wanted to avoid when deciding the amounts was making it seem to the Uniswap community like we were taking advantage of the fact that they have the largest treasury. If we had weighted by treasury sizes, we would’ve had to request the vast majority of the funding from Uniswap, which didn’t seem like the right approach. So we went for funding tiers of roughly $500k, $1m, and $5m. The tier and specific amounts for each were decided by having conversations to see how a few people at each community felt about the numbers, and that’s what we went with.\nWe thought newer DAOs in less established projects might struggle with significant asks. Since Uniswap and Compound are some of the most mature and established projects, we believe both DAOs are well-positioned to decide on a substantial allocation like this. We decided to ask for amounts that we considered wouldn’t break the bank for either one and make them the same, as part of the same tier.\\nWhile supporting Ethereum tooling is a positive, $5M in COMP tokens is a disproportionately large request, comprising 1.63% of the remaining COMP tokens, which are necessary to support user growth, community development, etc in perpetuity.\nI will be voting against this proposal; it is too large, and asks too much of Compound relative to the rest of the community.\\nAgree with this note–would look for a version of this proposal at a lower funding level \\nUniswap:Yes $5M\nENS:Temperature Check    Oppose > Agree  $5M\nYearn:Yes $400K\nSushi:No, not interested\nCompound: Agree > Oppose  $5M\nNo one wants to be someone who doesn’t support Ethereum tooling, but business is still business, we should be against it.\\nBased on the community’s feedback we’ll lower the amount in our proposal from $5m to $2m.\nWe submitted the proposal last night with $5m, but we’ve heard more comments since then and after discussing we think this makes more sense, so we’ll cancel and re-submit at $2m.\nThanks to everyone who provided feedback on this aspect!\\nYou forgot Curve, Vyper asks for donations in the Curve community.\nBitDAO Lido GnosisDAO … 6\\nWe submitted our $2m proposal\n\n  \n      \n\n      compound.finance\n  \n\n  \n    \n\nCompound 15\n\n  Compound is an algorithmic, autonomous interest rate protocol built for developers, to unlock a universe of open financial applications.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\\nCan anyone provide a bit of background as to how this proposal fits into any overarching budget, strategy, grants program or processes that currently exists at Compound?\nI noted a grants program/process for grant requests over/under $125,000 (still active?) - but am not seeing this proposal as part of that process? Would this be considered a grant, if not - why not? If so, why would this not be part of that process? This is not to say that the general idea behind the ask is not beneficial or positive, just that, as an organization, we would want the processes for giving out funding of this amount (grant or otherwise) to be transparent and fair to all those who may be seeking support for other worthy projects as well.\nWould be helpful if anyone more familiar with the community/structure could help explain how this proposal fits within those structures (any budget, strategy, grants programs in existence).  If any budget, strategy, grants process is still in infancy stages, query whether making a contribution of this size makes sense at this point.\\nI don’t believe this fits into any overarching plan. I’m a little surprised to see this get so much traction, given the unclear value proposition for the protocol.\\nOut of curiosity, what other communities are on the roadmap for this type of funding request? For example Aave core is listed as a user of Hardhat in the proposal, is that community also being commissioned for a donation? I didn’t see anything in snapshot or their forums.\nAlso, is Nomic planning to liquidate all of the COMP they receive from this request immediately after funds are transferred?\\nI am strongly opposed to this proposal as the proposition provides little direct benefit to the Compound Protocol writ large, and further, the ask is off by an order of magnitude.\nTo provide a simple example, let’s compare this proposal to what OpenZeppelin has committed to providing the Compound Protocol and community. OpenZeppelin has become the primary auditors of this protocol for the benefit of this community. They have assigned their own full-time engineers to audit the protocol and they have pledged to review all incoming governance proposals. Nomic Labs, on the other hand, is pledging to continue building the same tools they are already building. Nomic Labs is not pledging to focus the tools or building on the needs of the Compound Protocol. Yes, the Protocol benefits from a strong ecosystem of external teams and projects, built on the shoulders of giants. But if the community thinks $2MM is right donation for Nomic Labs, then should we also give equal- or greater-sized grants to the Ethereum Foundation for developing Ethereum, Joyent and Microsoft for making NodeJS and TypeScript, and Vint Cerf for his seminal work on TCP/IP. The DAO would be out of funds in a day if we scaled donations to match what is being considered here for Nomic Labs.\nWhile, as an engineer, I strongly believe engineers should be paid for their efforts in open-source software, Nomic Labs is asking for [in aggregate] $30MM, which at prevailing rates implies 150 engineering-years of effort. Not to belittle the effort, but Hardhat is, at its core, a tool which invokes Solc.js and executes a fork of EthereumJS. The tooling wasn’t used by Compound for a long time as there are multiple viable alternatives (Truffle, Saddle, etc). I even wonder if the push toward Hardhat came with this grant in mind.\nThis is hard to say, but I honestly find this proposal is effectively changing my viewpoint on B2DAO in general, as the grant is scaled significantly beyond what I would consider in reason, and the grant is riding the wave of good support that came from the amazing proposals by OpenZeppelin, Trail of Bits and ChainSecurity.  The protocol community should look at these decisions from a perspective of what would, in the long term, grow the value of the Compound Protocol and this community. On its face, this proposal offers little substantive benefits to the Protocol, while costing excessive amounts of limited resources.\nI am, and hope others are, voting against this proposal with prejudice.\\nA16Z:0x0f50D31B3eaefd65236dd3736B863CfFa4c63C4E\nA16Z:0x92E3F4891B37d1fAAE471091599721115893eD24\nNomic Foundation:0x8c64c3d24c4df13b451d5bc9292b75e5dba2e02a\n0xf50D transfer 65000 COMP to 0x92E3F4\n0x92E3F4 delegating 0x8c64c3d\n0x8c64c3d initiate a proposal Request a 2M$ donation\n0x8c64c3 and 0x0f50D3 Voted For\nWhy did A16Z not initiate the proposal itself, but lent it to the Nomic Foundation.\nIsn’t this a governance attack? It seems that they can ask for more donations to hollow out the protocol funds,\nMaybe I should avoid getting involved in protocols that A16Z has invested in.\\n\n\n\n hayesgm:\n\nBut if the community thinks $2MM is right donation for Nomic Labs, then should we also give equal- or greater-sized grants to the Ethereum Foundation for developing Ethereum, Joyent and Microsoft for making NodeJS and TypeScript, and Vint Cerf for his seminal work on TCP/IP\n\n\nI don’t think these are apples-to-apples comparisons. The Ethereum Foundation did a token sale to get funded and holds a significant amount of ETH as a result (of which they’re allocating $8M to this effort), so they’re not in need of funding for the time being. Providing funding to Vint Cerf in the past so that he could work on TCP/IP would’ve been impactful, but funding him today would produce no new value in TCP/IP. Microsoft needs no funding support.\nThe objective of funding public goods is making sure that highly-valuable projects that don’t attract returns-seeking investors are built, since the incentives structure isn’t there for the market to take care of it, and that’s what we’re proposing as a non-profit dedicated to Ethereum. We’re no longer a business, and we’re not asking for payment for the work we’ve done, but for funding that will be exclusively used to build new software for the ecosystem.\nIf no one funds the kind of projects we’re proposing they will simply not happen.\n\n\n\n hayesgm:\n\nWhile, as an engineer, I strongly believe engineers should be paid for their efforts in open-source software, Nomic Labs is asking for [in aggregate] $30MM, which at prevailing rates implies 150 engineering-years of effort.\n\n\nFor this framing to be fair it should be compared against what we’re setting out to do. I don’t think the amount we’re raising in isolation could accurately be described as being too much or too little without knowing what the team is planning to do with the capital. Compound Labs raised $33m according to Crunchbase, and I’m sure once the capital was deployed, more was needed to continue operating. The reality is that $30m for us won’t be enough either, and we’ll need to raise more in the future since we’re setting out to build a lot of sophisticated infrastructure, which we outlined in our proposal.\n\n\n\n hayesgm:\n\nNot to belittle the effort, but Hardhat is, at its core, a tool which invokes Solc.js and executes a fork of EthereumJS.\n\n\nThis is an extreme and inaccurate oversimplification of Hardhat, which isn’t the core of our proposal. At its core, Hardhat Network contains many of the same components a full Ethereum node contains, and an entire layer of Solidity tracing on top. As a simple example, we had to put a lot of time into building support for EIP1559, just like every other core Ethereum team did. EthereumJS provides just an implementation of the EVM (a single component), which doesn’t do anything for developers other than executing specific bytecode. It took Solidity 5 years to have proper stack traces and console.log(), with Hardhat being the first tool to introduce these features properly at scale. The reason wasn’t ecosystem laziness, but the fact that it’s a significant engineering effort to build passive runtime observation to trace Solidity at the opcode level with production scale quality. The same applies to Solidity error messages, which didn’t exist before Hardhat. None of this functionality is provided either by solc, or by EthereumJS.\nWe’ve built a very significant amount of code around them to provide the functionality the ecosystem relies on, and we’re proposing to build new alternatives to EthereumJS and solc. Both Slang & Rethnet are projects designed as platforms for the ecosystem to build more developer infrastructure. In the long-term, the biggest impact will come from the dozens of tools built on top of these, rather than Hardhat.\nThese challenges are explained at high-level in this piece about Slang & Rethnet, where we also explain how we’re turning what we built so far into something bigger.\nHardhat alone is a major engineering project to deliver with quality, polish, correctness, and stability to service >20k developers. The code is open-source for everyone to look at, but Hardhat isn’t the core of our proposal. The big picture combination of all of the projects and the bootstrapping of a non-profit engineering organization dedicated to Ethereum public goods is.\n\n\n\n hayesgm:\n\nNomic Labs, on the other hand, is pledging to continue building the same tools they are already building.\n\n\nThis isn’t a fair or accurate take on our proposal. The Hardhat everyone is using today is one component in a much bigger plan which we’ve outlined in the proposal. As one example, building a new Solidity compiler (Slang) from scratch is another major engineering project, which will benefit Ethereum and Compound indirectly in a significant way. In combination with Rethnet, these will give birth to many more tools, independent of Hardhat. These projects are in a very early stage, with Slang not having a single line of code written yet.\nWorking on a programming language, compiler, end-user tooling, and infrastructure for tooling developers is what building and maintaining a software development platform entails, and that’s what we’re doing for Ethereum. As a non-profit. This donation isn’t a post-facto award for the work we’ve done, but funding to create more value for Ethereum (and hence Compound).\nI appreciate you chiming in, but I believe this perspective is misleading to the Compound and Ethereum communities as it doesn’t consider the importance, potential impact, extent, and complexity of the work we’re doing. Talking to engineers involved in the development of advanced developer tooling could be helpful to understand the relevance and cost of our work from an unbiased point of view.\\nThank you for your response here, and I do appreciate the work that Nomic has done on a variety of high-quality tools. I think the key driver that’s not being discussed here is: why should the Compound Protocol support this work? Is it because some developer of the protocol may someday decide to use some of the technology built, maybe?\nHistorically, we’ve seen developers submit a proposal to the Governance system with the grant given simultaneous to execution. Those proposals have a clear value for the community to assess. This proposal, on the other hand, has no tethered return value. The Protocol is sending assets in exchange for the hope that some of the tools developed by Nomic turn out to somehow be valuable for future development of the Protocol. This isn’t even a streaming grant that assesses milestones over time- it’s a lump sum.\nThe benefits created by Nomic will, honestly, help new teams and projects in the ecosystem far more than they will help the Compound Protocol itself. That work should be applauded, but we would be better off donating as individuals, not as a DAO, to support it.\\nWe addressed this in the proposal:\n\n\n\n FrancoNomic:\n\nWhy Compound?\nGenerally, we think that allocating capital to the Nomic Foundation makes strategic sense for any protocol treasury that is aligned long term with the growth of Ethereum, and we’ve approached and will continue approaching several protocols.\nThe projects that the Nomic Foundation will deliver will create value for the entire ecosystem, including Compound. We’ll provide services to the Ethereum community that will:\n\nContinue the maintenance of critical infrastructure used to build most protocols (Hardhat).\nIncrease developer productivity for every team in the ecosystem.\nAccelerate developer onboarding to Ethereum, increasing the size of the experienced engineering hiring pool and making time-to-productivity shorter for new hires.\nAccelerate the pace of innovation and the number of products being built.\nIncrease market volume driven by new users and new products.\n\nWe believe this grows the market for everyone including Compound.\n\n\nThe line of thinking for why projects like Compound should support Ethereum itself is similar for why it makes sense for tech companies to heavily invest in open-source, albeit at a very different scale for now. Linux, v8, GCC, Chromium, LLVM, Rust, Node.js, etc are all examples of OSS public goods with heavy funding from many organizations building on top and benefitting from others also leveraging the created value. Compound itself exists thanks to these technologies being funded in the past. FAANGs all needed the internet to grow for their businesses to be able to grow, so they invested heavily into infrastructure that is reusable by others to empower the world to keep building tech.\nCompound is aligned and dependent on Ethereum being successful, and this requires the ecosystem to keep evolving.  Funding public goods grows the pie for everyone in the long-term, including Compound, but in a blockchain context where decentralization matters, I think it’s even more important for the funding for these efforts to come from a diverse set of backers, to allow for neutrality to flourish.\nDecentralization isn’t simply an architectural aspect, but also organizational. If the Ethereum world we’re all building is reliant on the EF being the sole provider of funding, then we could go in a direction of not being as decentralized as we could be on the most core component that underpins the entire ecosystem, and that poses many risks. What if EF changes? what if it’s captured? what if management changes and becomes really bad?\nIs the infrastructure that Compound is relying on both for its tech stack and for the growth of its business with certainty going to be effectively developed, maintained, and funded for the long-term, in a way that is strategically (see: decentralized) aligned with Compound and every other decentralized protocol? I don’t think the answer to this is a clear yes today, but I do think we can build that. An independent organization with diverse ecosystem funding being solely focused on helping Ethereum evolve into a world-class development platform is a really important aspect of increasing the odds of success, and that’s what we’re requesting funding for.\nI realize that Compound is working to achieve similar objectives for itself, as it should, but I’m sure you know that these things don’t happen quickly. They take time to build, and if we don’t plant the seeds today, then we won’t see the results in the future.\nLong-term, as long as Compound is Ethereum-aligned, it looks like Compound needs efforts like ours to be successful. I think it makes sense to allocate resources to make that happen.\\nTo add to this, Compound today is already relying on this strategy. Integrations in other apps and protocols are important aspects of driving protocol activity, and I know you’ve worked on improving the developer experience of integrators. Compound needs more people and projects implementing software to keep increasing protocol activity, and we’re all about empowering them to do exactly that.\\nMisread a point made by Geoffrey and my comment just added noise to the thread, so I removed it.\nIt seems Polychain has joined the room!\\nAlright, the community’s position is clear! I’m glad that more stakeholders have gotten involved in the discussion now. Looking forward to chatting with you all to further calibrate our proposal. Thanks for chiming in."
  },
  {
    "number_of_comments": 31,
    "postid": "c583251f-d054-450d-9942-efad8d02a08b",
    "posturl": "https://www.comp.xyz/t/comp-distribution-speeds/899",
    "combinedcontent": "Each block, a total of 0.1760  are distributed to suppliers of assets, and 0.1760  to borrowers; this is the COMP Speed parameter. COMP speeds are allocated by market, e.g. DAI is 0.0977, and ETH is 0.0016.\nWith the passage of Proposal 033 40, COMP Speeds are now set through governance (by calling setCOMPSpeed for a given market), and are no longer automatically/dynamically adjusted based on borrowing demand. This is an important change – the community now has the power and responsibility to allocate COMP by market.\nThe current allocation 39 is a relic of the moment that the proposal went into effect; the COMP Speeds will stay at the current rates, until they are set through governance.\nCOMP Distribution Speeds1088×1544 67.4 KB\nThe prospect of governance proposals to modify the allocation is exciting–before the community acts, here are a few guiding questions to shape how this tool is used:\n\nShould the sum of COMP Speeds match the current total,  0.1760? If markets are adjusted individually, should an increase in one market be offset by a decrease in another?\nHow should important collateral markets like ETH and WBTC, that historically have little borrowing demand, be allocated ? How would this change user behavior?\nShould assets that behave similarly be given similar COMP Speeds? Can assets like BAT, ZRX, and UNI, or stablecoins like DAI, USDC, USDT be grouped?\nHow can the COMP Speeds be used to increase the distribution to more users, and increase decentralization?\nShould the community set COMP Speeds infrequently, or actively calibrate them?\n\nAt this time, the COMP Speed applies equally to suppliers and borrowers–modifying that relationship would require a significant protocol change.\\nWould it be possible to dynamically adjust comp speeds? For example: borrow rate on USDC is 20% and no new supply is coming in, maybe increase comp rewards for a set # of blocks, possibly scaling every few blocks to sweeten the pot to incentivize people to supply more USDC. This would work for all the markets and would incentivize people to keep reserve ratios at healthy levels.\\nMy starting point on this would be “if it isn’t broken don’t fix it”. To me the current distribution speed and market allocation seems pretty good. Intuitively it makes sense – markets with higher interest rates receive larger amounts of COMP. I would like to see how the vesting effort plays out before making major changes.\nI don’t really want COMP to distort normal market behaviors. So that would mean you always would want the distribution APY to be lower than the market APY The one exception might be the COMP market itself. Distributing more COMP there could be a nice way to reward holders… or it may just get abused by farmers.\nThose are my first thoughts \\n-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA256\nOptimizing COMP Rewards\nWhy?\nAs Compound pioneered liquidity mining, it is only natural to ask if the distribution of rewards can be optimized. Why might the community decide that this makes sense? Firstly, it is likely — as often happens with market maker and liquidity incentive programs at centralized venues — that an issuer ‘overshoots’ and overincentivizes the either the supply or demand side of a two-sided market. This problem isn’t unique to financial asset incentives, either — even ride-sharing markets like Uber and Lyft are notorious for losing money due to excess payments to both the demand side (riders) or supply side (drivers). In Compound’s case, COMP emissions are used for a multitude of reasons, including but not limited to:\n\nLiquidity Incentives\nPaying for development work for the protocol (e.g. Proposal 031 5)\nPaying for risk management and insurance for the protocol (e.g. Proposal 032 9)\nStaking/bonding mechanism for Compound Chain 5\n\nGiven the fixed cap, finite supply of COMP, overspending on one of these components reduces the protocol’s ability to spend on other components. As such, it is prudent for the protocol to optimize how much it spends on liquidity to ensure that other uses of COMP are not drowned out. Moreover, liquidity providers already earn yield from borrowing activities and it is not clear how much of that liquidity would leave without active experiments. However, it is clear that liquidity incentives are one of the best ways to decentralize a protocol and maximize the number of active token holders — which should be taken into account when adjusting rewards.\nLet’s do some back of the envelope math to compute how much the protocol is currently spending on liquidity\n\n\n\n\n\n7 day moving average of the number of blocks produced by Ethereum per day: ~6514.29\n\n\n\n\n\n\n\n\nPrice of COMP:  ~$150 (USD)\n\n\n\n\n\n\n\n\nTotal COMP emitted per block (both supply and borrow sides): 0.352 COMP\n\n\n\n\n\nThis means that the protocol is spending ~$343,954.29 per day or $125,543,314.28 per year on liquidity incentives. Spending ~$125M to attract users is quite expensive! If we approximate the number of users of Compound by the an optimistic estimate for number of active addresses 3 (~300K) that means that the protocol is spending $416.67 per user per year on customer acquisition costs (CaC). These are numbers that would scare any investor outside of cryptocurrency!\nHow can we optimize incentives?\nNumerous other industries that manage two-sided marketplaces, such as ride-sharing, centralized trading exchanges, and gaming, deal with incentive optimization using data science. Given historical data and models for how users behave, market designers can effectively construct A/B tests. These tests could operate in the following manner (stylized and simplified for readability):\n\nAt time t, on-chain data and centralized market data is used to make a prediction of how much liquidity will change as a function of incentive (= USD price of COMP * COMP emitted)\nThis leads to a hypothesis test:\n\nNull Hypothesis: Reducing the COMP emitted to the Dai market by 10% will reduce liquidity by 15%\nHypothesis: Reducing the COMP emitted to the Dai market by 10% will reduce liquidity by much less than 15%\n\n\nA governance proposal is proposed (akin to Proposal 021 1) to reduce emissions by 10% to the Dai market\nIf passed, the data aggregated from monitoring emissions for some time interval T (say, 30 days) will provide us a way to estimate whether the null hypothesis can be refuted or not\nNew data from the test will be reincorporated into the model (first step) and used to construct another hypothesis test\n\nThe literature on designing such trials for online, two-sided marketplaces is vast, but none of this has ever been done using markets with full data transparency, like a DeFi protocol.\nHow to answer @rleshner’s questions\nPart of what we are working on at Gauntlet is something called Automated Governance 5. This is system that continuously runs simulations (improved versions of those used in the Compound market risk report and in our DeFi Pulse score for Compound 2) and estimates ‘safe’ and ‘unsafe’ parameter values. These parameter values can be for per collateral protocol parameters (e.g. collateral factors, reserve factors) as well as for COMP incentives.\nWe propose that all of @rleshner’s questions can be analyzed in a scientific and experimental manner using a system like this for proposing hypotheses. In my opinion, it is impossible to answer these questions for a novel, unique market like Compound without performing some experiments via adjusting COMP speeds. These experiments will naturally be able to tell the community if spending $125M per year is expensive or if it is necessary in the competitive DeFi environment. Moreover, experiments can easily tell us whether these speeds need to be adjusted infrequently or not. For instance, if an experiment to reduce emissions by 10% leads to the null hypothesis staying true, then we have accrued information that suggests that user behavior is insensitive to changes and speeds should not be adjusted too frequently.\nConclusion\nThe beauty of cryptocurrencies is that their data is completely transparent and public. This means that anyone who has a clever model can test it out without having to spend money on data (like in traditional finance) or harvest user data and make it nonpublic (like a tech company). However, someone has to actually produce this data and analyze it. The only way to do that is via carefully designed experiments whose results can be clearly interpreted, much like a clinical trial or A/B tests in centralized online platforms. Compound led the way by showing that yield farming was a viable methodology for protocol decentralization and it can lead the way in providing scientific rationale for parameter choices in the years to come.\n-----BEGIN PGP SIGNATURE-----\niQIzBAEBCAAdFiEEEJAuhmVduSEVN8uPKIPJySxksQ8FAl/tJfQACgkQKIPJySxk\nsQ/nfA//WTcQ7Sgu2z3iFjhfYDb1DXl0Qu969MB+eak7WOYhjKkF7b885he20Bgv\n4go1U+n7UsyRlfmvx33m5CNDNnBHFk+MYY01dkm548viuTEdJ68X3Wof7kOuUeqr\ndfTUal7cV+twHjLbolVORrBE3mAFcVcmuXbfhi8g+CBSx43437zz61KQVolQrlfL\ndotUtv6KGZFYrXPtP0wIuS7fZvSe3zLDIG8EttnxTR692u9EU3p11WP77IFvmpAg\naaV7PFnVyVnacHZYcTdDGGFOs+8+VO8OvRm4OOBD9D4gsLgYxBY3TUhoNDEw6i1V\np2AXIX0pA+ngsBgPjhFiKtTxapfKSQTd/z8lxmIVFsEpXWZ5mzQJRTiOBSRywRmE\n1TuVd9avi/yKtF2qC8fzi3+wyT99rKySuixropqR4/y0aG7+bU+0g9zbrJCCClXZ\nP04yblEJ8pHit0rrcqblqw5aQSUv6Uxu19dOK6g9l8X3j3epggHhA6nmNHNJlICg\nCHzM9md7hP0DsjYGtG+to2qloyHvcGVSNeMsNWcLmLvXAgn9IKOyXKoKxkWkIgT+\nextMTDEcVaAQLfHIZIa8jOQ8Q3zfSpRWIOM/zBUk4kFeSn795ti5HjpOlAo8Xf+c\neqT49pVqf8m90n/5XViRv+1fCqGHZ48CuA8fPfjNT5psC83WHv8=\n=wm0g\n-----END PGP SIGNATURE-----\\nStrongly agree with this. It’s not wise to anchor on the status quo, especially since this distribution was initiated without any foreknowledge of COMP’s price or its effects on lending/borrowing rates. Compound needs a more rigorous way of determining: what is the best COMP emission rate that maximizes the long-term value of Compound. If it’s higher than the current rate, increase it; if it’s lower than the current rate, decrease it.\nSticking with the status quo makes no sense if it was arrived at basically randomly.\\nI like Tarun’s idea of experiments to optimize the rate.\nAnd as Tarun also points out, we could reallocate some/much of this COMP to other (perhaps more productive) uses- the forefront for me being development work on the protocol.  For example, using COMP to fund initiatives into new lending products could expand the user base and assets on platform in way that’s more effective and enduring than a COMP distribution to protocol users.  At the very least, it seems like an attractive area to explore.\nPersonally, I could do without such a large COMP subsidy- it distorts the natural market that forms between borrowers and lenders (ie this protocol is already very useful without the subsidy) and also leads to a lot of unproductive actions within the protocol, such as yield farming.  The distribution is quite expensive and has unclear utility to protocol growth.  Running A/B tests and other such experiments will help quantify that utility to some extent, but I already have a strong hunch that some of that ~$125mm a year in COMP distributions could be better spent on funding/studying new initiatives/products.\nAt the very least, I’d like to see the COMP distributed adjusted such that Net Borrow Rate > Net Supply Rate for each token (by Net, I mean the rate net of COMP distribution and reserve).  It seems crazy to have positive carry in the same token on the same protocol.\\nVery well written, by @tarun but completely based on Hypothesis that COMP is about 150$ and that protocol is spending $ on liquidity incentives.\nReality is that COMP is about 150$ with current market free float. Do you really believe if we, for example, double market free float of COMP right now, price will hold? I sincerely doubt it. In future it might though, if market grows. However from observing current conditions we can also suggest that current release of COMP to market doesn’t move price much, and demand keeps up with selling pressure, which is likely less than daily emission, as some are probably accumulating.\nProtocol isn’t really spending any  on liquidity incentives, as it's merely distributing tokens, created from thin air and not really backed with any sort of value and which are 150 only as long as there is a demand at that price.\nHowever, even if we look at COMP from perspective of zero-value token, and merely an utility token for governance, do we think that all tokens, reserved for distribution, should be given out to liquidity miners? I believe not. And that is reason why i believe current structure should be adjusted. And that is also a reason why i believe airdrop to early users should be done as well. Not because i see it as giving out money, but because it’s by far better idea, than just give everything to liquidity miners. They are going to get major share anyway.\nSo from that perspective i think it’s very reasonable suggestion to decrease emission for dai market by 10% and observe results.\nWhat i would Suggest personally?\n\n\ndecrease daily emission by 10%-20% (That could be taken either exclusevly from dai market, or dai and usdc market)\n\n\nUse that 10-20% of emission for continious market sell for stable coin/eth/wbtc. Use the proceeds to create treasury. Hypothesis here is: Liquidity farmers are selling good part of daily emission to market and that doesn’t collapse the price. Protocol could as well sell that tokens personally, rather than giving them out to farmers so they could sell and have profits. What protocol should do with treasury? First it should direct that money to reserves of the biggest utilised markets on borrow side until reserves reach at least 1% of supply for every market. It would take quite a time, what to do later after goal is achieved could be decided at a later time. That will greatly derisk markets by seeding a reserves to be used in case of unexpected events, like Coinbase DAI price event. As natural reserves creation is going take forever with current reserve factors. And COMP reserves, are not really reserves as it’s not money until it’s sold, and you don’t now how much you could possibly get for it untill you actually sell. It’s like paper profits. And there’s no difference if supply is limited or not, as certanly it’s very much possible to create COMPV2 with different supply, some conversion rate COMP to COMPV2 and transition governance to new token. So it’s limited only as long as governance agree and want it to be limited supply token. Doesn’t mean it’s set in stone forever and impossible to change. Always nice pitch though, of course \n\n\nRedistribute remaining emission, rerouting part of emission for stable coin markets towards ETH and WBTC markets.\n\n\nMinor markets could be adressed later, but in general i think good approach would be take some percentage of total daily emission, let’s say 10%, and distribute it in equal portions to minor markets and observing the results. (Interesting here is would it boost utilisation or not) I believe there should be minimum portion of total daily emission every market should recieve regardless of utilisation.\\nA main reason of having large COMP rewards was to decentralize ownership of the protocol amongst protocol users. This is a major difference for DeFi protocols vs. the traditional comparisons above: customer acquisition cost is not necessarily what should be optimized. The $125m/year is about spreading ownership, not just attracting users. Decreasing the rate of rewards (w/o increasing it elsewhere) slows that down. There may be good reasons to slow down the decentralization, but I’m not really seeing that discussed.\nTo me, the larger question is: how decentralized is the protocol becoming through the distribution (and how can that be optimized)? There are many ideas floating around about this (like vesting). Ideally, this would also happen to go hand-in-hand with attracting many new users from the distribution and fixing some of the market distortions arising from the current distribution pattern.\\nThank you for this helpful reminder on how the COMP distribution aids in decentralizing the protocol.  Perhaps we could then just at minimum adjust the velocity of COMP distributed in each market such that Net Borrow Rate > Net Supply Rate for each token (by Net, I mean the rate net of COMP distribution and reserve). By righting these incentives you would be more likely to distribute COMP to folks actually using the protocol for its intended function vs sending it to folks who are yield farming or trying to get free carry.  Note that the COMP distribution economics in this adjusted case would still help decentralize the protocol, make the yield for lending better, and also  subsidize the cost of borrowing- just not so much it borders on a handout for those who lever up on both borrowing & lending simultaneously.\\nGreat set of comments & opinions–\n@johndoh this is how the distribution originally functioned; it scaled the COMP allocation as a function of the interest rate in each market. It was very easily gamed, as users/farmers attempted to use whichever market already had the highest interest rate; first, USDT, then BAT, then DAI. It’s possible that this model combined with vesting/cooldown (Gauntlet is doing fantastic work there) could be viable.\n@Sirokko and @aklamun bring up an extremely important point, which is that the distribution was originally (and still is?) intended to distribute COMP to users that want to be long-term stakeholders in the protocol; to participate in governance; not to “farm”. Changes to the protocol or market allocations that decrease the distribution to “farmers” and increase them to “users” (also to @lay2000lbs point) fit this goal.\nIncreasing the allocation to the large collateral assets (ETH, WBTC) and COMP (and in part, away from stablecoins that could lead to recursive leverage) could begin to achieve this.\nAnd to @tarun and @haseebq’s point–the community should prepare to experiment, improve, and adjust the COMP distribution to best achieve it’s goals.\nTaking all of this together, and speaking with a number of you in Discord, I’ve tried to synthesize a “baseline” set of allocations by market, which we can then run experiments against.\nScreen Shot 2021-01-04 at 12.09.47 PM1008×532 47.2 KB\nThis baseline:\n\nMaintains the current total distribution (0.1760)\nCreates a modest allocation to the primary collateral assets (which didn’t exist previously due to a lack of borrowing demand)\nStandardizes (but doesn’t materially increase) the allocation to secondary collateral assets\nMaintains large allocations to stablecoins, while setting DAI and USDC to be equal to eachother as a point of reference for future experiments\nIncreases the allocation to COMP\n\nDoes this jive with everybody’s thinking?\\n@rleshner I do not quite understand how you got the numbers in Supply and borrow, can’t be COMP per day, as if speed is same than it couldn’t be more than current about 2300 total  COMP. But regardless, i’m a huge proponent of simplicity, predictability and ease for understanding, so take a look at my suggestion and below i’ll explain how i arrived to that numbers and some reasoning behind.\nI group all existing markets into 3 Categories:\n\nImportant Collateral Markets\nImportant Stablecoin markets\nMinor markets\n\nFirst i took 10% of distribution (0.0176) and distribute it in equal portions to each active market. Which makes 0.00195 per market. That portion just goes there regardless of current utilization of that market.\nNext 10% of distribution goes to ETH market\nAnother 10% goes to WBTC market.\nBoth of theese markets are important Collateral markets.\nRemaining portion is split equally between 2 stable coin markets. USDC and DAI. USDT market is positioned in Minor market because it’s not usable as collateral and thus can’t possibly compete with USDC and DAI at Compound. It’s not going to be big regardless of demand for that token as long as it’s in that state.\nThere might be arguments that Collateral markets might need bigger share, but we should keep in mind that we already taking away some distribution from stablecoins, we should observe impact and don’t introduce fast radical changes.\nAs for 50/50 and 75/25 distribution reasoning is. I believe it makes no sense to micromanage exact numbers, as liquidity will always be faster than governance, 50/50 split existed for quite a time, people used to it, liquidity will be easy to adopt and adjust. Could be changed later if needed, but i think it’s a good basis point. At introduction there might be some earning opportunities in minor markets, but i expect money quickly to adjust and profitability will balance itself.\nI believe we should start with 75/25 for collateral markets, as borrow side of that markets was always on the low side. There might be some arguments that 25% might be too big for borrow side, but that’s for a reason. Here we not just follow market, we creating some initiative to bring more utilisation to that side. Not too much, that’s why it’s not 50/50, but big enough that we could see some interest there. I believe in long run, it beneficial to have somewhat more on borrow side of ETH and WBTC markets.\nI believe it’s good simple starting point. Can be easily adjusted from there if need arise.\n         Current            New            Supply         Borrow\n\nETH       0.0016        0.01955           75%            25%\nWBTC      0.0011        0.01955           75%            25%\nBAT       0.0000        0.00195           50%            50%\nZRX       0.0013        0.00195           50%            50%\nUNI       0.0006        0.00195           50%            50%\nUSDC      0.0675        0.063575          50%            50%\nUSDT      0.0057        0.00195           50%            50%\nDAI       0.0977        0.063575          50%            50%\nCOMP      0.0004        0.00195           50%            50%\n\nTotal     0.1760        0.176`\\nFWIW, I’m coming around to the idea that it would be fine for current Net Borrow Rate < Net Supply Rate, provided COMP vesting will happen in such a way that the user has to take a non-trivial price drift risk in the COMP component of the yield.  If that’s the case, then probably okay to just optimize for depth of collateral markets per unit COMP distributed with the aforementioned approaches.\\n\n\n\n rleshner:\n\n@Sirokko and @aklamun bring up an extremely important point, which is that the distribution was originally (and still is?) intended to distribute COMP to users that want to be long-term stakeholders in the protocol; to participate in governance; not to “farm”. Changes to the protocol or market allocations that decrease the distribution to “farmers” and increase them to “users” (also to @lay2000lbs point) fit this goal.\n\n\nI would like to throw out an idea on the point “allocations that decrease the distribution to “farmers” and increase them to “users” (also to @lay2000lbs point) fit this goal.”\nIf we define “farmer” as a user that frequently claims and sells COMP to compound farming returns, cash out, etc., would some sort of distribution model that “punishes” frequent claiming (rather than conventional vesting, more opt-in), be worth considering? Has this been brought up?\nI certainly lack the sophistication to model it formally myself, but intuitively, it seems that allowing some level of farming activity to subsidize liquidity (to some degree), while at the same time rewarding users who accumulate COMP (and perhaps especially those who vote with it) fits the goal of shifting distribution from farmers -> users.\nThe “punishment” (/pentalty) for the “farmers” (however that makes sense to codify, certainly lots of ways to do that… e.g. carry traders, frequent claimers, etc.) could even be distributed to treasury.\\n@Sirokko re: the original post:\n\nThe Supply and Borrow numbers are the market sizes (at the time of post), in $ million, as a point of reference.\nThe even split between suppliers and borrowers is how the protocol functions; adjusting the ratio would require changing the code of the protocol. I completely agree with you that being able to fine-tune the split by market (e.g. 75/25 for collateral) would be very useful and the community should bookmark this as a future protocol improvement!\n\\n@mrhen It’s indeed important to properly define things, so we could clearly understand each other. In case of COMP farmers, as i define it it’s an activity of supply/borrow  assets to Compound in that way, that it generates positive net flow achieved by selling distributed COMP tokens, measured in USD while minimising the volatility risks at same time.\nGood example of that strategy will be, for example supplying DAI and borrowing DAI at same time. Normally that operation makes no economical sense, as borrow rate you will be paying will be higher than supply rate that you recieve. However, if you account for COMP distribution, you have positive cash flow. For small users it makes not much sense, as returns aren’t that huge, but if you throw bunch of millions, you arrive to quite a steady profits.\nOf course, farmers are indeed users of Compound too, problem is that they not really bringing much value to protocol aside of high numbers, which could be called a sort of expensive advertisement. As if i bring for example 10 millions and borrow back 6-7 millions from that, i’ll draw high numbers on both sides, but actual liquidity i provide doing that is much smaller.\nSo, it’s not really about selling COMP or holding COMP, it’s about using Compound specifically for the purpose of capturing that distribution by activity, which isn’t profitable without that distribution.\nSold COMP to market isn’t really a problem for protocol. For every seller there is a buyer. COMP is just being redistributed from those who don’t want it, to those who want. There’s nothing wrong with that process. It doesn’t look like being huge impact on price valuation either.\nI don’t see the need to “fight” or penalise farming, as well as i believe vesting, isn’t really that much needed aside of vesting for initial venture capital… And that’s not to discourage development of vesting. Any development is great, as others might see utility where i don’t see one. And certanly the more instruments protocol could utilise, the better.\nI personally see vesting (and by that i mean basically delaying a COMP distribution by a certain period) as a proved inferior solution. I think it’s quite observable from other projects in DeFi, that “staking” works much better. In quotes is because while term is widely used in DeFi, it’s nothing to have with “proof of stake”. Basically usually it’s simple timelock of token in smart contract to collect more rewards.\nThat could be done in similar manner for COMP and it could recieve some portion of new distributed COMP if there is a need to provide incentive for holding COMP. That’s quite common workable solution in DeFi. Question here is what sort of benefits it can bring to protocol?\nFrankly, i’m much more concerned by lack of reserves in pools in comparison with TVL. Preparations for rainy days are usually done far before rainy days actually arrive. But that’s another topic of discussion.  Here we talk about Comp distribution adjustment, which is quite a priority since auto-adjustment is removed already.\\n\n\n\n Sirokko:\n\nSold COMP to market isn’t really a problem for protocol. For every seller there is a buyer. COMP is just being redistributed from those who don’t want it, to those who want. There’s nothing wrong with that process. It doesn’t look like being huge impact on price valuation either.\n\n\nI agree with this reasoning generally.  However, in the context of yield farming COMP to sell on exchange, Compound is punting on letting the protocol benefit economically from this sort of decentralization.  For example, if the yield farmer is just going to flip the farmed COMP, why not instead have the protocol just sell COMP directly via DEXes or something of the like?  That way the protocol could diversify its COMP to other tokens (through the COMP sale for another asset/token) while also decentralizing COMP governance to whoever purchases the sold COMP on DEX.\nAnd so, if there was a policy we could enact to reduce COMP distributions to yield farmers while keeping reasonable COMP incentives for actual “real” users of the protocol, we could then route the saved COMP to exchange to still diversify ownership but also diversify the Compound protocol treasury.\\nI’m with you on that also. I indeed think that it would be more beneficial for protocol to reduce emission of COMP by 10-20% and use it for direct market selling of COMP instead to seed a reserves for pools. And when reserves reach at least 1% of supply either decide on increasing reserves more, or direct funds to create treasury consisting of eth/wbtc/dai/usdc basket.\nI already suggested that on several occasions, but to have discussion more focused i’d suggest deciding on one thing at a time. Yes, reserves are important, treasury is important. Yet, shaping initial frame for new COMP distribution is important too. It’s not very productive and more difficult for everybody to understand if we will try to fit everything in one single proposal.\nI presented my suggestion about new distribution. Robert also suggested one. They are quite similar in numbers, but i believe mine is more clear about how numbers are delivered. Maybe you have some comments on suggestions on that? Might be you see a better model? Better distribution?\nI’m sure if people would give more participation and supply their reasoning, something better can be shaped, rather than just waiting untill somebody create a proposal with what they think is the best, just because there is literally no other suggestions.\nYou can hardly blame VC that they make proposals with what they believe is better if nobody else really bothers.\\n\n\n\n Sirokko:\n\nI presented my suggestion about new distribution. Robert also suggested one. They are quite similar in numbers, but i believe mine is more clear about how numbers are delivered. Maybe you have some comments on suggestions on that? Might be you see a better model? Better distribution?\nI’m sure if people would give more participation and supply their reasoning, something better can be shaped, rather than just waiting untill somebody create a proposal with what they think is the best, just because there is literally no other suggestions.\n\n\nI hear you on that and apologize for not translating how ideas would percolate down to a distribution speed or a proposal that closely ties to the topic thread.  That is good feedback. I’ll try to run some calculations in the coming days/weeks on how this overlay policy of Net Borrow Rate > Net Supply Rate would affect yours & other speed suggestions.  I have multiple kids in diapers and no childcare right now, so haven’t had much free time to do the level of work I’d like to do on this.  May take a bit longer  to get back with firm numbers.\nI should also re-note, I’m coming around to the idea that it would be fine for current Net Borrow Rate < Net Supply Rate, provided COMP vesting will happen in such a way that the user has to take a non-trivial price drift risk in the COMP component of the yield.  It is likely a superior way to reduce farming because it avoids adding another rule to the COMP speed system that may require relatively frequent adjustments/votes (since COMP price and other protocol variables would likely affect the Rates too often).\\n\n\n\n lay2000lbs:\n\nIntuitively it makes sense – markets with higher interest rates receive larger amounts of COMP.\n\n\nCorrect me if I am wrong, but here exists opportunity cost for Ethereum (example). Eth has a low-interest rate and that can disincentivize Ethereum holdings on the protocol (because of staking yield). I think a high LTV ratio is not enough incentivizing for holding because of volatility and aggressive liquidation mechanism.\nIt would be more logical to reduce the incentive to stablecoins on the deposit and lending side because they have a high-interest rate on holding and borrowing anyway. Perhaps this would partially solve the problem with - “stablecoin leverage farming” in which I see no point other than fictitious TVL pumping.\n\n\n\n rleshner:\n\nShould assets that behave similarly be given similar COMP Speeds? Can assets like BAT, ZRX, and UNI, or stablecoins like DAI, USDC, USDT be grouped?\n\n\nI think this is interesting idea\\n\n\n\n tarun:\n\nPaying for risk management and insurance for the protocol (e.g. Proposal 032 )\n\n\nIn this industry (early stage) this is  important use case for COMP token, but voting results on proposal show otherwise.\n\n\n\n tarun:\n\nThis means that the protocol is spending ~$343,954.29 per day or $125,543,314.28 per year on liquidity incentives. Spending ~$125M to attract users is quite expensive! If we approximate the number of users of Compound by the an optimistic estimate for number of active addresses  (~300K) that means that the protocol is spending $416.67 per user per year on customer acquisition costs (CaC). These are numbers that would scare any investor outside of cryptocurrency!\n\n\nIf you compare Aave vs Compound user adoption rate the data is worrying. Is that because airdrop on Coinbase Earn program? Active addresses with 3$ USDC?\nSimilar marketing tricks in the area of traditional business are less effective on decentralized protocols.\\n-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA256\nIn order to add some data to compare real borrowing to the suggested changes from @rleshner and @Sirokko, we tried to construct lower bounds on the amount of recursive leverage used. Recursive leverage is somewhat tricky to define, so we instead used a lower bound as a proxy. Before looking at the data, we will first define what recursive borrowing is (and how it is used to maximize COMP rewards) and then describe we computed a proxy.\nRecursive Borrowing Queries\nA recursive borrower (aiming to maximize COMP yield) will recursively construct an array of collateral quantities (C[0], C[1], ...,), borrow quantities (B[0], B[1], ...,), and transfers between fresh addresses (a[0], a[1], ...) as follows:\nBase Case:\nC[0] = depositCollateral(address a[0], block height h[0], transaction t[0])\nB[0] = borrow(address a[0], block height j[0], transaction s[0])\nTransfer(qty=B[0], from=a[0], to=a[1])\n\nRecursive Step (for i in 1, 2, ...):\nC[i] = depositCollateral(address a[i], block height h[i], transaction t[i])\nB[i] = borrow(address a[1], block height j[1], transaction s[i])\nTransfer(qty=B[i], from=a[1], to=a[2])\n\nsubject to\n∀ i:\n    h[i] <= j[i]\n    t[i] <= s[i]\n\nTheoretically, in a zero gas fee and infinite recursion world, the maximum leverage that one can get by iterating this infinitely many times is 1/(1-cf) where cf is the collateral factor. One can see this by writing out the geometric series for total quantity Q[i] = C[i]+ B[i] and summing it. For instance, if the collateral factor is 0.75, then this provides at most 4x leverage.\nIn real life, there are a number of reasons this cannot be iterated infinitely (finite wealth, finite address space, gas fees). But the rate of convergence to the optimal amount is exponentially fast in i (as a geometric series), so this is a reasonable approximation. Creating queries to track all such transfers is a heavy lift, so instead of computing the COMP issued for all recursive steps, we computed wrote a query to compute the recursive borrowed issued for a single step of this iteration. This is a lower bound on the true amount (e.g. Q[0] <= Q[0] + Q[1] + ...  since all quantities are non-negative) and provides some insights into the ‘farmer’ users versus the regular user. For brevity, we will refer to this quantity as the 1-step recursive borrowed quantity.\nData\nYou can explore the data more closely in this notebook 14 and the plot below the signature line. Note that we exclude USDT since it has a collateral factor of zero. We plot both absolute values (in USD) and relative values (e.g. % of a market that is 1-step recursive). It appears that at least 80% of the outstanding DAI borrow amount corresponds to recursive borrows. Moreover, ZRX also has a high percentage of borrow that is recursive borrow, even though the total borrow is low. It’s interesting that BAT had a high percentage of the recursive borrow when the yield farming started, which matches our intuition. We also included the data 7 days before and after 20% COMP reduction proposal 4 executed, and the recursive borrow amount did temporarily drop, but it went back up again due to overall DeFi market growth.\nChanges to the existing COMP distributions\nBoth @rleshner and @Sirokko have reasonable proposals (sans the fact that @Sirokko’s proposal, without modification, will rely on a rather large code change unless with stick with 50/50 distribution). The data we collected does suggest a few further changes that might be reasonable:\n\n\nReduce ZRX rewards to zero: Given the high fraction of ZRX 1-step recursive borrowing, it might make sense to reduce ZRX rewards to zero in a future experiment (e.g. once a baseline supply rate / TVL is established)\n\nDAI rewards can be reduced further: One future test should involve seeing if a further reduction of Dai rewards reduces 1-step recursive borrowing.\n\nCOMP rewards could be higher: @rleshner’s proposed COMP rewards are a lot higher than @Sirokko’s proposed numbers. One benefit of COMP rewards for COMP lenders is that they effectively provide a staking reward — holders of COMP lock-up their COMP in the pool and receive inflation rewards. This also incentivizes long-term COMP holding, to some extent, as yield can be realized on COMP added to the pool. Growing the COMP pool via COMP rewards seems like a simple (but incomplete) mechanism for rewarding real users vs. farmers who instantly sell.\n\nBarring further feedback from the community, we’re going to propose the suggested changes on Friday, January 8, 2021.\nAcknowledgments\n@htkao created the notebook cited above\n-----BEGIN PGP SIGNATURE-----\niQIzBAEBCAAdFiEEEJAuhmVduSEVN8uPKIPJySxksQ8FAl/2W0MACgkQKIPJySxk\nsQ+M7w/+KJ7coO9P7lcAlGaWPcNbGcbrr7JBZ6eQwbDWqxltQzvmnEzmM+z7kkM3\n+zo9df0693DjE6VY0MZXQ8PCEPxE7aJIU/aS7u99F50VwMncFp1KIpmP2+xfiVVg\nzNRliuu0Tz3xK0rp5HWF9q/b5xkrUyHjrs2yWRt4BYDw2VHy9y3dWrftxzV1aa69\n5UdBTw4gNMnJeoX94lHme+AXZVW5MfpJNL2QkeHDDjxaR7mNIqpzr8ubkILwSXE6\niubSQRQ5axHlTKJ+vTkL9aNgFIVsiHBzbb0v/ybaSvwtcM5qrm5TNe/V90eDkFR3\nP/bnfecwHfbnJZ73I8top7c3oQB5biFBnIjKSz2Lrk0L06E0C8dxBJcNSJxwIWHF\ncPe3Cqw0pKr1Vnl6JH74JSNuMKqo6SR3N4+oHreAuTFS0f5WlfYFqL3QNVTmDFTZ\nHLU/4Kx7y5AkLCH/Uk0/VYsaRZ3jH4uRU+SwgfxpK6C8CVGC6rsxZKIfa0zsWqFR\nGnRDfoYvjWTdvddY0Kuw/KaYOzPQXi69lSUytdgq2iPDYd58owh/Xk8vo83cZnOo\nicJA99ZBUFMdvJs6b25YsfIStZqOg94yV3ZwnkCikKVCHl2fclqTbvaKVlEihLSG\n70+l2sk27hwrimEq1wjp1W7Vq93hW5VXWR8U2iEE1TfeNcNiQok=\n=CD61\n-----END PGP SIGNATURE-----\nimage1114×835 37.5 KB\\nComp allocation manipulation and adjustments  through governance, which is controlled by less than 30 user accounts, as opposed to a systematic adjustment based on supply/demand/utilization seems like a clear reversal of a decentralized financial institution. Comp is resorting to big business tactics and will continue to loose membership growth and decline in wide spread adoption.\\nThanks for such a detailed writing, @tarun,  it’s appreciated. Let me elaborate a bit on some core principal i believe Compound distribution should follow. Since it was originally stated, the main goal of COMP distribution was to distribute it to users. And while i do like automation in distribution, it happened to be easily manipulated. It allowed to concentate distribution in certain markets, excluding part of the users from distribution pretty much completely. As a side effect, some other users had to follow and utilise leverage in attempt to compete, even if leverage wasn’t even their plan to begin with.\nWhile manual setting through governance might be called by some as inferior method than automation, it’s not nesessary true. As long as rules are clearly defined at shore, and followed after ship sailed it’s not that much different. Thus, i strongly believe, that there should be a bare minimum of daily distribution every market should recieve, regardless of utilisation. I propose that minimum to be 10% of daily distribution spread evenly to every market. (that i believe was what initial automation lacked. If there would be a minimum which coulnd’t be drained out, while other major portion would still redistribute depending on fees, things would shape differently). Yet, it’s not late to introduce that, and i think now is a good opportunity.\nThis would be my only argument against zeroing ZRX rewards. I see your point, though, and i think it’s good one. It’s good reasoning to do so. Yet, if we agree that there should be a minimum for every market, than that minimum shouldn’t go away, even for a good reason. If there would be a choice, i’d vote No, but i understand reasoning to vote Yes. (full disclosure i don’t have any ZRX in Compound and having or not having COMP rewards for ZRX market would not impact me  in any way)\nRegarding rewards for COMP market. In my proposal number is much lower than in Robert’s because i haven’t allocated anything above minimum that every market recieve. I’m not in opposition to that though, not at all. If we believe that there should be special initiative for COMP market, it can easily go on top. In a similar way how ETH and WBTC markets got their additional 10% on top.\nAnd now it comes to more important. I suggested 75/25 distribution as i believe that proportion would fit better to Collateral markets like WBTC and ETH, but since uneven split is impossible to implement currently without bigger changes to protocol here i see 2 main options:\nLeave distribution as it is and follow with 50/50 split. In that case to achieve additional COMP initiative, suggested by Robert, we just cut slightly into stable coin distributions to get that. I also cut a bit more in USDC and DAI to provide bigger initiative to USDT market. I don’t personally think we should increase USDT rewards higher than they are today, as i still see USDT as a minor market, but if needed, we could indeed cut deeper in USDC and DAI to arrive to same number as Robert suggest. But should we? In that example i left it pretty much unchanged from current number. I’d rather incentevise WBTC and ETH more than incentivise USDT at additional expense of DAI and USDC markets.\nETH       0.0016        0.01955           50%            50%\nWBTC      0.0011        0.01955           50%            50%\nBAT       0.0000        0.00195           50%            50%\nZRX       0.0013        0.00195           50%            50%\nUNI       0.0006        0.00195           50%            50%\nUSDC      0.0675        0.06005           50%            50%\nUSDT      0.0057        0.00595           50%            50%\nDAI       0.0977        0.06005           50%            50%\nCOMP      0.0004        0.00500           50%            50%\n\nAs an alternative, we can also decrease WBTC and ETH distribution from 10% per market to 5% per market, dumping that saved 10% daily emission back into DAI, USDC markets. Since we already taking away from ETH and WBTC i see no critical difference in cutting some from USDC and DAI and providing that to USDT, to bring it to number, suggested by Robert. Overall result will look much more conservative indeed, as distribution still remain havily concentrated in the stable coins markets.\nETH       0.0016        0.01075           50%            50%\nWBTC      0.0011        0.01075           50%            50%\nBAT       0.0000        0.00195           50%            50%\nZRX       0.0013        0.00195           50%            50%\nUNI       0.0006        0.00195           50%            50%\nUSDC      0.0675        0.06700           50%            50%\nUSDT      0.0057        0.00965           50%            50%\nDAI       0.0977        0.06700           50%            50%\nCOMP      0.0004        0.00500           50%            50%\n\nPlease, keep in mind, though, that while this distribution arrived to very similar numbers as initially suggested by @rleshner, it’s arrived from the other side. Important part here is having fixed part of distribution. Number for every market here consist actually from 2 parts. It’s a Sum of Fixed part + Incentive part.  Fixed part here is 10% of total daily emission (which could be increased in future if governance decides), which is evenly distributed to every market, which is 0.00195 per market. Incentive part varies, ETH and WBTC markets recieve 5% of daily emission each as incentive. COMP market recieve additional 1.7% of emission, USDT get about 4.4% of emission, USDC and DAI got about 37% of daily emission each as incentive.\nNow it’s kind of easier to see, that might be over 70% of all daily emission going exclusevly to stable coins isn’t such a great idea of distribution. Maybe it’s worth a try to go with first option, where WBTC and ETH recieve 10% each. I believe both ways are viable in the end. Even while we might see some movements in ETH and WBTC borrow side in attempt for market to capture that extra possible profits from COMP initially introduced to the borrow side.\nThough given how small is borrow side of ETH and WBTC in comparison to Supply side, and considering, that while rewards increase noticebly from what they are today, they are still a quite small portion of daily distribution in the end, i’d probably suggest to give a “go” to first variant.\nThat, i believe, summs what i have to say on topic, governance is very welcome to take it into consideration when forming a proposal.\\nI think it’s important that these rates be set dynamically rather than every other day by governance post. It’s important to have a dedicated team attack this issue from an economic and financial perspective, like how will this affect borrowing on other markets?\n  \n      arxiv.org\n  \n  \n    \n2006.13922.pdf 15\n\n1315.47 KB\n\n  \n  \n    \n    \n  \n  \n\n\nAccording to this study, compound is the first mover in rates and people will move their money if dissatisfied.\nAcademic research needs to be cited, evaluated and it can’t be done just by analyzing recuring borrows from comp folding. The RFR shouldn’t be seen as the level where you can borrow for free, rather taken from the bitcoin futures bonding curve. I love the fact this was proposed on a Friday afternoon with 2 days to vote, but an economic study this size needs to be of a higher and more DYNAMIC calliber. This is too big of a decision to have governance do on fridays, where decisions are set it strobe until anouther governance proposal comes through. I wish this could be postponed until a more effective and decentralized economic model is passed through the community, where contributors are comp users and everyday people, not bagholders who can quickly pass a governance prop on a Friday afternoon.\\nAlso I think the vesting model should be pinpointed a little better before this passes\\nA little late for this round, but here’s another allocation idea to consider: calculating COMP rewards for a given asset as max( reward rate from lending, reward rate from borrowing). Essentially, users can get rewarded for either lending or borrowing of each asset, but not both. This makes recursive leverage on the same asset unprofitable, which is reasonable because there’s no real economic value to it. It doesn’t necessarily stop recursive leverage altogether because you could still borrow and re-deposit different assets between different accounts. But doing that involves significant price risk, which makes it much more costly/risky vs. the current recursively leveraged Dai position.\\n@rleshner @sirokko Won’t this change in COMP distribution lead to more borrowing for all collateral markets (like ETH, WBTC, UNI, etc…) due to recursive farming?\ni.e. the DAI recursive farmers will probably shift to ETH (and some to WBTC) and probably to other collateral markets too so they can maximize their returns. So this will end up inflating all the borrow rates for collateral assets (similar to how it is already inflated today for DAI, USDC).\nIt doesn’t seem like a great thing if the borrow rates become artificially high for a lending protocol. Net Borrow rates will probably still remain somewhat competitive due to COMP rewards, but that doesn’t seem great for regular users who are actually using the lending protocol for what it is meant to be used instead of just farming COMP.\\nGood thought.  I’m definitely not an expert on yield farming, but I think farmers might sometimes use different addresses for borrowing vs lending and pass along the funds between the addresses, perhaps with some obfuscation transfer step so funds can’t easily be linked.  That way they can collect on both sides.\\nThat’s true, but is addressed. In particular, recursive leverage on the same assets would not be profitable b/c you wouldn’t be able to get the COMP rewards from both the deposit and borrow (since they’d have to happen in the same account). But recursive leverage across different assets (i.e., depositing one asset as collateral, but borrowing a different asset against it) could still earn COMP on both sides if you’re moving them between different accounts. This latter recursive leverage comes with a lot more risk though (e.g., the Dai price event in November) whereas recursive leverage in the same asset has very little risk. Overall, would seem to be an improvement toward lowering incentives for recursive leverage.\\nNice job on this rule update.  excellent approach\\nHave you decided on this quantity?\nETH       0.0016        0.01955           50%            50%\nWBTC      0.0011        0.01955           50%            50%\nBAT       0.0000        0.00195           50%            50%\nZRX       0.0013        0.00195           50%            50%\nUNI       0.0006        0.00195           50%            50%\nUSDC      0.0675        0.06005           50%            50%\nUSDT      0.0057        0.00595           50%            50%\nDAI       0.0977        0.06005           50%            50%\nCOMP      0.0004        0.00500           50%            50%\\nThese changes were implemented in Proposal 035 37, and enacted. Closing this thread, so new discussions can focus on experiments & changes from this base-case."
  },
  {
    "number_of_comments": 20,
    "postid": "75b24865-94be-4a4c-b8d4-93850764fcdc",
    "posturl": "https://www.comp.xyz/t/safety-and-gas-patches/1723",
    "combinedcontent": "Compound Labs is in the process of developing patches to the Comptroller and CToken in order to improve gas efficiency and security.\nThis is in-progress work, but I wanted to share the changes publicly as soon as possible so that the community may audit them and leave feedback while in active development and testing.\nGiven the limited scope of the changes, we recommend a community bug bounty and peer review in place of a formal audit.\nThe changes are as follows:\nComptroller Implementation\n#123 30: Markets with a collateral factor of 0% and borrowing paused would be considered deprecated by the Comptroller, and allowed to be completely liquidated. This allows the closure of all outstanding borrows and the removal of reserves in deprecated markets including SAI, REP, and future migrations.\n#125 28: A gas optimization for the claimComp function, significantly improving the gas cost of claiming COMP across multiple markets at once.\ncToken Implementation\n#124 35: Modifies the seize function to transfer 2.8% of a liquidation to cToken reserves, reducing the risk of cascading liquidations that could render the protocol insolvent. With each liquidation, the protocol’s ability to recover (or utilize reserves) increases. Note: this reduces the effective liquidator incentive to ~5%, the historical baseline.\nThank you for your review and comments!\\naaaaaaaaaaaaa left some good feedback, tether is an example of a big market with 0 collateral factor, which means we’d never be able to pause borrows (in case of emergency) w/o deprecating the whole market, which makes a lot of sense. so to deprecate markets, they suggested requiring 100% reserve factor.\nimplemented here Enable liquidations for all accounts in deprecated markets by maxwolff · Pull Request #123 · compound-finance/compound-protocol · GitHub 21\\nHey all,\nWe’ve been gaining confidence on these changes and are starting a bug bounty in advance of doing a proposal.\nFor the next 48 hours, reported vulnerabilities in branch #127 12 will be eligible for a bug bounty of up to $150k according to these terms 4.\nIf you have questions feel free to reach out on the #development channel in discord 2.\\nHey there,\nI’m not sure why this has only 2 replies, maybe it’s already done? If not, my questions…\n\n\n\n maxcwolff:\n\n#123 : Markets with a collateral factor of 0% and borrowing paused would be considered deprecated by the Comptroller, and allowed to be completely liquidated. This allows the closure of all outstanding borrows and the removal of reserves in deprecated markets including SAI, REP, and future migrations.\n\n\nWhere would these funds go? Could they be used to buy COMP then distribute that COMP to active suppliers and borrowers? Or maybe transfer those assets to be used for grants? Or what is the plan?\\nWe now have a bug bounty going through Immunefi for this patch: Compound Bug Bounties | Immunefi 10 !\nSee the bounty page at Immunefi for more details on accepted vulnerabilities, payout amounts, and rules of participation.\\nIsn’t it safer for the DAO to simply maintain a list of isDeprecated ctokens?\nEither hard coded, or at a dedicated storage.\nThe deprecation involves a DAO vote anyway. So might as well give it the final say on what is depreciated.\\nI think the point is that the DAO can explicitly mark things as deprecated using this combination of parameters which should only be used for this purpose\\n\n\n\n maxcwolff:\n\nModifies the seize function to transfer 2.8% of a liquidation to cToken reserves, reducing the risk of cascading liquidations that could render the protocol insolvent. With each liquidation, the protocol’s ability to recover (or utilize reserves) increases. Note: this reduces the effective liquidator incentive to ~5%, the historical baseline.\n\n\nI think this is probably a good idea. But it may be worth reviewing this from a market risk perspective. When the liquidator incentive was last at 5%, total borrowing on Compound (and in defi in general) was much lower, and gas costs have also increased substantially. Maker’s new liquidation system also clears debt much more quickly than the previous system (target of 1-2 hours vs 6 hours for old system).\nAll of this together increases the risk that liquidation will not be profitable with only 5% incentive, potentially allowing prices to fall further and push the market into insolvency. My inclination would be to increase the total liquidation penalty so that the liquidator incentive remains 8%, and the protocol liquidation fee is charged on top of that.\nThat being said, I think this change will be a net positive for Compound. More protocol revenue from riskier users (as identified by their position being liquidated) allows for charging less from safer users.\\nOn first read I was thinking @maxcwolff expressed a concern about that. But now I see he actually said why it is ok.\\n\n\n\n monet-supply:\n\nAll of this together increases the risk that liquidation will not be profitable with only 5% incentive, potentially allowing prices to fall further and push the market into insolvency\n\n\nSome empirical data suggest that the difference between 5% and 8%, currently goes to MEV anw.\nBut maybe th protocol could give up to 8% if the collateral is not sufficient for proper liqudiaton, and 5% otherwise.\nThis will guarantee 8% for the “hard liquidations”, while preventing some of the MEV for the regular ones.\\nthe funds would go to the collateral ctoken’s reserves directly, not into COMP. so they would serve as a safety buffer for each ctoken, subsidize rates, and could hypothetically be withdrawn by COMP holders for another purpose if they decide to\\ni agree, the ideal model might be something like an auction, where the market decides the discount. increasing the discount for increasingly insolvent positions is also an interesting idea.\\n(English) auction worked really bad for Maker (liquidations v1). Also it will require a new intermediary state when collateral and debt are owned by some auction contract.\nBut a simpler solution could be something in the flavor of:\nif(collateral / debt < 1.1) incentive = 1.08; else incentive = 1.05;\\nJust deployed to mainnet and making a proposal soon.\nNew Comptroller.sol : 0x75442Ac771a7243433e033F3F8EaB2631e22938f 8\nNew cERC20.sol 0x339B2D3bf0406DF82f8fa7B0d855a3f47562d8D7 4\nAs always you can verify the contract matches the PR by checking out the branch compound/2.9, compiling, and running npx saddle match 0x339B2D3bf0406DF82f8fa7B0d855a3f47562d8D7 CErc20Delegate -n mainnet\\nRedeployed a new cERC20Delegate. The previous one did not include CCompLike vote delegation functionality. Instead of deploying a new delegate that also has vote delegation functionality, I pushed the vote delegation into Cerc20 itself and got rid of CCompLikeDelegate.sol. This way every new CToken delegate will have the ability to delegate votes, but for non-COMP-like tokens it will do nothing. The upshot is that we only need one delegate and we can avoid this mistake in the future.\nYou can see the diff here 7.\nThe new address is 0x3587b2F7E0E2D6166d6C14230e7Fe160252B0ba4 3\\n\n\n\n maxcwolff:\n\nWith each liquidation, the protocol’s ability to recover (or utilize reserves) increases\n\n\num… Do you mean protocol use cToken reserve automatically in specific liquidation status? or community multisig can decide it(use cToken reserve in liquidation process) under emergency situation?\n===\nCurrently, I don’t have enough info for reducing liquidation incentive to support 48th proposal. as @monet-supply pointed out, I’m also concerned its impact from reducing it about 5%.\\nIn pull #124 3 CToken.sol seizeInternal() it emits a Transfer event for the seized CTokens, but the amount emitted in the event is the total amount seized, not the amount transferred to the liquidator. This will confuse off-chain analytics that expect the liquidator’s CToken balance to have increased by the amount in the Transfer event.\nInstead of\nemit Transfer(borrower, liquidator, seizeTokens);\nemit ReservesAdded(address(this), vars.protocolSeizeAmount, vars.totalReservesNew);\n\nit could be something like:\nemit Transfer(borrower, liquidator, vars.liquidatorSeizeTokens);\nemit Transfer(borrower, address(this), vars.protocolSeizeTokens);\nemit ReservesAdded(address(this), vars.protocolSeizeAmount, vars.totalReservesNew);\n\nThe LiquidateBorrow event might benefit from an upgrade also, as it currently shows only the total amount seized from the borrower, which is no longer the amount going to the liquidator:\nevent LiquidateBorrow(address liquidator, address borrower, uint repayAmount, address cTokenCollateral, uint seizeTokens);\n\nFinally, protocolSeizeShareMantissa is an internal constant, which makes it unavailable to on-chain logic. It would be nice if this were a public variable.\\n@dakeshi the cToken reserves increase automatically; the community / COMP token-holders will continue to manage reserves as they currently do.\nThe liquidation incentive was historically 5%; and the markets being updated (DAI/USDT/WBTC/UNI/COMP/LINK/TUSD) are less commonly liquidated collateral than Ether, the primary collateral asset. This should provide a safer approach to experiment & improve than updating all markets.\n\n@pyggie thank you for flagging this suggestion; we’ve reviewed it and agree that without changes, off-chain analytics could be impaired.\nGiven the simplicity of the change / improvement, and because we are still in the review period, we plan to:\n\nCancel the current proposal\nImplement the split in the transfer event\nResubmit the proposal, and recommend a bounty be paid to pyggie\n\nThis is a great example of the review period being put to good work. Thank you!\\nThanks for more detailed explanation @rleshner  especially this part.\n\nhe markets being updated (DAI/USDT/WBTC/UNI/COMP/LINK/TUSD) are less commonly liquidated collateral than Ether, the primary collateral asset.\n\nand I also agree to make separate proposals to help its decision and focus on specific topic.\nBut my question was not how to accumulate cToken reserve. I agree that increased reserve can reduce protocol risk but it is still unclear “What condition make trigger to use cToken reserve in liquidation process”. (even though it is possible to set some standard through Compound Governace) If we want to say it will be helpful to reduce risk of cascading liquidation with increased cToken reserve, I think that it should have some rule(code level or community multisig level) which cToken reserve can get involved in liquidation process.\\nHere’s the diff 3 splitting the Transfer events and making the protocol share public.\nHere’s the new deployment CErc20Delegate | 0xa035b9e130F2B1AedC733eEFb1C67Ba4c503491F 3\n@pyggie decided not to modify LiquidateBorrow event. It is emitted inside the borrowed token, not the collateral token which gets seized, the borrowed token would have to know if the collateral token is going to have a protocol share, and that gets complicated.\\nGauntlet will be voting yes on CP048. This change does two things:\n\nUpdates the Ctoken contracts to a new standard\nAdds in a “protocol cut” for liquidation fees similar to the reserveFactor\n\nMoving the Ctoken contracts to a new standard is great step forward for the protocol, ensuring that all markets benefit from recent CToken changes as well as making future development easier.\nRegarding the protocol cut, defined by the protocolSeizeShareMantissa, there are some concerns. This parameter is hard coded and regresses some of the functionality provided by the liquidationIncentive.\nThe liquidation incentive is an important lever to ensure that even when liquidity dries up and when gas costs are high, liquidators are paid enough to offset slippage, gas, and other costs.  At the same time, this is something that effects borrower UX.  The safer the protocol gets, the worse it gets to borrow on Compound - any active borrower will occasionally get liquidated and if this fee is unnecessarily high, this could be a disadvantage when trying to grow the protocol and attract more borrowing. Ideally, you are able to change this over time to balance these tradeoffs. If you take 2.8% off the top of the liquidation incentive, it becomes difficult to ever lower it.\nReducing this incentive does increase the risk for the protocol in the case of Black Thursday type events. However, Compound has held up well in recent smaller downturns and the protocol does have the ability to raise this again if it needs to.\nWhile we raise these concerns, we do not believe they are blocking issues for a change that also delivers a lot of positive value to Compound."
  },
  {
    "number_of_comments": 20,
    "postid": "8294ad99-1177-45d3-adbf-5bc6daa750d1",
    "posturl": "https://www.comp.xyz/t/vesting-for-the-compound-protocol/252",
    "combinedcontent": "Note: This post was written in collaboration with Peteris Erins @peteris and Hsien-Tang Kao @htkao\nBackground\nVesting COMP has been brought up recently in two forum discussions:\n\nCompound Proposal 022: Systematically Reduce Emissions Quantity 33\nFaster way to real community governance 20\n\nGauntlet has been working on designing a vesting mechanism whose parameters can be controlled by the community via governance. We started by looking at the designs used in other protocols: A number of other protocols have accrued rewards vesting, with Synthetix being the most well-known vesting mechanism. The design of cTokens provides both flexibility with regard to how vesting quantities should be calculated and presents trade-offs with regards to gas and code complexity. Given that the community will likely want to incentivize COMP for participation, liquidity, and to compensate for improvements to the protocol’s security and/or feature set, we believe in adding a vesting mechanic that can handle all of these use cases.\nThe liquidity provisioning use-case, pioneered by Synthetix and copied by virtually all of the new yield farming protocols, is most sensitive to the form of vesting used. In particular, if COMP holders want to supply their cTokens to a Uniswap, the accrued vested COMP impacts the value of a Uniswap liquidity provider (LP) share. Let’s walk through this before going through the different models of vesting.\nLiquidity Incentives don’t commute with vesting\nShould the COMP community decide to prioritize on-chain, non-custodial forms of liquidity, it will be necessary to use COMP to incentivize on-chain trading in the future. For provenance, recent (08/25/2020, 8pm UTC) Nansen 3 data shows the following distribution of COMP at exchanges, which indicates that very little is kept on-chain:\nimage1786×912 116 KB\nCompare this to the exchange distribution for Synthetix (08/25/2020, 8pm UTC):\nimage1774×894 119 KB\nLet’s explicitly see why this complicates the math for traders and liquidity providers. In this example, we are assume that a COMP or cToken liquidity provider stakes their LP shares into a Synthetix-like rewards contract 11 to earn the liquidity incentive. Suppose that one places 1 cUSDT and 1 cUSDC in the Uniswap cUSDT/cUSDC pool 3. Furthermore, suppose that in one day, the cUSDT earns 0.1% of interest and 1 COMP while the cUSDC earns 0.2% of interest and 2 COMP. Now consider two different vesting styles:\n\nCOMP is vested continuously: Uniswap LP share should be valued at the price of the underlying coins (USDT, USDC) plus the accrued trading fees and interest plus the value of COMP less the 95% percentile gas costs for a withdrawal transaction\nCOMP is vested discretely: Uniswap LP share should be values at the price of the underlying plus the accrued trading fees and interest plus the discounted value of the COMP less gas fees\n\nWhy do we have to discount (e.g. a la discounted cash flow 2) the COMP portion of the share? Since COMP is more volatile than the assets producing it (cUSDT, cUSDC), the holder of the LP share needs to discount the value of the share based on how long it takes to claim COMP. For instance, if COMP followed the weekly emission model of Synthetix and the time until the next reward claimed is T, then we would discount the COMP component of the LP share by a factor such as exp(-sqrt(volatility(COMP, USDT) * volatility(COMP, USDC))* T) [0]. In a sense, discrete vesting reduces the expected reward for LPs, even though it costs the network the same amount in terms of emissions.\nWhy do COMP voters have to care about this? Suppose that the community first chooses to do discrete vesting (e.g. a weekly distribution akin to Synthetix). If the community later decides to incentivize on-chain liquidity (as opposed to centralized liquidity, which dominates COMP holdings as of now), then the choice of how to distribute COMP to liquidity pools depends on the vesting structure. A vesting structure that has a number of parameters — length, vesting period, discounts – should be able to accomodate liquidity incentives. If on the other hand, liquidity incentives were given before a vesting choice was made, the protocol would have a harder time choosing what schedule is used as it would need to account for the existing implied discounting factor.\nTherefore, we believe that:\n\nA vesting schedule should be flexible enough to provide future liquidity rewards\nNo liquidity rewards should be administered until vesting has been set.\n\nA modest proposal\nGiven the desire for the community to have a mechanism for vesting earned COMP, we wanted to put forth two proposals for vesting. These two proposals take two extremes:\n\nHave a fixed period for vesting which saves on gas, but forces LPs to have to compute discount factors\nHave continuous vesting, which is more expensive, but makes math for liquidity provisioning much easier\n\nWe’ve written two pseudo-code implementations (Vyper-esque pseudo-code) of these two proposals that can be found here 23. Hopefully the intentions are clear and one can clearly compare the trade-offs. We believe that the first implementation is similar to the description of a vesting period by @jared in a prior forum post\nCall for feedback\nWe would love to hear feedback on these proposals and whether there are other features that are missing and/or if there is another application that causes these options to be malformed.\nFootnotes\n[0] This factor is chosen if we assume that COMP/USDT, COMP/USDC are geometric brownian motions with well-defined drift, variance. Traders will come up with their own discount factors, but a fixed vesting guarantees that the discount factor will be less than 1, provided they are rational. We note that technically you have to discount both continuous and discrete vesting, but because the discrete vesting has to account for volatility over a long time period it always will over-discount relative to continuous.\\nI’m having a hard time following this post and I think a more simplistic explanation would be helpful.\nPrimarily I’m confused if we are only talking about delaying the ability to claim rewards one is owed (this is what I consider to be vesting) or if we are also talking about providing bonus incentives to a certain subset of users who choose to lock their comp? Or if we are talking about both?\\n\n\n\n tarun:\n\nWhy do we have to discount (e.g. a la discounted cash flow) the COMP portion of the share? Since COMP is more volatile than the assets producing it (cUSDT, cUSDC), the holder of the LP share needs to discount the value of the share based on how long it takes to claim COMP\n\n\nnot sure I understand this point, could you expand further? COMP’s volatility vs farming assets doesn’t seem relevant to vesting\\nI believe (and @tarun correct me if I’m mistaken), that the simple “fixed period” vesting would be a regularly scheduled window to claim COMP, e.g. every 60 days. So, February end, April end, June end, etc.\nUsers would then be forced to judge the future value of COMP (and their desire to hold it in the future) vs using the protocol today. Purely capitalist yield farmers would find this to be more unpalatable than “natural” users. Taken together, the impact of fixed period vesting might be lower usage by farmers, and more rewards for long-term holders.\nDid I get that right?\\nI would like a more complicated solution, a vesting contract that allows the owner to delegate/vote before he could claim the COMP in this case a longer vesting period could work.\\nI think the current model works. If the goal of this is to limit the number of people selling COMP regularly, I think it is over complicated and unnecessary. Those who are interested in a vesting solution should write out why and then think what is the best change that can be done to accomplish that is.\nThe COMP launch has been a huge success. Any changes at this point should be carefully considered.\\nApologies if I mired the initial explanation with jargon. Here’s what I mean in hopefully simpler terms:\nAssumption: A user prefers to receive COMP now rather than later\nConcretely: For a user, receiving 1 COMP now is equivalent to receiving 1.1 COMP in a month if you can reinvest the 1 COMP you receive immediately and earn 10% in a month (e.g. via being a Uniswap LP, lending on a venue that lends COMP, etc.)\nAnalysis\n\nUsing this assumption, if you place a cToken in a Uniswap pool and the cToken is earning COMP every block vs. every month, there’s a value difference in the Uniswap pool share (e.g. LP shares with cTokens that vest COMP on every block are valued more than LP shares with cTokens that vest the same amount of COMP every month)\nOwning LP shares or placing cTokens in pools means that the value of the liquidity pool share that has a cToken with a COMP / block will be worth more than the one with COMP / month -> you’ll end up w/ different amounts of on-chain liquidity based on the issuance and/or lock-up\n\nThus, even if COMP never has an incentivized pool (like Synthetix), the choice of how vesting works effects the value of COMP when used elsewhere on chain. If a large portion of the community wants to utilize their COMP in other on-chain financial products, the vesting effect will be quite important to analyze carefully.\nSince the precise implementation of how vesting is added to the protocol will effect the time scale, frequency, and amount of user interaction needed for vesting, it is important to keep this effect in mind, especially if the community deems on-chain usage of COMP to be a high priority.\nAs an explicit example of what might happen if the user has to manually claim vested COMP rewards, consider the Balancer gulp() bug that led to losses due to a deflationary token. While COMP will not have losses of this form, there is an issue if claimComp() is called on cTokens in an LP share a long time post vest. Consider the following numerical example:\n\ncUSDT/USDT price is $1.01 so LP share is worth $1.01 [0]\nEarns an average of $0.01 COMP / block\n\nIf claimComp() is called every block/continuously vested, then the LP share’s value k blocks after vesting begins is $1.01 + k * $0.01. On the other hand, if the vest happens every 100 blocks, then the price of the LP share at block k is $1.01 + floor(k/100) * $0.01. And if the user forgets to call claimComp(), then there will be a bigger shock to the price of the pool share when the COMP is claimed.\nWe’re only pointing this out because the continuous (e.g. every block) vs. discrete (e.g. user has to claim ever week/month) vesting schedules will require different levels of COMP holder attentiveness, especially when COMP is in other protocols.\n[0] Slightly misleading in that you have to include Uniswap/gas fees on withdrawal, so the price might differ slightly. However, in a low gas price and 0% fee world, this is true.\\nThe main point of vesting is two fold:\n\nTo incentivize development and security of the protocol, as @arr00 has championed in multiple posts\nTo incentivize long-term liquidity provisioning\n\nWe strongly believe that spending the entire budget of the protocol on liquidity provisioning, as is the current status, is suboptimal. There are a number of components and facets that a protocol’s budget should be spent on, such as (but not limited to):\n\nSecurity\nIncentivized voting participation\nFeature development\nInsurance Fund / Reserve Management\n\nThere is much that DeFi protocols can learn from layer-1 communities that have faced similar issues, albeit without the consistent cash flows of DeFi.\\nI know that speaking about the COMP price is a bit taboo here, but this can’t really be addressed without bringing it up.\nI think the main issue here is an irrational market. The current value of COMP is highly inflated due to speculation and limited supply. The large farmers are more likely to be rational actors and that is why they most likely dump their COMP asap.\nVesting could have varying outcomes:\n\nFarmers leave, dropping the TVL massively, causing the price of COMP to plummet, shedding any remaining farmers.\nFarmers stay, COMP becomes more scarce, driving up the price of COMP, attracting more farming.\nFarmers leave, but COMP is more scarce, leaving the price about the same.\n\nIt’s anyone’s guess.\nBut the more fundamental issue is that governance and price speculation aren’t really compatible.\nIf you truly want COMP to be a governance token, charge a 10% fee every time it’s transferred. That way it can’t really be traded and will serve it’s real purpose of assigning voting rights to users of the protocol.\nIf you choose for price speculation, it’s fine to leave things as they are. Let the farmers fill the reserves and keep the TVL high. List some new assets to attracts more capital. Start talking about a way to reward holding COMP with dividends.\nThis is the real crossroads we’re at. And this seems to be the discussion nobody really wants to have. No need to make the contracts more gassy by adding more bells and whistles.\nAnyway, my 2 cents.\\nrelevant meme\nimage1024×683 207 KB\\n@tarun -  Vesting period is a (looking back) annoyance barrier and would only serve to delay selling, rewarding for holding $COMP is a good (looking forward) idea on the other hand.\nAs an example paying out $COMP dividends driven by the market conditions quarterly. Would be very interested to see in practice.\\nNot sure that keeping tokens in a Uniswap pool works as a proof that the tokens are being kept “on-chain” in the long term. If there are incentives to doing so, wouldnt there eventually be a service offered by a CEX to custody your tokens while they are inside a uniswap pool earning the incentive? Likewise, if earning COMP through a CEX service, they could offer to buy off your unvested tokens, messing up that incentive too.\\n-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA256\nAs a follow-up to this, we [@peteris, @jmo, Rei] just added a PR to the Compound repository for a vesting implementation. It proposes a change to the protocol to add periodic COMP vesting, as per the discussion in this forum post. This will enable the protocol to do the following:\n\nAdd vesting for COMP rewards that encourages long-term holding (e.g. akin to Synthetix’s vested reward)\nEnables governance to issue COMP grants out of the accumulated COMP (c.f. @rleshner’s post:  A Call to Action 3)\n\nThe current implementation, which will be proposed to be added, also adds a few other features:\n\nReduced Comptroller size (the size was within 100 bytes of the EIP-170 limit after borrow caps were added) using some of the suggestions from @jared’s post A Leaner, Meaner Comptroller 1\n\nIndependent vesting module: Other projects, not only those using Compound’s governance contract, can adapt the vesting module to their contracts. Contracts that do use Compound’s governance contract (e.g. Uniswap) can take advantage of vesting sooner rather than later\n\nWe are excited to get community feedback on this! \n-----BEGIN PGP SIGNATURE-----\niQIzBAEBCAAdFiEEEJAuhmVduSEVN8uPKIPJySxksQ8FAl+HGPgACgkQKIPJySxk\nsQ+U5w//bpdGale1/7nGNvCuSNzzoUFuJ9/IaadrZCxRHf6j+rkZpQs/1Nxbge+K\niW+qrU5c6JKNiA4dKuX9yDKlL/Bo93Wo3vcSeFbn/2xSohJcSV9bqq1FyDRx9xi6\noq+vU5nSFX1VMTPgw/8Odxcvw325ZlZOnF0rB1RLWj5LqvzmwsOQNTy+WHNRlJ73\nsDubBHHuqMnPJzS4JIIhoUmUS5q2Z31dQ40zjMufR58PEMSAwHeOA7iXuKJVTMjD\nnwGfhbB/YanXH5d2PkOkxvkMNIMxnpVOfh/2x1eSK5MKFsrF7pftmStu0B5tzPBL\nVGyPJho/2qqZKDcxnyZ9yWY3z3TB5kRxqNgUdNbE5UXZjO2Qf6fLBmTUPQ7CBJfY\ncYoC07WvAz7yI/SKhaxB3y5+QchU5ooYiDswfQE1047dKxTOLX3Dt73Psjh6pYLm\nOKitbZO0XQdi7Sw0DHjIzvxwyBHf4W+E1n+vIcqRrE8GqaAAQw20pWNT03DneT5S\n2SZWObSZlVJHYwA7I8pNrx+X16t+XtiiBjubfYUfVEHAQm245ECAsSo7YDljcFZC\nH2pIQiRfwdK1Iv4GqsgSAUvpJuDuuj/7xwQQOFSQScnYOhOa1TgfCnsj57wkdZZr\nr+hRBIBty9s0MWozl8oRXMqapvK+S47+CBkmLXG2hVBsZ11HAFk=\n=TvOa\n-----END PGP SIGNATURE-----\\n\n\n\n tarun:\n\nEnables governance to issue COMP grants out of the accumulated COMP\n\n\nSomething I believe is very important, but I don’t see any new function actually allowing for this.\\n-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA256\nIndeed — that’s the next piece of code we’re working on! Just needed to get a little space in the comptroller…\n-----BEGIN PGP SIGNATURE-----\niQIzBAEBCAAdFiEEEJAuhmVduSEVN8uPKIPJySxksQ8FAl+HIIgACgkQKIPJySxk\nsQ9Ktw//cUpzLu3rTngxzsjHX+/RLT9/WgQ4SsZYxJeYjs2MSqxcVcgCXYOlLmpa\nDDg75U/kxiv605Kht4nc6rOpl8hoC8eYWT+o++dSN/OwAUSV9UDMZdAg9RwGO7mC\nXeo1VQK9obV+vEO7AfSUzg92JhRfRMlidun9ik6pM0WwJPmhOKtiyXE77rncT6t3\njvPNjLPpyt2B8munTjEVceH5ZSw9g0s+Hg1cLDWChnDYAJtrv3GU0ckGmbscIpuS\nl7bBSp7+HsZN/+iM+WNewkusyMX/Wp9HORVxzsLUakjm3NJM/ICmI/8PFNUzZ/iJ\nOsy3AKadXNZnX0GBGHAVcCBqr0E5/NzmbzNKMD8kkYIk+2X5wK/UmvRnHXnpowsR\np2Y39+rCNbvdQe+UKzWNNgqIDLZQ//B/ko6ctDKfrs9egdq0P8IJLXjmSs9CBrKW\n461KANTHLgKEaBTPXYsCW35FwRVaGWij2AZt6r/amvZDeesiNhbVgF81h7B1cmYE\nEAUAc+NmSMQHdGq54hqWacsoubT7rcrUu+QVWdDmDLL846oGf/NVougOKY/pAZxq\nEJfG3LF9+jG/Va9r4IdNAPvocaUiT6j1Uj1lr/lYSBByPrGsa2/G7u01LAkYyPfq\nLd/SzVDvpgdqHd8VNlIKKxvXO+K4ctmVRjN0/XxPIVY3cKvQrEY=\n=A/Ab\n-----END PGP SIGNATURE-----\\n\n\n\n tarun:\n\nAdd vesting for COMP rewards that encourages long-term holding (e.g. akin to Synthetix’s vested reward)\n\n\nThis may encourage long-term holding, but it will likely drive users towards trading COMP derivatives to manage their risk. I am not convinced this fixes a problem or if there is a problem here.\\nSo if you mint COMP two days before general claim schedule, you basically did not vest your tokens.\nThis system looks inelegant to me because it does not apply the same vesting period for everyone.\\nI propose a bonding curve to distribute the COMP farmed on a 23 day basis.\n\nSmooth and time constraining.\\nLogically, the vesting structure should be refined first, my opinion is that the users of the project then have more control, and with that come security advantages (it is easier to simulate possible attacks and manipulation). Continuous vesting is a better solution if the main argument against that model is gass costs. However some parameters for vesting factors need to be defined, does not need to go to extremes.\nDon’t mind if I missed something and criticism of my opinion is of great importance to me as I am new to the forum.\\nI like the idea of fixed vesting periods. I would prefer quarterly as it would simplify tax accounting. It could potentially work like this. COMP earned from Dec-Feb would vest in Mar, Mar-May would vest in Jun, Jun-Aug in Sep, Sep-Nov in Dec.\\nWith ideas like this in a few years we can expect the protocol to become a commercial bank. Priority is given to security, decentralization in decision-making, incentives for project development and flexibility. If tax accounting is the main problem of the DeFi protocol it is only a matter of time before regulatory institutions take control."
  },
  {
    "number_of_comments": 9,
    "postid": "946a9996-707a-4355-b6f3-ff736344ccb7",
    "posturl": "https://www.comp.xyz/t/lower-proposal-thresholds-v2/2840",
    "combinedcontent": "\nBackground\nCurrently, the proposal threshold is 65K, which enables 12 addresses (excluding those on the whitelist) to make direct proposals to governance. Governor Bravo enables Compound’s community to change the proposal threshold (which was previously executed upon via Proposal 52 to reduce the threshold to 65K). Though there has been discussion on reducing the proposal threshold 3 following Proposal 52 that fell through, we believe that over the past several months we’ve seen several key issues that warrant the community revisiting lowering the threshold.\n\nCurrent Compound delegates are unable to create CAPs (unless they own 100 COMP), and as such current delegates are unable to participate in governance. Additionally, any users below 100 COMP in holdings are unable to even create a proposal (CrowdProposalFactory contract 5).\nCurrent Compound delegates are unable to lend support to CAPs (as they can’t delegate delegated funds), effectively limiting the ability of delegates to participate in governance if they’re below the 65K limit.\nCompound users are not incentivized to participate in governance & token-holders feel a lack of ownership in Compound. At this time, we believe reducing the proposal threshold is an important step of many towards increasing governance participation & contributions by community members in the long-term by reducing the friction to create proposals.\n\n\n\nIdea\nWe believe that the community should consider lowering the proposal threshold in order to increase the number of potential proposers & speed up the governance process. We’ve created a poll below that corresponds to potential voting thresholds and want to get community input on which proposal threshold Compound’s proposal limit should move to.\nPotential Thresholds\n65K (Current) => 12 Addresses\n50K => 24 Addresses\n10K => 41 Addresses\n1K => 59 Addresses\n100 => 75 Addresses\n\nWhat should the new minimum proposal threshold be?\n 65K (Current) 50K 10K 1K 10024votersResults\n\nOn January 14, if the community decides that Compound’s proposal limit should reduce from the current 65K (~$13M) limit, we (Blockchain at Berkeley) will take the result from this poll and put up a proposal that changes the minimum proposal threshold to the community-decided result.\nWe believe reducing the CAP threshold limit from 100 COMP (~$20K) to 1 COMP (~$200) would enable smaller token holders & delegates to be able to propose changes via autonomous proposals, but we believe this warrants more discussion (and consideration of potential negative implications) before being put to a vote.\\nGenerally, we think lowering the proposal threshold to 50k is a good next step.\nAs for the CAP limit, 1 might be low, but we wouldn’t oppose it either since it is rather arbitrary and has little downside. The CAP contract is external to the protocol: CrowdProposalFactory | 0x54a06047087927D9B0fb21c1cf0ebd792764dDB8 4\\nGenerally supportive of this initiative but would prefer reducing the CAP threshold by ~1 order of magnitude at a time (~10 COMP). I haven’t looked closely at wallet data recently but suspect that, unlike the case at the very top, the number of eligible wallets jumps exponentially from 10^2 to 10^1 and then to 10^0.\\n@allthecolors Because the CAP contract is external to the protocol anyone would be able to deploy a “new” CAP contract with the nonzero compStakeAmount that they wish (i.e. 10 COMP, 1 COMP, 0.1 COMP, etc.) As such, we’ll likely deploy a new CAP contract based off of consensus here on behalf of the community, but anyone could go ahead and deploy a new CAP contract if they wish to do so with a lower limit (w/ the disincentive being the contract deployment cost).\\nBased off of the poll, we’ll go ahead and move to put up a proposal in the next week that drops the proposal threshold to 10K & separately will be redeploying the CAP contract with @allthecolors  suggested limit of 10 COMP.\\nMust say, thats a really good proposal by Blockchain @Berkeley\nI’m pretty excited to see the governance process evolving with  more opportunities Compound users to participate.\\nDeployed the new GovernorBravoDelegate contract & passing sims as intended. Pending whitelist, will put up a proposal to reduce proposal threshold to 10K!\nContract\nNew GovernorBravoDelegate: GovernorBravoDelegate | 0x30065B703DE5d473975A2db5bBB790A23FD6EFbD 1\n\nReduced the minimum “setable” proposal threshold to 1K COMP\n\nSimulation\nWorked off of @arr00’s tests for whitelisting users, and proposal passes as expected (with newly deployed GovernorBravoDelegate contract).\nYou can see the tests here 3, and the paste of successful output here 2!\nPR\nI’ve opened a PR to reflect the updated GovernorBravoDelegate contract, the new simulations & updates to the ABIs.\nhttps://github.com/compound-finance/compound-protocol/pull/189 3\\nAfter discussing with community members, we’ve decided to put up a proposal to reduce the threshold to 25K to be closer to the weighted average of the votes by the community. Proposal is live here 6!\\nThis is a long-standing conversation in the community; the bar to creating proposals has been lowered multiple times (the introduction of Autonomous Proposals, lowering the threshold from 100k to 65k, and the introduction of a community-set whitelist), each time with the goal of making proposal creation more accessible while balancing protocol security.\nTo date, proposals with community support easily make it to the proposal stage; at this point, a proposal sponsor simply has to ask to be whitelisted for proposal creation. Case in point: this very same Proposal 89 5.\nLowering the COMP-based proposal threshold will make it easier for proposals like Proposal 84 (Coindesk article 13) to be introduced.\nWhile I strongly agree with the goal of widening governance participation, I don’t believe this is the right approach. Introducing changes to a protocol that manages $10B+ is a high-stakes task, and following the Proposal 62 5 bug, one that I think should be made more difficult, not less. I will be voting against lowering the proposal threshold, and encourage the community to find new ways to expand governance participation.\\nWith respect to @rleshner 's point, I am wondering what other measures could be taken to mitigate the protocol security concern, other than limiting those who can bring a proposal based on # of tokens/votes they control?  Basically, how can we ensure that those risks that can/should be a concern that have been are addressed by the appropriate mechanism?"
  },
  {
    "number_of_comments": 114,
    "postid": "a15448c4-5989-4215-ae31-419c4556c25a",
    "posturl": "https://www.comp.xyz/t/compensation-proposal-distribute-comp-to-affected-users-in-the-dai-liquidations/801",
    "combinedcontent": "\nNote: I’ve deployed the CAP for this proposal  116. If you support this, you can delegate to this address: 0x2f04664b18fb9b6d49124fcc876b52a4ba797718\n\nObjectives:\nFollowing the first proposal 22 to increase the DAI reserve factor with the goal of de-risking the DAI market and mitigating against future improper liquidations, this next step outlines the mechanics to compensate users for funds lost in the liquidation events of 11/26 by distributing 55,255 COMP (0.55% of total COMP supply) to affected users.\nBackground:\nCOMP distribution to users began on June 15, 2020, and has since distributed ~450K COMP, with approximately 4MM COMP remaining.\nBehind the scenes, the distribution works through two main contracts, the Reservoir 10, and the ‘Comptroller’ 5. The Reservoir is an immutable contract that exists outside the control of governance and drips 0.50 COMP per block into the Comptroller contract. The Reservoir continuously adds COMP at the drip rate and is independent of the rate at which the Comptroller distributes such COMP. The Comptroller contract controls the usage and distribution rate of COMP for a number of functions including but not limited to, i) distribution to borrowers and lenders for each market, ii) building a reserve 3 that can be used for community needs, and iii) voting, among other uses.\nCurrently, the Comptroller distributes 0.352 COMP/block (70%) on liquidity incentives and accrues the remaining 0.148 COMP/block (30%) as treasury reserves. At the current COMP price of ~$155, Compound’s treasury is growing at a rate of ~$149,000 per day, or $4.5MM per month.\nGauntlet’s prop30 3 added the ability 1 for the Comptroller contract to send COMP to a particular user, group, address, or contract selected by a governance vote. These changes as described 1 by @jmo, enable a plethora of use cases, including the use of treasury assets to compensate those liquidated in the Thanksgiving Event, as suggested 2 by @mrhen .\nMechanics:\nThis proposal uses the _grantComp function introduced by Gauntlet to distribute a set quantity of COMP to a Merkle distributor contract 6 that will automatically distribute the corresponding COMP to each of the wallets 20 affected—user’s don’t need to interact with any contract to claim this COMP; After the proposal’s execution, the relayer will claim COMP for each wallet according to the merkle distributor and distribute it.\nTo calculate the amount of COMP needed from the Comptroller for the compensation, this proposal uses the 14-day average closing price of COMP from CoinMarketCap as of the time of writing (12/8/20) to compensate users 8% of the liquidated amount.\nAs an illustrative example, if a wallet had 100 DAI repaid by a liquidator, then that wallet would receive 8 DAI worth of COMP, which equates to 0.064 COMP of compensation (assuming a 14-day average COMP price of $123.39).\nDistributor Methodology\nReferencing the DAI liquidations spreadsheet 21 published by @rleshner, a total of 85,220,406.43 DAI was repaid on 11/26/20. Applying the 8% liquidation penalty, this proposal would pay out a total of 6,817,632.51 DAI (in COMP). Taking the 6,817,632.51 DAI and dividing by $123.39 (the 14-day average price of COMP) equals 55,254.95 COMP distributed to users. Numbers here:\nScreen Shot 2020-12-09 at 08.30.591598×812 68.1 KB\n\nTo preview the COMP distribution per address, see here 20.\n\nOverall, this distribution of COMP represents only 0.55% of the fully diluted COMP 10M supply and less than 2% of the 24hr COMP volume traded globally, suggesting minimal market impact. Additionally, assuming the Comptroller earns 962 COMP per day, the 55k COMP distribution will be replenished in less than 60 days.\nI propose proceeding with this approach for the following reasons:\n\n\nCompensate users who lost funds: This proposal achieves the objective of compensating users who lost funds due to the abnormal DAI price action that took place on Coinbase, which reflected an off-mark DAI price relative to the other exchanges 20 and liquidity venues, as noted by members of the community.\n\n\n\nRestore borrower confidence: Based on forum discussions 6, it seems that the community believes that the liquidations were not in the spirit of how the Compound protocol should have affected users given the off-mark price of DAI. Community members and borrowers have expressed their frustration in the forums 3 and suggested compensation 4.\n\n\nAchievable: While the community acknowledges that the exact dollar value impact of the liquidation event varies per user, with feedback from core Compound contributors, this approach executes an achievable, straightforward, and timely compensation resolution rather than constructing an overly complex approach that would look to compensate an exact amount. This proposal is a Pareto optimal solution.\n\n\nParameter choice: The 14-day average price is a middle ground between the COMP price of 2 weeks ago and the current spot prices. Backdating a price too long, and it gives affected users COMP at an unfair discount relative to the current spot prices. Too recent could lead to material differences by the time the proposal goes live after the CAP, voting, and queuing period.\n\n\nRelatively small distribution: This proposal would not inflate the COMP supply and simply allocates existing COMP 2 in the Comptroller to affected users, as noted by @rleshner. The distribution represents a very small % of the total COMP supply (0.55%).\n\n\n\nNext steps:\nIf you support this proposal, you can delegate COMP to the CAP 116 at this address 0x2f04664b18fb9b6d49124fcc876b52a4ba797718\n\nDisclosure:\nFor full transparency, I was one of the 121 Compound users affected in the DAI liquidation event. I recently worked with the community to pass a reserve factor change 11 to the DAI market to de-risk it and prevent repeat events.\\nI’ve deployed a CAP that implements the changes in this proposal. You can find it here:\n\n  \n      \n      compound.finance\n  \n  \n    \n\nCompound 28\n\nCompound is an algorithmic, autonomous interest rate protocol built for developers, to unlock a universe of open financial applications.\n\n\n  \n  \n    \n    \n  \n  \n\n\\nisn’t 8 % a bit low. I think it should be 8,5 or 9 % to compensate for the inconvenience and possible losses due to price difference , the fact the funds where in accessible , …\\nI am all for the idea of compensation in any way.\nBut there is one nuance I would like to point out.\nThe only compensation mentioned was 8% fee for the liquidation of positions.\nBut it’s just a commission, and the larger problem was that Coinbase Oracle was attacked, and Security was exchanged to DAI for the unfair, non-market price.\nPeople lost not these 8%, but rather much larger sums because of the exchange rate difference.\nIn order to evaluate real damages we might consider so-called DAI rate deviation margin - and apply it to whole sum of DAI paid out by the liquidators.\nFor instance, average DAI exchange rate this month was about 1.04$\nMax DAI exchange rate during the attack 1.23$\nso, roughly speaking, the rate deviated for about 18% from the monthly average.\n\nReferencing the DAI liquidations spreadsheet published by rleshner, a total of 85,220,406.43\n\nI think we should add another 15339673,08 (18%) to the sum above, and use it as a final sum for compensation calculation.\nIf it seems too complex, we can round the sum to 100M (85M + 15M), and dispense 8% out of the total 100M between the damaged parties.\nIt is just a concept; if the approval for such scheme would be granted, we will have to make precise calculation, but I think you get the idea above.\\n@zino @Dmitry: You are correct. The community has acknowledged that losses varied per user based on a number of factors such as prices, liquidation timing, and collateral type. The losses incurred by users varied between 8% to 30% depending on the factors above.\nI think it’s important to zoom out and acknowledge a few points:\n\nCompensation is an act of good gesture by the Compound community and its stakeholders to restore borrower confidence and acknowledge the liquidations affected users in an adverse and unintended manner.\nSome have argued that users affected should not be fully compensated and that instead, they should get less than the 8%, or get 80c on the dollar, or such.\nConsidering the complexities of precisely calculating the exact losses per user, this proposal takes what’s known–the 8% liquidation penalty–and seeks, in good faith, to compensate users based on an achievable approach.\nThe actual compensation percent is ultimately a function of COMP price. If COMP price is greater than the twap price used to calculate the COMP taken from the controller, users will be marginally better off than 8%. Conversely, if COMP price is less than the twap price, users will be marginally worse off than 8%. The point being here being, that due to the delays and timelocks of the Compound Governance contract, it’s not achievable to get users an exact percentage compensation either way.\n\nThe code that executes this proposal has taken time to develop and would take several more weeks to go back and re-develop the code to incorporate new logic. In situations such as this one, community action and self-organizing play a crucial role in effectuating this proposal. A practical course of action is important before the Thanksgiving liquidation events and the frustration by the community becomes stale.\nThe deployed and currently live autonomous proposal (CAP) has a real shot at compensating users.\nYou can view the CAP here (https://compound.finance/governance/address/0x2f04664b18fb9b6d49124fcc876b52a4ba797718 16) and delegate votes to it at this address 0x2f04664b18fb9b6d49124fcc876b52a4ba797718.\\nIt seems my math might have been off indeed.\nThe total that should have been paid off is 85M paid out by liquidators plus 15M of exchange rates difference. But there’s just no reason to treat it as a base for 8% compensation. We have to add two things:\n\n8% liquidators fee collected on 85M paid out; and\nexchange rate difference calculated as I suggested before (DAI monthly average to max known Compound Oracle closing exchange rate), for the sake of the conversation let’s assume it in the range of 18%.\n\nIn this case total compensation should be about 26% (8+18) out of 85M. Basically, the figure mentioned in p. 2 and its calculation should be the topic of discussion. I would add the percentage of the exchange rates nevertheless, at least partly (maybe not in full, but at least 50% of it).\nIndeed, this calculation does not account for comp token (which is suggested as a compensation medium) price change.\\nI’m impressed with the efforts here, but wanted to weigh in that I am somewhat against this proposal. I think this sets a bad precedent for compensating a few borrowers for what is/was currently considered normal oracle/market behavior of the protocol. The protocol does not treat DAI as having a fixed price (as it does for USDC/USDT), and so without changing how the oracle/protocol works to address a change in policy first, I definitely don’t see how governance can pass this without creating worse problems down the road.\\nWhat kind of bad precedent will this do? The borrowers still lost a shit load of money after the proposed compensation; it’s not like the compensation made it become a good deal to get liquidated on weird price oracle events. You said there is just ‘a few borrowers’, but keep in mind it could have happened to more borrowers if Dai went higher or if Coinbase reports weird prices on other assets. It is also not completely impossible that lenders will get screwed in some different way one day. If this doesn’t get addressed it actually is setting a bad precedent. Next time an incident happen people can reference to this event to not compensate again, and eventually no one would feel comfortable using Compound if you can just get screwed randomly with no protection.\nAlso I disagree with your point that this event is “currently considered normal oracle/market behavior of the protocol”. This is just not true.You also mentioned that the oracle / protocol needs changing, which means you agreed that it is not normal.\nMaybe you are just arguing about the order of things being addressed. I agree both issues should be addressed and fixing oracle is more important than compensating a few people that no one cares about, but I don’t think these two issues have to block each other.\\n\nwithout changing how the oracle/protocol works to address a change in policy first\n\nin progress.\n@ kybx86\n“I am working on a series of proposals to 1) reduce the risk in the cDAI market  2, and 2) compensate users who were impacted in the DAI liquidation event  2. I am working with active community members, stakeholders, and users of the protocol to enact these changes.”\nAs a result the Governance proposal was accepted. (This proposal includes a community member that explains the DAI market risk. Increase DAI Reserve Factor from 5% to 15%):\nhttps://compound.finance/governance/proposals/31 7 (Passed)\n@ kybx86  solves a global problem. Not just compensation\\n@jared thanks for your thoughts. They are well taken. I acknowledge that the Compound protocol is not perfect today and that further steps are needed to improve its security and mechanics. Oracles (or more generally price feeds) have been one of the toughest problems to solve in finance and DeFi. Price perfection in real-time is a difficult thing to achieve particularly when markets move quickly. An oracle needs to simultaneously recognize the real-time price while also rejecting real-time prices that don’t reflect the true price–in some fashion, oracles remind me of the Heisenberg Uncertainty Principle in physics: the more closely you measure its position the more uncertain its velocity, and vice-versa. Price feeds suffer a similar curse: too fast and you can intake the wrong price; too slow and it quotes a stale price, creating a trade-off between speed and accuracy. Traditional financial markets have usually solved this problem through human reasoning (i.e. “the price is off”) or litigation (i.e. “you improperly liquidated me, so I’m going to sue”). These dynamics don’t exist on-chain yet. But I am of the mind that steps can be taken to improve the design of oracles AND liquidation mechanics. The UMA protocol is a great example of this, where liquidations go through an on-chain dispute process that allows members to vote on the validity and accuracy of a liquidation. (UMA is particularly interesting in this regard because it’s “priceless”, so there are no oracles at all).\nAs you know best, the Compound protocol and design have taken +2 years to develop and have benefited from user feedback and community use. Designing the perfect oracle and liquidation solution will take time, and require feedback from the community.\nTo that end, I am cognizant of the existing risks of the protocol and I think my first proposal around increasing the reserve factors across the DAI market (and others) is a strong first step in de-risking the protocol. Of note, I think you’ll see many DAI liquidity miners wind down their leverage (and reduce the risk to the protocol) since the wider spread between borrow and supply rates will make farming COMP less economical to them.\nCompensating users for a market-event that exclusively affected the Compound protocol in a timely manner is an important step towards improving the protocol. Other protocols that have been affected by adverse market events, such as Pickle, Yearn, Opyn, Hegic, and other trustworthy exchanges have reimbursed users for protocol hiccups. This compensation proposal, along with the DAI reserve change, is one of many future steps in engaging the community around important governance issues and working together on further improvements.\\n\n\n\n jared:\n\nconsidered normal oracle/market behavior of the protocol\n\n\nYou really think that is normal?\nThat priceof the DAI on one exchange is 30% higher than global price?\nAnd “quite by accident” protocol use only that exchange oracle?\nAnd “quite by accident” that exchange is an early investor in the protocol?\nAnd “quite by accident” that one hour-long manipulation frontrunners work harder than usually?\nYou think that problem is compensated protocol users with COMP tokens? Whois a better holder of native token than users of protocol?\nSomebody liquidation in ratio on 1 ETH: 377 DAI is normal? How long ETH price is above 400$? Somebody obviously need super premium price?\nWho was willing to buy a DAI for  1.3 when they could mint it for 1 from MakerDAO (Oasis)?\nI think you know who …\nAnd than we discuss about 8%?\nThis is a humiliation of true protocol users\\n@dabar90 please delegate your COMP votes to seek compensation!\\nI agree with you, i didnt expect 100% compensation but this is too low\\n@jared, the oracle printed a price that was way off market. That is not normal and not acceptable. The oracle needs better market coverage and probably much tighter bounds based on the Uniswap TWAP.\nAs far as addressing the oracle problem first, I am okay with that.\\nI am a comp holder, as well as a lender, and I agree this compensation is too low; when most users lost more than 8% (the liquidation fee). It is important to restore faith in the COMP protocol  and protect the long term health of our ecosystem in order to maximise the long term value of COMP. As a holder, I want to see short term dilution for long term stability and capital appreciation.\nDeFi is a competitive market and this is our game to lose. A lack of trust and confidence in COMP from this will cause serious long term impairment, and allow competitors to rapidly gain marketshare.\nI suggest compensation around the 15% range. This parameter should be tweakable easily. I believe more of the community will support this change, and believe this change is better for all of Compound.\nThe likely price impact of this heavier dilution is 2-4%. 2-4%, people. Think about what the cost of being seen as unsafe is…\nEven Ethereum bailed out the whole network for a bug in the DAO; the leading smart contract at that time.\\nI think this is a well reasoned rundown. My losses were around 25%-30% of the liquidated amount based on the false DAI price and liquidation fee combination. I also believe the compensation amount should be reasonably computed in a structured and logical manner.\n@kybx86 What rationale are we currently using to arrive at the current 8% value?\nI’d also like to point out that it is possible to have multiple compensation proposals and stage them in; potentially beginning with this proposal initially.\\n@4D_compound, thanks for your thoughts.\nThe 8% is coming from the liquidation incentive 1 liquidators get paid to close our undercollateralized positions. Basically, it’s the incentive fee for liquidators to run bots. This 8% incentive fee gets paid from the user’s collateral(s) balance. When a liquidator repays up to 50% of the undercollateralized loan, the liquidator gets back 8% in the collateral of their choice, so long as the user had it enabled as collateral type. Because users were borrowing DAI against many different types of collateral supported on Compound, user’s losses varied in non-trivial ways. So the 8% is compensating users for the liquidation penalty that was collected by the liquidator.\\nFrom a technical and functional standpoint this proposal works as follows:\nUsing the _grantComp function supported by the Comptroller, the proposal takes X amount of COMP from the comptroller to distribute it to the timelock contract. The COMP sits in the timelock, and after its approved, it then sends the COMP to the Merkle Distributor. The Merkle Distributor is a smart contract that distributes the COMP according to a merkle root. The Merkle Distributor used in this proposal is forked from the tried and proven Uniswap merkle distributor 5 which was used to distribute UNI, and further improved by Dharma.\nThis is the immutable merkle root 3, which shows every wallet and the hexadecimal COMP amounts.\nThe merkle root relays the COMP to each user’s wallets automatically and completely removes any technical claiming process or required interaction by the user. Each user affected would receive the COMP units in accordance to their liquidated amount. The COMP that would be distributed to each wallet is listed in this file 1.\\nSo if I’m reading it correctly, the 8% is the portion of losses that is guaranteed to have been suffered by each user as a result of the attack. In my case I had X liquidated for DAI for cost C1, but in another case a user may have had Y liquidated for DAI for cost C2, resulting in different total damages that were not simply a percentage of the amount liquidated (computable from the divergence of the incorrect DAI oracle price)?\nDo you have data for how much was lost for each liquidation?\\n8% premium + exchange rate difference + high gas fee when I tried to defend my collateral(because of front-runners).\nI have the impression that the community did not take the attack on the protocol seriously.\nIs there any more detailed report on the attack that includes information about reporters from the Coinbase open price feed?\\nI really appreciate your contribution in this case, I understand that it is difficult to determine the equillibrium between the affected users and the rest of the community however I would like to see better arguments of that part of the community that opposes or minimizes the compensation of liquidated users.\n8% is a sign of “goodwill” however it does not give me confidence to continue using the protocol. DeFi as a sector is at a very early stage and more complex attacks can be expected in the future.\nI do not want the security of my funds to be based on “goodwill”.\\nI’m not one of those who got liquidated. Indeed, it will be a BAD PRECEDENT. Bad precedent if this proposal isn’t passed. We would want to keep the reputation of compound strong, even stronger if this goes through. It’s easy for people to say things because they weren’t affected at all as result of unsecured borrowed asset price. Not to mention, it’s a stable coin that was the culprit of the event. If this doesn’t go through, then there will be no reason for compound to just turn down the community for future exploits. Just the NORMAL system you believe that It should not secure its lenders/borrowers.\\nvisualization (37)884×247 14.2 KB\nvisualization (39)811×576 42.5 KB\nI think this is relevant to a discussion regarding what might be fair compensation.\nBy Nov 24th, two days before the liquidation event, the largest liquidated looper whale had farmed and sold about 17,547 COMP, or about 2.16M USD given prices it claimed and sold at, minus around 160K between interests and gas costs. So even though the whale lost about 3.7M DAI with the liquidation, the account’s PNL after liquidation was closer to -1.7M USD.\nThis is the liquidation in question https://etherscan.io/tx/0x53e09adb77d1e3ea593c933a85bd4472371e03da12e3fec853b5bc7fac50f3e4 8\\nFascinating discussion here. I’m in favor of compensation as a grounds for recognizing abnormal market behavior but can definitely see the cases on both sides of this argument.\nTo me this is a great example of community coming together to rally for change through governance of which I encourage.\nBoth the compensation amount and future precedents here are definitely uncertain but in my mind I believe passing this would give other community members confidence that meaningful change can happen without the intervention of the core team which I believe is worth taking a punt on.\n\n\n\n kybx86:\n\nassuming the Comptroller earns 962 COMP per day, the 55k COMP distribution will be replenished in less than 60 days.\n\n\nThis gives me confidence that this proposal would not have significant implications. If the funds are largely idle I think this experiment is one worth pushing forward on (either in its current form or with slight revisions to the compensation amount).\nWill be keeping a close eye on this!\\nI’m not sure how much this should factor into the computation as ‘damages from false liquidation’. It seems like it is still a loss 3.7M DAI regardless of whether or not the victim was profiting or losing from using the platform. It’s the false liquidation event itself that counts.\\nthis is not the case of all the users that were liquidated wrongly , some users didn’t make as much comp as other to cover for the losses , 8% is way too low for what i have lost , my collateral was liquidated for a way below market price and given the price of Eth has recovered since that adds to the losses i had no intention for selling my collateral . If users believe that there’s an oracle (price feed) issue that needs fixing ,that means that the protocol didn’t behave as intended and that users lost their funds because of a bug in the system and since compensation will not have a huge impact on the protocol as a whole ideally the victims should get more than 8% which represent just the liquidation penalty .\\nWorth noting that +61% of the proposed compensation would go to just 2 COMP farming operations according to this plan.\\nHas anyone looked into the list that is going to be (or not) refunded to see if there are addresses that are extra/added by the governance proposer?\\nThis is certainly something important to consider. Please observe the following chart\nDAI Repaid(linear scale)1051×651 5.26 KB\nAlthough you can’t see them, for the entire length of the x address there are addresses that were liquidated that don’t show up because their relative size is essentially zero.\nThere is a disparity in the magnitude of losses that is so large that it makes those who didn’t lose millions in this event seem insignificant. I would like to see an amendment made to this proposal that incorporates this function which will weight the percentage of compensation with the magnitude of the losses:\npercent compensated = 1/(1000*(x+.0012))  +.0782\nObviously, the subsequent values would be multiplied to the pre-existing data on how much DAI was repaid. For an easy view of how this amendment would affect each affected account please view this modified google doc I made that does exactly what I described above:\n\n  \n      docs.google.com\n  \n  \n    \n\nCopy of DAI Liquidations 6\n\nSheet1\n\nLiquidator,Borrower,Repay Amount,Collateral cToken,cTokens Seized,Repay cToken,Transaction,evt_index,evt_block_time,evt_block,DAI Repaid,Percent of liquidation,percent of liquidation compensated,amount to...\n\n  \n  \n    \n    \n  \n  \n\n\nThe newly adjusted total COMPensation (get it?) would be 9,855,567.80 with a majority of users being compensated a majority of what was liquidated. Please see the document for a more detailed spread of what liquidated values get what percent compensation. The difference between the total in this proposal originally and the amendment is 3,037,935.29. That comes out to +20,951 comp which would result in a replenishing period of not <60 as calculated by @kybx86 but rather 81 days.\nIn my mind, this doesn’t come out to a significant amount of money relative to the size and growth rate of the treasury. Additionally, the result of this amendment will better accomplish goals 1 and 2 and will maintain the benefits of the original proposal: minimal market impact, short time to replenish, trust between the users of compound and the governance.\\nThis second allocation proposal seems more fair to the smaller liquidated accounts who lost more than the liquidation incentive due to incorrect DAI price, but still bails out the big farming operations that had been keeping their accounts at unhealthy levels for months while dumping COMP rewards. I think that instead of compensating the smaller accounts by increasing total compensation, it should come from the compensation originally allocated to the big farming operations, and perhaps cap the total compensation for any account to something like 500K USD value or lower. This would 1) compensate borrowers unfairly liquidated 2) cap the bailout on big recursive farmers 3) reduce total paid out COMP\\nI have voted against cause of 2 reasons:\n\nIf we compensate a bug, then the bug should be fixed first.\nI would like to see a more precise calculation instead of just going a flat 8%, using price of the assets when the liquidation happend.\n\\nHere is a proposal that keeps the suggested formula to increasingly compensate smaller accounts, but caps compensation to a single account at 500K USD value. Instead of increasing the total compensation, it saves the Compound treasury about 200k USD of value of what the OP @kybx86 proposed. Worth noting that this cap only applies to the 2 biggest accounts that were recursive COMP farmers (aka “looper whales”)\n\n  \n      docs.google.com\n  \n  \n    \n\nCopy of Copy of DAI Liquidations (weighted compensations) 6\n\nSheet1\n\nLiquidator,Borrower,Repay Amount,Collateral cToken,cTokens Seized,Repay cToken,Transaction,evt_index,evt_block_time,evt_block,DAI Repaid,Percent of liquidation,percent of liquidation compensated,amount to...\n\n  \n  \n    \n    \n  \n  \n\n\\nI’m mildly against the idea of reimbursement and strongly against this specific proposal. It’s very important we separate these two things because this proposal is terrible. However, I don’t think voting against this proposal should be interpreted as voting against some form of compensation. These are separate issues and there are more ways to do this!\nConsidering this proposal in theory\nProtocol insurance funds are a great idea! If the Compound protocol had one, it should be used in this case. However, the Compound protocol does not have an insurance fund. So now we are in a situation where we have not promised insurance but we may want to reimburse some losses anyway because we see it as being in the greater good of the protocol long term.\nThe protocol does have “reserves” for each market which have slowly been growing over the last couple of years. There are have been people in the community that have considered these to act like an insurance fund for the protocol. They have never actually been used in that way but it is a reasonable use case. Currently the total reserve for the Dai market is $1.3 million Dai, this is the largest reserve of any market and makes up a large portion of total protocol reserves.\nIf there is going to be any compensation, the amount of reserves in the Dai market should be the ceiling for it. This would be incredibly generous as it amounts to giving away a large portion of all reserves that have been earned in the last couple of years.\nConsidering this proposal in practice\nPractically this proposal amounts to a massive bailout for two yield farmers who have been vampiring value from the protocol.\n\nPer @wario’s comment 54% of this payment goes to one single yield farming operation and 61% goes to the top two yield farmers. The top yield farm was simply selling all accrued COMP (generating $2.15 million in profit for themselves).\nThese yield farmers are NOT long term aligned with the protocol. If they were they would not be selling their COMP.\nThese yield farmers were intentionally and knowingly engaging in risky behavior in order to earn millions of dollars by dumping COMP. With that reward comes risk that anything at that scale should clearly understand.\n\nThe idea of giving 0.55% of the total governance token supply to people who’s proven on-chain behavior is detrimental to the protocol is INSANE! We are literally giving extra power to the most short sighted actors in the system.\nCOMP is not money, it’s a governance token. We should not be giving away governance tokens to somehow try to act as financial compensation and the particulars of this situation only make it a worse idea.\nFor these reasons I encourage everyone to vote against this proposal. I also encourage @kybx86 to put forth a new proposal using the Dai reserves for a pro-rata compensation plan.\\n\n\n\n lay2000lbs:\n\nThe idea of giving 0.55% of the total governance token supply to people who’s proven on-chain behavior is detrimental to the protocol is INSANE! We are literally giving extra power to the most short sighted actors in the system.\nCOMP is not money, it’s a governance token. We should not be giving away governance tokens to somehow try to act as financial compensation and the particulars of this situation only make it a worse idea.\n\n\nThese 2 paragraphs should be emphasized alot more \\n\n\n\n lay2000lbs:\n\nThe idea of giving 0.55% of the total governance token supply to people who’s proven on-chain behavior is detrimental to the protocol is INSANE! We are literally giving extra power to the most short sighted actors in the system.\n\n\nThis is not true for all the users that were affected by the liquidation , also compound uses the COMP token as a collateral by doing that it’s already being used as some form of money .\\nvisualization (45)800×494 32.4 KB\nI need to correct what I posted above about the second largest liquidated account (0x189c2c1834b1414a6aee9eba5dc4b4d547c9a44c), it is not as clear cut. It was doing recursive borrow/supply of DAI to almost exactly the same amount, which is why the supply line of DAI in this chart is hidden behind the borrow line. But it was not dumping its claims as frequently as the big liquidated looper whale. LLW had 1525 claims, versus 46 claims for this second one. And the receiving addresses are also not as obviously exchanges as is the case for LLW.\\nAlso need to correct, the top 2 accounts would make up for 61% of the proposed comp distribution, not 75%, my mistake. Here’s the original sheet with an extra column for the proportion of repaid capital for each one https://docs.google.com/spreadsheets/d/1cJoviDbDpNljdqRbiUKMJnoRipTc66O6ZjrzuXu30jw/edit?usp=sharing 6\\nI want to make a point that’s been coming up. I’ve heard a lot of “fix the problem” then compensate users. While im not against the idea at all, my question to the community is: what IS the problem?\nTo an extent, we can agree the oracle needs work. But, I’ve heard a lot of let’s just cap the price of DAI at ± 5% bounds, and then that’s it. That fixes it; then we can move on. This couldn’t be further from the truth. In fact, proceeding in this manner would just mask the issues; if there comes a day when DAI does dislocate beyond these bounds, the protocol will suffer material losses, and people would be pointing the finger back to the oracle. So it’s not just about the oracle; it’s about re-designing from the ground up a self-sustaining risk framework that works across all kinds of environments, not only a subset of probability outcomes (because isn’t that what just happened?). This won’t be built in a day or week. It’s a multi-faceted problem requiring the best ideas to come forward and drive the protocol design.\nUMA has one of the best approaches to the oracle challenge: It’s a priceless design and its liquidations go through a dispute process–if we were to replay the events of Thanksgiving, having this dispute process active on Compound would have resulted in most of the liquidations to be disputed and reversed since it was clear that the global spot price of DAI was not $1.3. But that’s just a minor point.\nTo that end, the compensation proposal I’ve put forward is not uni-faceted nor final. In fact, it’s the second proposal I’ve put forward (with the first one having been the adjustment of the reserve factors, which’s now live).\nMy intent here is to 1) acknowledge Compound needs work. 2) Compensate all users affected, regardless of whether one’s moral compass likes/dislikes small users, large borrowers, looper whales, or whatnot. 3) continue to work towards the security and betterment of Compound.\nOne final point: While I’ve recently become more vocal on the forums, I’d like to share that I’ve been a long-term and loyal Compound user since V2 launched back in March 2019 (~2years), and most recently, since COMP distribution began, I’ve also been a COMP holder. I seek an outcome that’s a win-win for everyone involved, including the users affected, the community, and its stakeholders.\\nThis is a fairer compensation structure for smaller investors whose businesses is significantly damaged by the “DAI manipulation event”.\nBut also I agree with @blck first reason :\n\n\n\n blck:\n\nIf we compensate a bug, then the bug should be fixed first.\n\n\nAlthough this event damaged my business, but I think that for the benefit of the protocol, it is important to fix the bug first.\nI don’t know why are we waiting for the implementation of Chainlink oracles?\nCoinbase open price feed as the only source is a mistake, this raises the question: why was this solution voted on (proposal 019)?\nIs there a secret agreement with Coinbase?\nThis question was addressed to all those who voted for proposal 019.\nWhose fault is it?\n@blck what is your opinion about this situation and what kind of compensation structure would be fair in your opinion? I am asking you specifically because you are a more experienced protocol user.\\nThanks for all your efforts but based on the feedback from the community i feel like the community is willing to support the compensation as long as the compensation % is revised . there’s some good proposal from @wario and  @jacobdecatur also maybe it’s worth considering using DAI reserves or combination between $Comp and DAI reserves . in term of fixing the bug ,that’s a more complex issue the discussion to use chainlink has already started here chainlink price feed 8 .\nI think this two conversation can go in parallel and they are not mutually exclusive , the compensation proposal doesn’t depend on fixing the bug first , also increasing DAI reserves is a step in the right direction\\nI dont have any personal problem with chainlink however it had its own problem in the past (2 which is big deal for compound), while compound’s openoracle also has its advantages and disadvantages, those 2 issues would not happen with compound’s openoracle and thats already 2 “bug” that we avoided by not using chainlink.\\n@blck what’s your logic in linking the compensation effort to the bug fix as if the bug is not fixed the compensation can’t go ahead ?\\nthe compensation part i already said that each user “loss” should be calculated on spot price when the liquidation happend but with “normal” DAI price this would give us the exact “loss” of a certain user a range of 8% to 25%~ probably\\nNo strong thoughts on if it should be 8% or any other number, overall, I am for compensation. Maybe it is this proposal or a different proposal, but I think we should compensate users. The main reason I’d want to compensate borrowers is because it is still in the early days of DeFi and anything that builds borrower trust in the long term is valuable for the protocol. If the extent of the oracle risks were not clear to the end users, then it is on us as a community to improve our communication around the risks.\\nPolychain has decided to vote “AGAINST” Compound Proposal #032.\nWhile we sympathize with those who were affected by the dislocated DAI price and subsequent liquidations, we believe that CP #032 sets a potentially dangerous precedent for a few reasons:\n\nThis proposal provides reimbursement before a clear path to fixing the issue has been established. We would like to see a little more clarity around potential solutions before finalizing any form of reimbursement.\nThis proposal reimburses users even when the protocol worked as designed, and may encourage further systemic risk. We do not want to set the precedent that tail risk should be subsidized by the COMP governance process.\nThis proposal suggests reimbursement denominated in COMP to a small group of users affected by the issue, despite displaying behaviors that are not aligned with the long-term interest of the protocol. We would prefer a reimbursement proposal denominated in the asset that was liquidated, but are understanding of the additional technical complexity.\n\nWe would like to see continued discussion and proposals that work towards improving oracle robustness, implementing insurance funds to provide a finite backstop for unexpected protocol behavior, and exploring alternative methods of reimbursement.\nThe community has come to a loose consensus that the current oracle system worked as designed, but not as expected. The actual market price of DAI on Coinbase was accurate at the time of liquidation. Regardless of the outcome for CP #032, it is very clear to us that the governing community must work to reduce market risk and improve oracle resilience.\nWe are voting “AGAINST” this particular proposal, but remain open-minded as it relates to potential reimbursement. We applaud the efforts of @kybx86 to solve these key issues, and we look forward to working with them and the rest of the community to reach a possible solution.\\nDoes the protocol now use its own open oracle? It isn’t Coinbase “price discovery” solution?\nThe bug is in the Compound protocol or Coinbase open price feed?\nCoinbase states that is a normal market activity (30% stablecoin spike in one hour on their exchange while globally was stable).\nWhat is “bug” in this situation?\\nPantera Capital is voting AGAINST Compound Governance Proposal 32 to distribute COMP to users affected by DAI liquidations on Nov 25 2020 as compensation.\nFinancial markets are more prone to hidden tail risks when participants deploy instruments with high leverage or for uses far from their intended purpose. Such risks are difficult to measure, which leads to scenarios where returns are attractive and too low at the same time.\nAs a Compound user (note: not farmer or private investor), we’re responsible for knowing the potential risks involved in using the protocol, including faulty oracle feeds and illiquid markets. Compensating users encourages looser risk-taking when efforts are still underway to measure and mitigate such risks. It also undermines the ability to properly assess risk vs. reward in Compound markets by artificially deflating perceived risk.\nOpen protocols can aspire to be trustworthy, but they’re permissionless by design. Usage will inevitably go “out of bounds,” despite disclaimers or risk parameters. Compensating users when known risks materialize is unsustainable in the long term.\nWe’re supportive of solutions that are sustainable and healthy for the protocol, including:\n\nprotocol insurance funds, with explicitly-stated intent and sources (e.g., market reserves),\nmulti-oracle designs, and\nremoval of “safe max” language in Compound UI.\n\\n\n\n\n ekrenzke:\n\ndespite displaying behaviors that are not aligned with the long-term interest of the protocol\n\n\nCan you please give me the definition what kind of behavior is aligned with the long-term goals of the protocol?\nWhat is my mistake as an affected user in this case?\n\n\n\n ekrenzke:\n\nprotocol worked as expected\n\n\nIf the protocol is worked as expected, what isn’t worked as expected?\\nYou talk about risk management despite the obvious manipulation?\n\n\n\n franklin-pantera:\n\nCompensating users encourages looser risk-taking\n\n\nUsers didn’t liquidate because of risk management. If under normal circumstances you mean 30% volatility of stablecoin on a single exchange (not globally) then Compound protocol is not a healthy and safe place to hold user funds.\nWith this statement, you actually said that this protocol is useless. Given the interest on the deposit (0.04% APY) the only healthy feature so far has been the “borrowing LTV” of 75% on Ethereum.\nWith this kind of thinking, you will lose true customers and you will be left with yield farmers - and then we all know what will happen when a similar protocol offers them better incentives.\nWe are all aware of your connection to Coinbase Ventures (VC Funds clusters) and it would be bad for Coinbase to admit the malfunction of its product.\nConclusion: A compound protocol based on several VC Funds will probably never be a decentralized platform. Currently, the protocol functions as a joint stock company.\\nReally well thought out responses from those voting AGAINST here.\nWhile it makes sense this specific proposal is not going to lead to any compensation (and for good reason) I would be curious to explore ways to reward @kybx86 and all those involved in putting this proposal together.\n61% of compensation going to one farming operation and using COMP as payment seems to be the nail in the coffin and makes perfect sense to amend these points.\nI’d like to emphasize that with this vote failing, those involved in stewarding this convo are going to feel very defeated and these types of discussions are ones that we should always look to encourage and reward, even if the proposal itself ends up failing.\nI would be interested in exploring opt-in grants on a case by case basis to proposal authors that are settled onchain (meaning at least 1M in COMP was required) as to encourage amendments of this proposal and future participation from other politicians.\nThanks everyone for their input and excited to report on this proposal this week.\\n@kybx86 has done a very good job by starting a process that brings long-term value to the protocol - trust in the protocol.\nI hope that the proposal will be upgraded in certain elements and put to the vote again in cooperation with the big players because otherwise, we are spinning in a circle (proposal - reject).\nWe should be more concerned about the statements of certain VC funds that see the “Dai liquidation event” as risk management and in fact it is a complex manipulation of the protocol.\\n@coopahtroopa I appreciate your thoughtful engagement. Your feedback has been helpful throughout this discussion.\nAs a liquidated Compound user and COMP holder, I have skin in the game. My hope is that the community doesn’t move on from the objectives of this proposal should I stop driving it. The community should look inward to improve Compound’s risk framework and step up for the 121 users who were liquidated in the events of 11/26, regardless of who steers the boat.\n\nIf any member(s) in the community wants to work with me to craft a better compensation proposal, please reach out. (I’ve heard privately from a handful already, and look forward to more).\n\\nGreat point. Not enough people speaking about Coinbase’s role here, despite their explicit language to reject prices that deviate from the expected volatility of said asset 3.\nLuke Youngblood, who voted for prop32, is one of the main architects of the Coinbase price feed.\nLuke, if you read these forums, we’d be interested in hearing your opinion.\\n\n\n\n ekrenzke:\n\nThis proposal suggests reimbursement denominated in COMP to a small group of users affected by the issue\n\n\nI’d like to put on the record that the number of users affected was a non-negligible percentage of the userbase.\\nWould love to hear input from  Coinbase’s trade surveillance head apart from their “short” comment here.\n\n  \n    \n    \n    DAI Liquidation Event \n  \n  \n    Coinbase looked at the data from many different angles and concluded that not only were there no price manipulation alerts in our trade surveillance software, but we did not find any evidence of collusion or single actors that pushed the price higher. We believe our books properly operated according to the availably liquidity at the time which, in stablecoin markets, may become thin as price moves away due to the collateralized nature of these markets. And, Coinbase Price Oracle accurately repor…\n  \n\n\nWould also appreciate it if Coinbase can shed more details on the said event.\n@Elkins I was wondering if Coinbase can atleast provide anonymized data on said price event. Also,  if possible, could Coinbase also divulge the volume of orders needed to move the CB’s Dai market by 30% . Thank you!\\n\n\n\n franklin-pantera:\n\nAs a Compound user (note: not farmer or private investor), we’re responsible for knowing the potential risks involved in using the protocol, including faulty oracle feeds and illiquid markets\n\n\nPotentatial risk when somebody need cheap Ethereum, manipulate the market with front-running and false-reporting, get my Ethereum for 1ETH:377$ (because global price of DAI in moment is 1$) with nice premium of 100-150$ per Ethereum.\nAnd Pantera Capital said that I need to know risks in that  “fairytale risk story”.\nI didn’t know the risk of being robbed by protocol aggressive liquidation system powered by bots.\nI didn’t know that the “borrowing function” should not be used.\nI was used Compound before COMP token issuance because protocol had use case for my business and I didnt know that criminal activities are allowed.\nI just used lending/borrowing service(product) when protocol was manipulated - i didnt farm COMP, I was used stablecoin for business liquidity - what is the point?\nThat Alex Mashinsky (Celsius) REPUTATION worth more than Compound protocol? I know that worth more than Pantera Capital for sure.\nYour statement really raises the question: “Do we really need DeFi?”\\nNot sure, why you want to dig deeper into the events which happened at Coinbase. No matter which the exact reason for this market dislocation was, the root cause for the “liquidation event” is, that the open oracle uses Coinbase as a single price source. If you want to dig deeper, I think you should ask, why the open oracle ever is gone live with only one price source.\\n\n\n\n cryptix:\n\nwhy the open oracle ever is gone live with only one price source.\n\n\nBecause it was voted to go live with one oracle.  here:\n\n  \n      \n      compound.finance\n  \n  \n    \n\nCompound | Proposal Detail #19 3\n\nOpen Price Feed\n\n\n  \n  \n    \n    \n  \n  \n\n\n\n\n\n cryptix:\n\nNot sure, why you want to dig deeper into the events which happened at Coinbase.\n\n\nBecause I want to see the volume/cost used to move the coinbase dai market, and why no arbitrages on that price spike happened. In short I want to get to the bottom of how that spike happened as reference on ideas on how to better harden the oracle system\\n\n\n\n tonyotonio:\n\nBecause it was voted to go live with one oracle. here:\n\n\nSure, that’s obvious. But why was this proposal ever created? A good source for this answer could be the author of the proposal 5.\n\n\n\n tonyotonio:\n\nBecause I want to see the volume/cost used to move the coinbase dai market, and why no arbitrages on that price spike happened. In short I want to get to the bottom of how that spike happened as reference on ideas on how to better harden the oracle system\n\n\nHardening the oracle is very simple: just use more price sources. Check here 5 for a possible solution.\\n\n\n\n cryptix:\n\nHardening the oracle is very simple: just use more price sources. Check here for a possible solution.\n\n\nIm also in that discussion, I actually am for it but waiting for someone to submit code and start a CAP. Would  also gladly delegate my measly vote to it.\\nAgree, Coinbase can provide some data and stats about the event, but problem is that Coinbase Price Feed Data is a centralized part of the “decentralized” Compound protocol.\\n\n\n\n tonyotonio:\n\nI actually am for it but waiting for someone to submit code and start a CAP. Would also gladly delegate my measly vote to it.\n\n\nI am looking into getting something submitted. Will report back soon.\\n\n\n\n franklin-pantera:\n\nreturns are attractive and too low at the same time\n\n\nCan you explain to me please what you want to say with this statement?\nThis statement means that I use a platform with low returns but on the other side I want to be part of Compound community (because I additionally farm COMP)?\nThat makes me a high-quality COMP holder,\nand not a farmer whose main goal is profit (sell / dump COMP).\nThe point is that in the case of “DAI liquidation” every user who used the “borrowing service” was in danger.\n\n\n\n franklin-pantera:\n\nCompensating users encourage looser risk-taking\n\n\nI was liquidated two times in the last 16 months in Compound protocol but that was my fault, this time users are manipulated with planned attack.\nI don’t need charity from protocol,  I just want to be refunded the part of the funds that were taken away from me by that attack and be a more active member of the healthy community.\nI understand, your perspective on this situation is different but please can you tell me what is long-term impact on the protocol if real affected users will not be compensated?\\nNot sure if everybody saw this tweet 11 from Synthetix Founder Kain Warwick regarding proposal 032, But I thought it was worth sharing here:\n\nI voted yes on Compound proposal 032. This proposal would distribute COMP to offset losses incurred by liquidated DAI positions. I thought about this a lot over the weekend, and while I expect the proposal to fail here is my reasoning regardless. Factoring in all risk is probably the defi platform I trust most. Excluding SNX there have been times when 50% of my crypoassets have been on deposit there. I think it is one of the safest places in DeFi you can put your funds.  However, there are risks, and liquidations due to anomalous prices are foremost among these. The reason I voted yes is that I want to ensure there is skin in the game for all COMP holders, so they are hyper aware that the funds on deposit are at risk, and they are responsible. Right now there is indirect alignment between COMP holders and depositors, while this proposal sets an aggressive precedent I think it is one which is unlikely to be repeated in the future if all Compound stakeholders are aligned. This is also a good initiative to restore trust. The final point I will make is that I would highly suggest the Compound community at least discuss with Chainlink, the possibility of using their oracles as a backstop. And I say this as someone who holds no LINK, but as an SNX I am holder extremely aligned with CL security.\n\nI again want to applaud @kybx86 on the valiant effort on prop 032. While the proposal did not pass in current form, it helped to propel the conversation forward by giving us a reference point on which to base our discussions, offer alternative routes to compensation, discuss issue prioritization etc.\nThat said, that several notable entities who publicly shared their reasoning (i.e. Polychain, Pantera, Kain from Sythetix) all pointed at Compound’s oracle design as a key area that needs improvement, regardless of how they voted on this proposal. Obviously I am in agreement with the sentiment, and as stated before, I am looking into submitting a CAP along these lines. If anybody wants to work with me on this feel free to shoot me a message. Would love the support!\\nA valuable next step may be to summarize all the compensation proposals we have so far to make it easier to examine their merits.\\nThank you for your input, @ekrenzke . It is great to have your involvement and I appreciate you keeping an open mind.\n\n\n\n ekrenzke:\n\nThis proposal provides reimbursement before a clear path to fixing the issue has been established. We would like to see a little more clarity around potential solutions before finalizing any form of reimbursement.\n\n\nThis I agree with completely.\n\n\n\n ekrenzke:\n\nThis proposal reimburses users even when the protocol worked as designed, and may encourage further systemic risk. We do not want to set the precedent that tail risk should be subsidized by the COMP governance process.\n\n\nThe protocol worked as designed, but the design was clearly flawed. The Coinbase oracle printed an off-market price and Compound accepted it. Legitimate users were unnecessarily liquidated.\n\n\n\n ekrenzke:\n\nThis proposal suggests reimbursement denominated in COMP to a small group of users affected by the issue, despite displaying behaviors that are not aligned with the long-term interest of the protocol.\n\n\nI am not sure why you make this statement. I was liquidated and all I had done was supplied ETH/WBTC and borrowed DAI to the 80% limit which was described as a “safe max”. I was not doing anything particularly unusual other than using the protocol to borrow DAI. How is that behavior not long term aligned? Is the protocol not meant to be used?\nIf your statement does not apply to the entire set of users that were liquidated, but rather only the subset of liquidated users that were doing loop-farming, then I do agree with you.\nAll that said, I personally do believe that we can come up with a better proposal. 8% across the board makes DAI-DAI famers completely whole, while leaving “legitimate” users with as little as ~25% reimbursement on their losses. Effort should be made to understand the losses in more detail.\\nIt seems like if we modify the proposal, there is actually a chance that it would pass. and I do think if it passes it will be a very positive message overall and show that the community do have the power to protect users in events like this. If anyone is working on a revised proposal and need help, please let me know.\\nWe can simply add okex to the list of the reporters. They post prices but aren’t included. We could also add more reporters, chainlink to me just isn’t the best option because it’s very prone to human error, and had a problem with aave not too long ago.\\n\n\n\n TennisBowling:\n\nWe could also add more reporters, chainlink to me just isn’t the best option because it’s very prone to human error, and had a problem with aave not too long ago.\n\n\nYour comments about chainlink sound interesting. Would be great, if you could write some more details over in the chainlink thread 4.\\nthe proposal was not accepted by the VC funds due to compensation in the COMP token (mostly) as far as I understood from their statements.\nI understand them because the 2 biggest liquidated addresses would get big bags of governance power.\nI think the fairest calculation for compensation is suggested by @jacobdecatur and  @wario - weighted compensation -  in DAI.\nArgument against proposal by @blck makes sense\n\n\n\n blck:\n\nIf we compensate a bug, then the bug should be fixed first.\n\n\nI think that every component of a decentralized system should be decentralized, so in this “oracle case” it is necessary to plug in more price discovery sources. The protocol must use global cryptocurrency prices.\nI also wonder if anyone works on new proposal?\n\n\n\n michjun:\n\nIt seems like if we modify the proposal, there is actually a chance that it would pass\n\n\nI agree with you\\n\n\n\n TennisBowling:\n\nWe can simply add okex to the list of the reporters. They post prices but aren’t included. We could also add more reporters, chainlink to me just isn’t the best option because it’s very prone to human error, and had a problem with aave not too long ago.\n\n\nIm my view adding more reporters only nominally mitigates the situation, as simply taking the median across a select few exchanges still does not provide adequate market coverage (it also doesn’t take into account volume and liquidity differences across exchanges like data aggregators do).\nAs @cryptix said, would love to hear more detailed thought on this comment in the Chainlink thread 4 so I can better understand.\\nHi everyone, I’d like to first make an attempt at summarizing a couple of the concerns expressed by others in this thread, including @lay2000lbs  @tonyotonio  @ekrenzke @franklin-pantera  that might have resulted in this proposal not gaining the required support:\n\nSome of the largest liquidated accounts were using the protocol in a way that is not intended, and displayed behavior not well aligned with the long-term interests of the protocol (aka Recursive farmers, wash lenders)\nCOMP is a governance token, any compensation should come from the DAI reserves\n\nWith that in mind, I have been working on an idea for which I would like to solicit feedback from the community, on its soundness and on if it’s considered worth pursuing further.\nI’ve done some work to discern the accounts that were engaging in farming and to what extent if they were. By using the following formula we can quantify this for any Compound user. Let’s call it recursive factor for lack of a better name\n\nWhere:\nt = Tokens the user has borrowed or supplied as collateral\nCFactor = Collateral factor for the token in question\nSupply,Borrow = Self descriptive for the token in question and converted to Dollar value\nA couple of examples might help to illustrate how this works out:\n\nAlice supplies 10M DAI, subsequently borrows 7.5M DAI, then recursively does this again and again, and ends up with a total supply of 40M DAI and total borrows of 30M DAI. The collateral factor for DAI is 0.75, so Alice would have a recursive factor of 1\nBob deposits ETH for 10M worth of value, borrows 7.5M DAI, exchanges the borrowed DAI for ETH, which he again deposits to the protocol for another DAI loan, and so forth until his account ends up with 40M worth of ETH supplied and 30M worth of DAI borrowed. Bob is leveraged up on a long ETH and short USD position. His recursive factor is 0\n\nWhat determines the recursive factor is not how leveraged up a position is, but what proportion of their supplies and borrows are in what we could call “wash loans” which serve no seeming purpose other than farming COMP. So for the purposes of this metric, all stable coins in the system are counted as a single asset. Another example might illustrate this point:\n\nCarol supplies 10M USDC, subsequently borrows 7.5M DAI, exchanges the borrowed DAI for USDC, then recursively does this again and again, and ends up with a total supply of 40M USDC and total borrows of 30M DAI. The collateral factor for both DAI and USDC is 0.75, so Carol would also have a recursive factor of 1\n\nI analyzed all the liquidated accounts using this proposed metric, and have created a spreadsheet with the results. The columns in green are there just to demonstrate the formula described above. All the supplies and borrows are using coingecko USD prices for the day of the liquidation event, except for stable coin prices which I fixed at $1, and all account values are taken from a snapshot at block 11332733 which is 1 block before the first liquidation. I’d of course happy to share the code used so that it can be properly audited by the community.\nA summary of the results is that out of 122 distinct liquidated accounts, 87 have recursive factors of basically 0, and only 29 have a recursive factor greater than 0.50.\nI also added a column that computes one possible compensation scheme based on this recursive factor that results in a total compensation of 2.22M as opposed to the 6.8M originally proposed. There are other ways to use the recursive factor to compute possible compensation, and I don’t think this one is necessarily the ideal one, I added this one simply to get an idea of how much recursive farming could impact the proposed compensation.\nIt’s worth noting that both the supply and borrow columns for assets have been converted to USD value using the coingecko prices for that day, and the supply columns have been multiplied by each asset’s collateral factor\nThe spreadsheet in question can be accessed here\n\n  \n      docs.google.com\n  \n  \n    \n\nliquidated recursive factors 12\n\nrecfactors1\n\naccount,dai repaid,block,recursive-factor,rf formula,max value,s-b dif,prop compensation,dai b,dai s,usdc b,usdc s,usdt b,usdt s,sai b,sai s,uni b,uni s,eth b,eth...\n\n  \n  \n    \n    \n  \n  \n\n\nI’ve also made a simple web app that generates account history charts, as shared in my posts above, and also computes the recursive factor for any account, which can be accessed here 3 (It requires a browser with Metamask).\nIn addition to sharing the code used for this, there is further possible work that has been suggested by different members of the community, which might also be relevant for deciding on possible compensation schemes.\n\nCalculate the prices of liquidations, as some accounts that had non-DAI collateral seized lost more than the 8% liquidation incentive.\nCalculate what accounts would have gotten liquidated on that day due to other collateral assets crashing in price, had DAI remained at parity with the global market price, and to what amount they would have gotten liquidated if that is the case\nCheck if there are other accounts that might have gotten liquidated due to the DAI price, but for which the liquidator might have repaid a non-DAI loan. These would not have been included in the originally shared liquidations spreadsheet.\n\nI appreciate any feedback here, and if there’s some form of soft approval from the community, and no important issues with what I’m proposing, I’m happy to continue work on next steps, including possibly creating a new formal compensation proposal.\nFinally I’d like to sincerely thank @kybx86 for getting the ball rolling, not only on a proposed compensation, but I think more importantly on starting to address the issue of DAI market risk the protocol currently faces.\\nI’m a fan of all of these\n\n\nCalculate the prices of liquidations, as some accounts that had non-DAI collateral seized lost more than the 8% liquidation incentive.\nCalculate what accounts would have gotten liquidated on that day due to other collateral assets crashing in price, had DAI remained at parity with the global market price, and to what amount they would have gotten liquidated if that is the case\nCheck if there are other accounts that might have gotten liquidated due to the DAI price, but for which the liquidator might have repaid a non-DAI loan. These would not have been included in the originally shared liquidations spreadsheet.\n\n\nThey should be trivially computable from the transactions, and seem fair.\\nI will repeat more or less what I have written in the governance discord commenting the work of @wario but also the general handling of this liquidation event.\nFirst of all I am not trying to “throw him under the bus” as another user has commented there and I respect his work on this situation that is trying to get some money back to the users that got liquidated. Second, I would like to point out that I am biased as I was in fact one of those users and I was also “looping” (farming) but also hedging my position.\nNow to the main points that he has risen I disagree on some things. I basically disagree with the main idea which is - if I was to summarize it - comprised of two basic points: 1) spend around the amount there is in the DAI reserves, 2) categorise compound  users to “farmers” and “non farmers” and punish farmers.\nIn my opinion the appropriate way to handle this is: 1) ask if everything worked as intended? lets agree on that first, 2) if not, then fix it, 3) if it is fixed then calculate what each user lost in this incident and 4) go from there.\nFor what is worth, I am not an idiot and I see the good practical merits that his work is doing by trying to compromise different opinions of big COMP holders and for that I sincerely thank him because his work and its direction will get a % of the amount I lost back.\nNow to the main points he is making I disagree in particular with:\n\n\nThe categorisation of users. “Some of the largest liquidated accounts were using the protocol in a way that is not intended”. If this is not intended why do not you said so? And by “you” I mean compound in the MAIN PAGE. Secondly why dont you try to discourage it? Even now if you think that users should not be able to borrow and loan the same asset then code it like that or at least put a warning when users do it to say that this is not allowed/intended whatever. If this needs fixing then fix it. But doing nothing and afterwards using this as an excuse to divide us is not fair at all. Some would even say it is illegal.\n\n\nThe way this categorisation is supported by pseudomath. By pseudomath I mean the creation of the recursive factor that has generalisations and assumptions and does not describe specific important parts of the situation: a) Assumption: Someone that supplied USDC/USDT and borrowed DAI by leveraging his position is automatically named a “farmer” with recursive factor of 1. That is not true. She could be short DAI in that situation. Actually that is the only thing that we can say for certainty for this user “she was short DAI with leverage, DAI went to 1.3 USDC and she got liquidated”. The math formula takes for granted that this would not be the case at all. She is a farmer period. I agree that most probably she was but not 100%!  b) Hiding. The hiding of the riskiness of a users position. Does it matter if one would get liquidated when DAI went to 1.05  as opposed to a user that got liquidated because DAI hit 1.2? I would say YES. Because we are talking about some very different users here. One was prudent and the other was a gambler. DAI can move up and down but not to 1.2 or 1.3 (at least without messing the whole defi sector). The above math formula makes no such distinction on the repayment from what I can tell. Which user behaviour is healthier for the compound protocol?\n\n\nThe only category of users that has a 100% probability being a farmer is the one that supplied asset X and borrowed the exact X only. That is self explanatory I think. Note that those guys did not get liquidated…\n\nI particularly dislike the fact that the above does not answer the simple question: How much money the 120 or so users lost in this event and how much money this proposal returns to them (as a %). For me PERSONALLY it makes a hell of a difference to think/say “compound community covered their users by giving them back 30%-50%-100% of the money they lost and fixed that shit ASAP” versus “compound community gave 2% back to their users and still does not know if there is anything to fix”.\n\nSo, I see that the interests of big COMP holders and compound users are not really alligned in this and I will repeat that this saddens me as I believe this protocol is the gem of defi with the best security around that I STILL use. This saddens especifically when I see protocols like harvest.finance or YFV that try to compensate their users when they lost money without their fault without trying to divide them.\nWhat I suggest?\nFor starters I would suggest, first of all, to fix the oracle or whatever needs fixing so this does not happen again. After that I would suggest the big holders of COMP that decide all the proposals to discuss themselves if and how much money they want to give to us to help the trust of the compound protocol. After all, they created this protocol and/or took the risk to invest in it and guys like @wario busted his ass to put this proposal forward so as to reimburse guys that have contributed nothing to compound (like me) and just came to use it and profit from it by farming some, hodling a little and hedging a lot.\nWhatever you big COMP holders decide do not divide the users because that is certainly “not alligned with the protocol’s long term health”.\\nHi @Azalin, thanks for your continued feedback.\n\nThe categorisation of users . “Some of the largest liquidated accounts were using the protocol in a way that is not intended”. If this is not intended why do not you said so? And by “you” I mean compound in the MAIN PAGE\n\nIt is not fair to say that this is not discouraged, it is in fact impossible to do recursive farming using the standard Compound UI, people doing so are interacting with the contracts directly. Aside from people recursively farming with 2 different stablecoins, which is still possible from the standard UI.\n\na) Assumption : Someone that supplied USDC/USDT and borrowed DAI by leveraging his position is automatically named a “farmer” with recursive factor of 1. That is not true. She could be short DAI in that situation.\n\nYou are correct. In fact, I brought up this precise situation in the governance Discord, and should have included in the forum post. I’m fine with making that assumption, as it seems pretty clear to me that the person in the example is not “short DAI”, but just farming COMP, but I agree that there is no way to prove this.\n\nb) Hiding . The hiding of the riskiness of a users position. Does it matter if one would get liquidated when DAI went to 1.05 as opposed to a user that got liquidated because DAI hit 1.2?\n\nThe riskiness of each account is not the main intended purpose of what I’m proposing here, though it is implicitly accounted for. Given two recursive farmers with the same assets, the one with a higher leverage would naturally have a higher recursive factor since the difference between their supply and borrow will be smaller than the user with lower leverage.  However, if the community thinks this is important to account for, this formula could be easily amended to take into account the account’s health to explicitly factor in riskiness. I actually think this is a good idea.\n\nThe only category of users that has a 100% probability being a farmer is the one that supplied asset X and borrowed the exact X only. That is self explanatory I think. Note that those guys did not get liquidated…\n\nSome of these accounts did get liquidated, in fact that was the precise case with the largest farming  operation, given it was mostly doing recursive farming with DAI, but also had a small proportion of their collateral in USDC. If we count each stable coin separately, while using the same proposed formula, this account would still have a high recursive factor, though not pretty much 1 which is the case now when counting all stablecoins as a single asset.\n\nI particularly dislike the fact that the above does not answer the simple question: How much money the 120 or so users lost in this event and how much money this proposal returns to them ( as a % ).\n\nI understand and agree, and that is why I specifically suggested in the post this could be done as future work. Along determining what accounts would have gotten liquidated anyway regardless of DAI price staying on par with global pricing, as other collateral assets were crashing in price during the event.\n\nWhatever you big COMP holders decide do not divide the users because that is certainly “not aligned with the protocol’s long term health”\n\nI disagree with the suggestion that there is no difference between normal users and recursive farmers. Farmers are not providing anything to the protocol as they are both supplying and borrowing their own capital, and the interests they are paying is a tiny fraction of the COMP value they extract from the system. Perhaps more importantly, they are diluting the COMP incentives that would otherwise go to normal users. Today the vast majority of COMP rewards is most likely going to these farming operations, gaming the incentives at the expense of normal users. I do not understand why Compound users are not up in arms about this situation, I suspect it may be because it has not been measured publicly. I hope to start another thread in that regard and suggest a way to remedy the situation.\\nSounds like there are two separate issues here.\n\nWhat is the raw damage as a %.\nWhat modifiers should apply to this based on user behavior (e.g. recursive-factor)\n\nFor 1), the value should be computable based on the liquidation transactions and should include the value of the real damage in the false liquidation (what was sold to cover the manipulated DAI + the 8% penalty) minus the value of what would have been lost had the position been legitimately liquidated at real asset prices.\nOnce you have that value, we can discuss whether or not there should be a modifier applied to it like this recursive factor.\\nI think that If there is no community agreement in that there is a fundamental and objective difference between normal users and recursive farming operations, then there isn’t much point in calculating off-price damages at all, as that would only increase the originally budgeted compensation that has already been voted against.\\nI appreciate your time spent to balance a compromise between users and VC funds, but the calculated compensation is charity and humiliating for affected users.\nIt is difficult to categorize users and track how much their collateral values have fallen per asset, because the liquidation would not have taken place if the DAI had not been manipulated.\nNow we are trying to show via webapp how bad the user’s risk management is in order to solve the problem with CHARITY which realistically almost covers the costs of the gass fee in an attempt to save the position.\nUsers are not at a loss because of their risk management nor can you prove it with math because the problem is created by GLOBAL DAI PRICE vs COINBASE DAI PRICE. Any attempt to hide the manipulation and portray it as an internal problem puts the blame on a poor price discovery solution.\nThis situation has shown that a relationship between PROTOCOL OWNERS and USERS has been created which is equivalent to the relationship in traditional finance and business.\nYou can’t buy users with a few dollars, users are participate with confidence in the protocol and in the way it should work.\\nAgree with you, cover false liquidation cost + 8% is fair solution. Reducing that amount with pseudomath isnt solution.\\nThere is no proof that price was manipulated. Even if other exchanges did not show the spike, it does not prove manipulation.   One might be able to say that the Coinbase orderbooks were inadequate to take that volume, but then the VOL during the 2hrs during that spike 2.37M, 1.79M.  So it doesn’t even seem that their books were thin.\nRight now that spike would only amount to being circumstantial and would need further proof to show actual manipulation.\nTo note:  The 24h volume of the top5 exchange with direct dai - usd/usdc/usdt pairs are as follows:\n\nCoinbase DAI/USD - 6.881m usd\nBinance DAI/USDT - 6.380m usd\nCurve  DAI/USDC - 4.55m usd\n1inch DAI/USDC - 3.14m usd\nOkex DAI/USDT - 2.38m usd\n\nOne might give that using only Coinbase as the sole price oracle is inadequate, but there being a single price oracle is widely known. And one would expect those who did not use the default compound interface to do further research while going into something risky (Perhaps loop farmers were thinking this was virtually 0 risk return, but then you would probably have come across Dai’s black\nThursday and how MakerDao voted not to compensate liquidated vaults) .\\nThere is no proof because price discovery source is centralized?\nIf you think this is normal market situation then we all are in trouble.\n\n\n\n tonyotonio:\n\nOne might be able to say that the Coinbase orderbooks were inadequate to take that volume\n\n\nthan why compound protocol take that oracle solution?\nIf Compounds outstanding debt in DAI caused this event then we don’t have a reliable protocol.\n\n\n\n tonyotonio:\n\nTo note: The 24h volume of the top5 exchange with direct dai - usd/usdc/usdt pairs are as follows:\n\nCoinbase DAI/USD - 6.881m usd\nBinance DAI/USDT - 6.380m usd\nCurve DAI/USDC - 4.55m usd\n1inch DAI/USDC - 3.14m usd\nOkex DAI/USDT - 2.38m usd\n\n\n\nTo note,\nsomebody liquidated my collateral in ETH so info above are useless. I run normal business and why I need to put stablecoin in collateral if I borrow stablecoin?\nAnd why i was not able to protect my position at that moment? Why gas fee go up to 500 gwei and higher?\n\nSpike only on one exchanges\nProtocol use only that price discovery source\nSpike in DAI which is the largest  pool on the protocol\nFront-running elements in liquidation\nDAI spike last only about an hour ( enough for a hack )\n\nAnd you tell me this is normal market behaviour?\nWhat caused this DAI spike in Coinbase?\nWhy somebody buy DAI for 1.3$ if an another place can buy for 1$?\nYou don’t have arguments about this event and then the conclusion is that for example Coinbase price of  ETH can go from  500$ to  10$ at only one source for just an hour and that I need to accept that situation because I knew about the “oracle bug” on the protocol?\nIn your statement, you mean that this protocol was made for liquidators?\nA lot of you are scattered with big statistics that mean nothing when the situation is simple.\nAlso simple is reason why “community” vote against compensation:\n\nCompensation in COMP does not fit for VC funds and other big COMP holders\nExcept COMP we dont have resources for compensation\n\\nHi darbar, my intention is not to reduce compensation, but to separate users from farming operations that were using the protocol in a way that was not intended in order to game the incentives via farming which is not positive for the protocol and net negative for normal users who get diluted from rewards. I’m sorry that you feel this way and have to assume you must be in the farming category. I believe a significant part of the community has shown that they are not willing to bail out this type of users, and with good reason. I am hoping that a compromise can be reached so that normal users are not treated in the same way as users who were intentionally exploiting the incentive program for their own benefit by performing wash loans and knowingly keeping their accounts for months previous to the incident at unhealthy levels.\\nI am not in farming category. I am using protocol to stay liquid in hardware mining business.\n\n\n\n wario:\n\nI am hoping that a compromise can be reached so that normal users are not treated in the same way as users who were intentionally exploiting the incentive program for they own benefit by participating in wash trading and knowingly keeping their accounts for months previous to the incident at unhealthy levels\n\n\nI agree with you.\nBut without users in the farming category, protocol wouldn’t have this market cap and TVL.\nI have noticed before that there is no mechanism to regulate the “unhealthy” behavior of yield farmers especially since the hype has been built on that.\nIn the end, the real users are damaged.\\nI don’t think it is clear that spuriously inflating TVL via wash loans is good for COMP’s market cap and we must also consider that the constant dumping of COMP by farmers implies a significant amount of sell side pressure on the markets. If you are not in the farming category then what I am proposing would help in recouping some of the unfairly liquidated collateral due to Coinbase mispricing. I understand that the 8% incentive is less than what you might have lost due to the offprice liquidations, but believe this is better than no compensation at all.\\nI agree with you and appreciate your commitment in this case and I understand that it is difficult to align the interests of users and COMP holders.\nHowever, it is difficult to come to terms with the fact that the Compound protocol is centralized and is mentioned everywhere as a top DeFi project.\\n\n\n\n dabar90:\n\nThere is no proof because price discovery source is centralized?\n\n\nRight now the only “proof” we have as far as I’m aware is from this post from CB’s trade surveillance team, which I’m not entirely sure is verified. Regardless, definitely a little concerning to rely on such a centralized source.\n\n\n\nDAI Liquidation Event\n\n\nCoinbase looked at the data from many different angles and concluded that not only were there no price manipulation alerts in our trade surveillance software, but we did not find any evidence of collusion or single actors that pushed the price higher. We believe our books properly operated according to the availably liquidity at the time which, in stablecoin markets, may become thin as price moves away due to the collateralized nature of these markets. And, Coinbase Price Oracle accurately reported data throughout the DAI price increase.\n\n\\nI took a stab at modeling out this strategy.\nThe way I see it there are 4 questions we should reach agreement on in regards to compensation.\n\nWhat was the actual damages in raw numbers\nWhat percentage should be compensated (The proposal that @kybx86 suggested was intended to cover the liquidation incentive of 8%)\nWhat modifiers should be applied based on user behavior (@wario is suggesting modulating by how recursive the borrowing scheme is)\nHow to distribute it to affected users. (@kybx86 proposed using COMP, other proposals involve DAI reserves. The most accurate to the true damages of the event would be to use the asset that was seized in false liquidations)\n\nThis is a strawman model that is intended to get the methodology for a direct computation of damages correct. It uses input values derived from @wario’s data, the minimum USD values for assets on the day of the exploit. A real computation would use on-chain data, but should be in the same ballpark. This was just to determine a good methodology and if we move forward we can work on using accurate data to determine the final raw damages result.\nThe model divides users into two classes. Those that would not have been liquidated using correct DAI prices and those that still would have. The damages for those that would have not been liquidated is the extra collateral that was taken due to the incorrect DAI price plus the liquidation penalty from the false liquidation. For those that would have been liquidated anyway, the damages are simply the delta between the liquidation penalty from the liquidation that would have occurred and the liquidation penalty that liquidation that did occur. Let me know if this sounds correct.\nscript: https://pastebin.com/DcsXXxcJ 4\noutput: https://pastebin.com/y9s6997E 5\nThe total raw damages from false liquidations comes to around $7.5M direct loss of user value. This model also takes into account DAI-DAI liquidations, which results in less direct damages for the “looper-whales”, which was an objection to the previous proposal, so that worked out nicely.\nThe counterfactual of who would have been liquidated is the trickiest part of this model. Using @wario 's data and coingecko prices, there are only 7 users that would have been liquidated if DAI prices were correct and only one had > 5 figures liquidated. Since this real computation of this counterfactual requires assuming no user transactions, then it may be fairer to ignore it as it will have relatively minimal impact. This is something to decide if we move forward with it.\nOnce we have a good method for computing raw damages, we can then decide, the other points (%, modifiers, distribution method).\nAlso per the discussion above, even if there was no manipulation (which seems extremely unlikely and can only really be determined through the legal system) then there was still an oracle failure leading to false liquidations (otherwise, why bother fixing it if its not broken?). Compensation is part of the fix.\\nI would just like to clear up that this is not my data. What I provided was simply the state of liquidated accounts at block 11332733 taken from the Compound API’s account endpoint.\\nSome follow-on thoughts. It seems 1) (and potentially some modifications from 2) and 3)), and solving 4) by drawing from the DAI reserves (while simultaneously derisking DAI), would go a long way to resolving criticisms brought to the previous compensation proposal that prevented its acceptance.\nI would also like to mention the option of compensating in the original asset seized in the false liquidation as this would offset any opportunity cost damages not accounted for in my computation, but is as easily computable as USD is.\nFactoring this in results in the following https://pastebin.com/9CmuhcsA 4\nNext steps will be deciding on 2, 3 and 4 and crafting a new proposal.\\nCompound must compensate ,its not the fault of the investor due to price manipulation.\nCompound did not warn investor of the risk at their website otherwise i would have not used compound.\\nCan you explain to me why it is not ok to supply USDC / USDT and borrow Dai? For example, people may want some Dai to supply into Uniswap as an additional asset pair, but didn’t want to actually trade their USDC for Dai so they borrow it. Is that not a valid use case / legit purpose? Is the only use case that is aligned with Compound's future to long / short a certain asset?\\nIt seems you are proposing that people would want to pay 5.4% interest to the Compound protocol in order to borrow one stablecoin using another stablecoin as collateral to provide liquidity to Uniswap. Seems like a pretty bad financial decision to me. This contrived example also does not explain why the people doing these stablecoin to stablecoin loans would then do it recursively again and again.\\nThis is actually a very common trade for Dai. Arbitrageurs borrow Dai frequently with the intention of selling it high and buying it back lower, or vice-versa. This trade is the primary method by which Dai keeps its soft-peg to the USD. If the annualized return of arb’ing the Dai peg is greater than the APY of the loan, the arbitrageur makes money. Note that in theory this trade could be put on for any pair of stablecoins.\nI’m not saying that this point is particularly relevant to this conversation, but it can make sense for traders to use stable assets as collateral for loans against other stable assets.\\nWhy wouldn’t they just mint DAI from the Maker protocol itself given that the stability fee for minting DAI with USDC collateral is currently 0%?  I understand it could be argued that some uses cases exist for wanting for borrow DAI with USDC collateral, like the shorting DAI example explained some posts above, but being real that is not what is happening here.\\nThe stability fee on USDC was 4% up until last week. Other platforms can offer better LTV and different incentives. I’m not saying it made sense in this specific situation, just that money markets of exclusively stablecoins can exist organically.\\nTrue, point taken. I still maintain that this is obviously not what is happening here, what is happening is an a clear exploitation of the COMP distribution program. And it’s also mostly done by supplying and borrowing the same stablecoin asset where this use case does not apply.\\nI don’t know why this is a bad financial decision. Maybe I am just dumb? Right now Dai interest is 5.4% but it wasn’t that high before. It was about 4%. You also earn interest when you supply USDC and it’s around 2 - 4%. And you always supply more than you borrow. So net you are not really paying much interest, if any, borrowing Dai, as long as you supply enough USDC. And you can end up with diversified Uniswap pairs. And you earn a little bit of Comp on the side. Maybe not the smartest thing to do with your stablecoins; there are probably better ways to get Dai out there; but it is easy enough to do it on Compound, and it seems like a “safe” enough strategy with some returns because you have the least risk of being liquidated since both sides are stable coins… but obviously not lolllll. And now you get fucked, plus you also get people trying to paint you as a “yield farmer”.\nIn general I just find your definition of “yield farmer” very up to your own interpretation of what you think is ok vs not. Why do you get to define what is the “right purpose” when you borrow an asset? Also, let’s be real, Compound giving out Comp for participating users is in the equation for everybody, not just the “yield farmers”. I don’t know why you think some people are more noble than others.\\nAgree with you.\n“Healthy” and “Unhealthy” strategies are a subjective terms. In case that DAI went in the opposite direction, we will be talking about a “great” strategy.\nFor COMP holders stablecoin pair farming is a great thing when TVL and market cap come into question, but when manipulation on protocol happens and users are robbed then they put the blame on the user’s risk management.\nI personally dont use this strategy, but if protocol allows users to perform that strategy to pump TVL and after error declare that strategy is “unhealthy” then we have problem in protocols incentives mechanism.\nIn this case of manipulative liquidation, it is obvious that the position of the COMP holder is defended to the detriment of the user.\nIf a protocol error has occurred why do only users have to bear the cost of that error?\nIn every future hack/manipulation/explotation we can expect the same reaction from COMP holders?\\nYou are ignoring the fact that most of these farmers are using the exact same stablecoin to supply and borrow, including the largest amount of capital in this set of liquidations that is just 1 such farming operation, with 100M+ of DAI and only around 16M USDC collateral for its 80M+ DAI loan. I don’t get to define anything, I just point out that these operations are exploiting a program intended to distribute COMP governance tokens to its users. If you don’t see how this practice is exploitative, I honestly don’t know what else to tell you, but a protocol that only allowed these types of same-asset wash loans would be absolutely useless so it can’t be its intended purpose.\\nYou great point the problem and question are why Coumpound tokenomics allow that action?\nI think that stablecoin pair farming work great for COMP holders, because it has a positive effect on key metrics and COMP price.\nHow big would a market cap be without stablecoin farming?\nWith a given interest rate on non-stablecoin crypto-assets, Compound would not perform that well.\\nWell so I am telling you I am in this discussion because my address is classified by you as having a recursive factor of > 0.5 so obviously I am a recursive yield farmer, but I am not. I only supplied USDC and borrowed Dai, and all the capital I supplied was not leveraged again, so your method has some flaws. Sure there are people that may be “exploiting the program” whatever the definition that is, but just trying to tell you there are also possibly usecases outside your limited imagination, and it is not appropriate to classify people using a flawed blanket policy.\nI am also not sure why I need to be here defending myself or others. It almost feel like people being raped and then having to argue with people whether they are supposed to be raped because they exposed too much skin or are out drunk late at night etc. It is a distraction of this particular Dai incident as well. If you are not happy with recursive yield farmers, you should propose to change the incentive of the program and yield farmers will naturally move away. Investigating the effected users in this incident and not compensating their losses based on a random rule is not the solution here.\\nYou are right that the proposed method is making an assumption that would classify as recursive farmers people in the position you describe, and agree it would be wrong if that is the case. Perhaps there is indeed organic demand for that type of stablecoin - different stablecoin loan beyond farming COMP. One possible alternative would be to not bundle stablecoins together and only count recursive farming with each specific asset, I assume then you personally would have no problems with that given your explained situation? In either case, it seems that there are more voices against the idea of identifying recursive farmers that there are supporting it. I hope perhaps some other proposal might find some form of consensus that results in compensating users who were unfairly liquidated, and were also not exploiting the COMP distribution program given that was one of the reasons cited by different community members for voting against this proposal was not wanting to bail out those types of operations.\\nIs there any other source for compensate users beside DAI reserves?\nAre the reserves of other assets also considered? For example ETH - because in my case I was liquidated in ratio 1:377 (ETH:DAI). It will probably never be possible to buy ETH at that price again\nI understand that users cant be compensated in COMP and it is fair, users didn’t lost COMP tokens(mostly) in “DAI event”. But do you think that 8% isn’t too low?\nDo we have any other options as affected users?\nI also understand that full compensation is not possible, but covering only the liquidation premium is not acceptable. I perceive this as a poorly improvised insurance policy and the costs of liquidation are much higher.\nI am personally only down $ 70 because of front-running (my attempts to defend the position).\\n\n\n\n 4D_compound:\n\nThe model divides users into two classes. Those that would not have been liquidated using correct DAI prices and those that still would have. The damages for those that would have not been liquidated is the extra collateral that was taken due to the incorrect DAI price plus the liquidation penalty from the false liquidation. For those that would have been liquidated anyway, the damages are simply the delta between the liquidation penalty from the liquidation that would have occurred and the liquidation penalty that liquidation that did occur. Let me know if this sounds correct.\n\n\nThis totally makes sense\n\n\n\n 4D_compound:\n\nOnce we have a good method for computing raw damages, we can then decide, the other points (%, modifiers, distribution method).\n\n\nAgree, this should be the next step. What is the fairest way to calculate a real loss due to liquidation?\n\n\n\n 4D_compound:\n\nAlso per the discussion above, even if there was no manipulation (which seems extremely unlikely and can only really be determined through the legal system) then there was still an oracle failure leading to false liquidations (otherwise, why bother fixing it if its not broken?). Compensation is part of the fix.\n\n\nGreat conclusion, we were really spinning in a circle over whose fault it was.\nI was expected more clear and direct arguments from people who are against compensation in this case. Not because of the compensation itself but because of the general security of the protocol.\nI have been using Compound for more than a year and have not considered other options solely because of the protocol’s reputation for security.\nAfter all words here on the forum about “bug”, lately whitepaper of Compound Chain presented the same price discovery solution?\\n\n\n\n dabar90:\n\nAfter all words here on the forum about “bug”, lately whitepaper of Compound Chain presented the same price discovery solution?\n\n\nThat’s what I’m seeing too. It appears as though the validators will act as the “oracles,\" publishing data to each block (as an extension of the Open Price Feed). While I see how this results in fresher data as it no longer requires active posting, our market coverage appears to remain unchanged, so I assume we are still open to the same price manipulation. Also, I’m unclear if Uniswap TWAP will still be in place as a backstop, and how would that work in this environment if so.\nI know I sound like a broken record, but offloading this oracle concern to a 3rd party sounds like the prudent move, especially now given how big of an undertaking building a new chain alone is.\\nI am guessing this has been abandoned.\\nThis particular compensation proposal (#032 2) was voted down on December 14th. However, users like @wario have been looking at alternative methods for determining compensation that have been gaining traction. Perhaps they could fill you in on progress there?\nI personally think the oracle is the primary issue of concern and should be addressed first, but I have nothing against those affected being compensated given that this was an oracle failure.\\nI will be writing up an idea for a proposal soon and then possibly a formal proposal if it looks good.\nFalse liquidation compensation can happen in parallel while those with more technical knowledge address the faulty oracle issue.\\nAre we giving up on fixing this issue and making things right for affected users?\\nPosted an informal proposal that addresses the feedback from the previous proposal DAI Liquidation Event - #53 by 4D_compound 24\\nAgree, also we have problem with protocol governance decentralization. Early investors use token like a stock share and they don’t care for small users (small from their perspective).\nUniswap airdrop model is maybe solution (with few modifications) because in the existing model the average users are left at the mercy of VC funds.\nI think @wario  is doing its best to establish an equilibrium between users and large holders but the problem is that there are currently not enough DAI reserves to compensate. I think the proposal by @4D_compound and @kybx86 is a very good solution. In case the available funds are not sufficient from the DAI reserves @michjun stated a possible solution:\n\n\n\nDAI Liquidation Event\n\n\nhe only thing is from what I can tell, there is only ~2M in Dai reserve right now, so the compensation may need to be distributed over time\n\n\nDistribution over time is a quality solution for the long-term interest of the protocol as it will provide security and keep users and their resources on the protocol.\\nCan’t Compound notify Chainlink of it’s incorrect price feeds?  Chainlink is supposed to slash rewards of malicious oracles or oracles who give incorrect data.  Where do those slashed rewards go?  Shouldn’t they go to the projects/people who were affected by the mistake?\nIf not, then:\nIs it possible to use the interest generated from the DAI reserve to partially compensate the addresses in which were liquidated?  Maybe could compensate them upon acceptance of the proposal from the interest held in reserve, then allow the interest to build up for 6 months - 1 year and have another distribution of DAI (or cDAI).\nCould even distribute the amount of COMP those users would have received from supplying/borrowing DAI since the ‘liquidation event’.  Just giving those accounts COMP for compensation just doesn’t make sense to me.  Especially since whoever was the bad actor on Chainlink could be one of the accounts that was liquidated, giving them more governance ability on Compound could be just what they want.\\n\n\n\n ekrenzke:\n\n\nThis proposal provides reimbursement before a clear path to fixing the issue has been established. We would like to see a little more clarity around potential solutions before finalizing any form of reimbursement.\n\n\n\nDepending on how often the Compound protocol gets price information from oracles, we could do an average or exponential average over the last ‘n’ prices.  I would say that at least 10 of the last prices from the oracle(s) would be needed to minimize liquidations during extreme flash crashes/pumps/market manipulation.  The side effect would be that Compound’s asset prices could be estimated.\nAnother solution that may help is to check prices on multiple markets, DAI/ETH, DAI/USDC, DAI/USD and weigh them equally.\nOr, to keep things simple. Stablecoins tied to USD are usually within a percent and are designed to be as close to the dollar price as possible.  So make their value be $1.  If Compound is getting prices from Coinbase, then $1 USDC is already equal to $1 USD. This would probably lower gas fees as well since there is a lot of code within the Comptroller’s smart contract making sure the market price has not changed.\\nI will also post info from @kybx86 here. I ask the community delegate votes.\n\"The CAP has been deployed here  8.\nThis CAP needs 65k COMP delegate votes until it becomes a formal proposal.\nIf you support this proposal, please consider delegating to it.\nVIEW CAP & DELEGATE  8 \"\\n@Dmitry Thank you.\nThe CAP is live and it needs votes to get to 65K so it can be made into a formal proposal. The CAP already has 3 votes and ~12K COMP, please delegate 5 to it to get it across the line:"
  },
  {
    "number_of_comments": 10,
    "postid": "90f076da-b812-4bfb-9761-fb953f32bc83",
    "posturl": "https://www.comp.xyz/t/compound-v3-weth-comet-risk-parameter-updates-2023-02-08/4053",
    "combinedcontent": "\nSimple Summary\nA proposal to adjust one (1) risk parameter (supply cap) across one (1) Compound V3 wETH Comet asset.\n\nSpecification\n\n\n\n\nParameter\nCurrent Value\nRecommended Value\n\n\n\n\ncbETH Supply Cap\n20,000\n30,000\n\n\n\n\nAbstract\nThese parameter updates are a continuation of Gauntlet’s regular parameter recommendations as part of Dynamic Risk Parameters 2.\n\nMotivation\nThis set of parameter updates seeks to maintain the overall risk tolerance of the protocol while making risk trade-offs between specific assets. Gauntlet has published a blog post 1 on our parameter recommendation methodology to provide more context to the community.\nTop 30 borrowers’ entire supply\n\n3398×1008 281 KB\n\nTop 30 borrowers’ entire borrows\n\n3398×1002 269 KB\n\nThe cbETH supply cap of 20,000 is 99.8% utilized, and we propose increasing the supply cap to 30k.\nDaily open, high, low, close ratio between cbETH and WETH from 2022-08-24\n\n1300×756 89.6 KB\n\ncbETH-WETH price has been trending upwards, reflecting the market’s confidence in a successful Shanghai upgrade rollout. As a result, there are decreasing depegging risks.\nAdditionally, liquidity for cbETH/ETH is sufficient as of 2023-02-08, with 4k cbETH able to be swapped for ETH at 1.6% slippage across DEXs, as shown below. This allows liquidators to quote blocks of 4,000 cbETH from the protocol and sell with a profit after discount.\n\n942×878 77.8 KB\n\n\nNext Steps\n\nTargeting on-chain proposal next week.\n\nBy approving this proposal, you agree that any services provided by Gauntlet shall be governed by the terms of service available at gauntlet.network/tos.\n\\n\nRecommended Value 35000\\nThe current market dynamics is confusing.\nOne can deposit cbETH with a LTV of 90% with emode enable and borrow ETH at an apy of 3.67%.\nWhile on compound the borrowing cost is 4.48%.\nThe deposits capacity has not been reached on AAVE. So I don’t see any incentive for ppl to borrow more on compound in the near future.\\nI agree with @RapidsCapital, and I think that there is room to decrease a little bit the borrowing rate at optimal utilization. What does Gauntlet think about also updating the IRM, now that we have a little bit more data on this market ?\\nEmode borrowing at 90% LTV on Aave is only for wstETH and is not available for cbETH\\nwhen we are going to vote on this?\\nProposal is now up:\n\n  \n      \n\n      compound.finance\n  \n\n  \n    \n\nCompound 13\n\n  Compound is an algorithmic, autonomous interest rate protocol built for developers, to unlock a universe of open financial applications.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\\nGiven the recent developments, wanted to provide some additional color for transparency to the community:\n\nEven though DEX liquidity has dried up recently, there is still high liquidity ($15M) on CEXs.\nOur models show that it would require around > 7% depeg for insolvencies to occur.\n\nAs such, we will not pull down the proposal. If the community has a particularly conservative risk tolerance (e.g., if the community wants to heavily discount CEX liquidity), then the community may wish to vote no on this proposal.\\nTo clarify: the on-chain vote for this forum post, which is Proposal 149 1, accidentally links to the wrong forum post. Proposal 149 is the on-chain vote for this forum post.\\nIn favor of this proposal.\n\nLiquidity for all of DEX pools containing cbETH is standing at $24M as of right now.\nSlippage for cbETH → USDC is sitting under 2% for trades up to $8M. Liquidating the biggest cbETH collateral position wouldn’t be a major issue.\nIncreasing supply cap doesn’t pose significant risk for the protocol given current market usage.\n\nSource: cbETH risk assesment on Warden Finance platform 5\n\nScreenshot 2023-02-16 at 4.19.31 PM1300×1271 104 KB\n\\n\nimage728×350 32.7 KB\n\nRecommended Value 38000"
  },
  {
    "number_of_comments": 14,
    "postid": "aa744215-78c0-4cc4-8dd8-4e7554894a02",
    "posturl": "https://forum.makerdao.com/t/gait-follow-up-synthesis-document/22733",
    "combinedcontent": "Hi @BLUE @JanSky @ldr @cloaky @0xDefensor @ecosystem-team @vigilant @truename\nThe next 2 weeks are to be focused on spinning up working groups. One auxiliary to-do is choosing quality data from last week’s submission for inclusion in the synthesis document 5. If you’d like, please submit a limited amount of high-quality selections in a reply below. (Here is an example of how to format your submission.)\nPlease submit by EOD UTC Sunday, Nov 19th, so that we have time to compile into the doc.\n(We’ll ping you here and on Discord to invite you to our next sync meeting.)\\nHi @BLUE @JanSky @ldr @cloaky @0xDefensor @ecosystem-team @vigilant @truename\nBonapublica AD has the following repositories available for your consideration Github 2\nFor node A.1.4.3, A.1.4.4, and A.1.4.5 we have created the following:\n\n\nElement Analysis: Node A.1.4.3\n\n\nFacilitatorDAO Action Examples: A.1.4.3.0.3\n\n\nElement Analysis: Node A.1.4.4\n\n\nFacilitatorDAO Action Examples: A.1.4.4.0.3\n\n\nElement Analysis: Node A.1.4.5\n\n\nFacilitatorDAO Action Examples: A.1.4.5.0.3\n\n\nVisit our repository 1\nNext Key Objective: Rewrite Article 4:\n\n\nRevised A.1.4 for NextGen Atlas Json-WIP.\n\n\nRevised A.1.4 per intructions.\n\n\nAdded Supporting Root for Child Documents and FacilitatorDAO Action Example Directory.\n\n\nCreated 10 Element Analysis\n\n\nVisit our repository\nNext Key Objective: Rewrite Article 5:\n\n\nRevised A.1.5 per intructions\n\n\nAdded Child Documents to A.1.5 (missing from synthesis document)\n\n\nAdded Supporting Root, and FacilitatorDAO Action Example Directory\n\n\nCreated 3 Element Analysis\n\n\nCreated 3 FacilitatorDAO Action Examples\n\n\nVisit our repository\n\nGPTS\nWe are also developing our personal GPTS, aiming to transfer as much as possible from the old Atlas to our own JSON-WIP. While this process is not yet perfect and requires further refinement, it serves as an effective practice method for learning how to transmute from the old Atlas to the NextGen.\nNextGenAtlas GPTS1055×685 51.1 KB\nResilience Fund928×644 106 KB\nElixir925×657 85.7 KB\\nI believe there may have been a misunderstanding regarding the purpose of this post. The intention is for those who are tagged to nominate last week’s submissions of others for inclusion.\\nWe know that. During Friday’s GAIT call, ACs were asked to do hands-on work during the current down time, and we went ahead and grounded last week’s submissions and expanded our knowledge with Element Analyses and Action Examples. Our attempt here is to produce high quality work, and have the teams tagged here review the work. Our sole motivation is to contribute to the data integration phase.\nThank you.\\nHere are Cloaky’s nominations:\n\n0xDefensor Element Analysis - [Target Document A.1.5.1] “Prescriptive and self-justifying methods”\n(Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #6 by 0xDefensor)\n0xDefensor Element Analysis - [Target Document A.1.5.2] “Serves as a directive”\n(Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #6 by 0xDefensor)\n0xDefensor FacilitatorDAO Action Examples - [Target Document A.1.5.2] Both Action Examples under this Target Document\n(Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #6 by 0xDefensor)\nEcosystem Team FacilitatorDAO Action Examples - [Target Document 1.4.3] Both Action Examples under this Target Document\n(Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #9 by ecosystem-team)\n\\n@ldr @JanSky @cloaky @Le_Bateleur @votewizard okie mayve you pik me this time i make element analysis perfect . I think i am expert now please look at my work i spend last night and this morning makin perfection please review my work\n{\n\"A.1.5.4\": {\n        \"Name\": \"Aligned Voter Committee (AVC) Active Status eligibility requirements and internal processess\",\n        \"Version\": 1,\n        \"Type\": \"Section\",\n        \"Components\": {\n            \"Content\": \"AVCs must follow a standardized internal governance process for determining membership, creation of Aligned Governance Strategy, Aligned Scope Proposals and other decisions. This process is contained in the Aligned Voter Committee Active Data Section. \\n AVC Members vote on AVC decisions based on their relative ownership of verified MKR. A vote takes one week and resolves with acceptance or rejection based on the amount of verified MKR that votes for each option, unless a simple majority is reached in which case the decision is accepted or rejected instantly.\\n New AVC Members are included based on an application that is either accepted or rejected through an AVC decision. The effective verified MKR of members is specified in their application, and can only be increased or decreased through another application that must be accepted by another AVC decision. The effective verified MKR of the creator of an AVC is determined when they create the AVC. If an AVC Member’s blockchain account has MKR in excess of their effective verified MKR, they don’t gain additional voting power in the AVCs internal governance process, but it does count towards AVC rewards qualification. If an AVC member's blockchain account has less MKR than their verified amount, they are automatically removed from the AVC.\\n Existing AVC Members can be removed through an AVC decision. \\n A minority quorum of AVC Members based on verified MKR can trigger an AVC split. GOV5 must specify the minority quorum to ensure it serves its role well. All AVC Members then vote to support one of two new AVCs, with the naming rights and infrastructure inherited by the side with more MKR in support. The AVC Members of each of the two new AVCs is determined based on who voted to support each side. The Aligned Governance Strategy Links from AD are divided proportionally into two new Protocol Delegation Contract, each containing an amount of delegated MKR that is proportional to the MKR in support for each side of the AVC split. \\n AVC Subcommittee Meetings must be scheduled through an AVC Decision.\"\n        },\n        \"Last_Modified\": \"2023-08-16-19:07:00\",\n        \"Child_Documents\": [],\n        \"links_to\": {},\n        \"linked_from\": []\n    },\n    \"A.1.5.4.0\": {\n        \"Name\": \"Supporting Root\",\n        \"Version\": 1,\n        \"Type\": \"Supporting Root\",\n        \"Components\": {},\n        \"Last_Modified\": \"2023-11-09-00:00:00\",\n        \"Child_Documents\": [\n\"A.1.5.4.0.2\"\n],\n        \"links_to\": {},\n        \"linked_from\": []\n    },\n    \"A.1.5.4.0.2\": {\n        \"Name\": \"Element Analysis Directory\",\n        \"Version\": 1,\n        \"Type\": \"Element Analysis Directory\",\n        \"Components\": {\n            \"List of Elements and their Element Analysis Documents\": {\n                \"Standardized Internal Governance Process Element Analysis\": \"A.1.5.4.0.2.1\",\n                \"Effective Verified MKR Element Analysis\": \"A.1.5.4.0.2.2\",\n                \"AVC Split and Minority Quorum Element Analysis\": \"A.1.5.4.0.2.3\"\n            }\n        },\n        \"Last_Modified\": \"2023-11-17-00:00:00\",\n        \"Child_Documents\": [\n\"A.1.5.4.0.2.1\",\n\"A.1.5.4.0.2.2\",\n\"A.1.5.4.0.2.3\"\n],\n        \"links_to\": {},\n        \"linked_from\": []\n    },\n    \"A.1.5.4.0.2.1\": {\n        \"Name\": \"Standardized Internal Governance Process Element Analysis\",\n        \"Version\": 1,\n        \"Type\": \"Element Analysis\",\n        \"Components\": {\n            \"Element\": \"Standardized Internal Governance Process\",\n            \"Analysis\": \"The Element, \\\"Standardized Internal Governance Process\\\" is a pivotal concept within A.1.5.4 that necessitates precise definition to maintain the integrity and functionality of Aligned Voter Committees (AVCs). This process forms the backbone of AVC operations, influencing membership determination, the creation of governance strategies, scope proposals, and other pivotal decisions. By invoking the term \\\"standardized\\\", the Atlas underscores the imperative for uniformity and predictability in procedures, ensuring that all AVC actions and resolutions are consistent with the established norms, irrespective of the individual AVC. The standardized nature of this process is instrumental in upholding the Spirit of the Atlas, ensuring that all actions and decisions made by AVCs are not only universally aligned with the broad strategic vision of the NEWDAO ecosystem but are also transparent, fair, and neutral, thus eschewing individual biases or incentives that could potentially derail the collective governance mandate. The phrase, \\\"Aligned Governance Strategies\\\" in A.1.5.2 highlights the necessity for AVCs to function within a framework that is not only methodical but also adaptive, allowing for strategic foresight and the ability to address emergent challenges within the NEWDAO ecosystem. Furthermore, A.1.5.1 emphasizes the role of AVCs in monitoring and adjusting the alignment between the various governance facets, which is only feasible through a well-structured and standardized internal process. By embedding this process within the \\\"Aligned Voter Committee Active Data Section\\\", the Atlas ensures that the procedures are not just theoretical constructs but are actively practiced and accessible, fostering a culture of accountability and continuous improvement within the AVCs. In context, \\\"standardized internal governance process\\\" serves as a bulwark against misalignment and malicious acts, fortifying the AVC's capacity to act decisively and with clarity, thereby preserving the equilibrium of power and the overarching objectives of the NEWDAO governance structure. This element is intrinsically linked to the broader sections of the Atlas, including A.1.5.1, A.1.5.2, A.1.5.3, A.1.5.5, A.1.5.6, A.1.5.7, and A.1.5.8, each providing additional context and depth to the AVC's operational ethos. It is through this interconnectedness that the AVCs can embody the collective wisdom and strategic intent of the NEWDAO, ensuring that the governance process remains robust, equitable, and aligned with the long-term strategic goals of the ecosystem.\"\n        },\n        \"Last_Modified\": \"2023-11-09-00:00:00\",\n        \"Child_Documents\": [],\n        \"links_to\": {},\n        \"linked_from\": []\n    },\n    \"A.1.5.4.0.2.2\": {\n        \"Name\": \"Effective Verified MKR Element Analysis\",\n        \"Version\": 1,\n        \"Type\": \"Element Analysis\",\n        \"Components\": {\n            \"Element\": \"Effective Verified MKR\",\n            \"Analysis\": \"The Element, \\\"Effective Verified MKR\\\" within A.1.5.4 is a term that carries substantial weight in the governance structure of AVCs. This element quantifies the influence each member possesses in the voting process, thereby shaping the internal decisions of AVCs. The word \\\"effective\\\" suggests the practical, actionable impact of the MKR, which must be \\\"verified\\\" to ensure its legitimacy and current standing within the ecosystem. This verification is crucial to prevent inflated or misrepresented voting power, which could distort decision-making and lead to misalignment with the broader governance objectives.The verification process serves as a safeguard against potential manipulation or misrepresentation of voting power. It ensures that the MKR counted towards votes truly reflects the current holdings and commitment of AVC members, thereby aligning with the Spirit of the Atlas which emphasizes transparency and fairness. This process also mitigates the risk of \\\"phantom votes\\\" by inactive or non-committal members who might otherwise exert undue influence on governance outcomes. Effective Verified MKR is rooted in the notion that governance power should be proportional to the stake and engagement within the MakerDAO ecosystem. It aligns with the content in A.1.5, which positions AVCs as deeply aligned with MKR holders, ensuring that they act as true conservators of the governance process. The concept also resonates with the principles laid out in A.1.5.1 and A.1.5.2, where AVCs' role in proposing modifications and formulating strategies is predicated on a universally aligned, transparent, and equitable approach to governance.Moreover, the treatment of Effective Verified MKR in cases where a member's blockchain account does not match their verified holdings demonstrates a commitment to fairness and alignment with the Atlas's principles. It ensures that AVC members cannot leverage unverified holdings to influence decisions, reinforcing the need for accuracy and up-to-date verification in reflecting a member's actual influence within the AVC. In the broader context of governance, the element of Effective Verified MKR supports the strategic perspectives outlined in A.1.5.7 and risk management policies in A.1.5.8. It enables the AVCs to vote and operate within a framework that is not only aligned with the MakerDAO's strategic vision but also resilient against micromanagement and biased conditions that could undermine the governance process.\"\n        },\n        \"Last_Modified\": \"2023-11-17-00:00:00\",\n        \"Child_Documents\": [],\n        \"links_to\": {},\n        \"linked_from\": []\n    },\n    \"A.1.5.4.0.2.3\": {\n        \"Name\": \"AVC Split and Minority Quorum Element Analysis\",\n        \"Version\": 1,\n        \"Type\": \"Element Analysis\",\n        \"Components\": {\n            \"Element\": \"AVC Split and Minority Quorum\",\n            \"Analysis\": \"The Element, \\\"AVC Split and Minority Quorum\\\" reflects a crucial mechanism for structural adaptability within the AVC framework. An AVC split is a significant event that bifurcates a single AVC into two distinct entities, redistributing governance power and potentially altering strategic directions. The minority quorum is the specific threshold of verified MKR ownership required to initiate such a split, a safeguard against impulsive or unilateral decisions that could destabilize governance. In the broader context of the Atlas, the AVC's ability to split is an acknowledgment of the dynamic nature of governance, allowing for evolution and diversification in response to the ecosystem's needs. It is a manifestation of the principle that while the AVCs should be cohesive units, they must also be flexible enough to accommodate divergent strategic perspectives when necessary. This ensures that the AVCs remain true to the Spirit of the Atlas, which is to align with the day-to-day Letter of the Rules while also being adaptable enough to incorporate marginal improvements. The minority quorum plays a critical role in ensuring that any split is not just the result of a fleeting majority opinion but rather a substantial and considered decision by a significant minority of stakeholders. It embodies the Atlas's emphasis on detailed Universal Alignment interpretations by stipulating that such a structural change must be rooted in a profound strategic shift rather than transient factors.This element harmonizes with the content in A.1.5, A.1.5.1, and A.1.5.2, where the AVCs' purpose is to enhance the governance process through universally aligned, transparent, and equitable strategies and proposals. It echoes the Atlas's call for AVCs to be governed by principles that prevent micromanagement and preserve long-term alignment artifact strength, as noted in A.1.5.1 and A.1.5.8. The AVC split mechanism and the definition of a minority quorum are fundamental to maintaining a balanced and aligned governance structure within MakerDAO. It supports the strategic perspectives outlined in A.1.5.7 and mitigates the misalignment risks detailed in A.1.5.8 by ensuring that significant governance shifts like AVC splits are executed with a clear, aligned, and justified mandate from the community.\"\n        },\n        \"Last_Modified\": \"2023-11-17-00:00:00\",\n        \"Child_Documents\": [],\n        \"links_to\": {},\n        \"linked_from\": []\n    }\n  }\n\n\n\n\n\n cloaky:\n\n\n0xDefensor Element Analysis - [Target Document A.1.5.1] “Prescriptive and self-justifying methods”\n(Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #6 by 0xDefensor)\n0xDefensor Element Analysis - [Target Document A.1.5.2] “Serves as a directive”\n(Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #6 by 0xDefensor)\n0xDefensor FacilitatorDAO Action Examples - [Target Document A.1.5.2] Both Action Examples under this Target Document\n(Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #6 by 0xDefensor)\n\n\n\n@cloaky this have many many error you mayve ask @ldr to check format and you find many many error parse no good\n\\n\n\n\n Ikagai:\n\n@cloaky this have many many error you mayve ask @ldr to check format and you find many many error parse no good\n\n\nYou are right. There are indeed some formatting errors. I was primarily focused on the content in my nomination.\\nBLUE’s recommendations:\n\n\nBonarepublica Target Document A.1.4.4 “Encouraged Anonymity” Element Analysis\n(Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #8 by Bonarepublica)\n\n\n0xDefensor Target Document A.1.5.2 “AD is accused of going against the Aligned Governance Strategy” FacilitatorDAO Action Example\n(Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #6 by 0xDefensor)\n\n\n0xDefensor FacilitatorDAO A.1.5.2 “Are the conditions fair or unfair?” Action Example Target Document\n(Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #6 by 0xDefensor)\n\n\nCloaky Target Document A.1.5  The submitted package by Cloaky of EA and AEs.\n(Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #13 by cloaky)\n\n\nCloaky Target Document A.1.5.1  The submitted package by Cloaky of EA and AEs.\n(Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #13 by cloaky)\n\n\nVigilant Target Document A.1.51  “Broader Alignment Structure”\n(Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #15 by vigilant)\n\n\nQuestion: How effective is it to create action examples based on broadly described scenarios?\nWe have a general question and are interested in hearing various perspectives on it.\nWe have seen a quite common scenario also among the better rated examples.\nThis is where the details from input and output leaves a lot of room for interpretation due to its lack of specificity, these cases are so to say kept open so they will catch more instances.\nOur concern and questions about this is\n\nif we can be leaving too much room for interpretation in the Action Examples?\nwhere are the boundaries?\nTo what extent can an AI tool discern the difference between meeting formal requirements and the underlying intent?\nAnd what can we do in our drafting of Action Examples that may help the AI tools become better at actually getting the underlying intent?\n\\nOn behalf of Endgame Edge: here are our selections of quality data for inclusion in the synthesis document.\nWe saw significant improvement over Week 2 with regard to Element Analyses (good mechanics - well written and structured, logical, contextualized, etc.) However, we observed that most entities/teams are still struggling with creating valuable Action Examples.\nIn addition to solid mechanics, we wanted to highlight data that also evidences synthetic thinking and poses provocative big-picture questions. These are the potential flywheel benefits attending data creation. 0xDefensor and Ecosystem created some good examples of this.\nElement Analysis\nCloaky [Target Doc A.1.5] “Normal Conditions” Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #13 by cloaky\nBlue [Target Doc A.1.4.3] “Governance Attack” Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #11 by BLUE\nEcosystem [Target Doc A.1.4.3] “Slippery Slope” Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #9 by ecosystem-team\n0xDefensor [Target Doc A.1.5.8] “Unjustified” Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #6 by 0xDefensor\nJanSky [Target Doc A.1.5.8] “Hidden conflict of interest” Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #2 by ldr\nBonapublica [Target Doc A.1.4.4] “High levels of opsec” Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #8 by Bonapublica\nFacilitatorDAO Action Examples\n0xDefensor [Target Doc A.1.5.8] “Are conditions fair or unfair?” Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #6 by 0xDefensor\nEcosystem [Target Doc A.1.4.3] “Slippery Slope Realignment” Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #9 by ecosystem-team\\nHere are my recommendations:\n\n\nBlue Element Analysis [Target Document A.1.4.3] “Warning reason”\n(Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #11 by Blue)\n\n\ncloaky Element Analysis [Target Document A.1.5.0] “formal powers”\n(Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #13 by cloaky)\n\n\ncloaky Element Analysis [Target Document A.1.5.0] “normal conditions”\n(Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #13 by cloaky)\n\n\nldr/Jansky Element Analysis [Target Document A.1.5.8] “AVCs Overstepping Their Roles”\n(Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #2 by ldr)\n\n\\nHere are my selections. I’ve only included ones that have not yet been submitted.\nElement Analysis\nikagai [Target Doc A.1.5.0] “subjective definition of Universal Alignment” Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #3 by ikagai\necosystem [Target Doc A.1.4.3] “Held to the Highest Standard” Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #9 by ecosystem-team\nBLUE [Target Doc A.1.4.3] “Immediately” Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #11 by BLUE\nFacilitatorDAO Action Examples\nBLUE [Target Doc A.1.4.1] “Anon dilemma” Element Analysis and Action Examples for Nov 10 Atlas & GAIT call - #11 by BLUE\\nAs per our previous meeting, I have formulated a guide that encapsulates my thought process when crafting Element Analyses and FacilitatorDAO Action Examples. I believe this guide may not be as comprehensive or useful as the Flywheel guide, but I am sharing it in the event that it highlights any areas where I may be falling short.\nElement Analysis\nThe first item on the agenda is to identify an Element suitable for Analysis. This refers to an Element with a meaning that is not entirely evident or one that could be intentionally misinterpreted by a misaligned actor to serve their own agenda. If the Element can have multiple interpretations, but the surrounding context in the target document restricts it to one meaning, then it does not necessitate Analysis.\nSubsequently, the Element must be clearly defined. What is the Element’s significance within its context? Why is this meaning crucial to the Atlas? Additionally, it is imperative to explicitly outline what the Element does not signify. The objective is to ensure that it cannot be plausibly interpreted in any other manner, even if someone were to make a concerted effort to do so. Furthermore, it is advisable to reference other pertinent sections of the Atlas that aid in contextualizing the meaning.\nFacilitatorDAO Action Examples\nWhen creating FacilitatorDAO Action Examples, the initial step is to narrow down the sections of a Target Document that a FacilitatorDAO can be expected to act or refrain from acting on. From these, endeavor to select a scenario related to these sections that is impactful, likely to occur, and/or not thoroughly covered by the Atlas text. Subsequently, commence describing the scenario in as much detail as possible. It should not be at a similar general level to the Atlas itself. For instance, instead of stating “an AVC imposed biased conditions that favored this other entity” when the Atlas outlaws biased conditions, create a specific scenario such as “it was discovered that an AVC’s desired qualifications for a Scope Advisor position were directly tailored to match the qualifications of a business entity in which a member of the AVC held equity.” Once this descriptive scenario is fully articulated, place it in the input section. Following this, complete the output section with the FacilitatorDAO’s response. How did they react, and was it the appropriate course of action? Was it severe enough or too severe? With these considerations in mind, determine whether it is aligned or misaligned in the label. Subsequently, taking all this into account, decide on a penalty paid in stablecoin that corresponds to the level of misalignment, if misaligned. It is crucial to elucidate the level of misalignment in the Penalty_Reason, as this holds greater significance than the penalty amount. The penalty amounts will need to be adjusted later once there are sufficient Action Examples, and a comparison of Penalty_Reasons will be beneficial in this process.\\nGood work on the Element Analyses @Ikagai. We’re currently using your content to develop some ideas!\nEspecially liked this\n\n…reflects a crucial mechanism for structural adaptability within the AVC framework. An AVC split is a significant event that bifurcates a single AVC into two distinct entities, redistributing governance power and potentially altering strategic directions. The minority quorum is the specific threshold of verified MKR ownership required to initiate such a split, a safeguard against impulsive or unilateral decisions that could destabilize governance. … the AVC’s ability to split is an acknowledgment of the dynamic nature of governance, allowing for evolution and diversification in response to the ecosystem’s needs. It is a manifestation of the principle that while the AVCs should be cohesive units, they must also be flexible enough to accommodate divergent strategic perspectives when necessary.\n\\nHello everyone!\nI’m pleased to share the link to Endgame Edge’s GitHub repository link 1, now updated with the latest high-quality examples crafted by the community. You can view these recent inclusions in the log file 1.\nWe greatly appreciate any feedback you can offer.\nAuthors: @Le_Bateleur & @votewizard\nThank you!\\n      Ty you my friend "
  },
  {
    "number_of_comments": 17,
    "postid": "76b38a4c-1e66-4c3a-9bab-afc2692dbe41",
    "posturl": "https://www.comp.xyz/t/change-my-mind-compound-protocol-is-moving-too-slowly/1107",
    "combinedcontent": "I realize this might be a bit “off the path” for this forum, but I’ll give it a go. I’d like a “change my mind” view of a viewpoint I’ve been thinking about for Compound: The protocol is progressing too slowly.\nChanges are not happening at a “defi-pace”. I realize this does count as a positive for the stability and probably trust for a lot of involved parties, but it also does pose the question if Compound can keep up with the other protocols like Aave.\nWhat is the in pipeline for Compound? Short term? Long term? Why should I believe Compound will be the leading platform in the time to come?\nI hope to be thoroughly impressed \\nWorking on it now, would you like to play a role?\\nI agree with you, and don’t think this is “off the path” for a forum. Yes, stability is important for maintaining user trust, but it must properly balance with innovation for the project to be a success.\nThe reaction to the oracle issue 4 highlights this concern most for me. It has been deliberated on for months, meanwhile our users’ collateral remains prone to another DAI liquidation event. Now we are looking at building a medianizer, which will take even more time, rather than quickly fixing the situation with a 3rd party oracle like Chainlink.\nThat said I think many in the community share your sentiment and are working toward creative solutions toward improving governance moving forward. The most notable development of late was @arr00  creating a gasless voting mechanism (compound.vote 6). I feel confident there will be more with similar talent and skills who step up to the plate.\\nChainlink alone won’t work, single point of failure.\\nThat’s not true. Chainlink is a decentralized oracle network consisting of hundreds of public nodes. Other oracle projects feature only a small number of nodes, creating extremely minimal Sybil-resistance or decentralization.\nThere is a reason Aave, Synthetix and other top DeFi projects who are moving significantly faster than Compound utilize it.\\nNothing wrong with Chainlink, great product, but like you said, everyone is using it, then, this is bad for decentralization.\\nCoinbase price feed and Compound governance structure are decentralized parts of protocol?\nNobody cant tell us what was going wrong on “DAI liquidation” event, why?\nIt would be useful to have insight into on-chain data when something goes wrong.\nEspecially when it comes to your funds. Do you agree?\\n\n\n\n tokenera:\n\neveryone is using it, then, this is bad for decentralization.\n\n\nThat’s like saying Ethereum isn’t decentralized because most of DeFi uses it, which of course isn’t the case.\nDecentralization comes from the number of independent nodes in a system, and right now both Ethereum and Chainlink are leaders in their respective spaces, which is why they are the most trusted and used.\\nSo comparing Ethereum to Chainlink is incorrect: different tech and built for different purposes.\nLet’s go back to this “DAI liquidation” issue, what are the possible approaches?\n\nUse both Coinbase and Chainlink oracle. What if one of them got hacked, Dai goes to $3? Then avg of two $1.5, not solving it.\nAmend the contract. Since stable coin like Dai eventually goes to $1, delay liquidation, and set aside some Comp for risk control?\nWarn user the liquidation risk. Market volatility is a norm, maybe offering some insurance coverage?\n\\nYou get the point, but\n\nbecause of Coinbase price feed people was liquidated. Liquidation  price was 377:1 (DAI: Ether). Fact is that you can mint DAI on Oasis whenever you want - for 1$. Coinbase didnt provide single piece of info for that event (we can compare this with Coinbase support efficiency). Check Ethereum price now and you can imagine user losses, provided that it is not a bad risk management of the user - the protocol simply failed.\nDelay liquidation is great solution, Aave has similar approach with 5% delay. Why Compound implementation looks like place for liquidators and users are not protected (users with real business)?\nPlease, you cant shift responsibility on users risk management. Stablecoins exists for reason - to reduce volatility, thats stablecoin purpose. I write enough about that in other threads - i will not repeat myself.\n\nI as a Compound user am DISAPPOINTED with the approach of the token holder (giants). They dont care about anything except their crypto bags. They didnt answer any questions about the biggest liquidation event and they didnt provide any data about that. The relationship with Coinbase is more important to them because there is money and we all know that Coinbase and DeFi are opposit terms.\n@TragedyStruck is brought out great questions about protocol and i also want to be impressed with answers\\n\n\n\n tokenera:\n\ncomparing Ethereum to Chainlink is incorrect: different tech and built for different purposes\n\n\nYes - they are different tech, but their approach to decentralization (via a vast network of nodes) is analogous, so the comparison stands. Neither become less centralized because more people use it. In fact, more usage encourages more nodes to join and actually increases security.\n\n\n\n tokenera:\n\nWhat if one of them got hacked, Dai goes to $3?\n\n\nHow would this occur with Chainlink? It’s prohibitively expensive as nodes are not sourcing data from any single source, but rather from a variety of aggregators who are in turn sourcing from the entire market.\nThat said…I do think you bring up some good points regarding a liquidation delay, insurance coverage etc. But in regards to the oracle we should just keep is simple so we can move forward QUICKLY.\\nTo be honest, I don’t know how Chainlink works, and having hard time understanding these marketing material, but anyone can provide market data service, and it doesn’t require blockchain at all.\\nBut on blockchain data is transparent ( on Coinbase isnt ).\n\n\n\n tokenera:\n\nI don’t know how Chainlink works\n\n\nAnd you cant criticize something without arguments. So, if you dont have knowledge about project, you cant bring out valid argument.\n\n\n\n tokenera:\n\nanyone can provide market data service\n\n\nThats not true, protocol need data price oracle that reflects the price of asset closest to the global price of that asset. I understand that task is very hard because exist trade-off between flexibility, trust and cost for set-up reliable oracle. For example existing Compound solution isnt reliable because relies on centralized system.\nUnreliable data price oracle allows a hacker to arbitrate with $30 of Ethereum and flashloan, and makes damage to the protocol and users.\\nValid points. Can you provide some details with an example? For an asset like COMP, where does it get all prices? How does it validate the prices? And how to calculate the final price? Are these information on the blockchain as well?\\nI am follow simply logic.\n\nMarket price is equillibrium of buyer and seller preferences and their perspective of value\nIn “DAI liquidation” event, price of DAI reaches 1.3$ on Coinbase open price feed but global market price was still 1$\nI am very interesting who will pay 1.3$ for DAI when he can pay 1$ everywhere or mint DAI on Oasis (1$). Is this normal and rational behaviour?\nSome people here say that was a structural DAI problem but didnt - because if this is true global price will also spike to 1.3$. In scenario where global price spike to 1.3$ we can talk about bad users risk management.\nIn this case that is clear maniplation with front running elements and of course we cant get transparent information from Coinbase because they are centralized platform.\nFor Chainlink or Maker oracle solution data are available ( deep dive in Oracle and Medianizer threads )\n\nIn my opinion Chainlink is best solution in short-medium terms but in long term best option is that Compound build own oracle. Everything is better then present solution. But we also know close cooperation between Compound and Coinbase because Coinbase Venture is early investor in project. Because of that I dont expect quality solution for that problem.\n@getty has started building a medianizer and I am  supporting it because in the current situation this is the best solution (situation si that majority dont want 3rd party solutions)\\nI’m looking for answers and facts, not a story. Thanks for your input.\\nMaybe you need to read few threads on this forum and no just show up here with 20 words. Do you homework or even read Chainlink whitepaper but I think that every text longer than 20 words presents difficulties for you.\nGood luck\\n\n\n\n tokenera:\n\nTo be honest, I don’t know how Chainlink works.\n\n\nThis is pretty clear. I’m all for a debate, as long as it’s in good faith, which I don’t feel this was.\nAnyway, thanks for trying to explain the situation in more detail @dabar90, I’m sure others will find it valuable."
  },
  {
    "number_of_comments": 56,
    "postid": "fa9a9eb8-ebd9-43fb-bdaa-d8e60108b269",
    "posturl": "https://forum.makerdao.com/t/cvc-creation-sovereign-finance-cvc/20868",
    "combinedcontent": "CVC Creation: Sovereign Finance CVC\nFollowing the procedure described in §4.4.1 of MIP 113 24, unaffiliated CVC Member SEED Latam hereby formally requests the creation of the Sovereign Finance CVC.\nDear Maker community,\nFrom Latin America, we have witnessed first-hand the economic and political issues of the different countries, such as political corruption, obstacles in the financial system, devaluation of local currencies, and annual two or three-digit inflation. Therefore, we have adopted and acquired DAI 4 and cryptocurrencies as a solution to overcome these challenges, advocating for decentralization, transparency, and security of a currency that would not erode our incomes and savings by the mere determination of a few.\nThree years ago, we started DeFi Latam 2 to bring crypto and web3 users to the DeFi ecosystem. Now, with SEED Latam, we work collectively to build the path toward decentralization and digital sovereignty.\nSEED Latam 2 is an organization that seeks to promote, support, and accompany the communities and leaders of Latin America. We are not mere entities but part of a vast ecosystem composed of communities, educators, developers, researchers, artists, entrepreneurs, students, and enthusiasts who promote knowledge and critical thinking about web3.\nOur first adventure in governance was with Optimism, where we achieved significant support from the community to make our voice heard. Evidence of this was when we were alerted about the funds obtained from subsidies used for self-delegation of the voting power 3.\nWith the End Game’s new governance system, we believe it’s the right time to join and participate. As Constitutional Voter Committee (CVC) we can propose, debate, and contribute with our perspectives to support Maker’s transformations and path towards decentralization.\nThat is why today, after being recognized as an unaffiliated CVC member, we introduce you to the Sovereign Finance CVC.\nWhy Sovereign Finance?\nCryptocurrencies were created with the idea of returning the power of money to individuals. And we believe that MakerDAO should position itself and lead the growing financial revolution that enables autonomy, independence, and self-governance.\nThis voter committee’s central values will be:\nGrowth and Adoption: We will drive and support all initiatives that allow for massive growth and adoption of DAI while reducing risks associated with the protocol, particularly assets that could compromise its economic stability. During pre-game and early end game, we believe DAI’s peg should stay as close to 1:1 as possible, a crucial factor to achieve much wider adoption, especially in developing countries; as a stable currency, it should convey safety and trust to DAI holders to achieve this purpose.\nLong-term Sustainability and Revenue: We will support and promote all initiatives and proposals that develop reliable, secure, and revenue-stable products for the protocol. It is necessary to provide healthy and stable sources of income that keep the DAO and protocol actors and participants aligned to guarantee that MakerDAO remains a relevant project over time.\nSecurity and reliability: We will also be alert to potential points of excessive centralization that could affect the protocol. Allowing us to guarantee optimal security, thus transmitting greater security for DAI holders, partners, and participants in the protocol.\nEthos and commitment: We will actively contribute to the spread of DAI to all corners of Latin America and the world. We are committed to participating in the forum discussions. debates and in all activities carried out in the DAO. We will promote and spread not only DAI but also everything that surrounds the MakerDAO protocol in Latin American communities. Our ethos is aligned with the values of MakerDAO, and therefore our commitment goes beyond mere participation in governance.\nThis CVC supports the vision and roadmap outlined in the Endgame towards decentralization. We believe that it is currently useful and beneficial to accept certain trade-offs between centralization and profit for sustainable growth in the short and medium term. However, we firmly believe that decentralization is a requirement that will define the success or failure of the protocol in the long term. Therefore, we expect ecosystem actor’s proposals that introduce new business models to Maker to have clear definitions regarding duration, expected benefits, and how they propose to transition to a fully decentralized model that will not pose risks to the protocol.\nAs an organization committed to the principles of decentralization, autonomy, and self-governance, we believe that supporting public goods is a crucial component of creating a sustainable and inclusive financial ecosystem. MakerDAO has the potential to be a leading force in the cryptocurrency community by actively contributing to the development and maintenance of public goods that benefit the broader community.\nPublic goods, such as open-source software (nowadays focusing on AI), educational resources, and infrastructure improvements, are essential building blocks for the growth and adoption of cryptocurrencies. They provide a foundation for innovation, foster collaboration, and promote accessibility for all participants in the ecosystem, regardless of their background or resources.\nBy supporting public goods, Maker DAO can help ensure that the benefits of decentralized finance are widely accessible and not limited to a select few. We recognize that investing in public goods may require trade-offs in the short term, but we believe that the long-term benefits far outweigh the costs. Maker DAO can contribute to the sustainability and resilience of the entire cryptocurrency ecosystem by actively engaging in efforts to support public goods.\nIn this regard, we commit to preparing educational content that promotes MakerDAO and easily explains to MKR holders how to participate and involve in governance. We share the view that MKR holders’ participation in governance is a key aspect of the protocol’s success.\nAlso, being aware that not all Maker users understand the English language, in addition to complying with the subcommittee meetings as per the Maker Constitution (next to be Atlas Inmutable Alignment Artifact) and the Arbitration Scope Framework, we commit to holding monthly subcommittee meetings in the Spanish language where we resume the topics discussed in our weekly meetings. We also encourage the other CVCs members to participate and engage through us with the Spanish-speaking community. All ideas should be discussed! Finally, we will share our Governance Strategy Document and Scope Framework position document in Spanish.\nRegarding the dovish v hawkish dilemma, we prefer to go with a balanced strategy for MakerDAO that would aim to maintain stability while also promoting growth and innovation. By finding the right balance between these two objectives, MakerDAO could build a sustainable and successful platform that attracts new users and drives innovation in the decentralized finance space.\nSome examples are:\n\n\nMaintain a stable peg while allowing for some flexibility: Maintaining a stable peg for DAI is crucial for the credibility of the platform, but some flexibility could be allowed to accommodate market fluctuations.\n\n\nExpand collateral options to attract more users and increase revenue streams while maintaining conservative requirements to ensure that loans are well-secured and unlikely to default.\n\n\nAdjust interest rates to balance stability and growth: During periods of high volatility, interest rates could be increased to maintain stability, while during periods of low volatility, rates could be lowered to stimulate growth.\n\n\nFinally, we are aware that Scopes are currently under review and that modifications are being introduced right now. Once approved, we will start working on the document that defines our position on each Scope and proposes any additional modifications (Alined Scope Proposal). Also, we are preparing our Aligned Governance Strategy document 4.\nWe invite MKR holders and all those interested in the future of MakerDAO to participate in Sovereign Finance CVC and collaborate actively. Our goal is to explain our positions clearly and help shape the future of MakerDAO.\nConstitutional delegates: By following us, you are placing your trust in a committee that will advocate for a pluralistic, growth-oriented, transparent, and secure protocol.\nThe subcommittee meetings organized by Sovereign Finance CVC will take place on Thursdays at 18:00 UTC. Once Sorvereing Finance CVC gets the active status, we will share the meeting schedule as indicated by the latest modification to MIP 113, soon to be enacted. All meetings will be recorded and published on the Maker Forum.\nWe are excited to have the opportunity to contribute our knowledge, ideas, and experience to this initiative, hoping that our work will be of great influence on the protocol.\\nBravo! ReFi CVC looks forward to being in community with you. Once you are confirmed and active, to the extent it takes time to accumulate a bench of following CDs/ADs, the Regenerative Finance CVC would count any support provided by its following CDs/ADs to Sovereign Finance CVC as strong participation in furthering the policy objectives of ReFi CVC.\n@Bonapublica @Frontier_Research @truename @UPMaker @PBG @Libertas @WBC\n(Especially if any are Spanish speakers and by attending your Subcommittee meetings would be supporting enhanced alignment among CVCs across language differences). cc @SpaceXponential @DaveringtonPLLC\\nSo far as I can tell, you’ve met all the CVC creation requirements. We’ll have you added to the list shortly.\nBe aware that in order to display your CD/ADs on the portal effectively, we would need a logo and a short bio / description. You can see examples of these in the delegate profiles that are currently live on the voting portal.\nIf you have any questions, feel free to get in touch.\\nHi! \nThank you very much for your support, we really appreciate it. We think that we have lots of common points with Regenerative Finance CVC, so definitely looking forward to cooperating!\nWe are @pedro_breuer @Marian @Cryptochica @Imlola @AxlVaz  @anonbuilder @Jadmat  \\nHi @longforwisdom, thank you very much for confirming us as an active CVC. In this drive you can find the logo: Sovereign Finance - Google Drive 7 (thank you @Loco_Pacha for the design).\nOur bio will be:\nName: Sovereign Finance CVC\nWhy Sovereign Finance?\nCryptocurrencies were created with the idea of returning the power of money to individuals. And we believe that MakerDAO should position itself and lead the growing financial revolution that enables autonomy, independence, and self-governance.\nThis voter committee’s central values will be:\nGrowth and Adoption: We will drive and support all initiatives that allow for massive growth and adoption of DAI while reducing risks associated with the protocol, particularly assets that could compromise its economic stability.\nLong-term Sustainability and Revenue: We will support and promote all initiatives and proposals that develop reliable, secure, and revenue-stable products for the protocol.\nSecurity and reliability: We will also be alert to potential points of excessive centralization that could affect the protocol. Allowing us to guarantee optimal security, thus transmitting greater security for DAI holders, partners, and participants in the protocol.\nEthos and commitment: We will actively contribute to the spread of DAI to all corners of Latin America and the world. Our ethos is aligned with the values of MakerDAO, and therefore our commitment goes beyond mere participation in governance.\nThis CVC supports the vision and roadmap outlined in the Endgame towards decentralization. We firmly believe that decentralization is a requirement that will define the success or failure of the protocol in the long term.\nAs an organization committed to the principles of decentralization, autonomy, and self-governance, we believe that supporting public goods is a crucial component of creating a sustainable and inclusive financial ecosystem. In this regard, we commit to preparing educational content that promotes MakerDAO and easily explains to MKR holders how to participate and involve in governance.\nRegarding the dovish v hawkish dilemma, we prefer to go with a balanced strategy for MakerDAO that would aim to maintain stability while also promoting growth and innovation. By finding the right balance between these two objectives, MakerDAO could build a sustainable and successful platform that attracts new users and drives innovation in the decentralized finance space.\nOur tagline for Maker Forum front end (start.makerdao) will be: “Unlocking Financial Freedom: MakerDAO’s Path to Sovereignty”\\nSovereign Finance CVC made its first formal decision:\nMessage:\nMon May 22 12:00:00 UTC\nBeing SEED Latam the only member of Sovereign Finance CVC, hereby we take the first formal CVC decision as per MIP 113. With this decision I schedule the dates of the first six Sovereign Finance CVC meetings to:\nMay 25th, Thursday, 6 pm UTC\nJune 1st, Thursday 6 pm UTC\nJune 8th, Thursday 6 pm UTC\nJune 15th, Thursday 6 pm UTC\nJune 22th, Thursday 6 pm UTC\nJune 29th, Thursday 6 pm UTC\nThese dates belong to Q2 and are subject to change in the future by means of a vote by Sovereign Finance CVC members. Q3 meetings will be scheduled in the last Q2 CVC subcommittee meeting.\nSignature Hash: 0x3502291344246b03dc89ba2e5f5a5dd6693627f8e1e40a662d7e8dac9971f4bf3b0ecfbcc2b6593f4eae45d929a0c80a97c3484f1dda531c2ea3d4d5b11795971b\\nThank you, can you let us know where you would like to hold the meetings and I can add it to the calendar?\nThe main two used so far are Zoom and Discord calls - it’s up to you.\\nHi Patrick! Our intention is to conduct the meetings on Discord. We’ve noticed that a specific channel for Sovereign Finance AVC has been created, but it is currently limited to text communication only. Could we please have a voice channel added as well?\nThank you!\\n\n\n\n seedlatam:\n\nCould we please have a voice channel added as well?\n\n\nDone, will get it added to the calendar now too\\nOur first subcommittee meeting will take place this Thursday on Discord 4 at 18:00 UTC. We invite MKR holders, delegates, and ecosystem actors to participate and interact! Your input is invaluable to the success of the AVC!\nSince this is our first AVC meeting, the agenda will consist of a brief introduction to SEED Latam and the goals we have set for creating the AVC. Then, we will share our view on Rune’s post on the 5 phases of the endgame and a brief comment on the recently approved MIP102c2-SP7 approval (since each specific Scope will be discussed in the following meetings). Finally, we will have a special guest, @Sebix  from Gov-Alpha, who will be sharing the remarkable work they’ve been doing in the Ambassadors program. They will also present their proposal to continue as an ecosystem actor.\\nThank you everybody for coming to our first AVC subcommittee meeting! You can find a record of the meeting here 6.\nYou can see our\nNext meeting we will discuss MIP 113: Governance Scope 2.\nSee you on next Thursday!\\nOur second subcommittee meeting will take place today on Discord  at \n        \n          \n        \n        June 1, 2023 2:00 PM\n      . We will be discussing of MIP 113: The Governance Scope 5.\nWe invite MKR holders, delegates, and ecosystem actors to participate and interact! Your input is invaluable to the success of the AVC!\\nThank you everybody for coming to our second AVC subcommittee meeting! We introduced MIP 113: Governance Scope 1 and shared a couple of our Scope improvement proposals. You can find the proposed template for Governance Advisory Council here and the recorded meeting here 2. Don’t hesitate on leaving your comments! This will form part of this quarter AVC Aligned Scope Proposals document.\nNext meeting we will discuss MIP 106: Support Scope.\nSee you next Thursday!\\nOur third subcommittee meeting will take place today on Discord at \n        \n          \n        \n        June 8, 2023 2:00 PM\n      . We will be discussing of MIP 106: The Support Scope. 2\nWe invite MKR holders, delegates, and ecosystem actors to participate and interact! Your input is invaluable to the success of the AVC!\\nThank you everybody for coming to our third AVC subcommittee meeting!\nYou can watch the recording of the meeting here 1.\nWe introduced MIP 106: Support Scope Bounded Mutable Alignment Artifact 2 and shared a couple of our Scope improvement proposals.\nNext meeting we will discuss MIP107: Protocol Scope Bounded Mutable Alignment Artifact.\nHere is our Notion 3 with useful information.\nSee you next Thursday!\\nHi everyone!\n Today marks our 1-month anniversary on the forum!\nWe’ve just started, but we’re happy \nToday we have our 4th meeting at 6pm UTC, don’t miss it!\nHere is our Notion 2 with important information.\nImportant information for the 4th call here 1. Today we will discuss Protocol Scope.\nSee you later!\\nThank you everybody for coming to our 4th AVC subcommittee meeting!\nYou can watch the recording of the meeting here 3.\nWe introduced MIP 107: Protocol Scope Bounded Mutable Alignment Artifact\nNext meeting we will discuss MIP104: Stability Scope Bounded Mutable Alignment Artifact 3.\nHere is our Notion 3 with useful information.\n Have a nice weekend!\n See you next Thursday!\\nSovereign Finance AVC made its second formal decision:\nMessage 1:\nMon June 26th, 04.00 PM UTC\nBeing SEED Latam the only member of Sovereign Finance AVC, hereby I take the second formal AVC decision as per MIP 113. With this decision, as committed in our governance strategy position document, I schedule the date of the first Sovereign Finance AVC meeting in the Spanish language to:\nJune 29th, Thursday 11 pm UTC\nSignature Hash:\n0xa0512f0b78155509fa214d0ee4913c120d1cc9a3b811d787208b44600e2c24075b08a57a6413c4d238eb7468899d7001b4ae05516883630fbb277974309638861c\\nSovereign Finance AVC made its third formal decision:\nMessage 2:\nThursday June 29 00.40 AM UTC\nBeing SEED Latam the only member of Sovereign Finance AVC, hereby we take the third formal AVC decision as per MIP 113. With this decision I schedule the dates of the third quarter Sovereign Finance AVC meetings to:\n7-6 06:00 PM UTC\n7-13 06:00 PM UTC\n7-20 06:00 PM UTC\n7-27 06:00 PM UTC\n8-3 06:00 PM UTC\n8-10 06:00 PM UTC\n8-17 06:00 PM UTC\n8-24 06:00 PM UTC\n8-31 06:00 PM UTC\n9-7 06:00 PM UTC\nThese dates belong to Q3 and are subject to change in the future by means of a vote by Sovereign Finance CVC members. Q4 meetings will be scheduled in the last Q3 AVC subcommittee meeting.\nSignature Hash:\n0xc15eb6121a9c2badfaeac4a8c6bc81941ed4e9d51aa6bcb26444b2b7209ba15907eeb1872040bdb78e8f87506499a6ddba360b7ee0104d4f92e74d9a258137b91c\n@Patrick_J Please confirm if its okay and if possible add it to the calendar \\nHello everyone!\nHere’s a recap of the topics covered during our 7th Sovereign Finance AVC Subcommittee Meeting on July 6, 2023 in the Sovereign Finance AVC Voice Channel on the MakerDAO Official Discord.\nTopic: MIP113: Governance Scope Bounded Mutable Alignment Artifact 3\nDiscussion Topics: [DRAFT - V1] Q3 - Sovereign Finance Aligned Scope Proposals 4\nWe kindly invite you to join us in a discussion where we will present our proposals and seek your valuable feedback. Your input and insights are highly appreciated as we aim to refine and enhance our proposals on the following topics:\n\nAtlas Interpretation Process:\n\n2.2.1: Atlas interpretation precedent approved through MKR vote is contained as Strengthening subelements of this clause. A majority of Governance Facilitators can trigger a vote to add a new subelement if it is necessary to resolve Atlas ambiguity.\n2.2.2: Atlas Interpretation precedent made directly by the Governance Facilitators is contained in 2.2.2.1A.\n1. An MKR vote should generally decide interpretations of the Atlas unless it is a minor issue that the Governance Facilitators can clarify.\n2. Any participant in Maker Governance can propose an interpretation of the Atlas.\n3. Proposed interpretations undergo a 30-day review period, during which community feedback is welcomed and the applicant must respond to inquiries.\n4. The Governance Facilitators can extend the review period by 15 days if needed, with a published justification.\n5. At the end of the review period, the Governance Facilitators announce whether an MKR vote will be called, along with the reasons for their decision and the voting preferences of the facilitators involved.\n6. When the Governance Facilitators interpret the Atlas without an MKR vote, they must provide a justification in the forum and indicate their voting preferences.\n\nScope Artifact Appeals:\n\n3.1.1.1: The Governance Facilitators can by consensus directly edit a Scope Artifact to align its content with the Scope boundaries and other Atlas requirements such as neutrality.\n3.1.1.2: A majority of the Responsible Facilitators can trigger an MKR governance poll to implement an edit to the appealed Scope Artifact that will align it with the Scope boundaries and other Atlas requirements such as neutrality.\ng. Governance Facilitators can directly edit a Scope Artifact if it involves Active Elements or Template Elements, based on the consensus of the facilitators.\nh. When Governance Facilitators make edits to a Scope Artifact, they must provide a rationale on the forum and indicate their voting preferences.\ni. An MKR governance poll is required to implement an edit to an appealed Scope Artifact if it involves Strengthening Elements and Budget Elements.\nj. When an appeal is made by an ecosystem participant on the forum, the Governance Facilitators have a 30-day review period, during which community feedback is encouraged. The petitioner must respond to inquiries.\nk. The Governance Facilitators may extend the review period by 15 days, if necessary, and provide justification on the Maker Forum.\nl. At the end of the review period, the Governance Facilitators will post the response to the appeal, indicating whether an MKR governance poll will be conducted and providing reasons for the decision. They will also specify the voting preferences of the Governance Facilitators.\n\nAlignment Conserver Derecognition\n\n4.1.1: Derecognition notices are issued publicly and justification and reasoning must be provided by the issuing Governance Facilitator.\n4.1.2: Derecognitions are recorded by adding the identity and known aliases or associated entities to 4.1.3A.\nm. If an Alignment Conserver engages in misaligned, serious, malicious, or negligent acts that jeopardize or could have jeopardized the Maker Ecosystem, they will be discharged.\nn. If an ecosystem participant believes that an Alignment Conserver has engaged in such misconduct, they can submit a request for derecognition on the forum, providing detailed information about the conduct.\no. If the allegation appears credible, the accused Alignment Conserver will be immediately suspended for the duration of the proceedings.\np. The accused Alignment Conserver has 15 days from the date of the complaint to present their defense.\nq. After the Alignment Conserver has made their defense or after 15 days without a defense, the Governance Facilitators have a 30-day review period, during which community feedback is encouraged. The reporter of the complaint must respond to inquiries.\nr. The Governance Facilitators may extend this deadline by 15 days if necessary, providing justification on the Maker Forum.\ns. At the end of the review period, the Governance Facilitators will post the response to the complaint on the Forum, indicating whether the derecognition of the accused Alignment Conserver is approved or rejected, along with reasons for the decision. They will also specify the voting preferences of the Governance Facilitators.\nt. Once the derecognition of an Alignment Conserver is decided, it will be included in list 4.1.8A.\n\nAlignment Conserver Formal Warnings\n\n4.2.1: Formal warnings are issued publicly and justification and reasoning must be provided by the issuing Governance Facilitator.\n4.2.2: Formal warnings are recorded by adding the identity and known aliases or associated entities to 4.2.3A.\nu. If an Alignment Conserver engages in a negligent misaligned act that is not serious or malicious and does not jeopardize the Maker Ecosystem, they will receive a formal warning from the Governance Facilitators.\nv. If an Alignment Conserver repeats actions that have resulted in multiple formal warnings, it may be considered serious misalignment conduct and could be reported for derecognition.\nw. Once an Alignment Conserver has received a formal warning, it will be included in the 4.2.1.3A list.\n\nPrime Delegate and Reserve Delegate Slots\n\n2.6.3.1: The Governance Scope Artifact specifies the number of PD slots, and the number of RD slots. There is always an equivalent amount of both. The number of slots must be increased over time if the income level of PDs and RDs increase above their baseline. The number of slots must be decreased over time if the income level of PDs and RDs decreases below their baseline. The baseline should generally increase over time if the number of PD and RD slots are increasing, ensuring that the internal operations of PDs and RDs also grows larger over time as the number of PDs and RDs increase.\nx. Add a mathematical formula to automatically set primary delegate and reserve delegate slots when the price of $MKR goes up or down.\nWe have provided the playlist link for all our previous meetings (English and Spanish).\nThe next meeting will be on Thursday, June 13th, and we will discuss the Support Scope.\nRemember that you can find all the information (important documents, our proposals, slides, etc.) on our Notion page 1.\n See you on Thursday! \\nHello everyone!\nPlease join us for the Sovereign Finance AVC Subcommittee Meeting tomorrow:\nDate: July 13, 2023 Time: 2 pm EDT/6 pm UTC\nWhere: Sovereign Finance AVC Voice Channel on the MakerDAO Official Discord.\nTopics: The Support Scope 1\nWe are going to present the templates we have been working on and kindly request your feedback. And our insights on the Support Facilitators.\nLast week, concerns were raised regarding the process for approving an emergency response to legal action against MakerDAO. In light of this, we are preparing a proposal to establish a streamlined process for onboarding legal counsel to prevent a recurrence of such a situation.\nFinally, we will also discuss @cloaky 's proposal and the questions he has raised and ask for comments on them.\nEveryone is invited to join and participate in our Subcommittee meetings as always.\nWe are excited to see you tomorrow!\nNOTE: Recordings of last week’s meetings are here in our Notion.  2\\nHello everyone! Here’s a recap of the topics covered during our 8th Sovereign Finance AVC Subcommittee Meeting on July 13, 2023, in the Sovereign Finance AVC Voice Channel on the MakerDAO Official Discord.\nTopic: The Support Scope\nWe presented our proposals and templates we have been working on in order to improve the Scope and kindly request your feedback.\nThe first component, 2.1.1 2, focuses on the allocation of resources for supporting AVCs based on their size and focus. We’re adding, 2.1.1.1, resources should be equally distributed among the five largest active AVCs, and beyond that, resource allocation should consider the focus of the AVC. Another addition, 2.1.1.2, Support Facilitators should regularly communicate the criteria used for prioritizing AVCs resource allocation.\nThe second component, 7.3.1 1, refers to the process of submitting proposals for becoming an Incubating Ecosystem Actor. What we’re proposing outlines specific requirements for the submission, such as publicly posting a recognition submission message on the Maker Governance Forum, including certain information in the submission message, and following a provided template.\nRegarding the concerns raised about the process for approving an emergency response to legal action against MakerDAO DAO Resolution for the purpose of retaining Perkins Coie LLP as counsel - Ecosystem Discussions - The Maker Forum (makerdao.com). In light of this, the third component, in Section 10 about Resilience Fund 4, we’re adding 10.6, an Emergency Defense Request process to address legal or regulatory actions against MakerDAO. The proposed additions establish a process for participants of Maker Governance to submit Emergency Defense Requests, justifying why standard processes cannot be followed, providing a provisional budget, and using a template for the submission. If approved, the request would be treated as an urgent situation, a governance poll would be conducted, and funds would be drawn from the Surplus Buffer to support the defense.\nNote: This is just a summary of our proposals, not the actual proposals 2.\nFinally, we were delighted to have Pyramidmaker from Solidi Labs, the newest Ecosystem Actor, join us as our guest. He talked about the current progress of Maker’s projects and the development of L2, RWA, and their post on the forum.\nAdditionally, We invite you to read our ADs Cloacky’s proposal and PALC’s proposal 2, please let us know your insights.\nYou can access both the slides and the recording of our 8th Sovereign Finance AVC Subcommittee Meeting on our Notion page.\nImportant: We have created a [DRAFT - V1] Q3 - Sovereign Finance Aligned Scope Proposals - Google Docs 2 to compile all of our proposals, and we are actively updating it based on the valuable feedback we receive from Ecosystem Actors. This allows us to incorporate their input and refine our proposals to ensure they are comprehensive, well-informed, and aligned.\nWe appreciate the ongoing collaboration and participation of our Aligned Delegates: @cloaky , @PALC , our newest AD: @BLUE , and all Ecosystem Actors.\\n\n\n\n seedlatam:\n\nFinally, we were delighted to have Pyramidmaker from Solidi Labs, the newest Ecosystem Actor, join us as our guest. He talked about the current progress of Maker’s projects and the development of L2, RWA, and their post on the forum.\n\n\nHello and thank you for this detailed summary of your activities! Is there a recording where we can view the presentation by Solidi Labs, or find materials that were shared?\\n\n\n\n goodnews:\n\nHello and thank you for this detailed summary of your activities! Is there a recording where we can view the presentation by Solidi Labs, or find materials that were shared?\n\n\nHi!\nThanks for your words!\nYou can watch the recording of our last Subcommittee Meeting in this YouTube link 4, at the end you will find the presentation by Solidi Labs.\nAnd you can also find both the slides and the recording of all our Subcommittee Meeting on our Notion page 3\\nHey everyone! Let’s do a quick recap of what we covered during our 9th Sovereign Finance AVC Subcommittee Meeting on July 20, 2023, held in the Sovereign Finance AVC Voice Channel on the MakerDAO Official Discord.\nTopic: The Support Scope (MIP 106)\nFirst up, we went over the updates to our Operational Manual, which lays out the expected Code of Conduct for AVC Members and Aligned Delegates, along with specific rules and guidelines for Aligned Delegates.\nThen, we had a presentation from Anonbuilders, who shared some exciting proposals for enhancing the DAO Toolkit. Their ideas include adding a Smart Contract Explorer interface, a Security Audits and Reports module, as well as a set of tools for Monitoring, Analysis, and Visualization of both the protocol and governance. You can find all the proposals and more details here: [DRAFTV4] Sovereign/finance Position Document Proposals - Google Docs.\nWe also talked about plans to implement an Executive Proposal Spell Checker to ensure the accuracy and professionalism of all executive proposals. This led to an interesting discussion about the Sidestream Ecosystem Actor post in the forum, which explores the idea of “Craftless Spells” – a no-code tool to automate part of the spell creation process in the MakerDAO ecosystem. The goal is to reduce delays, inconsistency, and workload for spell crafters while still allowing manual creation of complex content.\nNote: We’d love to hear your thoughts and any updates you might have on these.\nLast but not least, we had the pleasure of having – Wouter, Apeiron, and Emre from Powerhouse! Wouter shared some valuable insights about the DAO toolkit and gave details on what they’re working on, the toolkit’s features, the challenges they face as ecosystem actors, the improvements they’d like to see in Scopes to make their job easier, besides, he provided us with details on the progress made with GAIT/CAIS, and way more! Their input was incredibly beneficial, and we learned a lot from their contributions.\nIf you missed it, you can catch the recording of this meeting here 1.\nFor those who want to review the slides, they’re available here. And of course, you can find all these necessary links and more in our notion here 2.\\nLooking ahead, we’ve got another meeting coming up this Thursday:\nDate: July 27, 2023 Time: 2 pm EDT/6 pm UTC\nLocation: Sovereign Finance AVC Voice Channel on the MakerDAO Official Discord 1\nTopic: The MIP 104: Stability Scope 2.\nWe’ll be discussing the modifications to the Base Rate, Smart Burn Engine and EDSR. Link to the proposal: MIP102c2-SP13: MIP Amendment Subproposal - Maker Core - The Maker Forum (makerdao.com).\nBesides, we’ll have @hexonaut from Phoenix Labs as our special guest for this meeting. Sam will be sharing insights on SubDAOs, Phoenix Labs, Spark Protocol, and more. Feel free to bring your questions about these topics to the meeting.\nFriendly reminder to our ADs: @cloaky @PALC @JAG @BLUE @Penguin.Soldier to bring your proposals to the meeting. We’re looking forward to discussing them with you! See you there!\\nThis weeks topic is the Stability Scope, not the Accessibility Scope as you write.\nYour link and MIP no is correct.\\n@BLUE You are right!\nWe’ve edited the post to reflect the correct weeks topic, Stability Scope. Thanks!\\nHey everyone, here’s a recap of our meeting last Thursday, July 27, 2023, at 2 pm EDT/6 pm UTC.\nThe main topic of discussion was MIP 104: Stability Scope.\nWe had @hexonaut as a guest, diving into various aspects of Spark Protocol and Phoenix Labs. We covered the requirements for ALM (Asset-Liability Management), parameters that influence DAI emission and burning on Spark, and the revenue distribution between Spark Protocol and MakerDAO. Moreover, @hexonaut shared his insights on the potential impact of Money Market innovation based on Univ4 and similar mechanisms like LLAMMA. He also talked about the implementation of Spell with a different structure. Then, we discussed areas for improvement in the Stability Scope concerning Spark Protocol’s work. Of course, we couldn’t miss the opportunity to ask about his opinions regarding the DSR (Dai Savings Rate) and EDSR (Enhanced Dai Savings Rate). It was very interesting to hear his thoughts on these mechanisms that play a vital role in stabilizing DAI’s value.\nNext up, we discussed the modifications made to the Base Rate, Smart Burn Engine, and EDSR in the MIP102c2-SP13: MIP Amendment Subproposal:\nStability Scope Edits:\nBase Rate formula edits:\nSection 3.1: The formula for the Base Rate is updated. The new formula is calculated as follows: ((Yield Collateral Yield Benchmark - 0.7%) * 0.78 + Stability Collateral Yield Benchmark * 0.12).\n3.1.1A: The Base Rate is 3.79%.\nDecreased DSR Spread:\nSection 3.2: The Dai Savings Rate (DSR) Spread is decreased to 0.5%. The DSR is updated to be equal to (Base rate - 0.5%).\n3.2.1A: The Dai Savings Rate is 3.19%.\nModified ALM (Asset-Liability Management) Tiers:\nALM Tier 1 Collateral: No modifications\nALM Tier 2 Collateral must be able to convert to Cash Stablecoins within at most 2 weeks with an expected slippage and realized loss of at most 1%. The slippage percentage modifies from 5% to 1%.\nALM Tier 3 Collateral must be able to convert to Cash Stablecoins within at most 12 months with no slippage. The time requirement is increased to 12 months with no slippage, and the slippage percentage is removed in the updated version.\nALM Tier 4 Collateral: The time requirement for converting to Cash Stablecoins within at most 12 months with no slippage is maintained in the updated version.\nALM Tier 5 Collateral: Deleted.\nFinally, we shared PALCs document on the Request for GOV12.1.2 edit to the stability scope to quickly implement Enhanced DSR. Overall, it discusses the EDSR as a tool to drive adoption and growth of Dai and DSR, aiming for a solid user base and successful implementation of future offerings like SubDAO farming. It also addresses potential risks and management measures to maintain effectiveness in the strategy.\nWe mentioned the Smart Burn Engine (SBE) and the Elixir focusing on the difference in surplus since SBE was activated.\nLastly, we address some questions and concerns with our ADs: @cloaky, @PALC, @Penguin.Soldier, @JAG and @BLUE.\nIf you missed it, you can watch the recording here 2.\nFor those who want to review the slides, they’re available here. And of course, you can find all these necessary links and more in our notion here 2.\n See you on Thursday! \\nHey everyone,\nJust a quick heads-up that our next meeting is scheduled for tomorrow, August 03, 2023, at 2 pm EDT/6 pm UTC. You can join us on the Sovereign Finance AVC Meeting channel.\nThe main focus of our discussion will be MIP 108: Accessibility Scope.\nWe’re excited to welcome a special guest, @ChrisB, the CEO of Summer Finance 1. Chris will share insights about the rebranding of Oasis, the implications of rebranding, lessons drawn from Oasis, and much more. If you have any questions about these topics, please feel free to bring them to the meeting.\nAlso, we’ll be discussing Rune’s edits to the Accessibility Scope (MIP102c2-SP13: MIP Amendment Subproposal). In his own words: \"with significant increased budget for the Launch Project to handle additional expenses and take advantage of all growth opportunities”.\nLastly, we’ll touch upon @Cloaky’s proposal: MIP102c2-SP12: MIP Amendment Subproposals.\nA friendly reminder to our ADs: @cloaky @PALC @JAG @BLUE @Penguin Soldier, please come prepared with your proposals for discussion. We’re eagerly looking forward to your contributions!\nSee you all tomorrow!\\nHey everyone,\nWe hereby inform you that our 13th subcommittee meeting, originally scheduled for August 17, 2023, at 6 PM UTC, has been rescheduled to August 14, 2023, at 6 PM UTC.\nThe reason for this change is that all members of SEED Latam, therefor Sovereign Finance AVC, will be attending ETH Argentina as attendees, organization volunteers, speakers, and masters of ceremony. We believe that this event is crucial for the development of crypto in the country and the region, as hundreds of Latin Americans will be mobilizing to this conference.\nWe cryptographically signed this decision:\nMessage 1:\nThursday August 3 16.48 AM UTC\nBeing SEED Latam the only member of Sovereign Finance AVC, hereby we take a formal AVC decision as per MIP 113. With this decision, I re-schedule the date of the subcommittee meeting scheduled for August 17, 2023, at 6 PM UTC to August 14, 2023, at 6PM UTC.\nSo third quarter Sovereign Finance AVC meetings:\n7-6 06:00 PM UTC\n7-13 06:00 PM UTC\n7-20 06:00 PM UTC\n7-27 06:00 PM UTC\n8-3 06:00 PM UTC\n8-10 06:00 PM UTC\n8-14 06:00 PM UTC\n8-24 06:00 PM UTC\n8-31 06:00 PM UTC\n9-7 06:00 PM UTC\nSignature Hash: 0xe235890017d747530739af3bae6ccc36289d09ac931291350d4ee704bd3eeaf963e3479f6d8210dd48d6d98062bb5160e74d7fca50800d7784ee59f6ab788aeb1c\nWe hope this change does not cause any inconvenience to our attendees and the delegates who follow our AVC! @cloaky @BLUE @PALC @Penguin.Soldier @JAG\\nHey everyone!\nLet’s go over the highlights from our 11th AVC Subcommittee Meeting that took place on August 3rd, 2023, at 2 pm EDT/6 pm UTC in the Sovereign Finance AVC Meeting channel 1.\nOur main focus during the meeting was the MIP 108: Accessibility Scope.\nWe had the pleasure of hosting a special guest, @ChrisB, the CEO of Summer.Fi 1. Chris shared invaluable insights into the rebranding journey of Oasis, the implications of such a significant change, lessons that emerged from the process, and much more.\nHere’s a recap of some key points @ChrisB shared with us:\n\nTrust in Oasis originated from its Maker connections.\nThe transition from Oasis to the Summer.Fi 1 rebranding maintained the integrity of the smart contracts, without any changes.\nDai’s original purpose was to address real-world issues and provide liquidity through the Maker protocol.\nDuring rebranding, it’s crucial to uphold the core values that define the brand’s identity.\nThe brand’s vision, mission, and core values should remain consistent post-rebranding.\nRebranding established tokens can pose challenges due to the trust they’ve built over time.\nRebranding can be perceived as starting from scratch, potentially impacting trust-building.\nThe return on investment in rebranding may not always be immediately clear.\nUser feedback and satisfaction play a pivotal role in driving growth.\nSatisfied users often become the most vocal advocates for a product.\n\nBesides, we also delved into Rune revisions to the Accessibility Scope (MIP102c2-SP13: MIP Amendment Subproposal). In his words, the revisions included \"With significant increased budget for the Launch Project to handle additional expenses and take advantage of all growth opportunities”.\nFor those who missed the meeting or want to revisit the discussions, you can find the full recording and presentation slides on the Accessibility Scope 1.\nImportant note: We’ve set up a [DRAFT - V1] Q3 - Sovereign Finance Aligned Scope Proposals document where we’re collecting all our proposals. We’re continuously updating it with the valuable feedback we’re getting from Ecosystem Actors. If you have any insights or feedback, we welcome your thoughts. This helps us integrate your input and improve our proposals, making sure they’re thorough, aligned with the Atlas, and well-informed.\nLast but not least, please don’t forget to mark your calendars for our 12th AVC Subcommittee Meeting, happening next Thursday 10/08 about the MIP 113: Governance Scope. We can’t wait to continue our discussions and dive deeper into governance matters.\nAnd to our esteemed ADs: @cloaky, @PALC, @JAG, @BLUE, and @Penguin.Soldier : as always, we kindly ask you to come ready with your proposals for discussion.\nSee you all on Thursday!\\nHey everyone!\nJoin us for our upcoming 12th AVC Subcommittee Meeting today, August 10, 2023, at 2 pm EDT/6 pm UTC on the Sovereign Finance AVC Meeting channel where we will be delving into the MIP 113: Governance Scope 2.\nHere’s a glimpse of what we’ll be covering:\nSummary of our proposals:\n\nMIP 113 Component 2.2: Atlas Interpretation Process Exploring the framework of the process.\nMIP 113 Component 3.1.1: Scope Artifact Appeal Process Understanding the process framework.\nMIP 113 Component 4: Alignment Conservers Detailing the process for warnings and formal downtrends.\nMIP 113 Component 6.3: Prime Delegates and Reserve Delegates Time Slots Unveiling the mathematical formula for establishing PD and RD time slots.\n\nAll these proposals can be found more in detail in the [DRAFT - V1] Q3 - Sovereign Finance Aligned Scope Proposals document 1. This document is currently a draft, and we aim to present these proposals as AVC at the end of Q3. Your feedback as Ecosystem Actors is invaluable in this process.\nWe also have a vital topic to discuss:\n\nGrowth AVC without Active Members\n\nWe’ll delve into a post made by @Patrick_J from GovAlpha, the Governance Facilitator. Patrick discusses the implications when the Growth AVC lacks active members. This includes transitioning contracts, severing links, and creating new contracts in response.\nPatrick also suggests the need to review the Atlas and Scopes for managing similar situations better in the future.\nAdditionally, we’ll be discussing:\n\nGovAlpha Emergency Response Recommendations\n\n@LongForWisdom has outlined a set of recommendations for emergency response procedures. We’ll delve further into this, along with @0xDefensor post on creating an anonymous emergency notification system using Telegram and IFTTT. This system aims to ensure efficient communication during emergencies for governance facilitators and AD delegates.\n\nAnd finally, but certainly not least, we’ll delve into Rune’s Request for GOV12.1.2 edit to the stability scope to quickly modify Enhanced DSR based on observed data.\n\nA friendly reminder to our ADs: @cloaky @PALC @JAG @BLUE @Penguin.Soldier, please come prepared with your proposals for discussion.\nSee you all in a few hours!\\nSubject: Highlights from the 12th AVC Subcommittee Meeting: MIP 113 Governance Scope\nWe sincerely appreciate your participation in our 12th AVC Subcommittee Meeting.\nHere is a brief overview of our key takeaways:\n\n\nProposals Recap: We revisited our proposals, which can be found in the [DRAFT - V1] Q3 - Sovereign Finance Aligned Scope Proposals document 1. As this document is currently in draft form, we are aiming to present these proposals as AVC by the end of Q3. Your feedback, as valued Ecosystem Actors, is crucial to this process.\n\n\nGrowth AVC Considerations: Our discussions included the scenario of Growth AVCs lacking active members. Patrick’s post shed light on this, outlining the implications and necessary actions such as contract transitions and creation, who also considered refining the Atlas and Scopes for better management of such situations in the future.\n\n\nActive AVC Status and Governance Rules: The status of an AVC was discussed in relation to its fulfillment of eligibility requirements. It was highlighted that failing to meet these requirements could lead to marking an AVC as inactive or pending status.\n\n\nEmergency Response Recommendations: LongForWisdom’s recommendations for emergency response procedures were discussed, covering detection, coordination, technical and governance responses, and communication. 0xDefensor’s post on an anonymous emergency notification system was also explored, aiming to enhance communication during critical situations. Flexibility during emergencies and the role of the Governance Facilitator were considered important factors.\n\n\nModification of GOV 12.1.2 in Stability Scope: Regarding Rune’s request to modify GOV 12.1.2 for rapid EDSR application, it was agreed that the request holds potential. However, concerns were expressed about misuse and the need for clear guidelines to prevent circumventing proper governance processes. Feedback from Ecosystem Actors emphasized the importance of defining “significant negative outcomes” and considering mini-votes for granular decisions.\n\n\nFor those who missed the meeting or want to revisit the discussions, you can find the full recording and presentation slides on the Governance Scope.\\nA quick reminder that our 13th AVC Subcommittee Meeting is scheduled for today, Monday, August 14th, at 2 pm EDT/6 pm UTC in the Sovereign Finance AVC Meeting channel. We will be focusing on MIP 106 Support Scope 1:\n\nDAO toolkit MIP 106 Component 3: DAO Toolkit Core Development 1\nLayerzero proposal: Resilience Fund, Lawyer Registry & Legal Research processes\n\nYour participation is highly valued, and we look forward to your insights and contributions during the meeting @cloaky, @PALC, @JAG, @BLUE, and @Penguin.Soldier and all Ecosystem Actors.\nSee you in just a few hours!\\nSubject: Highlights from the 13th AVC Subcommittee Meeting: MIP 106 The Support Scope\nHere is a brief overview:\n\nRecap of our Support Scope proposals:\n\n-MIP 106 Component 2.1: Governance Process Definition: criteria for facilitators to support the most valuable AVCs for governance security.\n-MIP 106 Component 7.3.1: Incubation Proposal Submission Process: This refers to a proposal submission process related to incubation. It may outline guidelines and procedures for submitting proposals to support the incubation of new projects, ideas, or initiatives\n-MIP 106 Component 10.6: Emergency Defense Request: This component outlines a process for quickly approving and providing funding for legal defense in response to legal or regulatory actions taken against MakerDAO. This is a mechanism to ensure that MakerDAO can respond effectively to legal challenges.\n\nLayerzero proposal: Resilience Fund, Lawyer Registry & Legal Research processes\n\n-The proposal suggests adding a new feature or aspect to the Resilience Fund, Transition Funding for DAO Legal Defense: This suggests that the Resilience Fund could be used to temporarily finance legal defense expenses directly related to the DAO. In other words, if the DAO is faced with legal or regulatory actions, the Resilience Fund could provide funding to cover the costs of legal defense. However, the proposal acknowledges the need for a separate “Claim Protocol” and SOP to be developed specifically for situations where the DAO becomes the target of legal or regulatory action.\n-Additionally, the proposal mentions that the Resilience Technical Committee will play a role in defining the rules and processes associated with this proposed addition, helping to elaborate on how the transition funding for legal defense will be implemented and managed within the Resilience Fund.\n\nRune MIP102c2-SP13: MIP Amendment Subproposal \n\n“Establishing Spark as a predetermined brand for SubDAO TWO.”\n6.3: Incubating SubDAO brands\n5 of the 6 incubating SubDAOs do not have a brand identity and are referred to with the following code names: FacilitatorDAOs: ZERO, ONE, AllocatorDAOs: THREE, FOUR, FIVE.\nOne incubating AllocatorDAO has an established brand: Spark. To promote its community development Spark can have stand alone communication channels managed by the Support Facilitators and their infrastructure.\nNote: For those who missed our 13th AVC Subcommittee Meeting or want to revisit the discussions, you can find the full recording and presentation slides here.\\nA quick reminder that our 14th AVC Subcommittee Meeting is scheduled for today, Thursday, August 24th, at 2 pm EDT/6 pm UTC in the Sovereign Finance AVC Meeting channel. We will be focusing on the MIP 107 Protocol Scope:\n\nWe will highlight the most relevant aspects of Rune’s post: Friday Atlas and Governance AI Tools community calls.\nWe’ll have ** Kris Kaczor from Phoenix Labs**, we’ll discuss the following topics:\n\n\n\nExperience in Multichain Ecosystem Development\n\n\nProgress of Spark Multichain Development\n\n\nInitial Chain Consideration and Selection Criteria\n\n\nApproach to Connecting DAI Across Chains\n\n\nExisting Infrastructure Utilization\n\n\nIn-House Development Approach\n\n\nChoice of Cross-Chain Standard\n\n\nContemplated Security Measures for Bridge Hacks\n\n\n*If you have any questions about these, please feel free to bring them up during the subcommittee meeting. Remember, our goal is to gather all the relevant information that will help us better understand this scope.\n\nAnd finally, but certainly not least, we’ll share our dao toolkit proposals focusing on the functionalities and features of the “Smart Contract Explorer” and the “Executive Spell Checker.”\n\n*A kind reminder to our ADs: @cloaky @PALC @JAG @BLUE @Penguin.Soldier, please come prepared with your proposals for discussion.\nSee you all today at 2 pm EDT/6 pm UTC!\\nHey everyone!\nWe’ll share the highlights from our 14th AVC Subcommittee Meeting that took place on August 24th, 2023, at 2 pm EDT/6 pm UTC in the Sovereign Finance AVC Meeting channel.\nOur main focus during the meeting was the MIP 107 Protocol Scope 1.\nWe also spotlighted key elements from Rune’s recent post: The next generation Atlas and Governance AI Tools\nMoreover, we had the pleasure of hosting a special guest, Kris Kaczor, the co-founder of L2beat and now part of the Phoenix Labs team. Kris shared his knowledge about the experience in the progress of spark multichain development. He delved into critical areas such as the criteria for selecting initial chains, strategies for interlinking DAI across various chains, utilization of existing infrastructure, the rationale behind choosing cross-chain standards, and even contemplated security measures to safeguard against bridge hacks.\nHere’s a quick summary of some key points Kris shared with us:\n\nEnabling subDAOs to explore different chains is a key milestone in multichain development.\nThe conduits system is crucial for realizing multichain development for various subDAOs.\nA multi-chain strategy approach was published on the forum, with a 2-phase permissionless deployment strategy.\nThe first cross-chain deployment of Spark is close (phase 1).\nRisk assessment groundwork for Maker DAO’s exposure on different chains is ongoing.\nInitial chain selection considers security and demand.\nMaker teleport is designed for more efficient large DAI withdrawals from layer 2.\nThe possibility of conduits injecting DAI liquidity into various protocols is considered.\nCross-chain standard choice revolves around permissionless experimentation.\nNeed for clear resources explaining bridged tokens and risks.\nSpark deployments’ pace depends on existing deployments’ outcomes.\nGnosis chain’s permissionless deployment is upcoming.\nFocus on wallet development is advised for the vault side.\nSdai integration is more significant than DAI integration.\nUsing existing bridges and layer 2 to minimize additional risks.\nThe goal is to monitor and manage risks for the entire domain.\n\nLast but not least, although we couldn’t share our DAO toolkit proposals related to the “Smart Contract Explorer” and the “Executive Spell Checker”, we plan to share these in our upcoming Protocol Scope Subcommittee Meeting. Detailed insights into these proposals are available in the [DRAFT - V1] Q3 - Sovereign Finance Aligned Scope Proposals document 2. Please note that this document is currently a draft, and we aim to present these proposals as AVC at the end of Q3. Your feedback, as valued Ecosystem Actors, is of utmost importance in shaping this process.\nFor those who missed the meeting or wish to revisit the discussions, you can find the full recording and presentation slides here 1.\\n15th AVC Subcommittee Meeting - MIP104: Stability Scope\nWe extend a warm invitation to all Ecosystem Actors for our 15th AVC Subcommittee Meeting, taking place today on Thursday, August 31st, at 2 pm EDT / 6 pm UTC. The meeting will be on Sovereign Finance AVC voice channel. Our central focus for this session will be MIP104: Stability Scope.\nDetailed discussions on key subjects:\n-Adjustments to stability scope parameter\n-DSR DAI source distribution #2 - 29/8/2023 \n-Smart Burn Engine Parameters Update from BA Labs \nAn exciting feature of this meeting is our special guest, Sébastien Derivaux from Steakhouse Financial. Sébastien brings a wealth of knowledge in the realm of RWA, an area currently under intense discussion within MakerDAO. If you have queries about RWA and related topics please share them during the meeting. Our objective is to gather comprehensive insights to enrich our grasp of matters pertinent to this scope.\nDear ADs, if you have questions regarding Participation requirements for ADs that follow Sovereign Finance AVC, we encourage you to either post your queries on the forum or present them during the meeting.\nPrepare for enriching conversations. See you later at 2 pm EDT / 6 pm UTC!\\nHey everyone!\nHere are the highlights from our 15th AVC Subcommittee Meeting held last Thursday, on August 31, 2023, at 2 pm EDT/6 pm UTC on the Sovereign Finance AVC voice channel 1.\nDuring this session, our primary focus was the MIP104: Stability Scope.\nSince a day before we posted our Participation requirements for ADs that follow Sovereign Finance AVC we dedicated the first few minutes of the meeting to encourage our ADs and Ecosystem Actors to comment, and give us feedback in the post.\nWe mentioned the most relevant information regarding the DSR DAI source distribution and the effect on DAI in the DSR and on user positions posted on the forum by Rema from BA Labs:\n\n\nThe implementation of EDSR and the new EDSR Upper Limit decrease from 8% - 5% had a positive impact on DAI demand.\n\n\nDSR Deposits Distribution Categories that include: Maker Core, Spark, Lending Market, PSM/AMM, and Holder.\n\n\nAave v3 ETH market accepting sDAI as a collateral asset is the most notable recent implementation. Other implementations include Pendle, Wormhole Bridge, IPOR, Sommelier, dForce, and more.\n\n\nPreparation for the upcoming SubDAO token farming in Endgame.\n\n\nNew PSM reserves can be allocated towards yield opportunities in order to decrease cash outflow from the EDSR initiative.\n\n\nAn exciting feature of this meeting was our special guest, Sébastien Derivaux from Steakhouse Financial. Sébastien brought a wealth of knowledge in the realm of RWA, an area currently under intense discussion within MakerDAO. Our consistent objective remains to gather comprehensive insights to enrich our grasp of matters pertinent to this scope. Some topics mentioned during the meeting:\n\n\nRWA transparency depends on community and governance decisions.\n\n\nMaker Dao is on a journey with RWA, and it’s just the beginning.\n\n\nSébastien encourages focusing on the bigger picture and goals rather than specific cases.\n\n\nComplexity of private credits vs. financing the US government.\n\n\nMaker Dao’s direction should be determined collectively, and private credits could empower underserved regions.\n\n\nThe importance of clarity from the community on what they want is crucial.\n\n\nThe potential of on-chain smart contracts and RWA to create a more efficient financial ecosystem.\n\n\nHaving high-quality RWA assets like t-bills and corporate bonds on the balance sheet could generate liquidity.\n\n\nThe speaker advises having healthy discussions in the community to determine Maker Dao’s direction.\n\n\n*Next Protocol Scope Subcommittee Meeting we’ll discuss the Smart Burn Engine Parameters Update from BA Labs.\nFor those who missed the meeting or wish to revisit the discussions, you can find the full recording and presentation slides here 1.\\nWe’re excited to invite all Ecosystem Actors to our 16th AVC Subcommittee Meeting, happening today, Thursday, August 7th, at 2 pm EDT / 6 pm UTC. You can join us on the Sovereign Finance AVC voice channel 1. Our main focus for this session will be MIP108: Accessibility Scope.\nWe are pleased to announce @tobalgarcia from Maker Growth Core Unit Team, as a special guest for this meeting. Tobal serves as the Growth Core Unit Content Leader and brings a wealth of experience in Marketing, Growth, and Business. We encourage all our ADs and Ecosystem Actors to participate actively, share your questions, and bring your insights.\nHere are the key topics we’ll discuss with Tobal:\n\nThe Future of the Growth Core Unit within MakerDAO\nEssential Considerations for Rebranding\nProgress in the Development of Our New Brand\nUpdates & Alphas\n\nWe’ll also mention the DAO Redundant Storage and IPFS (Led by LFW and Hernandoagf), about exploring strategies to reduce MakerDAO’s reliance on centralized storage, such as GitHub, through IPFS. We will provide insights into the concept and its current status.\nAdditionally, we will touch upon comments from our ADs @BLUE & @cloaky, with a focus on Components within the Scope of the week that they believe need to be reviewed, and explain why and in which direction.\nIn terms of our AVC Subcommittee quarterly requirement, this meeting marks the completion of the 10-week cycle, covering each of the 5 Atlas Scope Bounded Artifacts twice.\nA brief scheduling note: There will be no meetings for the following two weeks. However, we have a special meeting planned for September 28th at 2 pm EDT / 6 pm UTC, and we will provide more details on this in the coming weeks.\nWe look forward to your presence in our last Subcommittee Meeting from this quarter in just a few hours!\\nNew communication channel \nDiseño sin título - 2023-09-12T050431.0131500×500 595 KB\nWe just launched our English governance channel!\nSo far, we have conducted all our communication in Spanish outside of the forum. However, we believe that having an English channel can help us showcase our work and connect with teams from different regions of the world   \nOur DMs are open here and on our new Twitter profile. Thank you very much \n\nFollow us  https://twitter.com/SEEDGov 2\n\\nHey everyone!\nHere are the highlights from our 16th AVC Subcommittee Meeting held last Thursday, September 7, 2023, at 2 pm EDT/6 pm UTC on the Sovereign Finance AVC voice channel 1.\nDuring this session, our primary focus was the MIP108: Accessibility Scope.\nWe were delighted to host an incredible guest, @tobalgarcia, who shared insights with us about strategy for introducing new branding elements, emphasizing the autonomy of subdaos, the need for diversity among subdao brands, addressing complexity for different user types, using gamification for engagement, and the overarching goal of making the ecosystem accessible to a wide range of users.\nIf you missed it, you can watch our subcommittee meeting and view our presentation slides here 1.\nDuring the meeting, we had an interesting debate between @SebVentures, representing @steakhouse, and @pedro_breuer, a Sovereign Finance AVC member. The participants discussed the complexities of MakerDAO’s governance, transparency issues, and the need for improved communication to help stakeholders make informed decisions, potential solutions, and better community engagement.\nHere are the key points from the debate:\n\n\nTransparency: Sebastien emphasizes the importance of transparency within MakerDAO. He points out that Steakhouse has made efforts to provide detailed information about transactions made.\n\n\nComplex Governance: Sebastien acknowledges the complexity of MakerDAO’s governance system and the challenges it poses.\n\n\nRole of Steakhouse: Pedro raises questions about Steakhouse Financial’s role and responsibilities within the ecosystem. He seeks clarity on what tasks Steakhouse is assigned as an ecosystem actor.\n\n\nCommunication Challenges: Pedro expresses concerns about the difficulty stakeholders face in understanding complex financial reports and information related to RWA. He believes that effective communication is essential for stakeholders to make informed decisions.\n\n\nSuggestions for Improvement: Pedro suggests that better communication and information dissemination are needed. He proposes that the DAO consider hiring someone responsible for effective communication to bridge the gap between technical experts and stakeholders.\n\n\nTokenized Stables: Sebastien discusses the idea of tokenized stablecoins, such as tokenized T-bills. He argues that these tokens could provide greater transparency and accessibility to asset information, making it easier for stakeholders to understand and trust the assets backing DAI.\n\n\nComplexity and Transparency: Sebastien acknowledges the complexity of the MakerDAO ecosystem and the challenges in providing simple, easily digestible information to stakeholders. He suggests that simplifying the framing may remove important details, but focusing on what truly matters is crucial.\n\n\nCommunity Engagement: Pedro expresses the importance of stakeholders having access to detailed information to make well-informed decisions. He seeks advice on who to rely on for additional information when needed.\n\n\nEfforts and Limitations: Sebastien highlights the limitations of Steakhouse Financial, emphasizing that they don’t have access to all the information required for transparency. He also mentions the challenges of working with third-party entities in the traditional financial world.\n\n\nFuture Direction: Sebastien and Pedro discuss the potential benefits of moving towards on-chain structures and greater third-party involvement for transparency and asset tracking.\n\n\nDisclaimer: We consider this debate to be of significant value to the Maker Ecosystem. As Seed Latam, we aim to convey the most useful information for the DAO’s benefit. Our intention is not to take sides, neither favoring @SebVentures nor @pedro_breuer. Instead, we strive to present the information gleaned from these fruitful discussions, which contribute to the growth of Maker. Together, we are all actively shaping the future of MakerDAO.\nYou can view the complete debate here 2.\\nDear Aligned Delegates: @cloaky, @PALC, @BLUE, @JAG @Penguin.Soldier, @AVC_Member @Aligned_Delegates,\nWe kindly request your input/feedback on our Q3 [DRAFT - V2] Sovereign Finance Aligned Scope Proposals Document 3. Feel free to share your thoughts and comments directly within the document. Your feedback is valuable to us, and we genuinely appreciate your time and insights.\nWe will keep this feedback period open until Tuesday, September 26th.\nThank you in advance for your contributions.\\nWe warmly invite all Ecosystem Actors for our special meeting, happening this Thursday, September 28th, at 2 pm EDT / 6 pm UTC on the Sovereign Finance AVC voice channel.\nOur primary agenda for this session is to delve into our proposals for the Q3 Proposals Document. We’re carefully reviewing the feedback and insights provided by our ADs @cloaky @BLUE on our Q3 [DRAFT - V2] Sovereign Finance Aligned Scope Proposals Document 2.\nA friendly reminder to our @Aligned_Delegates who haven’t yet shared their comments: the deadline for providing us with your feedback, is tomorrow, September 26th. Your input is crucial to our collective progress, so please ensure you submit your insights by then.\nWe look forward to a productive and engaging discussion during the meeting. Your participation is highly appreciated and essential to our continued growth. See you on Thursday!\\nWe hereby inform you that Sovereign Finance AVC made a formal decision:\nMessage 3:\nFriday, September 29, 2023, 14.46 PM UTC\nBeing SEED Latam the only member of Sovereign Finance AVC, hereby we take a formal AVC decision as per MIP 113. With this decision, we schedule the Q4 dates of the Sovereign Finance AVC meetings to:\n10-5 06:00 PM UTC\n10-12 06:00 PM UTC\n10-19 06:00 PM UTC\n10-26 06:00 PM UTC\n11-2 06:00 PM UTC\n11-9 06:00 PM UTC\n11-16 06:00 PM UTC\n11-23 06:00 PM UTC\n11-30 06:00 PM UTC\n12-7 06:00 PM UTC\nSignature Hash:\n0x213d9377bcef15095f2237c54e5506a8c886745e292099a411d610788df2302b1ad4b1253ef39b6934ae5305177dde4646aff12a7390499233b6330ed4402eeb1c\nThese dates belong to the Q4 subcommittee meetings and are subject to change in the future by means of a vote by Sovereign Finance AVC members. Q1 2024 subcommittee meetings will be scheduled after the last meeting of Q4 2023.\nPlease confirm everything is fine @votewizard @JanSky @Le_Bateleur\\n\n\n\n seedlatam:\n\nPlease confirm everything is fine @votewizard @JanSky @Le_Bateleur\n\n\nI have verified the accuracy of this message and made the necessary changes to the meeting calendar to align with the provided dates. If you have any inquiries or issues, please do not hesitate to reach out.\\nAs an AVC member, 0xRoot makes a formal AVC decision as per MIP 113. With this decision, we confirm the Q4 schedule for the Sovereign Finance AVC meetings as follows:\n\n10-5 06:00 PM UTC\n10-12 06:00 PM UTC\n10-19 06:00 PM UTC\n10-26 06:00 PM UTC\n11-2 06:00 PM UTC\n11-9 06:00 PM UTC\n11-16 06:00 PM UTC\n11-23 06:00 PM UTC\n11-30 06:00 PM UTC\n12-7 06:00 PM UTC\n\nTx Hash: 0x51ecb0a260fc6daa6b75d74669fa7d4facb941727568ff7ff7b7d8fedaef0703\nLink: https://etherscan.io/tx/0x51ecb0a260fc6daa6b75d74669fa7d4facb941727568ff7ff7b7d8fedaef0703 2\\nWe’re excited to invite all Ecosystem Actors to our 17th AVC Subcommittee Meeting, happening today, Thursday, October 5th, at 2 pm EDT / 6 pm UTC.\nYou can join us on the Sovereign Finance AVC voice channel. Our main focus for this session will be MIP113: Governance Scope.\nHere are the key topics we’ll discuss:\n\n\nAD’s comments in their Public Docs about this scope @cloaky, @PALC and @BLUE\n\n\n(Draft) Proposal based on those comments\n\n\nProcess for ADs who wish to follow a different AVC\n\n\nAVC Member Participation Rewards\n\n\nHere, we share our ADs’ public documents we’ll be discussing during today’s call:: Cloaky’s public doc, PALC’s public doc, BLUE’s public doc.\n@AVC_Member @Aligned_Delegates\nWe look forward to your presence in our first Subcommittee Meeting from Q4!\nYour participation is highly appreciated and essential to our continued growth. See you in just a few hours!\\n18° AVC Subcommittee Meeting - MIP 106: Support Scope\nWe extend a warm invitation to all Ecosystem Actors for our 18th AVC Subcommittee Meeting, taking place tomorrow Thursday, October 12th, at 2 pm EDT / 6 pm UTC. The meeting will be on Sovereign Finance AVC voice channel. Our central focus for this session will be MIP106: Support Scope.\nWe’d like to express our gratitude to our ADs @BLUE @cloaky @PALC for their valuable input. We’ve taken their comments into consideration for this upcoming meeting.\nDetailed discussions on key subjects:\n\n\n‘Peer-to-Peer’ recruitment model that seeks to decentralize headhunting in the recruitment process. The model aims to incentivize professionals to maintain broader networks, promote inter-industry collaboration, and diversify hiring pools.\n\n\nGenesis Templates for FacilitatorDAOs, AllocatorDAOs, and MiniDAOs including Incubation Objectives for Ecosystem Actors, and a proposed solution to address who will be responsible for filling in this information.\n\n\nTemplate for Milestones and Transparency Reporting to help Ecosystem Actors enhance transparency in their reports.\n\n\nSovereign Finance AVC Internal Governance Process for determining membership, creation of Aligned Governance Strategy, Aligned Scope Proposals and other decisions.\n\n\nYour participation is highly appreciated and essential to our continued growth. See you in just a few hours! @AVC_Member @Aligned_Delegates\\n19° AVC Subcommittee Meeting - MIP 107: Protocol Scope\nWe invite all Ecosystem Actors to our 19th AVC Subcommittee Meeting, taking place tomorrow Thursday, October 19th, at 2 pm EDT / 6 pm UTC.\nThe meeting will be on the Sovereign Finance AVC voice channel. Our central focus for this session will be MIP107: Protocol Scope 1.\nGet ready for an exciting Subcommittee Meeting! We’ve got a real treat for you this time – two fantastic guests: Sam Hart and 'Buffalu ,’ who are all set for an engaging discussion. Our focus? It’s a bit of a hot topic - Maker’s strategy for its implementation on ‘NewChain’ in the 5th phase of the Endgame. We’re hosting a friendly debate to explore which blockchain, Cosmos or Solana, is the best fit as the technology foundation for NewChain.\nThis one promises to be quite the debate. Don’t miss out!\nYour participation is highly appreciated and essential to our continued growth. See you tomorrow!  @Aligned_Delegates @AVC_Member\\n20° AVC Subcommittee Meeting - MIP 104: Stability Scope\nWe invite all Ecosystem Actors for our 20th AVC Subcommittee Meeting, tomorrow Thursday, October 26th, at 2 pm EDT / 6 pm UTC. The meeting will be on the Sovereign Finance AVC voice channel. Our main focus for this session will be MIP104: Stability Scope.\nMike from Balancer will be joining us to share his insights on Jadmat’s proposals and the potential benefits for Maker.\nIn addition, we’ll discuss about:\n\nMaker Parameter Adjustments\nDAI Multichain Update\nSBE Transaction Analysis\nSummary of the proposed changes\n\nBig thanks to our ADs @BLUE @cloaky @PALC for sharing their comments on this scope. Your participation plays a vital role in our ongoing growth.\nSee you all tomorrow!\\n4th Sovereign Finance AVC call IN SPANISH!\nWe’ll be connecting with our Latin community to discuss the most recent updates from Maker and SoFi.\nDate: Tomorrow, Thursday, at 10 pm UTC / 6 pm ARG\nLocation: Sovereign Finance voice channel\nTopics:\n\nUpdates about SubDAOs\nInsights on DAI and TVL\nAtlas and GAIT\nA recap of SoFi AVC’s work over the past few weeks\n\nIf you speak Spanish (or not) come join us!\\n21° Sovereign Finance AVC Subcommittee Meeting - MIP 108: Accessibility Scope\nDate: Tomorrow Thursday, November 2nd, at 2 pm EDT / 6 pm UTC.\nWhere: Sovereign Finance AVC voice channel.\nOur main focus for this session will be MIP 108: Accessibility Scope.\nTopics:\n\nMakerDAO resolution detailing key decisions about incorporating the Mars Foundation, appointing service providers, intellectual property agreements, and the deployment of front-end applications like Spark FE and Sakura FE.\nComprehensive documents related to the Launch Project and Protocol Engineering knowledge. These documents, totaling 10, provide insights into various aspects of the Executive Process and Spell Crafting. They cover definitions, guidelines, principles, and processes, with a focus on enhancing community input and serving as resources for the Atlas tooling phase. Ultimately, the goal is to integrate this knowledge into the Maker endgame framework for the benefit of the community.\n\nSovereign Finance AVC’s proposals for this Scope:\nThe language in the Accessibility Scope is currently unclear in several aspects since there are no enough information. We have proposals to eliminate ambiguity and provide clear clarifications within the Scope. These proposals, which encompass more details about SubDAO Frontend Standards, covering aspects like security, user experience, and required features, and the Launch Project, including Cloaky’s and BLUE’s proposal, will be presented during the upcoming Subcommittee Meeting.\nWe invite all Ecosystem Actors to join us, your active participation plays a vital role in our ongoing growth. See you tomorrow!\\n22° Sovereign Finance AVC Subcommittee Meeting - MIP 113: Governance Scope\nDate: Today! Thursday, November 9, at 2 pm EDT / 6 pm UTC\nLocation: Sovereign Finance AVC voice channel\nScope: Governance - MIP 113\nTopics:\n\n\nAlignment Conservers Code of Conduct: The Q3 Sovereign Finance Aligned Scope Proposals Document includes that Governance Facilitators should develop a code of conduct that specifically defines the behaviors prohibited for each of the Alignment Conservers.\n\n\nRune’s proposal on the Governance Scope: “AVCs must devote time and energy towards discussing and understanding how the Next Generation Atlas will work and how to incorporate AI efficiently into long term governance and AVC’s operations.”\n\n\nLast Friday’s GAIT call on AVCs focusing on Section Improvement.\n\n\n@cloaky  and @BLUE comments on Scope components.\n\n\nDear ADs and AVC members, we’re looking forward to your thoughts on these topics! See you at 2 pm EDT / 6 pm UTC!\\n23° Sovereign Finance AVC Subcommittee Meeting - MIP 106: Support Scope\nDate: Thursday, November 16, at 2 pm EDT / 6 pm UTC\nLocation: Sovereign Finance AVC voice channel 1\nScope: Support- MIP 106\nOn this opportunity we will be joined by @layerzero who will provide us with insights about Maker’s legal resiliency strategy, the various associated projects and the following steps.\nWe will also share our Aligned Scope Proposals and comment on our delegates work! @cloaky @BLUE\nDear ADs and AVC members, we’re looking forward to your thoughts on these topics! See you at 2 pm EDT / 6 pm UTC!\\nHi everyone!\nThanks for joining us on our last Subcommittee Meeting were @layerzero introduced us to  Maker’s legal resiliency strategy. For legal reasons, this time we did not record the meeting, nor did we share it on youtube. We hope you understand! If you have any questions or need information about what was discussed, feel free to ask us, our Aligned Delegates, or layerzero themselves.\nAlso, we invite you to read this forum post were all the relevant information was published.\nMoving to tomorrows meeting:\n24° Sovereign Finance AVC Subcommittee Meeting - MIP 107: Protocol Scope\nDate: Thursday, November 23, at 2 pm EDT / 6 pm UTC\nLocation: Sovereign Finance AVC voice channel \nScope: Support- MIP 107\nOn this opportunity we will be discussing the following topics:\n\nUI for Smart Contracts Idea\nGearbox Risk Framework\nPullUp Labs Quarterly Update\nSpell Minor Error - Post-Mortem Analysis\n\nWe will also share our Aligned Scope Proposals and comment on our delegates work! @cloaky @BLUE\nDear ADs and AVC members, we’re looking forward to your thoughts on these topics! See you at 2 pm EDT / 6 pm UTC!"
  },
  {
    "number_of_comments": 10,
    "postid": "53b5d817-1453-4c03-a76a-90b7c193cf96",
    "posturl": "https://www.comp.xyz/t/adding-comp-as-an-asset/117",
    "combinedcontent": "Has COMP been considered to be added as an asset? Obviously with a collateral factor of 0%. Personally I would love this, but I also think it may be a bad idea for the protocol.\nAre there any guidelines on requirements to add an asset? If there isn’t it may be worthwhile to discuss what these should be to ease the many ‘what about token X’ posts that are surely coming…\\nWhy do you say obviously 0% collateral? The biggest issue when adding new tokens are:\n\nAccurate price feed\nSufficient liquidity\nGeneral token security\n\nOnce compound transitions to the open oracle, we will have a price feed for COMP. The rest is up to the community.\\nAh, because of low liquidity you’d want to prevent liquidations as much as possible. I figured 0% collateral prevents liquidations due to the price going down. But now I see this isn’t really an issue. The real issue is a price rise and liquidators having to source COMP in a market with low liquidity. So you’re right… collateral factor isn’t so important. Now I see why sufficient liquidity is key. Thanks.\\n“Rule Number 4, I know you heard this before\nNever get high on your own supply” -The Notorious B.I.G. from the Ten Commandments\\nAdding support for COMP is in line with the mission of the protocol; as with any asset, it requires a price feed (coming soon) 8, sufficient market liquidity , and community interest (which, given the repeated chatter in Discord, seems extremely high).\nThe primary question that the community should ask, is how borrowed COMP could interfere (or, improve) Governance:\n\nProposals require 100k COMP to be held for the life of the proposal; this prevents flash-borrow attacks, but not borrowed COMP attacks.\nVoting weight is measured at the beginning of a proposal; you can’t borrow COMP to sway an ongoing vote.\nCould risks be limited with a borrowing cap?\n\nIf I had to guess, Bart’s motivation is enabling new mechanisms to short COMP 13. That’s not a bad thing (efficient markets are strong markets), and a borrowing market for COMP could unlock new use-cases that haven’t been considered, too.\\nI think it might be good time to come back to discussing addition of COMP market, though i would like to look at it at different angle.\nFirst i would like to address market liquidity. That’s a moot argument. It was presented with WBTC first AND proven wrong. There’s not sufficient market liquidity BECAUSE there’s no market implementation. So as long as there’s no market, liquidity WILL be low obviously. Was same with WBTC for month with liquidity never coming. Compound COULD fix it itself with enabling  WBTC as collateral, but that was actually addressed first by MAKER team, by adding the WBTC vault, and as soon as there appeared usability, capital went there creating liquidity, hitting their vault caps multiply times.\nConcern about WHAT if someone would be able to borrow COMP from market and use it for creating proposals/ voting. I guess we never know before it comes, AND the thing is IT DOESNT ALREADY MATTER what Compound do. Since CREAM.finance already implemented that market and it already can be used for that, and while liquidity might not be enough yet, it will get there eventually no matter if Compound itself create that market or not. Besides you don’t really need to borrow COMP, you can borrow some stable coin and temporarily buy COMP with that. And that not necessarily would be more expensive than borrowing COMP, likely even cheaper, might even end in profit if market buying push price high enough, and then you can slowly sell it back higher. So that possibillity kind of always was there, and probability will just increase with more COMP created and entereng market daily.\nBut that’s not my main point. I want to higlight the benefits of having COMP market.\nMany people mentioned that it would be nice if Compound protocol would reward COMP holders. And i think this is great way how. Compound could adapt some of the methods “food tokens” utilise and benefit from them. If someone not aware how it’s done, the idea is quite simple: User provide COMP to supply side of COMP market and recieve cCOMP tokens. User than “stake” (timelock) that tokens and recieve reward in COMP distributed to the pool of that “staked” tokens. For visual example how it could work, you can check cream.finance and their native CREAM token. By that initiative basically several things are achieved, creating liquidity in supply side, locking that liquidity (user can’t unstake it prematurely and thus can’t remove COMP from supply pool) and rewarding of long term holding of COMP.\nBut more than that, i believe the system could be improved. Compound could have longer timelock on staked pool tokens. Like 3 months, 6 month, 1 year which could recieve bigger rewards for longer lock of tokens. Maybe even locked tokens could give weighted votes in governance. For example, 1 year locked COMP tokens could have 2x voting weight or something like that, ensuring that long standing commitment have a bigger voice out there.\nNow  from where  COMP tokens to reward that initiative should come? Well that’s quite simple, from that same pool of tokens which are now distributing via liquidity mining initiative. I see couple of ways to do that, we can either reduce emission speed OR “cut the tail”, keep emission speed same but shorten the time, like not 4 years of distribution, but like 3 years, 9 months. Either way is fine and could be voted on details.\nMaybe some of you noticed that i was strongly against reducing emission. Well i’m not against reduction itself, i’m against “just reduce it so we could possible use it for something else”. I believe we should FIRST have that something else, and then we could reduce emission to secure funding. But if there’s nothing else existing yet, than emission shouldn’t be touched yet as well.\nAnd from financial stand point ability to “stake” cCOMP tokens timelocking them in staking pool will remove a portion of tokens from circulation on market likely helping the price. But that’s not really main point, rather an additional benefit for COMP community.\nLast it’s a bit strange that Compound don’t support market for it’s own governance token. Is it not enough confidence in own token?\\nI’d like to get this done soon. I think we should deploy the current development branch version of CCompLikeDelegate and CErc20Delegator using this commit 3. It uses previously deployed code for the CErc20Delegator for reasons stated in this PR 1.\nThings we need to discuss:\n\nCollateral Factor\nBorrow Cap\nInterest Rate Model\nReserve Factor\n\nMy opinion:\nFor the most part, I am basing these off existing non-stable cTokens.\n\nCollateral factor of 60% matching that of other decentralized non-ETH/stable collateral\nBorrow Cap of 80k to avoid governance manipulation\nJumpRateModelV2 2% base rate. Kink at 80% util. Rate at kink? Rate at 10% util?\n20% reserve factor\n\\nArr00,\nThis is exciting, thanks for stepping up to make a new cToken & proposal!\nFor Collateral Factors, historically this has been an analysis based on the liquidity and volatility of the asset. Gauntlet has done extensive work analyzing these risks 1 for the Compound protocol (cc @tarun), and their input here could be very useful.\nA Borrow Cap makes a lot of sense, and is a great way to test the new technology! This could start low and increase as the market scales, and there is more information on the market’s impact on Governance. Throwing a simple poll in to gauge the community’s thoughts:\n 0 COMP (can’t borrow) 10,000 COMP 25,000 COMP 50,000 COMP 80,000 COMP No Borrow Cap8voters\n                      \n                      Choose up to 6 options.\n                     \n                \n                Results will be shown on vote.\n               Vote now!\nThe USDT Interest rate model is a good place to start; it’s already deployed, and becoming canonical. That being said, more work is needed to analyze the optimal rate models for collateral assets.\nIn terms of Reserve Factors (always a contentious topic), 20% is lower than the RFs on collateral assets, but seems to be in-line with where the community wants to take RFs going forward for collateral.\\nI’m starting to think that might be a bit more tricky for governance token. To the point that we might be possibly interested in having 2 different markets for COMP tokens. In some way it applies to other governance tokens too, but since COMP is native governance token of Compound that should be our primary concern. My primary focus on having COMP market isn’t really about providing borrow opportunity, but more of:\n\nEnabling COMP holders to use their tokens as collateral\nPreserving ability to vote, by keeping voting power from that tokens during supply\nPotentially providing reward initiatives by locking COMP tokens for different time-frames.\n\nAnd here we come to a problem if we are having a borrow side of COMP pool. We can’t preserve voting power for original owner, because the very same tokens could be borrowed from pool and borrower would expect voting power to come with them.\nThat could be addressed if we not allow cTokens from that pool to be borrowed. So basically they would be able to serve as collateral AND preserve voting power to original Supplier, as well as be timelocked in that pool, but by doing so we lose abililty for them to be borrowed.\nOr we could allow them to be borrowed like traditional markets, but then supplier have to forfeit their voting rights for the duration of his supply.\nOr maybe there could be 2 types of cComp, one with preservation of voting power and another with not. What is more important for future of protocol, preserving voting power for suppliers, or ability to borrow COMP as i don’t think we could have both in one cToken. Or am i missing something here?\\nYes, I agree with your concerns; however, due to implementation constraints, enabling COMP as collateral while still using it for voting would be difficult and require a lot of new code.\nThe cCOMP market will have the ability to delegate its liquidity. If the community would like, we can setup the delegation to the community multisig and have off-chain voting based on a cCOMP snapshot. This will allow cCOMP holders to still have a say in governance. This is a solution to the issue you identified: maintaining voting ability while allowing borrowing.\nIn the future, I hope to see additional ways to use COMP in the Compound ecosystem such as staking COMP to receive more COMP. In this scenario, we will setup a system in which the user will maintain their COMP voting/delegation rights. I don’t know if it will be possible to use the staked COMP as collateral, but at the moment I assume not.\\nI have deployed the cCOMP contract using here 6. A new JumpRateModelV2 with the following parameters:\n\nBase rate of 2%\n20% at kink\nkink at 80%\n100% at 100% utilization\n\nThe implementation for this cCOMP contract is the same contract as is used by cUNI."
  },
  {
    "number_of_comments": 21,
    "postid": "208f5eba-3725-4ce7-a025-36d585f9f1b6",
    "posturl": "https://www.comp.xyz/t/risk-parameter-update-february-2021/1144",
    "combinedcontent": "After the healthy discussion over the past week or so about the WBTC collateralFactor changes, people mentioned a few key risk factors:\n\nLiquidity\nVolatility\nUser behavior (for example, the collateral ratios maintained by borrowers)\n\nOver the past few months we’ve seen liquidity improve for many tokens in Compound, while volatility has also increased substantially. How do you determine if the increase in volatility is offset by the increase in liquidity? How have user-chosen collateral ratios affected any added risk? These factors interact in complex ways, and we believe the only way to understand how these myriad conditions affect risk is by running stress tests for the protocol.\nWe’ve gone ahead and run a set of stress tests of Compound to try to set collateralFactors that better balance the risk and capital efficiency of the protocol. We’re looking forward to doing this more frequently going forward. By setting parameters more frequently, the protocol should be able to take on more risk as we will be standing by to increase collateral requirements as market conditions change. We’ll submit a proposal for these parameters early next week, but want to leave this on the forum for a bit to get more feedback in the meantime.\nProposed Collateral Factors\n\n\n\n\n\nCurrent\nRecommended\n\n\n\n\nDAI\n75%\n80%\n\n\nUSDC\n75%\n80%\n\n\nBAT\n60%\n65%\n\n\nCOMP\n60%\n60%\n\n\nETH\n75%\n75%\n\n\nUNI\n60%\n60%\n\n\nWBTC\n75%\n60%\n\n\nZRX\n60%\n65%\n\n\n\nAs we mentioned in the discussions around the WBTC collateralFactor, WBTC poses the largest risk to the protocol and collateral requirements should be increased. However, as the community just voted to change this, we’ll leave this parameter out this proposal, and include it in the next one. We’ll also be making some changes to COMP speed, as these collateralFactor changes could encourage more circular borrowing in the protocol:\n\n\n\n\n\nCurrent COMP Speed (COMP/block)\nRecommended COMP Speed\n\n\n\n\nDAI\n0.067\n0.050\n\n\nUSDC\n0.067\n0.050\n\n\n\nStress Test Results\nOur stress tests are described here 13, but to recap they:\n\nDeploy the Compound contracts in a test environment\nSubsample the current distribution of borrowers to create a representative set in the test environment\nAdd synthetic price trajectories for each asset that are modelled from historical market data\nAs the prices change, positions become eligible for liquidation, and agents in the test liquidate the collateral on Compound via a slippage curve fit to real data from centralized and decentralized exchanges\nRun simulations comprising 1-4 hundreds of times to predict potential insolvencies in Compound\n\nWe’ll focus on the main output of the simulations, which is “Net Insolvent Value” per collateral type. This is highly dependent on price volatility, which we vary across the x-axis below. A “Volatility Scalar” of 1 means that the stress tests used price paths that matched the historical volatility of each asset, a scalar of 8 corresponds to price paths that are 8 times as volatile as recent asset prices. For reference, the historically bad volatility of March 12/13, 2020 corresponds to a scalar of 7 or 8.\n1600×1458 81.4 KB\nZRX, DAI, USDC, and BAT have a very low risk of seeing insolvency, even under March 12 like conditions. We’ll recommend increasing the collateralFactor (and lowering the collateral requirements for borrowers) for those assets.*\nETH, COMP and UNI also carry risk, but after investigating further, that risk was mostly determined by the accounts relying on WBTC as collateral as well, so we’ll recommend those parameters remain unchanged. Accounts using multiple collateral types can create complications in data analysis, but we look at individual accounts to add color to the data.\n*One thing to note. The issues that were seen with DAI price fluctuations 5 late last year are still possible. Borrowers using DAI as collateral or borrowing DAI should be careful when approaching the collateral limits.\\nI very much dislike a concept of packaged deals, as this is very common way of manipulations. Change for collateral factors and change of COMP distribution speeds are different parameters and shouldn’t be packaged together inside one proposal. But rather voted separately, presenting voters opportunity to express their opinion on each change separately.\nDecreasing distribution initiatives to DAI and USDC will indeed likely to decrease recursive stable coin farming, but i think it deserves bigger discussion. Certanly even after previous adjustment stable coin markets are kind of overincentivised, but such big of a cut is unjustified in my opinion. I suggest we proceed slowly and observe results. Instead of cutting it in 20% in one move by going from 0.067 to 0.050 we should consider gradual decrease instead and observe impact on markets. And maybe return to that discussion next month, when we have data on results.\nOn the other hand, collateral markets are kind of underincentivised still, WBTC and ETH certanly could get more initiative. Previous adjustment sparked some interest in Borrowing side, which, in turn, moved supply interest into something more noticeble than zero. Currently, Borrow side of Collateral markets can boast sligtly negative interest on Borrowing of ETH and WBTC. Which is justifiable, as with current bullish expectations, going short feels a risky position for most. And yet, Borrow side of that markets is important for Compound, not only it generates Reserves for that markets, but it also provides interest for Supply side. As well as provide more balanced utilization. If we add to ETH and WBTC what we substract from DAI and USDC, not only it might help to adjust risk, borrowers are taking, but it will also likely move APY on supply side to higher numbers, bringing more liquidity for protocol. There’s also some concerns that together with big increases in valuation of ETH and WBTC we might observe actual decrease of WBTC and ETH, supplied to protocol, COMP distributions could help here too.\nSo on topic of COMP speed, i’d suggest consider something like that:\nDAI         0.067        decrease  to      0.062\nUSDC     0.067        decrease  to      0.062\nWBTC     0.01075    increase   to      0.01575\nETH        0.01075    increase    to      0.01575\nWhile incentives on stable coins are likely could be cut more,  i don’t think incentives for WBTC and ETH should be increased in bigger steps. Predictable distributions is strong feature of Compound protocol. And i think making small incremental adjustments on the way would bring protocol much further in the end.\nDecreasing total COMP emissions per day i believe is completely different topic. And should be discussed further. Recent surge in COMP price, while brought a lot of excitement not necessarily is here to stay and not nesessarily a call for fast action.\nI’m not quite convinced that both increasing of collateral factors for stable coins and WBTC collateral factor increase are coming at good time, as markets look quite overheated. At the same time i’m not convinced that WBTC liquidations is that big of a problem. ETH/WBTC pair on Sushiswap alone is over 700m of liquidity and another 300m liquidity is at Uniswap. I wasn’t a supporter of WBTC Collateral factor increase, but i wasn’t against it either.\\n\n\n\n Sirokko:\n\ncollateral markets are kind of underincentivised still, WBTC and ETH certanly could get more initiative\n\n\nagree, since ETH staking and very low interest rate on supply, the incentive to supply ETH needs to be strengthened. In addition to a more drastic increase in interest rates, higher COMP distribution on ETH supply is a good idea. Are there any other options?\\nThis analysis is great and really appreciate the easy to understand chart. I think more frequent quantitative analyses should be the standard of risk management for DeFi applications like Compound.\\nIt is not really our intention to create a package deal, merely to change the protocol in a way that is more or less neutral to existing rewards .\nCurrently there is a lot of recursive borrowing in Compound that provides little value to the protocol. Recursive borrowers will for instance post DAI collateral, and then borrow USDC, having a position that is paying interest to Compound in return for liquidity mining rewards. With the current collateralFactor (cf) of .75, this means that farmers will earn COMP commensurate to 1.75x of their funds deposited. Farmers often leverage multiple times when doing this, in this case they would then borrow 75 DAI against every $100 of borrowed USDC, and so on.\n\n\n\n\n\nAction\nCurrent Reward\n\n\n\n\n1\nDeposit 1000 DAI as collateral\n1000 * 0.067 COMP / Block\n\n\n2\nBorrow 750 USDC\n750 * 0.067 COMP / Block\n\n\n3\nDeposit 750 USDC\n750 * 0.067 COMP / Block\n\n\n4\nBorrow (.75 * 750 DAI =) 562.5 USDC\n562.5 USDC * 0.067 COMP / Block\n\n\n5\nDeposit 562.5 USDC\n562.5 USDC * 0.067 COMP / Block\n\n\n\n…\n…\n\n\n\nYou can quickly imagine that you can keep doing this over and over again to achieve farming rewards equal to L:\nL = 1 + 2(cf) + 2(cf)^2 … (cf)^n\nAs\n\uD835\uDEBA(cf^n), from n=0 to inf. =  1 + (cf) + (cf)^2 … (cf)^n\nAnd when cf < 1:\n\uD835\uDEBA(cf^n), from n=0 to inf. =  1 / (1-cf)\nwe have that L approaches the following as the number of recursive borrows increases:\nL = 2 / (1-cf) - 1\nGiven cfs of .75 and .8, the maximum leverage is:\nL(0.75) = 7\nL(0.8)  = 9\nThis means that if the cf is increased, the maximum leverage (and therefore farming reward) is now ~29% higher than before. Now we want changes to risk to be neutral towards rewards. This would imply a reduction of COMP rewards commensurate to the maximum leverage the cf enables.\nWith current COMP speeds of .067 COMP / block for DAI and USDC, lowering rewards ~25% to .050 COMP / block should offset the increased farming rewards. Given that people are not recursively borrowing a literally infinite amount of times, we rounded up a bit.\nNow, as you notice, just lowering the rewards in one market is not risk neutral. The problem is that we’ve seen these rewards greatly encourage borrowing across specific collateral types, and so we’re hesitant to further encourage something like WBTC collateral, which we believe poses a systemic risk. Then that leaves other markets like ZRX and BAT, but which are also receiving needed cf updates.\nWe’re happy to entertain any separate proposals for how to further distribute COMP, but right now, we’re making the changes we feel best balance the interests of the protocol users, COMP holders, and community members. The changes are made with the broad brush strokes provided to us by Compound governance. We try to make the proposal as targeted as possible, and try to avoid omnibus proposals that needlessly couple unrelated changes. However in Compound, risk is tightly coupled to reward, and we are doing our best to change one with minimal impact on the other.\\nIt’s good argument, that intention is merely to change protocol in a way, which is neutral to existing rewards. I’l take it for now. But keeping that in mind, let’s look at what you propose from broader perspective.\nFirst of all, we all agree that there is quite a position of recursive borrowing, which debatefully not that much valuable to protocol. I believe that is a bit more complex, but i think we could agree that it’s surely not THAT valuable, considering share of COMP distribution that strategy recieves. Currently stable coin markets recieve over 70% of COMP distribution, while all others combined get less than 30%. And that is while stable coins are less than half of supply, but majority of borrow side. Additionaly good share of stable coin markets is represented by recursive borrowing.\nI don’t see recursive borrowing as plain evil though. Yes, it wasn’t intended, but it’s still legit usage of protocol. It drives adoption and brings liquidity at the very least.\nI’ve read Gauntlet research paper and i understand your position that Compound protocol can handle increasing collateral factor for USDC and DAI market. But while it doesn’t create sistematic risk for protocol, it allows users to take more risk.\nAnd that is coming not at exactly calmest period for cryptocurrency markets. That is what i mean when saying that it’s potentially not the best time to allow users to take more risk.\nBut then who is main beneficiary of increased collateral for stable coins? COMP farmers, utilising stable coin strategies. We can pretty much safely assume that, as we do not see that stable coins are used for collateral to borrow volatile assets like ETH, or WBTC or others. Stable coin borrowing absolutely dwarfs everything else. Increasing CF for USDC and DAI is basically adding more initiative to recursive farming, like it wasn’t big enough already.\nSo point one: While Compound protocol could safely handle bigger CF for DAI and USDC markets should collateral factor be increased at that markets conditions, while main benefeciary of such move indeed would be recursive COMP farmers?\nIf we agree, that recursive farming isn’t that valuable, why push initiative basically promoting to use it more? USDC-USDC and DAI-DAI are most safe strategies who can easily take more risk from increased CF. USDC-DAI and DAI-USDC are less so due to\npotential DAI volatility, but still it’s nowhere near comparable to volatile assets.\nNow lets come back to your suggested solution for that.\nIt’s hard to estimate exact share of recursive borrowing in stable coins, as users can easily use multiply addresses, but just for a sake of simplicity let’s take it’s 60%. Like 60% of stable coin markets consists from recursive borrowing. That number doesn’t need to be accurate for purpose of that discussion, just for pure simplicity.\nThen you say if we increase CF, than those participants, can safely increase their leverage and potentially could get up to 29% more in COMP rewards for their capital invested. That if all of them do that, which might or might not happen. But then\nwhere do that bigger rewards coming from? Is protocol paying more? Nope, protocol distribute exactly same amounts. Ok, so bigger rewards for recursive farmers coming because they increase their share in stable coins markets. So basically, having\nsame capital invested, they will potentially be able to increase their share in USDC and DAI markets and thus dilute share of other 40% participants which utilise other strategies. So they will get more per capital, than pure non-recursive usage like\nSupply ETH - borrow USDC for example. But that is worst-case a potential outcome. So, potentially, 40% of non-recursive part of stable coin markets would get less of COMP distribution than before, which at maximum might be up to 29% less.\nAnd then your suggested solution is to cut COMP distribution to USDC and DAI markets, which guarantee decrease of 25% in rewards for every participant of USDC and DAI markets, unless you are specifically a recursive farmer, in which case you can make it up for yourself by doing more leverage and then your rewards per capital invested will stay same.\nSo the only loosers here are users, who are not recursive-farmers. Also such change imply that we actually decreasing total COMP distribution per day. As we decrease distribution speed for USDC and DAI markets and not putting that distribution anywhere else. This is certanly not neutral change to existing rewards.\nI’d say in that case the treatment is worse than desease. Not only we introduce CF change wich directly benefit specific strategy, but we go further, and then cut rewards for everybody else, who isn’t using that strategy. I wonder how that go well with statement that recursive farming isn’t that much valuable for protocol. Why then we introduce initiatives stimulating that activity.\nI would say if we go with increasing CF fo USDC and DAI we should let market decide about decreasing/increasing rewards proportions between recursive/pure borrowing. And not instead execute literally the worst possible outcome for other non-recursive users.\nI have no comments on your proposed CF on ZRX and BAT markets, these are relatively small markets and proposed change is rather conservative as well. I don’t see problems with that change for now.\nAnd on topic of decreasing recursive borrowing my position is same. I believe we should allocate bigger portion to other markets. In current conditions Collateral markets (WBTC, ETH) at the expense of USDC, DAI markets. Decreasing distribution on stable coins always should have decreasing effect on recursive borrowing as you only can leverage till it’s profitable enough, lower rewards lead to less sustainable leverage. Increasing rewards to Collateral markets at same time lead to bigger utilisation and make other patterns of usage of Compound more rewarding as well, even if slightly.\nIn the end, i would like to mention that i absolutely like job, Gauntlet is doing for Compound protocol, as well as involvment with community. But i believe this particular changes as they just were drafted certanly need considerations.\\n\n\n\n Sirokko:\n\nNot only we introduce CF change wich directly benefit specific strategy\n\n\nTo make one thing clear- raising collateral factors should help all users\n\n\n\n Sirokko:\n\nI believe we should allocate bigger portion to other markets.\n\n\nWe’re looking forward to changing the compSpeeds in future proposals as well\\n\n\n\n jmo:\n\nTo make one thing clear- raising collateral factors should help all users\n\n\nThat’s not true. Raising collateral factor for USDC and DAI benefits exclusevly specific portion of users who use USDC and/or DAI as collateral. And since 80% of borrowing side consists of stable coins, we can safely assume that primary beneficiaries are recursive farmers. (since if you using stable coin collateral most likely you still borrow stable coins with it, and that have no economic sense aside of COMP farming) How raising collateral for USDC and DAI should help a user, who isn’t using DAI or USDC as collateral? For example, users, who supply ETH or WBTC and borrow USDC or DAI? Maybe i’m missing something here?\nDecreasing distribution speeds of COMP for DAI and USDC markets, decrease rewards for ALL users of stable coins markets on the other hand.\nAnd that was the main thing i wanted to highlight in my previous post.\\nI want to preface my comments that I wish I had more time to do a thoughtful, independent analysis of each asset.\nTLDR: I would vote no on this proposal if it were made as is.\nWhy:\n\n\nI don’t like package deals. If you want to adjust multiple collateral factors at once, I think it is possible (unlikely) to do that. If you want to change multiple comp speed variables at once, I think it is possible (likely) to do that. Adjust multiple collateral factors and comp speeds in the same proposal is setting a bad precedent. If you had ample evidence for these changes, perhaps I would change my mind.\n\n\nI don’t see a reason detailed in this proposal to decrease the comp speed for Dai and USDC.\n\n\n\n\n\n jmo:\n\nWe’ll also be making some changes to COMP speed, as these collateralFactor changes could encourage more circular borrowing in the protocol:\n\n\nPerhaps you meant to say discourage? Either way, I disagree.\n\n\nI read through your February Market Risk Assessment. While the document is very well written and the first  14 pages do a great job outlining the protocol, I do not think the paper’s evidence is sufficient for justifying these collateral changes. While I am generally a proponent for increasing the collateral factors, I also believe sufficient evidence needs to be provided. My main concern for the review is what volume numbers you are using for each asset. Throughout the paper, $100m is used as the ether volume, but I believe that wildly underestimates today’s liquidity. The paper also cites volume stats from 2019 & 2020, which is well out dated by now. The only significant historical stat worth looking at is volatility. Estimated volume numbers should be as current as possible.\n\n\nI think the analysis also significantly discounts the state of liquidators and their sophistication. Liquidators are no longer just users. Whole protocols/dapps are being built to be liquidators.\n\n\nWith the limited research I have done per-asset, I think the CF can be increased for DAI, USDC, and ETH. If I had to include a fourth, it would be UNI.\\nFirst, I’d like to applaud @jmo, @tarun and team for beginning the process of methodical, consistent changes to risk and incentive parameters. This is a welcome improvement; thank you for stepping up \nProcess\nI agree with @Sirokko and @getty that packaging multiple sets of changes together makes it more difficult to:\n\nAnalyze the impact of changes / experiments\nExclude changes that have public opposition\n\nAs the community gets into the groove of governance, it makes more sense to start slow; a proposal with specific risk changes, then a proposal with specific incentive changes (or vice versa). If they can be separated, they should be, in my opinion. This goes for a category of changes, too: if we’re modifying risk parameters, why not start with one, and measure the impact of it, before changing many?\nRisk\nOver the past few days, the risk of the protocol has increased significantly; there is now $7.9B of assets supplied, and $3.50B borrowed from Compound. One month ago, we had $4.5B supplied. Take a step back – these are staggering numbers and velocity.\nPrior to increasing risk (by increasing CFs), I believe as a community we should evaluated every additional safeguard possible, to decrease the risk of the protocol. This morning I laid out one strategy for improving the liquidation system 6, and there are many other ideas/approaches. Optimizing CFs (and capital efficiency) is a tool for growth; now is not a time to put the petal to the metal, in my opinion–we already have extreme velocity. We should add better navigation systems, cameras, and tires to this vehicle, first.\nIncentives\nIn Proposal 35, your team laid out a great framework 4 to think about, and model incentives. Prior to changing incentives, let’s revisit your framework and scientific method; deliberately experiment with incentives, and measure the results.\nThe idea of modifying the COMP distribution is sound; we could modify one stablecoin, and one collateral market (taking @Sirokko’s idea of re-allocating it to collateral), and measure the results. Decrease DAI, increase WBTC, for instance (or, USDC / ETH, etc). How will this change ETH vs WBTC, or DAI vs USDC? This is great data to arm ourselves and the community with.\nConclusion\nThis is a great direction to set the protocol on–by getting the methodology correct up front, we’ll have a recurring system that works well to calibrate the protocol.\\nWe’re happy to start slow, so we’ll break these parameters changes up into a couple of proposals as you mention. Given gas costs and also the long lead time for protocol changes, I hope that we can build confidence to bundle changes in the future.\nTenatively, we’ll suggest the following schedule:\n\n\nTomorrow - Update the ZRX and BAT collateral factors. I know it feels like the “pedal [is] to the metal” but for these markets, the pedal very much is not.\n\nNext Week - Update the rewards for the USDC and another market\n\nLater Next Week - Update the WBTC collateralFactor as this is a case where the pedal may in fact be through the metal\n\\n\n\n\n getty:\n\nThroughout the paper, $100m is used as the ether volume\n\n\nAs mentioned in my original post, we use live market data for these values. We’re actually working on sharing some of these risk factors (like liquidity/volume) via a Dashboard, which I will post about later today.\\nWe’ve published the first proposal here:\n\n  \n      \n      compound.finance\n  \n  \n    \n\nCompound 14\n\nCompound is an algorithmic, autonomous interest rate protocol built for developers, to unlock a universe of open financial applications.\n\n\n  \n  \n    \n    \n  \n  \n\n\n(Ethereum Transaction Hash (Txhash) Details | Etherscan 3)\nThanks again for all of the feedback!\\nHey @jmo , thanks for the analysis plot, a few questions:\n\n\nHow is volatility parameterised. Are you taking it as Gaussian or Power Law or something else?\n\n\nHave you done a simple convexity analysis? One example:\na. Calculate collateral levels with a 20% drop in WBTC/ETH prices within a time gap delta_t.\nb. Calculate collateral levels with a 40% drop in WBTC/ETH prices within a time period delta_t\nc. Check whether collateral is convex up or down comparing a. with b. (you can also check convexity with time). If it is convex down, the platform is under-collateralised. If it is convex up, it is buffering and may be sufficiently collateralised.\n\n\nHave you included second order effects of continuously falling WBTC/ETH prices as liquidations occur?\n\n\nHave you looked at second order effects of DAI (which itself will face collateralisation challenges in a downmarket)?\n\n\nGenerally, it strikes me that - at current collateralisation levels - a lot of trust in the liquidation mechanisms is required. I’ve laid out some high level ideas and suggestions here:\n\n  \n      \n      Pinotio.com – 13 Feb 21\n  \n  \n    \n\nAre DeFi Lending Platforms like Compound and Aave market neutral? 7\n\nLending platforms tend to be heavily long crypto and would do well to monitor collateralisation in extreme downmarkets.\n\n\n  \n  \n    \n    \n  \n  \n\n\\nI’m deeply saddened to see this fail because not enough votes were cast.  While I definitely support keeping proposals small and simple, I wonder whether the small number of votes was because the economics of the change were too small or affected too small a group, such that voters didn’t want to bother wasting time or gas to vote.\nMaybe worth pointing out again that everyone can vote for free at comp.vote, and also highlight the site whenever we link a new proposal.  I definitely forget to use it sometimes, even though it saves a few dollars for each vote.\\nWe’re going to propose again in the next day or two. I think as gas prices increase, this will happen more often, so we’ll just try to reach out directly to a few token holders to see if we can push things over the line for this proposal.\\n\n\n\n Pinotio.com:\n\nHow is volatility parameterised. Are you taking it as Gaussian or Power Law or something else?\n\n\nOur price paths in simulation use Geometric brownian motion, using historical data to set the volatility.\n\n\n\n Pinotio.com:\n\nHave you included second order effects of continuously falling WBTC/ETH prices as liquidations occur?\n\n\nWe then add in a “price impact” from liquidations that we train from data on Coinbase Pro and Messari:\n\n(This picture is from an old market snapshot but should give you an idea)\\nThanks @jmo ,\nRe Point 1. I guess I’m just saying what you know here, but geometric Gaussian won’t cover price jumps or power law type behaviour - the specific behaviours that would likely give liquidations most issues?\nRe Point 3. Thanks for this plot showing the fits. Is it possible to share a point where you highlight 2017 and 2020 crash data among all of the other data points.\nCurious to get your answers to Qs 2 & 4 (even if you could give me a sense of what kind of convexity you see in the models that would be worthwhile).\nAdding one more question - 5:\n\nHave you built a list of assumptions covering what is not in the model but could put the liquidation mechanism at risk? Could you share that list?\n\\n\n\n\n Pinotio.com:\n\ngeometric Gaussian won’t cover price jumps or power law type behaviour\n\n\nWe’ve used jump diffusion in the past but it’s not super useful. Collateral factor of 80%?  Add in some 25% jumps, yeah liquidators come and force insolvency immediately. Highly volatile GBM price paths cover a wide variety of scenarios. Jumps just don’t clearly make the model more accurate, though is a fun corner case to look into. It’s my understanding that power law price movement is used to approximate the exact sort of liquidation spiral we explicitly simulate, why did you want to look into it?\nRegarding future model features, we plan to share more info on that over time. Right now, we want to use these models to make Compound better, and our focus is on pushing parameters changes that do so. We’re more than happy to answer questions from community members that help them understand why the model is valid, but at the same time encourage everyone to do their own analysis as well. You should feel free to do any of the complexity analysis you mention above, I’d be delighted to read it.\\nHow do we feel about raising stablecoin reserve factors?\nDAI is at 15%, and with currently high rates and utilization, reserves are building up at a rate of $30M+ per year. USDC and USDT are both gaining reserves at a steady clip, but I don’t see the harm in laying on the gas by increasing the reserve factors for these assets.\nIf USDC and DAI have their collateral factors raised, increasing the reserve factor could also play a role in reducing returns from circular leveraged positions.\\nI think rasing CF for DAI and USDC isn’t going to be justifiable any time soon. Assets in Compound growing at rocket ship speed, increasing risk more in that conditions kind of so-so idea.\nAs for reserves, that makes sense, as reserves are kind of at a low side relatively to value of assets locked. I’d say raising USDC reserve factor from current 7.5% to 10%  Makes sense. USDT kind of lesser market, zero CF, it could be left as it is.\nI m not convinced DAI reserve factor should be increased further for now. As well as in general, i’m not convinced that we should adjust that parameters every couple months either.\nThat being said, while increasing reserve factor makes sense for USDC market, due to it’s size and current low reserves, for other stables it’s probably fine, but open for discussions. I initially was for setting higher reserve factor for Dai than it currently is, but it was decided to go with 15%. Should it be put at 20% just couple of months down the road later, not sure.\nThinking about it, may be it makes sense to create a sort of policy on reserve factors and maybe CF as well, to be reviewed let’s say every quarter. 3 month long enough period for data collection, and some sort of schedule will help with predictability of adjustments.\\nThanks @jmo , I’ve linked the article I did write above on how I would think about collateralisation and yes, it would be great to get your feedback on that."
  },
  {
    "number_of_comments": 17,
    "postid": "6fcf3b91-b276-40e4-9995-5c0edf23a505",
    "posturl": "https://www.comp.xyz/t/add-market-rai-reflex-index/2045",
    "combinedcontent": "With all of the talks of regulation, specifically, stablecoin regulation, I believe it is time to start supporting stablecoins that are not pegged to the USD.\nUSD-backed stablecoins have great centralization risks. While they provide great utility in terms of liquidity and while they fuel the growth of the ecosystem, the US government has an awful lot of power over them. Additionally, they have larger attack surfaces with owner accounts being able to mint, freeze, transfer, or burn the stablecoins their contracts control (depending on the stablecoin).\nWhile DAI is a great stablecoin without the centralization concerns of fiat-backed stablecoins, it has two primary drawbacks.\n\nIt’s pegged to the USD and the US government may try to regulate it because of this.\nIt’s unable to use negative interest rates as a tool to stabilize its price.\n\nThis is where Rai Reflex Index comes in. It is backed solely by ETH and it utilizes positive and negative interest rates to keep the price stable. Furthermore, it uses a PID controller to stabilize the price. For those of you who don’t know what that is, it’s the same integral-calculus-based mechanism cars use to ensure that all the tires provide the same velocity, ensuring the car always goes straight forward when the wheels are pointed straight. PID controllers are very effective at providing stabilization.\nWith a trading volume of only $5M and a TVL of only $96M, the PID controller is doing an excellent job of stabilizing the price of RAI - currently floating around $3.\nAdding RAI to Compound will give it greater strength and legitimacy. The more people using RAI and the more liquidity of RAI within common DeFi protocols, the more stable its price will be.\nFurthermore, adding RAI to Compound will help in reducing centralization risks of USD-backed stablecoins, giving users an alternative in addition to DAI.\nWhile RAI’s controller is currently controlled by a multi-sig, there are plans in motion to drastically minimize governance 1.\nLet’s get the discussion going!\nLinks:\n\nWebsite 3\nRAI tl;dr 1\nGitHub 2\nCoinMarketCap 2\n\\nThanks Tyler for posting this!\nI wanted to mention that Reflexer is aware RAI is still in its infancy and adding it to any new market means it will need to have a 0% collateral factor.\\nI think this could be a great inclusion and not too much risk with a zero CF.\\nI am totally for this! Reflexer is great asset to reduce the risk of stabelcoin regulation. The PID theory is a proven one.\nDAI is these day backed by 60% USDC. What if regulators demanded that Circle blacklisting USDC inside collateral DAI.\nSo we must adopt to very quickly changing environment.\\nSupportive of adding RAI as a decentralized non-pegged stable asset! More decentralized stablecoins on Compound will help reduce regulatory risk throughout the DeFi ecosystem and provide more liquidity and optionality for users\\nDon’t think RAI has the maturity to be considered at the moment. Just my thought\\n\nDon’t think RAI has the maturity to be considered\n\nWhat kind of risk does immaturity bring?\nIs there high liquidation risk for users?\nAnd, what risks would not be solved by a 0% collateral factor?\nIs there a brand risk?\nDoes it add any regulatory risk?\\nIt looks like the price oracle provider Chainlink doesn’t have an oracle set up for Rai.\nSo there may need to be some work with Chainlink to set one up.\\nHere it is: https://data.chain.link/ethereum/mainnet/crypto-eth/rai-eth 8\nNote: That website is using the wrong logo but the price feed is for RAI Reflex Index.\\nHi all, I wanted to restart the discussion about having RAI on Compound.\nRecently Idle Finance added incentives for RAI depositors and we think it would be great to combine the Idle and the Compound integration in order to bootstrap the Compound market.\nRAI also started to get traction 1 as a reserve asset and there’s currently more than $45M worth of RAI on other lending markets (counting recursive lend and borrow).\nWhat are next steps to see if the community is interested?\\n\n  \n      \n\n      OpenZeppelin blog – 13 Nov 20\n  \n\n  \n    \n\nGEB Protocol Audit - OpenZeppelin blog\n\n  The GEB protocol built by the Reflexer Labs team, is a stablecoin project based on the core design principles of the Maker's Multi-Collateral Dai (MCD) system. It brings new, exclusive features and is aimed to improve efficiency and user experience.\n\n  \n    Est. reading time: 33 minutes\n  \n\n  \n\n  \n    \n    \n  \n\n  \n\n\nLooks good.  But additional local auditing by compound team would be helpful.\\nI’m not sure I agree with Ripple’s relevance re: RAI. RAI is an ETH backed, real stablecoin and has no 1:1 peg with fiat. What people call “stablecoins” are simply pegged assets.\\nThis is why adding RAI to Compound is very important, because of its plans of “ungovernance” - governance minimization.\nThrough ungovernance and having Rai use a non-upgradeable contract with no centralized controls, it would be very hard to regulate directly.\nIt’s important to remember that the SEC does not rule over the whole of crypto and neither does any centralized body. Compound is becoming more and more decentralized along with many other projects, suggesting that Compound protocol is not exactly American. I say this as a Canadian with no ties to the US.\nThe more Compound and its users adopt censorship-resistant stablecoins, the less control the US government has over it.\\nHi Compound Community!\nMy name is Salomé, I am a Swiss digital nomad and a member of Idle.finance DAO’s Treasury League. This proposal just got caught up in the radar of our community and I decided to jump on board to provide some info and help here.\nLets build money Legos Sers! \nI would like to highlight that Idle.Finance 1 is available to give support to the Compound community for moving forward with this proposal, either on the development or communications side. Since RAI is already integrated within the Idle strategies, the implementation process here should be fairly quick. If you have any queries, just let us know!\nTo the Stablecoin discussion I would like to add that the stablecoins market continues to innovate, so one-size-fits-all regulation is a poor choice if we want to take advantage of the technology and curtail its risks. We are still very early in this game.\nFor example, we have to separate Fiat pegged stablecoins from flat stablecoins that are not pegged to any fiat but use another reference instead (inflation, basket, electricity price, ETH etc).\nStablecoin issuers generally use one of two mechanisms to keep stablecoin value pegged to that of a reference asset:\n\n\nAsset-backed stablecoins maintain asset reserves in fiat or crypto as collateral\n\nAlgorithmic stablecoins use the automated execution of smart contracts to maintain price stability\n\nAsset-backed stablecoins have raised concerns about security and whether they create direct contact points with the traditional financial system that might cause systemic risk.\nAlgorithmic stablecoins face primarily technological and operational risks, such as whether their smart contracts function as expected – It is possible to mitigate the unintended consequences of the protocol’s economics, incentives, and vulnerability by identifying and mitigating possible disruptions from malicious actors.\nFeel free to have a look at a research paper I wrote a while ago (PDF) Applications of CBDCs and private stablecoins: Comparative analysis 1\\nRAI offers an alternative to centralized stablecoins that DeFi needs and it would see significant use on Compound.\nThe ungovernance process aims to remove the ability to change key parameters and have a fully automated system. This means that RAI holders and minters won’t have to take into account governance risks when using the system. It should also prevent governments from directly interfering with RAI/Reflexer.\nUngovernance is a huge divergence from almost every other project in the DeFi space and a necessary hedge given the current regulatory climate in the United States and elsewhere.\n\n  \n\n      twitter.com\n  \n\n  \n    \nJake Chervinsky 1\n@jchervinsky\n\n\n  RAI is vastly underutilized within ETH DeFi. For a variety of reasons, we should change that.\n\n\n\n  11:06 AM - 16 Sep 2021\n\n    \n      \n        \n      \n      391\n    \n\n    \n      \n        \n      \n      47\n    \n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nFurthermore, I think RAI would see significant use on Compound on both the borrow and supply sides. I won’t get too in-depth into RAI’s PID mechanism but basically, if the RAI market price is above the “redemption price” (e.g. the target price that the system wants to achieve) then it begins to decrease the amount of collateral needed to mint RAI, which creates more sell pressure. Conversely, if the market price is below the redemption price, the system increases collateral requirements.\nThis means that during times of volatility there can be significant demand for borrowing and lending of RAI, which would generate significant revenue for Compound.\\nQuite supportive of this one! RAI would be a great inclusion and the exposure that Compound would bring would really be a bump to its visibility.\\n@FoxRon please stay on-topic.\\nNo problem. I understand you, I am not an american either, but now, at this moment, literally next year, the White House will show us a vector of direction."
  },
  {
    "number_of_comments": 12,
    "postid": "c08569cb-7107-4cf6-8dc1-0411dd58c042",
    "posturl": "https://www.comp.xyz/t/gauntlet-reserve-factor-optimization-framework/3283",
    "combinedcontent": "As planned in an earlier forum post 26, we have updated our dynamic parameter methodology to include recommendations for the reserve factor, as well as our usual parameters. To outline our thinking behind this addition, we split lending parameter optimization into a two step process:\n\nBalancing losses due to insolvency with capital efficiency\nBalancing the tradeoff between increasing supplier APY and growing reserves to cover insolvency losses\n\nIn the first stage of our modeling, we focused on optimizing capital efficiency within the community’s acceptable risk tolerance. Our primary levers in this optimization are the Collateral Factor and Liquidation Incentive, which most importantly allow us to manage the overall Value at Risk (VaR) of the protocol. We seek to balance VaR against borrow usage, which we use as a metric for the capital efficiency of the protocol. Since losses to a lending protocol occur during insolvencies - events where collateral is unable to cover the full value of a liquidated loan - controlling the frequency and severity of such cases is key to managing the overall value at risk. Within our simulation framework, the Collateral Factor drives the likelihood of a given loan triggering its liquidation threshold. The Liquidation Incentive parameter then drives the cost of liquidating the collateral and by extension the severity of insolvency loss.\n\nScreen Shot 2022-05-31 at 4.51.05 PM1008×718 18 KB\n\nA schematic view of the first stage of our modeling process is shown in the diagram above. Starting from the top left, we search through a range of lending parameters, which serve as inputs to the simulation. The simulation output then determines the level of risk and protocol efficiency for the trial set of parameters, which are evaluated against the protocol objectives. Finally, we can adjust the input parameters based on feedback to more closely meet the objectives. Now that we have reviewed the process used so far, let’s consider how the reserve factor changes things.\nHow does the Reserve Factor fit in?\nWe are now adding a layer to the existing simulation logic that allows us to reflect the second step of lending protocol risk management: building sufficient but not excessive reserves to absorb losses that do occur. Our model assumes the protocol would like to build reserves greater than insolvency losses by a small margin, and includes this as an optimization objective. As reserves are built and losses occur in the simulation, a higher setting for the reserve factor allows us to absorb more losses while maintaining reserve coverage, while a lower setting for the reserve factor would require losses to be managed more tightly.\n\nScreen Shot 2022-05-31 at 4.51.15 PM1320×736 24.4 KB\n\nA schematic for the new model structure is shown above, incorporating all the existing elements of the initial model and the new reserve factor layer. This illustrates where the reserve factor fits downstream of the simulation logic and how reserves are measured against losses as a model objective. As we continue to gather data on reserve factor adjustments, we expect to also incorporate the second-order effect of the reserve factor on the simulation itself, which is shown here by the dashed line. Because setting aside reserves locks up some assets that would otherwise be available to protocol users, there may be an indirect effect on user economics and thus capital efficiency from this. To refine our model here, we plan to recommend incremental adjustments to reserve factors that will gradually move reserve growth towards protocol targets while allowing us to further study indirect effects as the data becomes available.\nOur Recommendation and Reasoning\nRecommendations:\nWe recommend decreasing reserve factors for USDC, USDT, and TUSD from 7.5% to 3.75%, and decreasing all other reserve factors by 500 bps (for example, decreasing ETH reserve factor from 20% to 15%).\nReasoning:\nThe one-week rolling average VaR for Compound in a Black Thursday magnitude event is $0.30M.\nThe protocol has an existing reserve pool of $42.87M, broken down as follows:\n\nScreen Shot 2022-05-31 at 4.58.06 PM2494×970 98.8 KB\n\nThe protocol is expected to build reserves at a $3.86M annual rate should the reserve factors remain unchanged, broken down as follows:\n\nScreen Shot 2022-05-31 at 5.04.55 PM2458×1164 103 KB\n\nSince the existing pool and growth rate are more than sufficient to cover a very severe loss event, we believe that the optimal level of reserve factors is well lower than they are currently. By implementing the recommended cuts, the protocol could reduce its estimated annual reserve growth rate from $3.86M to $2.33M, reallocating $1.53M of funds to suppliers and thus incentivizing more users to lock collateral in Compound. As we continue to improve our model, we look forward to updating the community on further progress and fine-tuning our reserve factor recommendations in a future forum post.\nBy approving this proposal, you agree that any services provided by Gauntlet shall be governed by the terms of service available at gauntlet.network/tos.\\nI’m in favor of reducing the current reserve factors.\nWith the total funds in Compound having been much larger in the past, the protocol has effectively already banked about six years worth of reserve (at the current protocol size) in the last year alone.\nAm I correct, @klinsuain, that if the overall total borrows and overall total supply went up, say 3x, and all other things stayed equal, then we would want to raise the reserve factor again?\\nThanks @dvf. Changing the reserve factor up or down would depend on the insolvency risk. Borrow and supply amount are factors that go into insolvency risk, but there are a multitude of other factors as well (e.g. user collateralization ratios, liquidity of the borrow asset in secondary markets, asset volatility, etc). Our simulations incorporate these data and provide insight on insolvency risk, and reserve factors will be adjusted accordingly.\nAs an illustrative example, if the supply increases 3x and the borrows increase proportionally, then there will be more total interest going to the reserves. If the insolvency risk remains unchanged, then it may be reasonable to decrease reserve factor even more. If the insolvency risk scales proportionally, our models may recommend to keep the reserve factors the same. If the insolvency risk increases at a higher rate than the reserves, then it may be prudent to increase the reserve factors.\\nAs a heads up to the community, Gauntlet will publish an on-chain vote on Sunday, June 12, by 5 PM PT for these reserve factor changes outlined above.\\nUpdate from the Gauntlet Team:\n\n\nLinked Proposal: Compound 25\n\n\nWe are putting up 10 actions due to that being the limit for proposals.\n\n\nShould the vote pass we will look to proceed with the remaining 7 we had prepared, as we cannot put up two proposals at the same time.\n\n\\nBefore i present you my opinion, let me tell you story. Imagine you are an owner of retail store. And one day manager comes to you with a presentation of his idea. It’s a very nice presentation, with graphs and maybe even some math. But the core idea of that presentation is he suggest you to cut profit margines for your store, and  the money, saved from cutting down your profits, be used to pay more for your suppliers, so then suppliers could bring more supplies to your store. And then you look at very logical presentation and look at your supplies, and then you ask just a one question. Ok, but wait a minute, i look at supply of one thing (usdc) and i see it utilised by about 35% and i look at supply of another thing (dai) and see it utilized at about 45% and i as an owner would prefer it actually about 60-75. And you seriously suggest to cut our profits, in favor to bring more of the dudes i don’t even want to have? I was actually thinking how to get rid of 1/3 of them, so utilisation would go up and remaining ones would actually earn more? How long that manager would keep it’s job i wonder?\nSo now, when you see the picture, let’s talk why this proposal is bad for protocol and why pretty much anyone in their mind should vote against. I don’t know where Gauntlet got that idea, that protocol reserves are there only in case of some unlikely insolvency moment. It’s not. It’s earnings of the protocol, which can be used to cover insolvencies, but also it can be used to fund development, or create a paycheck for some teams doing some job for protocol, like Gauntlet itself. So irony is that they trying to chop a branch on which they are sitting themselves.\nFor those who might not fully understand, reserves actually work like a sort of a tax protocol puts on a revenue, it collects from Borrowers. And the Borrowers side is actually the ones who actually create a profits. Be it in speculations which they do with borrowed assets, or maybe they use Compound as a long term cheap funding for their business. That reserves rate is actually very small to be honest. If USDC, for example, have Borrowing APY about 2%, that 7% is taken from that 2% APY and the rest is distributed to suppliers. Do not be decieved by seemingly big numbers of current reserves, much of them were collected from vastly higher APY, which used to be even double digits at a times. It takes years to collect any meaningful amounts with such taxation rates. And Gauntlet servicies aren’t coming for free, not to mention there is also OpenZepellin with 1M$ every quarter. How the heck you guys imagine to be able to collect your own paychecks, especially when COMP tokens will run out, and they going to run out fast especially if markets go into prolonged bear, as the cheaper the COMP tokens become the more of them protocol would need to spend to cover costs.\nIt wasn’t very smart idea to rely only on COMP tokens to begin with, as they are generally limited, and it’s not in the best interest of protocol to spend it’s treasury tokens when price is very much in bear territory. But in any case they WILL run out eventually, and the costs for protocol WILL NOT disapper, they can actually go higher due to inflation. 3.86M per year of reserve growing ain’t even going to cover OpenZepellin alone not to mention anyone else.\nI strongly advice to vote NO for such proposal, as it undermines protocol long term revenue, putting it in much worse position as it is currently. And to be frank, potential increase of yield for suppliers going to be as negligeble, that pretty much every individual supplier would need a microscope to notice it. Even if we take all the potential savings of 1.53M and put it exclusevely on stable coin markets, that would be somewhere around 0.05%APR increase, which of course, would’t make anybody to earn more, as if amount of suppliers increase, the APY will go down to reach pretty much similar balance point it was at before.\nSuppliers are not a priority for Compound, they not earn anything, that is expenses that borrowers pay for capital. And if there is no borrowing demand, the suppliers have to go anyway, till yields can reach equilibrium at lower point.\nIf anything, protocol should consider increasing reserve factors on primary earning assets (stable coins) and unifying them to be in line with each other. The primary guidence here should be that rate of taxation on borrower payments should be low enough to be considered not prohibitive, which for stable coins is probably something not higher than 10%. For collateral assets like wbtc and eth that could be much higher, as those assets do not really produce any meaningful yield for supply side, and most suppliers would’t mind to not recieve those pennies at all, as it’s all in range of less than 0.1% APY. Thus reserve factor could be all the way to 50%. If some suppliers are not happy with not recieving yields they are very welcome to withdrow and go anywhere else, as for collateral markets there is negligeble borrowing demand, and if there are no borrowers, you don’t really need many Suppliers. Minor markets are negligeble for Compound as reserve assets, and thus reserve factor for small markets could (up to governance) potentially be zeroed without any significant downside for protocol.\nThose people who plan to support decrease of protocol revenue, you have been warned.  If we want to have a sustainable future-proof financial system, we couldn’t build it in same way as current pretty much collapsing financial architecture is. Protocol have to be able to generate enough profits to maintain it’s own development, bug bounties, risc analytics, and do that without relying on VC, or tokens issue or similar temporary measures. Think about it, and vote wisely.\\nNo one stood up to challenge Gauntlet. Gauntlet initiated many proposals with low utility by modifying tiny parameters, but the optimization of protocol revenue was negative, and cancelling the employment of Gauntlet was also a good proposal.\nIt can save a lot of money for compound protocol.\n\n  \n      \n\n      DeBank\n  \n\n  \n    \n\nDeBank | Your DeFi wallet 10\n\n  A multi-chain portfolio tracker that supports the largest number of DeFi protocols.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nBy the way, this is the risk point that leads to huge bad debts in the protocol.\\n@Sirokko makes a strong argument against Proposal 109. Currently, most markets are over-supplied, and decreasing reserve factors (to raise supply rates, and slow the growth of reserves) does not seem like the correct direction (at this moment). I plan to vote against Proposal 109, but I’m very excited and grateful that Gauntlet’s proposal is kicking off an important discussion, and the work @klinsuain is proposing is a needed improvement.\nIf there are more reserves than value at risk, it might benefit the safety of users to look at alternate approaches to manage excess reserves, e.g. reallocating from individual markets. A great example are the following markets:\n\nBAT: $1.48m borrowed, $1.13m reserves\nZRX: $1.08m borrowed, $0.43m reserves\nUSDT: $487m borrowed, $3.07m reserves\nUSDC: $304m borrowed, $13.26m reserves\nETH: $11.73m borrowed, $0.79m reserves\n\nGiven the markets above, should the protocol consider converting/migrating reserves from BAT/ZRX into USDT, etc, which has the worst ratio of borrowing/reserves? Or to convert excess reserves from BAT/ZRX to be able to cover bad debt from decreasing price scenarios (USDC) and rising price scenarios (ETH)? Generally, the super-majority of borrowing demand (and risk) comes from borrowing stablecoins, which are the asset most likely to not be returned via bad debt.\nIn addition, I’m looking forward to the near-term release of Compound III which could have its own reserve needs that could be bootstrapped from the excess reserves in the current protocol.\\nI was going to vote no anyway cause for the simple reason, the protocol currently pays more for services like openzeppelin and gauntlet than the reserves grow, so there is no point to reduce it further, but nice to see others have objections aswell.\nThe protocol will eventually run out of COMP, then we will have to use the reserves to be able pay for services.\\nThanks for the constructive feedback @Sirokko @rleshner @blck. As you all have highlighted there are numerous considerations for the protocol now and in the future. What we hope to agree on is that active governance through new proposals is a prerequisite for the sustainability or bootstrapping of future markets mentioned above. This proposal makes tradeoffs, which if passed will provide additional data to glean from subsequent user behavior.\nWe are happy to assist or facilitate reversing these reductions and potentially increasing RFs in the short-run (weeks/months). In the long run, migrations and Comp III can and should be considered. We look forward to those discussions.\\n\n\n\n klinsuain:\n\nBy implementing the recommended cuts, the protocol could reduce its estimated annual reserve growth rate from $3.86M to $2.33M\n\n\n@klinsuain, appreciate your work in adding Reserve Factor to the simulations.\nQuestion: If collateral asset prices keep falling, then borrow capacity (in raw USD) will decrease, and, as a result, protocol’s interest income will decrease too, even if the Reserve Factor stays constant. For example, if ETH falls 50% from here, will the estimated annual reserve growth rate of $3.86M still hold good? What assumptions are you using in this estimation?\nBased on the current market conditions, and also because VAR doesn’t adequately account for long tail events, I agree with others that it’s best not to reduce the Reserve Factor at this time.\\nShort answer is yes. Like everything else in the protocol the borrow amounts and borrow APYs are constantly changing, and as a result the annual reserve growth rate of $3.86M that we quoted at the time of the analysis is also variable (currently it’s down to $2.68M). Worth noting that as borrows decrease and reserve growth rate decreases, expected insolvencies will likely decrease as well, offsetting the effects to some extent.\nEvery time we generate recs we do so based on the current projected annual reserves, expected insolvencies, and current reserve pool. Should expected insolvencies or reserves change materially enough to affect our recs, we’ll cancel the proposal and create a new one with updated reserve factors. These are easy levers to change as they only immediately affect supplier APYs and reserve growth, as opposed to collateral factor changes which can have an impact on whether or not accounts are liquidated.\\nGauntlet is considering the community feedback above and will return to the community with an updated proposal."
  },
  {
    "number_of_comments": 22,
    "postid": "68cdc269-9192-4380-8d55-77f58baec0b8",
    "posturl": "https://www.comp.xyz/t/penn-blockchain-delegate-platform/3468",
    "combinedcontent": "Delegate Address: 0x070341aA5Ed571f0FB2c4a5641409B1A46b4961b\nForum: @pennblockchain\nEmail: pennblockchain@gmail.com\nExternal Website: http://pennblockchain.com/  8\nRunning Thread\nHere, we’ll start a running thread where we voice our opinions regarding our decisions to different proposals on the forums. We also encourage the other university groups involved with compound governance (@ucla, @blockchaincolumbia, @madhavvats(ChicagoDAO), Blockchain at Michigan, etc.) to create a delegate platform as well.\nOverview\nFor Background, hello everyone! We’re Penn Blockchain (@PennBlockchain on twitter), a leading, completely student run blockchain DAO from The University of Pennsylvania for both our undergraduate and graduate schools. The club currently has over 100 members and we’re expecting many more each semester!\nPenn Blockchain has committees covering Governance, Research, Education, Business Development, and Development/Web3. On the governance side of things, our team has different members leading governance initiatives for different protocols we have delegations for. Other current Penn Blockchain delegations include Maker, Aave, Uniswap, DyDx, Optimism, etc.\nEach week, governance leads will share updates with the club about new proposals and we will all debate about what to do before voting with our wallet. Our internal voting process takes place on the weekends and is an equal weighted democratic process. Proposals that will expire before the weekend are debated on our internal messaging platforms with a majority vote being our overall end vote.\nWe look forward to staying active within Compound and thank the community for their hard work!\\nReturn Accidentally Sent Funds #3\nWe voted FOR: Following the precedent set in #2 and rules outlined in #1, we vote to return these accidentally sent funds with the penalty.\nEnd Getty’s Contributor Grant\nWe voted FOR: Why work for Compound when you can create a Compound?  In all seriousness, we appreciate @getty’s contributions and thank him for his valuable work during the early days. We wish you the best of luck with Interest Protocol \\nEnable Sending ETH from Timelock (Revised)\nWe voted FOR: Interesting quirk with the Governor that we were not aware of. Glad this functionality was added in and we thank @arr00 for his contribution and well deserved COMP bounty.\nFYI: @compoundteam, if there’s any other quirks that need a-fixing, we’d love to take a crack at it!\\nInitialize Compound III (USDC on Ethereum)\nWe voted FOR: We’re excited for Compound v3 to come live and this was an easy no brainer. We want to congratulate the team for their hard work and also look forward to what this next version will bring!\\nCompound Oracle Upgrade v3\nWe voted FOR: Crucial to update the protocol’s oracle contract to v3 as well, switching the anchor market from UNI v2 to v3.\\ncETH Risk Mitigation\nWe voted FOR: Our reasoning is very similar to our reasoning for Aave pausing ETH borrows. The extra revenue generated from a couple days of high ETH borrows we fee is not worth the inherent risks around the uncertainty of the initial post merge defi landscape.\nRisk Parameter Changes for FEI\nWe voted FOR: Our hearts go out to eh FEI/Tribe team and hope they can peacefully wind down. There is no need for FEI on Compound anymore.\nInitialize Compound III COMP Distribution\nWe voted FOR: As the emphasize revolves to Compound III, we believe this incentive shift is a good move in reaching that goal.\\nWe have officially rebranded our Compound governance as FranklinDAO!\nEverything will stay the same, just in the spirit of Web3, our outward facing name is now FranklinDAO!\nPS: If a community admin can change our name to “FranklinDAO (UPenn) Delegate Platform” that would be very appreciated!\\nGauntlet <> Compound Renewal\nWe voted For: This was an overwhelming yes! We have seen the great work the team has done for Compound and think it should be continued for the following year. The analysis speaks for itself and after a nice talk with the team, we’re looking forward to their updated risk dashboard and new features in the future!\\nIncrease Compound III Supply Caps\nWe voted For: We think Compound III is ready to increase supply caps following the Gauntlet proposal. We look forward to the continuous growth of v3!\\nRisk Parameter Changes for FEI: Part 2\nWe voted For: In favor of the necessary steps needed to wind down FEI markets as the project unwinds.\\nAdjust cCOMP and cUNI Parameters\nWe voted For: Pending the analysis put forward by Monet and forum post/discussion, we believe the adjusted parameters are sufficient and should be inplemented.\\nPause Supply for Compound Assets\nWe voted YAE: Voting yes as a safety precaution in light of recent news regarding price manipulation possibilities. Will reassess to increase and adjust parameters in the future, but feel right now these more conservative estimates are safer.\\nLiquidation Event Handling And Collateral Reserves\nWe voted FOR: Thanks for the team for catching this minor “error.” This update will make analytics and etherscan view for liquidations easier to manage and see!\\nRisk Parameter Updates for 1 Collateral Asset\nWe voted FOR: Voting in line with our prior reasoning across Compound and Aave regarding adapting collateral factors to ensure safety and liquidation risks.\\nCIP-1: Compound Improvement Proposal Adoption\nWe voted YES: We’re happy to see the adoption of CIPs! Having been Maker and Aave delegates for a while now, this \"x\"IP structure is great for access and clarity and we’re excited to see what proposals come to be!\\nBorrow Caps for Compound V2 Assets\nWe voted YES: After reviewing the discussions between Paul from Morpho and the Gauntlet team, we are glad to see only the most volatile assets in this formal proposal, and are in agreement that these borrow caps should be changed accordingly. When it comes to an eventual proposal for the stable assets with a lot more on chain liquidity, we think maybe a compromise between the two parties is appropriate, or a slight increase in the caps recommended on the forum post.\nNote: Ethereum Transaction Hash (Txhash) Details | Etherscan\nWe submitted with about 2 minutes left, and high gas, but looks like on chain transaction failed. Might be a bug or UI issue where the voting technically ended before the UI showed it ended.\\nCGP 2.0 by Questbook\nWe voted YES: Looking forward to see the startup of the new grants program. We believe season 1.0 went pretty well and are excited to see the newly improved version. Also would love to get more involved and possible take on a research grant ourselves in the future!\\nRisk Parameter Updates for 1 Collateral Asset on the Comet USDC Market\nWe voted FOR: We are in favor of this Gauntlet proposal to increase the supply cap of COMP on the USDC market to increase to 900,000 as we trust their models’ judgements and think this is a reasonable change.\\nCGP Hackathon Sponsorships in 2023\nWe voted YES: We think these hackathon sponsorships are appropriate and look forward to seeing what projects arise from the tracks!\\nInitialize cUSDCv3 on Polygon\nWe voted FOR: We’re excited to see this first cross chain deployment of Compound III and look forward to seeing the eventual deployments across other chains as well! This deployment will bring millions and millions of more TVL and liquidity to Compound and we’re excited to see Compound go crosschain!\\nCompound V2 → V3 Migration Phase 1\nWe voted FOR: We’re looking forward to Compound V3 over the next few months and thank the team and all respective service providers for their hard work. \\nRisk Parameter Updates for V2 and V3 WETH Comet\nWe voted FOR: Voting to change the risk parameters for cWBTC and cbETH as specified in the proposal.\\nAnnouncement to the Compound community from the governance team at FranklinDAO.\nAs part of an effort to strengthen our internal organization and operational security, we’re migrating our delegate profile (franklindao.eth) to a hard wallet at 0xCc878369b26127BD4bC1B0B465bC8bE9b92a4c62.\nOur old address (0x070341aA5Ed571f0FB2c4a5641409B1A46b4961b) will remain active in protocol governance for the next month until June 15, 2023, so as to give everyone ample time to re-delegate their tokens.\nWe appreciate all of the holders that have entrusted us with their voting power and look forward to meaningfully improving the projects we all use on your behalf.\nWe are always accessible via the forums or Twitter @franklin_dao for any questions!"
  },
  {
    "number_of_comments": 16,
    "postid": "8a358e0b-67ce-4b99-9e64-42bc02bf9548",
    "posturl": "https://www.comp.xyz/t/acala-x-compound-chain-gateway-to-polkadot/1349",
    "combinedcontent": "tl;dr: Acala wants to build a Starport on Acala as a gateway into the Polkadot ecosystem. The purpose of this post is to share our proposal and receive feedback from the community.\nIntroduction\nAcala 14 pioneered the idea and implementation of a full-stack (Substrate + EVM) specialized blockchain focusing on decentralized finance (DeFi), and created multiple DeFi primitives that became a DeFi hub and infrastructure serving the Polkadot and Kusama ecosystem: multi-collateralized stablecoin using DOT and cross-chain assets like BTC, integrated liquidation process using Off-chain workers and a hybrid of built-in AMM and auctions, tokenized staked assets e.g. Liquid DOT (LDOT), and bring-your-own-gas where fees are payable in any supported assets like stablecoins.\nPolkadot is a layer-0 protocol that merely provides shared PoS security and cross-chain communication for its connected parachains. Other chain-specific and application-specific logic, such as EVM and smart contracts, are implemented on parachains. Acala’s parachain plays the role of DeFi hub and a landing pad that aggregates assets and liquidity from a variety of blockchains.\nAcala is a firm believer and a long-term builder for the multi-chain future - reducing liquidity fragmentation, increasing composability and DeFi accessibility to everyone, and sharing the same vision as the Compound Chain/Gateway 40. The Acala team has been building on and contributing to the Substrate code-base for 2+ years, and therefore would like to contribute to Compound Chain’s effort by deploying Acala Starport as a gateway into the Polkadot ecosystem, making such Starport implementation also available to all Polkadot parachains, and providing tooling to read events from and to both chains.\nProposal\n\nAcala Starport1852×1044 177 KB\n\nAcala would like to propose the following:\n#1 Deploy Acala Starport as the gateway into the Polkadot Ecosystem\nAcala will deploy Starport as a Substrate pallet, whereby all assets are downloadable to the Starport will be available to all DeFi protocols available on Acala, in the Acala EVM useable by smart contracts, cross-chain capable with Polkadot and its parachains, and later available in Ink! 4. Acala will also contribute to extend and make the events and offchain workers more generic for varied blockchains.\nWe will integrate the Polkadot{js} extension which is the predominant signer/wallet for the Polkadot Ecosystem to the Compound Cash Dashboard 28. We will also integrate the Compound-Acala service into the Acala DApp 10 and/or the mobile app via Polkawallet 11.\nAll of the Acala Starport code would be generic enough to be adopted by any parachains and Substrate-based chains for that matter, with a goal to accelerate the innovation for bridging more cross-chain assets together.\n#2 New collateral asset: DOT\nWe’d like to propose DOT as a new collateral asset on Compound Chain. DOT can be trustlessly transferred between Polkadot Relay Chain and the Acala parachain via Cross-chain Message Passing (XCMP) 9. Polkadot Relay chain keeps the DOT balances of each parachain, and the balances are updated when a transfer happens between the Relay chain and a parachain, as well as between parachains, all trustlessly. Since Polkadot Relay chain does not host any logic (smart contracts or pallets) other than those essential to a Relay chain, namely PoS consensus and cross-chain communication, DOT uploaded to Compound Chain will have to be done on a parachain. Shared Protected Runtime Execution Enclaves (SPREE) 4, which is on Polkadot’s roadmap (later the year), will add another level of security guarantee to this.\nAcala’s source code has been audited 2 and any new features will also be audited before launching on mainnet. Acala’s governance is managed on-chain for both voting and execution. We employ delayed execution for runtime upgrades and parameter changes on-chain. Monitoring can be easily set up to alert any suspicious behaviors as another added layer of protection for DOT integrity.\n#3 New collateral asset: Liquid DOT (LDOT)\nIn addition to DOT, we’d like to propose another native asset as collateral on Compound Chain - Liquid DOT (LDOT). LDOT ****is the tokenized staked DOT (similar to DAI-and-cDAI) bearing Polkadot’s staking yield. Polkadot’s economy has a targeted staking ratio of 50%, parachain bonding of about 30%, and the remainder is liquid. With an average staking yield of 20% APY, DOT is relatively expensive to be used directly in DeFi. And one also would not want to sacrifice network security for yield farming.\nTherefore Acala created the Liquid Staking protocol that allows users to trustlessly stake DOT, and use the tokenized staked DOT, LDOT, in trading, borrowing, lending and other DeFi activities. We believe with its trustless and yield-bearing nature, LDOT would be a quality and popular addition to the Compound Chain collateral set.\n#4 Download CASH and a potential fee token\nOnce CASH is downloaded to the Acala Starport pallet, it would immediately be plugged into Acala’s DeFi protocols, e.g. listed in AMM DeX, and also be cross-chain capable via cross-chain consensus (XCM) 2 to the Polkadot Relay Chain and other connected parachains. Acala will also make CASH available as a pre-compiled ERC20 contract readily composable in the EVM.\nBeyond that, we also propose to make CASH a native fee token on Acala. With pooled peer-blockchain-assets-to-protocol liquidity, the interest-bearing CASH can be expected to be a popular asset that users would hold and use to pay for things. It’s a great usability booster for CASH users to transact on Acala without the need to hold Acala/Polkadot native assets.\nMotivation\nPolkadot is a layer-0 blockchain scalability solution that provides network security as a service (“shared security”) and trustless cross-chain communication as a service to all parachains connected to it. Each parachain can be customized and optimized for its domain (i.e. Acala is optimized for DeFi), and many DApps (in the form of Substrate pallet or smart contract or both) can be deployed on each parachain.\nAcala aims to democratize finance and make DeFi more accessible to everyone with a full-stack chain to DApp approach to delivering the user experience that would not otherwise possible. Acala’s suite of DeFi products has gained significant traction through its slick DApp 10 design and native mobile experience via Polkawallet 11 (a founding member of Acala, Polkawallet has 45,000+ installs and a recognized tool by Polkadot 2). Acala’s live testnet (TC5) has 50,000+ new accounts, and 780,000+ signed extrinsics (See Subscan) with a large and ever-growing community. Acala has completed three security audits 2 from Trail of Bits, SRLabs and Slow Mist. As soon as parachain slots become available on Polkadot, Acala is ready to launch mainnet.\nCompound is one of the most respected and established protocols in DeFi, serving many users and as a DeFi primitive to many other protocols on Ethereum. Compound Chain is a significant step towards serving a much wider audience by bridging together digital assets from a variety of blockchains. Therefore it is beneficial to all three ecosystems for us to bring Compound’s service to the wider Polkadot ecosystem through Acala’s parachain.\nThe Acala team is one of the earliest Substrate Builders, members of the team (e.g. @xlc 3) are also top contributors (outside of Parity) to the Substrate code-base. Not only would we deliver high-quality code and products, we will also make the code generic enough to be applicable for all parachains and Substrate-based chains for that matter. We have done this before - we are the creator of the open-runtime-module-library  - a community repo that includes a set of common-good pallets to accelerate application development on Substrate. We have also pioneered XCM implementation for fungible tokens 2 which has been adopted by many parachains testing on Rococo (Polkadot’s parachain testnet).\nWe can’t wait to make this happen!\nDevelopment Plan\nMilestone #1 Starport pallet\n\nSupport lock to upload asset\nSupport execTrxRequest to execute transaction request on Compound Chain\nSupport invoke to execute notices from Compound Chain\nNecessary Governance features\n\nMilestone #2 Offchain Worker & Message Passing\n\nDevelop Substrate RPC client for Offchain Worker\n\nSupport subscript events to fetch and decode events from a Substrate full node\n\n\nBasic XCM asset handling in Compound Chain\n\nSubstrate based chain will use XCM as crosschain asset identifier\n\nhttps://github.com/paritytech/xcm-format 2\n\n\n\n\n\nMilestone #3 Integration & Oracle\n\nEnd-to-end integration\n\nRelease public testnet\n\nCompound Cash Dashboard 28 to support Acala Starport, assets and Polkadot{js} extension\n\n\nAdd LDOT and DOT price feed to the Open Price Feed\n\nMilestone #4 Dapps\n\nLaunch Compound-Acala service in Acala web and/or mobile DApps\nAcala Starport information will be readily queryable\n\nEffort Estimation\nTotal effort: 25 weeks (1 FTE)\n\n\nMilestone #1: 8 weeks (1 FTE Engineer)\n\nMilestone #2: 12 weeks (1 FTE Engineer)\n\nMilestone #3: 6 weeks (1 FTE Engineer)\n\nMilestone #4: 6 weeks (1 FTE)\n\n1-2 week FTE Designer\n4-55 week FTE Full-stack Engineer\n\n\n5 weeks FTE Product Manager\n\nDelivery estimate: ～3 months as M1, part of M2 and M4 can be developed concurrently. If we put 2 FTE Engineers on the project, then delivery can be shortened to ~2 months.\nRough bare cost: USD$111k~\nConclusion\nThe Acala team firmly believes that this contribution will be beneficial to Compound Chain, Acala and Polkadot ecosystem creating a win-win-win situation with a more evolved and cheaper money market service, more collateral asset variety, and much wider user adoption across multiple blockchains. We may request small amount of grant from the Compound treasury and Web3 Foundation to partially fund the initiative, but in principle, we are keen to carry out the plan regardless.\nWe hope to hear your thoughts, feedback and suggestions in the thread below.\nhttps://acala.network/ 15\nhttps://github.com/AcalaNetwork 8\nRegards\nBette 3 & Bryan @Acala\\nI think this is really exciting and would love to see Acala supported to start building the Polkadot Starport to Gateway. Also, I would like to welcome Bette, Bryan, and the rest of Acala to the Compound community, I believe they will prove to be major assets to us.\nTheir team has a great deal of experience and expertise in Substrate, and are very forward thinking. I can tell by the details of the proposal how much thought they have put in to understand the Gateway system and integrate with it in a really high quality way. I also applaud them for putting forth the first real Starport proposal to the community, as a really positive way to introduce themselves and start discussing the idea.\nI think there is probably some overlapping discussion to be had with the Compound Grants Program 13, which makes this especially relevant as that proposal is up for a vote now. I would love to see @sukernik or other committee members chime in with thoughts or feedback on how to approach this!\\nWhile I am cautiously optimistic the grants program will pass I’ll hold out commenting on that aspect until the voting has closed.\nGauntlet has been in talks with the Acala team for quite some time. We have been impressed with their rate of development. The Acala team and Compound community seem to have similar goals on governance, risk, and development. This is a great proposal and I look forward to hearing feedback from other community members.\\n\nWhile I am cautiously optimistic the grants program will pass I’ll hold out commenting on that aspect until the voting has closed.\n\nA little confused by this, wouldn’t it be good for voters to understand how the committee would approach it?\\nMy comment was not to skirt discussion about process. I did not want to assume CP040 would pass and side track a project Acala sounds like they will proceed with regardless. The initial and subsequent posts by @sukernik in Compound Grants Program 2 lay out our planned process so far. This will surely evolve. For a proposal with as large of scope as this one I would be tapping dev/data science on the Gauntlet team to weigh in. Maybe M1-M4 warrant separate grants, revision, or removal of certain aspects. I’m confident the Acala team and the grants committee would want to continue this conversation in the forums transparently.\\nThis is a fantastic proposal @bettec. It’s clear, concise, and well-documented. Bravo!\nTo mirror the comments of @jared and @inkymaze, I do think this proposal would be eligible for the Compound Grants Program 2 which is currently undergoing a community vote 4. Having said that, we don’t want to put the cart before the horse — let’s wait for the community to finish voting on the grants program before the committee begins to formally evaluate proposals!\nIn the meantime, maybe I can ask some questions in my capacity as a Compound community member (rather than the lead for the grants program).\n\n\nCould you please provide more details on how each milestone would benefit Compound and its community? I think it’s important for the community to know exactly how it stands to benefit from the work you propose to do.\n\nAre there challenges that you know of today that would make it difficult for you to achieve these milestones on time (e.g., challenges with R&D, lack of FT resources)? It’s important to list out these difficulties in advance for two reasons: (i) acknowledging them ahead of time makes it easier to prepare for the challenges, and (ii) perhaps Compound’s community can help with these challenges.\n\nI also agree with @inkymaze on the funding process. If the grants proposal passes and you apply for a grant, I do think it makes sense to apply for new grant each time a new milestone is met. For example, you could apply for a $10-$30k grant upfront to give you the resources to get started, and then apply for further grants as milestones are delivered.\nFor full transparency, I should also mention that I invested in Acala while I was at DCG. I know and respect the team, and I believe they can deliver what they say they will deliver. It’s also important to note that I have no economic exposure to Acala (and as far as I know, none of the grants committee members do either…if that’s not the case, they can chime in themselves).\\nCool,really like your thoughts~\\nThanks for the warm welcome and feedback.\nWe have approached the grants program to discuss this. Since the grants program is still being shaped up, we intend to proceed the design and development of the Starport in parallel of the discussion with the grants program. We will report back.\\nYup, that makes sense. This Proposal is more to lay out our overall intention so folks can have a full picture of deliverables and outcome. We will certainly need to engage relevant stakeholders and parties along our design and development process, whether that results in separating the milestones, or add additional (asset etc) deployment milestones. We will report back.\nThanks again for the feedback.\\nThanks Larry. Also applaud your transparency.\nTo answer your questions re benefits\n\n\n#1 Develop Acala Starport pallet\nThis will essentially bridge Compound Chain and Polkadot, allowing accepted Polkadot assets to be uploaded to Compound Chain as collaterals, and allowing CASH to be downloaded and used in Polkadot ecosystem. Polkadot in itself is layer-0, so it relys on parachains to deploy business logics like bridging and smart contract executions etc. Acala as the DeFi hub parachain offers a set of DeFi primitives such as stablecoin lending, AMM DeX and liquid staking, and therefore provides good asset variety and liquidity as a Starport to Compound.\n\n\n#2  Develop Offchain Worker & Message Passing\nThis will essentially allow any Substrate-based chain be able to bridge to the Compound Chain. This is certainly a common-good development that will enable many subsequent bridges to Compound Chain, although each Starport deployment will still subject to governance, the barrier of deployment is significantly lowered.\n\n\n#3 Integration & Oracle\nThis will ensure Compound Cash App works seamlessly with Acala to bring new assets (e.g. DOT and LDOT) on as collaterals, so as to download CASH to Acala. I can foresee this milestone can be broken down further to add economic and risk analysis for each new collaterals namely DOT and LDOT, and set up the initial parameters so as ongoing monitoring and adjustment procedure.\n\n\n#4 integrating Compound Cash into Acala Dapps\nThis essentially exposes the Compound Chain money market services to the Acala (and therefore Polkadot) users and community. In addition, integrating CASH into Acala DApps, using it to pay for transaction/gas fees will further the demand and utility of this new asset. We believe this will result in a positive trickle-up effect for the three communities: Compound, Acala and Polkadot.\n\n\nTo answer your questions re challenges\nAs we are one of the earliest and most experienced Substrate development teams, and after a thorough study of Compound Chain’s source code, we don’t foresee technical challenges. We do however anticipate potential challenges in setting up a brand new collateral asset class (DOT and LDOT). We will likely need help from experts like Gaunlet and the Labs team to help profile the new assets; in addition, we also need help to bootstrap initial liquidity - we can come up with liquidity programs, but it’s likely that it’d need support from multi-parties to make it work effectively.\nOther than that, in order to add CASH as a fee token on Acala, which would be a great use case of CASH and benefits the Compound community, we will need help to add CASH liquidity to our AMM DeX which is used as a unified fee settlement layer. Again we can come up with liquidity programs, it’s likely to be an effort of multi-parties.\nRe the grants program\nWe’re open to the suggestions and would also like to have a further discussion to learn more about the grants program. We’ll report back once having a chat with you.\nAgain thanks for the warm welcome and feedback~\\nThanks for the in-depth reply @bettec.\nFor everyone else following along — the Compound Grants Program scheduled a call with the Acala team to walk through the proposal in greater detail. If this proposal gets funded, we will be sure to update this thread with the details.\\nFor folks who are following along, I’d like to report back that we had a call with the Compound Grants Program, further detailed our plan and benefits to the COMP community with this initiative. And just heard back from @sukernik that the grant has been approved \nDetails of the grant as per below:\nTotal Grant approved: $35k\n\n1/5 upfront\n1/5 on completion of milestone 1\n1/5 on completion of milestone 2\n1/5 on completion of milestone 3\n1/5 on completion of milestone 4\n\nThe rationale behind this is that while the initiative is partially funded by the Grants Program, but it can hold Acala accountable for the entire delivery which is in the best interest of the Compound community.\nThanks again for everyone’s input, also shoutout to the Grants team especially @sukernik 's effort for moving this work forward. We will continue to update everyone here as we progress.\nRegards\nBette\\nawesome proposal! Cant wait to see it in prod.\nThanks for that proposal, Acala team!\\ntl;dr We have completed the first milestone of building a Starport pallet to upload & download Acala/Polkadot/CASH assets between Compound Gateway and Acala network (testnet), code merged & local testnet available \nCC: @sukernik from the grants team\n\nMilestone 1 Starport Pallet\nThis delivery includes\n\nSupport lock to upload asset\nSupport execTrxRequest to execute transaction request on Compound Chain （this is done on the Compound Chain side, on Acala chain, we just need to support the notice arrived #3 )\nSupport invoke to execute notices from Compound Chain\nNecessary Governance features\n\nWhich essentially enables the following features\n\nUpload Acala chain assets e.g. DOT, LDOT via the Starport to Compound Chain\nDownload Compound Chain assets e.g. Compound CASH to the Acala chain via Gateway Notice\nPreliminary administrative functions such as setting supply cap, setting authorities to verify Gateway Notices etc.\n\n\nLocal Testnet\nLive testnet integration won’t be available until Milestone #3 is delivered. Currently you can try the above features via Acala local testenet.\nHere’s the code & the guide 2.\n\nNext Step\nMilestone #2 is about cross-chain communication, specifically between the Compound Gateway Substrate chain and the Acala Substrate chain.\nThere are multiple options available to achieve this, including using Cross-chain Message Passing developed by the Parity team, alternatively using off-chain query service like Subquery and potentially other options. There were numerous exchanges between the two dev teams to finalize the design.\nIn addition, here’s a list of open issues 3.\nWill report back as we progress further.\n\nTestnet Examples\nUpload Acala asset e.g. DOT to Compound Chain\n\nstarport upload1446×725 107 KB\n\nDownload Compound Chain asset e.g. CASH\n\nstarport download1426×713 98.9 KB\n\nRegards\nBette  1 & Bryan 2 @Acala\\nCongrats @bettec - super amazing to see the connection between Compound Gateway and Acala! Can’t wait to see where this goes\\nFantastic update @bettec!\\nSuper exciting news !! Keep up the great work !"
  },
  {
    "number_of_comments": 10,
    "postid": "ab378ced-9f28-49b1-8c74-4a469493b036",
    "posturl": "https://www.comp.xyz/t/set-wbtc-collateral-factor-to-75/959",
    "combinedcontent": "This change increases the Collateral Factor of WBTC from 60% to 75%.\nOn October 1st, Proposal 24 7 was passed to raise WBTC’s Collateral Factor from 40% to 60%. According to Defi Pulse 2, on October 1st, there were ~$1B of WBTC in circulation. Today there are $4.38B of WBTC in circulation. In addition to having a much larger supply today, WBTC liquidity has significantly grown since Proposal 24 was passed.\nUniswap 2: $150M\nSushiswap: $256M\nBalancer 1: $200M\n1inch: $160M\nIn addition to growing liquidity on decentralized exchanges, centralized exchanges are continuing to adopt WBTC. Shortly after Proposal 24 passed, Coinbase added WBTC markets. As well, Binance continues to be a growing hub for WBTC/BTC 3 trading.\nWith the significant increase in WBTC circulation, liquidity, and infrastructure, it is safe for Compound to increase Collateral Factor to 75%. From a growth/business standpoint, AAVE  3currently has its Collateral Factor set to 75%. Compound Finance needs to remain competitive to avoid losing market share.\nIn order for the autonomous proposal to become a full-fledged proposal, it needs to reach 100K votes. Please delegate your votes by visiting app.compound.finance/vote  13 and delagating your COMP to 0x05c55b58b044342e89c343d2d6d53c08d26500bb 27\\nNice to meet you.\nI am in great agreement with your idea!\\nI support this,\nAlthough the WBTC is not trustless, by holding the token in protocol, we have already taken that risk. Increasing collateral factor may have only positive implications for protocol.\\nEchoing that, I support this.\\nMy main concern with upping the collateral factor here is the inherent price volatility in WBTC and BTC more generally.  (W)BTC can easily move 30% percent in a few days. As an extreme example, I think BTC moved 20%+ on March 12th 2020 alone.  We might run the risk of more forced liquidations than we’d like with a Collateral Factor at 75%.\\nIf price volatility of BTC or ETH force liquidation (global market price, not Coinbase market price), I will call it “healthy liquidation”.\nI am more concerned when stablecoin have a volatility of 30% (on one oracle).\n\n\n\n jthrack:\n\nMarch 12th 2020\n\n\nCompare losses from that event with DAI manipulation event where people lost around 90 million.\nI would interpret these 15% higher collateral factor as additional protection against liquidation. Because if the borrowed amount will remain the same, this move is excellent protection against liquidation.\\nJust chiming in here. I fully agree an increase in the WBTC collateral factor is due but think it’d be ideal to go to 70% first. Regardless, I will still support 75% if it comes up to a proposal.\\nWe (Gauntlet) want to provide some needed context on our vote and messaging around it.\n\nWe ran stress tests, and the current parameters for WBTC are pretty aggressive. Most of the risk of insolvency in the protocol comes from large WBTC positions:\n\n1600×1458 81.4 KB\n\n\nNet Value Insolvent measure the relative risk to the protocol from each asset pool. We pull live liquidity data from centralized and decentralized exchanges as well as the current collateral positions to ensure this value reflects the best available information.\n\n\nThe Compound WBTC pool has many times more WBTC than other protocols, creating higher BTC risk in Compound. While Compound can increase its capital efficiency for borrowers of other assets, this current change increases the risk of overall protocol insolvency substantially.\n\n\nWe made a mistake yesterday when casting our vote and are unable to change it. We’re building infrastructure now to ensure that future votes are always verified and cast as intended. We’ve voted without issue on almost every other proposal and are looking forward to completing tooling that makes sure this doesn’t happen again.\n\n\nWe’ll be making a new proposal soon with collateralFactors for each asset type that are supported by our stress tests that should increase the capital efficiency for less risky assets. Community proposals like this one are great see, and we want to support community-driven initiatives any time we can. It was a tough decision not to support his proposal, as supporting the community and increasing the capital efficiency of Compound are causes we are very much behind.  We’d love to increase the WBTC collateralFactor but our analysis doesn’t justify this change.\nMore info on how our stress test models Compound and liquidation spirals can be found in our report 9.\\nI am shocked.\nYou guys supported the CAP proposal from the beginning. I find it hard to imagine that you voted “Yes” by accident. It isn’t easy to make that kind of mistake, and you quickly voted for this proposal when it went live. You had given the community every reason to believe you supported this change until now.\nEven reading your post, your concerns are unclear.\n\n\n\n jmo:\n\nThe Compound WBTC pool has many times more WBTC than other protocols\n\n\nI don’t see how other protocols success/utility has a factor in Compound Finance’s decision making. What matters is if the liquidators can safely handle WBTC being used as collateral. We have more than enough evidence to support this.\nWhile your report shows your processes, it does not mention WBTC once.\\nI’d be happy to explain anything that is unclear:\n\nIn general, we try to vote quickly so other people can read our analysis before voting. After we realized we set the wrong bool value, we were hoping there was some way for us to change our vote (we even tried to call castVote again, but as you might know, that will fail), but we were unable to find a way, so posted here to clarify.\n\"I don’t see how other protocols success/utility has a factor in Compound Finance’s decision making. \" - The risk to Compound’s solvency is mostly due to liquidation failures. The quantity of an asset on Compound increases both the likelihood of a liquidation spiral as well the size potential loss in the case a spiral occurs. Maker, which has a limit of just over $200mm for WBTC, just has to sell less WBTC in the case the price drops, and carries less risk*.\nOur report focuses ETH but the methodology can be applied to any asset, and we pull live market data for each asset that we stress test to ensure our model is as accurate as possible.\n\nSadly, WBTC != BTC, especially not over short timescales. It has less than 1% of the BTC market cap and ~5% of the trading volume. WBTC is different than other assets in that people can create/redeem it from BTC (I think we talked about this on Discord a little while back). This might help create more liquidity for the asset, but we haven’t incorporated this functionality into our models yet. We hope to do this soon, however because BTC block confirmations are pretty slow, and liquidation spirals happen quickly, it’s unclear the impact this would have on recommended collateralFactors.\n*All other things being equal, which they are not since Maker uses auctions for liquidations. This is why in our stress tests, we run the actually Compound contracts in a modified version of geth to ensure these nuances are handled correctly.\\n\nThe top 10 cWBTC users are over 69% of the total cWBTC use. These users are well collateralized, intelligent, and fast-acting. These are NOT the people we need to worry about. The primary reason to increase the collateral factor is for when the market does go down, it gives a little extra breathing room, and it is great for people interacting with the protocol programmatically and can run close to the CF limit while constantly adjusting to avoid liquidation. An 8% liquidation penalty and another 17% of cushion are more than enough wiggle run."
  },
  {
    "number_of_comments": 14,
    "postid": "106258a1-4eee-4eaf-a0e9-ef0263fa99bf",
    "posturl": "https://www.comp.xyz/t/deploy-compound-v3-on-optimism/4075",
    "combinedcontent": "\nDeploy Compound v3 on Optimism\nPreamble\nType: Meta Process\nTitle: Deploy Compound v3 on Optimism\nAuthor: FranklinDAO Governance (prev. Penn Blockchain)\nProposal Introduction\nPoint of contact: @pennblockchain & @t-op (OP Labs)\nDescription: We propose the deployment of Compound III onto Optimism for the community.\nGrant Application: Yes, approved under Multichain Track (Live on Questbook)\nAbstract\nFranklinDAO Governance proposes the deployment of Compound V3 on the Optimism Network. We believe that by integrating Compound V3 into the Optimism ecosystem, both platforms can derive mutual benefits from working with each other’s innovative environments. This proposal will outline the Optimism L2 blockchain, its features, and demonstrate why integration with Optimism will be beneficial to the Compound community. We welcome feedback from the community on the proposal, including suggestions on how it can be improved.\nAbout Optimism\nOptimism is a L2 blockchain on Ethereum that provides optimistic rollup services for transactions. Optimistic rollups combine multiple transactions that occur on the Optimism L2 blockchain and secures their validity through Ethereum’s L1 consensus mechanism. It is able to achieve this seamless integration between L1 and L2 through its implementation of EVM equivalence, allowing for rapid transaction finality and full compatibility with Ethereum development tools. Optimism addresses the primary issue preventing the growth of protocols built on Ethereum by improving transaction throughput while maintaining network security.\nOptimism operates with a public goods funding mechanism, which allows for protocols deployed on Optimism to receive funding based on their contribution to the network. A portion of profits generated from operating centralized sequencers are allocated to projects that the Optimism DAO retroactively determines provide value to the Optimism ecosystem. This public goods system creates an economic feedback loop that funds the operations and innovation occurring within the Optimism ecosystem.\nThe Optimism Foundation has recently announced its OP Stack, a modular blockchain development environment that provides a framework for all protocols that operate within the Optimism network. Sets of open-source modules can be utilized to build new L2 and L3 “op-chains” that can be adjusted for any DeFi use case. Developers that use OP Stack are not restricted to a specific consensus mechanism or security technique and can instead access different modules similar to switching across APIs. OP Stack will essentially operate as a “Superchain” that is capable of supporting a wide variety of different blockchains capable of interchain interaction. The first stage of OP Stack will be launched in Optimism’s next network upgrade titled “Bedrock”.\nOptimism is currently focused on launching its “Bedrock” update, which will simplify many aspects of the system to reduce both complexity and costs. Specifically, Optimism developers are focused on reducing code complexity while mtainiting EVM equivalence to keep the seamless integration between Optimism and the Ethereum mainnet. EVM equivalence is also needed to promote continued innovation in Optimism’s ecosystem since it allows development on Optimism to occur seamlessly. However, the primary focus of Bedrock is incorporating the foundations of modularity through the early stages of OP Stack. The flexibility of this OP Stack network infrastructure will be critical to the continued growth of the network.\nNon-Technical Evaluation\n\nTVL on the Chain: $800M+: Optimism TVL - DefiLlama 4\n\nNumber of protocols on chain: 100+ protocols: Optimism TVL - DefiLlama 4\n\nAverage number of transactions per day: 400,000k+ transactions: Arbitrum Unique Addresses Chart | Arbiscan 2\n\nNumber of unique addresses: 2.8m+ addresses: Arbitrum Unique Addresses Chart | Arbiscan 2\nNumber of unique active users: https://pro.nansen.ai/multichain/arbitrum 2\n1 day: 80,000\n7 days: 310,000\n30 days: 670,000\n\nProposal Motivation\nDeploying Compound III on Optimism would be immensely beneficial to the community, allowing Compound to become a core pillar of Optimism’s rapidly growing presence in the DeFi space. Optimism’s technical developments in transaction scalability and security would contribute significantly to the newly launched Compound III and its user base. Integrating with Optimism will also expand the Compound ecosystem to include the high volume of users and protocols that currently operate within the Optimism ecosystem. Partnering with one of the leading Ethereum rollup solutions will allow Compound to establish itself as the flagship leader among competing lending protocols. Below are some critical improvements that Compound deployment on Optimism would bring.\nProposal Rationale\n\nSignificant Improvements to Transactions: Optimism rollups are responsible for $1B+ in gas fee savings across all transactions conducted on its chain. Protocol fees for performing transactions on Optimism range between 1% and 10% of the typical Ethereum mainnet. These fee reductions combined with near-instant transaction finality makes Optimism a market leader in rollup technology. Compound would significantly benefit from these improvements in transaction time, allowing for COMP to be traded at higher frequency and cheaper costs. Gaining this advantage in transaction speed and security would contribute significantly to the momentum of Compound III’s launch.\nEarly Entrant to OP Stack System: The OP Stack is highly disruptive DeFi technology that is currently in early development stages by the Optimism development team. OP Stack is a set of open-source modules that allow for the simplified development of L2 and L3 blockchains by switching across these modules. Unlike other L2 systems that are limited by their specific technical consensus and validation models, OP Stack provides versatility to any blockchain use-case created on the OP Stack “Superchain”. The opportunity to become an early entrant in this networked collective of modular blockchains would cement Compound as a thought leader in the DeFi space.\nParticipation in Public Goods Mechanism: Optimism incorporates a public goods funding mechanism that helps retroactively fund work done by Optimism community users to improve the network. This economic feedback helps fund the critical network infrastructure and innovation of protocol developers that allows for Optimism to continuously innovate. Compound deployment on Optimism would support the funding of open-source projects on Optimism, creating positive momentary feedback that funds users in both the Compound and Optimism communities.\n\nIncentivised Adoption\nThis project will qualify under Optimism’s realm of incentivized project adoption. Given the scope and magnitude of such a deployment, similar projects in the past have received millions in protocol incentives. Compound will be treated similarly and will be eligible to qualify for these incentives as well to spur adoption in the ecosystem. Pending expected adoption and volume, up to, and if not more, ~$2m of incentives can be allocated in the form of OP and given directly to borrowers and lenders, distributed by OP Labs. This number is subject to change, and will be distributed over multiple years.\nSecurity Considerations\nOptimism’s optimistic rollups are one the most secure scalability solutions available today as they rely purely on proofs to inherit the security of Ethereum. There is a general L1<>L2 communication bridge which will support arbitrary message passing and secured by optimistic proofs and Ethereum consensus.\nImportantly, the OP Labs team will be building out additional safety functionality and monitoring off & on-chain activity. Security is a top priority for Optimism and they have currently worked with many tier-1 auditors in the past, especially in the review process for the bridge code. Audits will be conducted before each major upgrade. Besides audits, the team offers a substantial bug bounty program 2.\nSome risks to consider with Optimism itself:\n\nOptimism’s sequencer is centralized\nThe next version of the fraud proof system is still under development\nFurther risks an analysis can be found here: Optimism – L2BEAT 1\n\n\nLicense Exemption\nWe are requesting an exemption that will allow Optimism to obtain a Compound Business Source License (BSL) to use the Licensed Work, update compound-community-licenses.eth, and deploy it on the Optimism Network, provided that the deployment is subject to Ethereum Layer 1 Compound Protocol governance and control.\nCopyright Waiver\nCopyright and related rights waived via CC0 2.\\nThanks for the proposal @pennblockchain!\nWe are in full support and think this is a win for both Compound and Optimism. Assuming this vote passes, do you expect to deploy this before or after the Bedrock upgrade?\nDo you also plan to post the OP grant nomination on behalf of Compound?\nThanks\\nThanks for the comments @WintermuteGovernance! Seeing how fast the Optimism team is working to get bedrock deployed and as to not set unreasonable deployment time parameters, we expect this to most likely be deployed after bedrock, subject to developer bandwidth.\nOnce deployed, yes, we are in contact with the OP Labs team and plan to post the OP grant nomination as we believe it would attract beneficial yields and liquidity for both parties!\\nHope everything goes well and we support this proposal!\\n@pennblockchain Thanks for posting this proposal. I know that Compound Labs has a PR for Optimism and we’ve already done some audit work in anticipation.\nFYI @Bobbay_StableNode - I’m curious if you think this proposals lines up with your Multichain Deployment Process CIP 6.\\nI reviewed this proposal before it went live and since the Multichain Deployment Process CIP 10 hasn’t been officially adopted, it follows a majority of it and I think it is suitable in its current state.\nI would add this section to it @pennblockchain\n\nProposal Introduction\nPoint of contact(POC):\nThis will be the POC through which Compound Community will request further information.\nOverview of Proposal:\nDescription of the proposal, including timeline, team, and benefits to Compound\nGrant Application:\nDid you apply for a Compound Grant? (Approved/ Rejected/ Did not apply)\n\nFor transparency, they have been approved for a compound grant which can be seen on Questbook.\\n@cylon Thanks for the commments. @Bobbay_StableNode added in the section! Adapted a small bit to make it not redundant, but will be sure to have this in future. Thank you.\\nCompound Labs has been doing the work publicly for a deploy on Optimism. I think the process for cross-chain grants should be re-examined, I don’t think this makes much sense.\\n\nGauntlet Initial Asset Recommendations - Compound v3 Optimism USDC Comet\n\n\nSummary\nWe provide two options to the community below. Option 1 is very conservative for the purpose of testing out Compound V3 mechanics. As such, the conservatism is less so derived from market risk (which is Gauntlet’s focus) but more so on the smart contract and other technical risks. Option 2 is less conservative and assumes that the community does not need to test Compound V3 mechanics on a new chain.\nOption 1: Very Conservative (Test out Mechanics)\n\n\n\n\n\nWETH\nWBTC\nOP\n\n\n\n\nSupply cap\n3k ($4.97M)\n50 ($1.22M)\n700k ($1.70M)\n\n\nLiquidation Factor\n50%\n45%\n40%\n\n\nCollateral Factor\n45%\n40%\n35%\n\n\nLiquidation Bonus\n5%\n5%\n7%\n\n\n\nStorefront price factor: 80%\nIR Curve: Same as Ethereum and Polygon USDC comets\nOption 2: Conservative (Assume mechanics are working, then gradually increase aggressiveness of parameters)\n\n\n\n\n\nWETH\nWBTC\nOP\n\n\n\n\nSupply cap\n15k ($24.86M)\n90 ($2.20M)\n1.5M ($3.64M)\n\n\nLiquidation Factor\n82.5%\n75%\n50%\n\n\nCollateral Factor\n80%\n70%\n45%\n\n\nLiquidation Bonus\n5%\n5%\n7%\n\n\n\nStorefront price factor: 80%\nIR Curve: Same as Ethereum and Polygon USDC comets\nThe supply caps are set as a function of on-chain liquidity and can be increased after the initial launch. The proposed LFs for the initial listing are set conservatively while still being capital efficient enough for usability.\nBy approving this proposal, you agree that any services provided by Gauntlet shall be governed by the terms of service available at gauntlet.network/tos.\\nHi @Bobbay_StableNode, hope you’re doing well.\nWanted to follow up on this thread.  It seems pretty important and that it would be helpful to get this done asap.  Would you happen to know the progress and what we can do to move it along?  Thanks.\\nHi Jared, would you please provide an update for the deploy on Optimism?  Thanks.\\nThe Bedrock update is currently scheduled to take place on the 6th of June.  Hopefully we can look forward to Compound v3 on Optimism relatively soon after this date.\\nlooking forward to have the OG lender platform on Optimism. I want to put my LSTs to work there.\\nis there something blocking this?\\nNo, this was deprioritized due to the bedrock upgrade blocker, which was released recently. It might be some time before it reaches the top of the queue. You can follow along with progress in the github repository pull request draft."
  },
  {
    "number_of_comments": 29,
    "postid": "24eba15f-c823-449c-8642-848382deee6b",
    "posturl": "https://www.comp.xyz/t/compound-finance-comp-rewards/2072",
    "combinedcontent": "Last Wednesday’s governance call brought attention to how the treasury’s COMP is being managed/spent. The original intention by Compound Labs was to distribute COMP tokens to users of the protocol and attract more users to the protocol. The protocol is currently rewarding 2252 COMP per day. That is worth roughly $900k. Most of the COMP rewarded to market participants ends up being sold 19. For example, Yearn Finance is has about $1B in assets that are being used to farm on Compound.\nWe need to find new ways to build the community and distribute the treasury’s massive COMP position in a way that directly benefits the protocol and its tokenholders. It has been over a year since the incentives began. I think it is time for the community to take a different tack.\nOff the bat, I will say I am against burning COMP and doing an airdrop. I think a burn is the least efficient way to distribute funds because it does not attract new users. An airdrop to users who have historically used the protocol is better than burning but not much better, in my opinion.\nRobert Leshner posted in the Discord, “my personal view is that any change should be methodical, not a huge step. And, any reduction, should be paired with new ways of distributing COMP to users to increase the community, grow the protocol, and improve the protocol.”\nBelow are a few ideas for the community to consider/debate. Please post which ideas you like/dislike.\n\nReward voters with one or more votes for each proposal they cast a vote. All types of votes would qualify for the reward. It would be some kind of pro-rata their voting power up to a limit, so people don’t just make a zillion wallets to participate in governance and so already large holders don’t receive outlandish rewards. Here is a link 25 for how many COMP the top 100 voters would get if they voted. Feel free to copy the sheet and adjust the parameters. Very roughly speaking, I think we could see +250 new voters.\nSeparate supply and borrow side COMP rewards. This has already been brought up in the forums 4 a couple of times. I think it is a great idea. Incentivizing borrowing doesn’t make a ton of sense, so shifting it towards the supply side would better distribute COMP and improve protocol liquidity.\nSupersizing the grants program: Larry and I have been discussing what the grants program will do in September when the trial period ends. We think building out a full-time team (notice plural) to run the program and take a more active role in development would be awesome. We haven’t talked too much about what it will look like, but it likely follows what the Uniswap Grants program is doing. They have a three-person full-time team working and have been making some impressive headway. I won’t say much on this because we need to flush out a more comprehensive plan, but I will say we are looking for a full-time grants lead and others interested in working full-time on grants. DM Larry or me if you want to talk more about that.\nWe could start a program to help cover fees related to using Compound. Balancer did something like this where they covered ETH fees associated with trading. A long, long time ago, I did some research on this to see if we covered 80% of ETH fees associated with Compound functions what it would cost the protocol. I could revitalize some of that work if the community likes the idea.\nWe could distribute additional COMP rewards to users who also participated in governance. Balancer was the first protocol to start doing this and saw an immediate increase in governance participation. Users who were farming BAL that also participated in governance received extra BAL. We could implement a program that rewards farmers with an extra 2% (open to suggestions) if they vote in the latest completed governance proposal. I think this is a great idea to get users of the protocol also engaged in governance.\n\nI want to note that ideas 1, 4, & 5 would likely be done off-chain. I could develop a script that analyzes activity (open-source) that generates a list of addresses and comp rewards. That list would be published on IPFS and be used for a merkle root drop similar to Balancer’s weekly claim or Sushi’s vesting program. We could do it weekly, bi-weekly, or monthly. To manage this, we would likely use the existing community multisig (or make a new one) to control the COMP and verify the IPFS data/claim. Here is a link to the Balancer site 6 and repo.\nHere is my rank of the above ideas (1 being best - 7 being worst):\n\nSupersize the grants program\nSeparate compSpeed into supply & borrow.\nDistribute additional COMP rewards to protocol users who also vote.\nCompound protocol usage fee reimbursement.\nReward people for voting\nAirdrop (bad idea)\nBurn (worst idea)\n\nSince I didn’t go into detail here, feel free to ask questions. I think I will make a poll a week from today after people have had a chance to ask questions, so we can try to gain a consensus on what people like. If we can reach a consensus, @elee and I can develop it (although we’re getting pretty busy), or I would be happy to work with the community or hand it off altogether to someone.\\nWould delegated COMP votes count for the the delegator or for the voter?  Delegating one’s votes to a representative that the user trusts is an important option in COMP governance and so we wouldn’t want to punish smaller holders (or large holders) simply for participating in a legitimate governance process.\\n\n\n\n getty:\n\nI could develop a script that analyzes activity (open-source) that generates a list of addresses and comp rewards.\n\n\nThis might be more involved than what you are envisioning, but it may be worth looking into SourceCred 5. I know Maker utilizes them 3 to reward governance and forum participation programmatically on a weekly basis.\nRegardless, I agree with your overall statement in that we should be weighting COMP rewards more heavily to those who are actively participating in the community rather than those farming via Yearn who sell it instantly on the market. It just makes sense and we know from Balancer that it is a positive incentive loop.\\nIt would be based on total votes and not COMP holdings.\\nMore rewards for the same small group of large comp holders that currently exist.\nOligarchy comes to mind. But agree comp burn is purely a valuation tool not for governance.\\n#5 , love it… engagement is super important\\nThanks Getty! I’d like to share my .02 DAI\nTL;DR: the protocol’s current incentives are structured around incentivizing capital, not users. This has been the logical approach to bootstrapping liquidity in the first year of decentralized governance, but I agree with Getty that the protocol would be better served to begin realigning incentives so that some fraction of them are more squarely targeted at the engagement of users.\n…I hear the counterargument that splitting capital across a number of wallets (so-called Sybil attack) makes any distinction between capital and users impossible. I have stated before and continue to argue that Sybil attacks on user-focused incentivization strategies can be made unprofitable by logarithmic distribution of incentives (e.g. +0.1 COMP per factor of 10 in whichever metric the incentive is based on) as long as gas costs are finite and measurable.\nI am marginally supportive of idea 1 (paying active voters with COMP) as a temporary measure, ideally decaying in size over time, because the potential engagement upside is worth it. If even a small fraction of users who earned COMP through Coinbase Earn participated in governance (could even be integrated with Coinbase Earn as a follow-up tutorial), it would be a major win for engagement and a significant return for those users under the formula @Getty proposes. As stated in my TL;DR, I would argue for a logarithmic distribution (or a piece-wise step-function distribution approximating it). I would still support @Getty’s distribution if the community prefers it. Importantly, I would not support a distribution model in direct proportion to COMP holdings: that goes back to incentivizing capital over users.\nI’m excited that there is already a PR for idea 2 (split borrow/supply streams for COMP rewards) by @arr00 and @TylerEther. I hope this will be proposed and adopted as soon as due diligence is completed. It’s my second-favorite idea of the bunch.\nIdea 3 (expanding the Grants program) is my favorite: it does more to serve the protocol and tokenholders than the other ideas while incentivizing the best kind of user engagement: converting users into developers, program managers, community evangelists, what-have-you, all roles that help build the community alongside the protocol. While I am a recent beneficiary of the program, I don’t anticipate applying for another grant any time soon because of time constraints; I think that reduces any conflict of interest I might have in encouraging expansion of Compound Grants.\nI support Idea 4 (partial reimbursement of costs to interact with the protocol): it values users over capital and makes a huge difference for smaller participants. It’s my impression that the perspectives of small users are massively under-represented in these sorts of governance discussions. From a small user perspective, even a partial gas reimbursement is a major user incentive and would make a huge difference, especially with engagement in governance (yes, there is comp.vote, though it is still a bit hard to discover for new users).\nI am opposed to Idea 5 because the reward structure incentivizes capital over users, which is what I’m arguing we should be trying to push beyond. If I borrow $1M of assets on Compound and my friend borrows $1K of assets, I should not be rewarded 1000x the amount of bonus COMP rewarded to my friend for the same action/effort/cost. I would be more supportive of this idea if it were recast as a gas reimbursement for voting with rewards disbursed relative to ETH spent to vote (not relative to amount of COMP farmed).\nMany of the largest COMP holders appear to be opposed to Idea 6 (airdrop to early users). Going back to my main argument that the protocol should seek strategies to incentivize users, not just capital, I think folks are underappreciating how powerful of a community-building opportunity we have before us to start the process of incentivizing real users with an acknowledgment of the protocol’s earliest adopters. These are our evangelists and educators, the folks who can share their experiences with the protocol with their friends, the folks who will actually bother to write letters to their legislators when a bill misunderstands what smart contracts are or what decentralization means. The protocol doesn’t owe them anything, but that doesn’t mean it’s not in the protocol’s best interest to afford them a meaningful opportunity to engage in governance that doesn’t require them to market buy (like many of them probably have) while whales quickly farmed enough COMP to sell to them or submit a CAP.\nWe can incentivize current and future users to do the things that we want them to do by spending COMP now to reward them for those behaviors; but when the incentives run dry, typically so do the incentivized activities. Loyalty is a scarce resource in DeFi, though fortunately Compound’s “lego” has gained the loyalty of many DAOs whose protocols rely on Compound under the hood.  Still, there’s no cohort on the scale of 10K people we can count on more to do the things we can’t incentivize in a targeted way, or to carry on with those things beyond the available runway of COMP incentives, than the protocol’s earliest users.\\nA great thread @getty.\nI’d like to suggest taking decisions that support users that hold small amounts of COMP. It’s users that form a community and that spread the word. Also, a small COMP holder today, might be a big supporter in the future…\nHere are some concrete suggestions to complement what has been said and suggested above:\n\nEnable to vote for for free when using Coinbase wallet.\n(Unfortunately this isn’t possible yet through https://comp.vote/ 3 )\nEnable a cheap withdrawal/transfer of COMP from the compound app to the wallet.\n(At the moment it’s simply not economical to use the official claim COMP function. Without COMP in the wallets there is no way to vote/to get engaged, if I am not mistaken.)\nShifting COMP rewards to the supply side.\n(To attract more users that wil then learn more about Compound and the community.)\n\\nMaybe one other idea to add to the (on-chain) list would be that of support for COMP as a backstop - and enabling incentivizing users (beyond the odd $30m in ctoken reserves) to create an ‘equity-tier, insurance-like’ fund.\\n@allthecolors well said.\\nI really like the goals that @getty , @allthecolors and others have laid out.\nBroad goals:\n\nIncentivize participation in governance and voting\nIncentivize new users over new capital.\n\nI’d like to lay out two more broad goals that are worthy of COMP distribution:\n3) Maximize protocol revenue even at the expense of TVL (the Uni-V3 strategem)\nLosing the recursive farming TVL on collateral assets is not a big loss as it doesn’t add much to net liquidity. COMP farming TVL was arguably more important when TVL was what made a protocol viable, but those days are long gone. Revenue is more important. So eliminating the 50/50 reward is a big step towards more efficient use of COMP. It will also reduce selling pressure from pure farming strategies.\n\nSupport and expand the protocol (especially V3 - Gateway)\n\nIncentivizing use of V3/Gateway is critical for making Compound a key protocol for every single smart contract chain. We’ve seen the huge opportunities that arose on BSC, Polygon, and Solana & should work hard to cement Gateway as THE key liquidity provider and bridge between chains. So, incentivizing supply on Gateway is critical, and to a lesser extent incentivizing borrowing.\nBorrowing is already very lucrative to those who are sophisticated enough to do it. Borrowing stables enables long positions and yield farming. On the volatile side, shorting alone is a massive incentive; it allows massive optionality that hasn’t been fully realized by retail users.\nShorting lets you delta hedge, which could cover a large % of your ETH or BTC holdings and insulate your borrows during market crashes. This is arguably more important than making money with naked shorts.\nThe major gap on the borrow side is sophistication of users and usability of tools. Things like Instadapp fill a need in that regard. The space has the potential to evolve beyond interest rate arbitrage and yield farming. This is where the grant program can step in big-time! Grants should be supporting projects not only on the lending side but also targeted towards borrowers.\nThere could be small projects targeted towards making borrowing from Compound more convenient (better UX), widespread (e.g. credit cards or payday loans type deal), or more tightly integrated into swap platforms (e.g. like Impermax on a bigger scale and multichain)\\nshoutout to @getty and @allthecolors for caring.\nIf you take a look at who are the biggest market participants, you get Yearn, Instadapp accounts, and some other friends who like to hang out in the HF (1.00-1.05) range.   Yes, they stabilize rates, yes, dumping COMP is kosher, its a free market, but is this the way to run a business?\nI mean compound isn’t a business, its a bunch of bytecode and magic internet money, so there’s no precedence. That’s why we must create precedence. Compound was the first to liquidity mine in money markets, and became a large portion of the money markets because its giving away 10 times as much as it makes, kind of.\nEther we act like a bank and figure out our balance sheet prospectus, start compiling by Basel 3/4\n(We mostly are, our reserve ratio is above 7% on average and who knows about Tier 1 and Tier 2 equity) or, we act like a business and stop giving equity away for no reason. Around 30% of emitted daily is dumped on on chain, which is fine, but it should be addressed. I think a slp/LP/BLP COMP/ETH token as collateral would help align incentives in that regard.\nBut honestly, cut 100% of COMP distribution, we need a clean slate to help gateway and other bridges, lets stop giving away over 600 COMP a day to Yearn, and I don’t know, give it to ourselves?\nsome dashboards I made\nhttps://app.flipsidecrypto.com/dashboard/y-dai-AcpK-S 6\nhttps://app.flipsidecrypto.com/dashboard/y-usdc-_WiNbL 7\\nI’m a big fan of the kyber pool governance model. (Except for the burn capability)\n1.Governance trust less pool staking. ( delegator keeps all staking rewards)\n2. Voting epoch occurs every two weeks. Increased voting periods, greater rewards, greater participation.\n3. Minimum 2 week staking period to qualify for voting rewards.\n4. Votes required,l to approve proposal based on % staked, not % of total supply.\n\n  \n      \n\n      GitHub\n  \n\n  \n    \n\nGitHub - protofire/kyber-pool-master-proxy-contract: KyberDAO - Smart... 1\n\n  KyberDAO - Smart Contract Proxy For Pool Masters. Contribute to protofire/kyber-pool-master-proxy-contract development by creating an account on GitHub.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\\nI might be missing something here. How is it a bad idea to reward early users? What would comp be without the risk those people took. People could’ve lost all their money but they used the protocol anyway and trusted comp. Does that not mean anything to the founders?\nDo yall worry that airdropped users will just dump the tokens? vest it if you have to. Then while it’s vested give those users an early credit so they can make proposals and stuff.\nIf it’s more governance participation you’re looking for then I believe giving power to your day 1 supporter would do that.\nI wish there was more discussion on this other than “we just don’t wanna do that”\n\n\n\n getty:\n\nOff the bat, I will say I am against burning COMP and doing an airdrop. I think a burn is the least efficient way to distribute funds because it does not attract new users. An airdrop to users who have historically used the protocol is better than burning but not much better, in my opinion.\n\n\\nThis topic is okay. I like 2, 3 (if there is any way to make it non-gameable), 5 is the same as 3 basically and I do like the Airdrop idea if it can be done in a way where users can only claim it if they continue to use the protocol & we have it to where zero COMP will be essentially burned due to non-accessible accounts.\nWith that said, I’m with @massnomis for turning this protocol into a business (or at least a self-sustaining protocol that will always have a COMP balance in which can be used for stuff, like for paying @getty and @allthecolors). Just think, if Compound runs out of COMP, what does this forum look like?\nI see there being an unlimited (sort of) supply of COMP that can help with distributing the COMP rewards to suppliers/borrowers. The COMP earning rewards on the borrow side of the COMP market could actually be used as a stream of income for continuing rewarding Compound users.\nEveryone’s first take at this market (me included) see the borrow cap or the distribution of COMP rewards as something we need to stop immediately. However, looking at it from a POV where it was Compound Governance in control of the Borrowed COMP, it kind of makes sense as a revenue stream.\nI say that Compound should depreciate the current COMP market and create a new one where our community adjusts the amount of COMP borrowed to maximize COMP rewards / incentivize COMP suppliers. This could be a non-collateralized debt position which would be secured by the Compound Treasury. Not only could this keep the COMP rewards going longer (forever), but also could implement something like a quarterly vote to where the community chooses a proposal to back with the borrowed COMP.\\nFantastic ideas @getty !\nI very much support idea 3 - supersizing the grants department. We could do a trial run of a decentralized grants department and have it overseen by a few key individuals. That way, we can collectively generate the best ideas to grow Compound and even further, the DeFi ecosystem.\nI’m against incentivizing people to participate in governance. We don’t want ill-informed voters and/or people simply voting with the herd. Rather, I think we should push to get people to delegate their votes to active governance members - the people who know the protocol the most and have skin in the game. We could reward these delegates for the governance work they’re doing.\nIn talking to friends, the #1 reason why they aren’t using DeFi and why I don’t really encourage them to use it is because of gas fees. Gas fees are acceptable for those with $10k+ in crypto, but it’s hard to justify paying $20+ to deposit when they’re only playing with a few hundred dollars. So I’m all for gas fee reimbursements and hope that doing so creates [more] precedence for other protocols to do the same.\nWe could even do more in regards to gas fees… possibly enable minting/borrowing/redeeming/repaying by signature? Then have Compound Labs batch execute these events? Off-topic though.\\nGas improvements are coming. Decentralized, open-source batch smart contract. Along with other gas improvements which are rewarded through RFPs. I don’t like the idea of spending more on gas fees. When DeFi goes mainstream, gas fees will be the least of our worries.\\nThat’s interestin discussion overall, as indeed COMP distributions have a lot of things to improve. I see kind of a lot of focus on rewarding voters, that in some way mentioned both in 1 and 5 ideas, so i want to start with that.\nLet’s say it straight: amount of voters is really massively irrelevant. It really doesn’t matter if people vote or not and especially very small, “dust amount” holders. They have absolutely zero impact on outcome, and in general, holders of single-digit comp should rather be delegating than voting themselves. Voting power is what matters, not amount of voters. Voting power is concentrated in very few accounts. How and if they vote DO matter, remaining 10-20% (that isn’t exact number) of voting power spreaded across hundreds of adresses does not. That’s the facts. Participation doesn’t really matter and that goes directly from distribution of voting power and thus anything spend on incentivising useless action is wasted money. Who we going to fool here with massive participation when every single decision can be made regardless of participation of masses.\nHowever, i fully support everybody who want to execute their voting power. Either by directly casting a vote, or by delegating their voting power to their representative. That’s their right.\nBut i’m really curious: Does nobody really thought why participation isn’t that massive? Aside of people just thinking that their vote matters not?\nThe problem here isn’t because people are so ignorant. It’s just COMP token design itself is so bad and outdated. It kind of based on assumption that COMP token is valueless governance token, which primarily would be held in wallet. And that kind of\nassumption never worked from the very beginning. As soon as it hit AMM and become paired to assets which have value, it gained value as well, regardless of initial vision.\nSo right now COMP is financial asset, which do have a value which also “kind of” could be used in governance.\nThe problem here is that there are 2 basic real life usecases for that type of financial assets:\n\n\nIt can be used as collateral\n\n\nIt can be used for LP at AMM DEXes like usiswap, sushiswap, etc.\n\n\nAnd interesting enough if you try to use it in any of this ways, you are instantly unable to participate in governance by design. Because in both cases you can’t keep token in wallet. And thus it instantly looses it’s utility in governance.\nFortunately Compound is Lending platform by itself, so Collateral usecase is relatively easy to solve with already pretty much existing code. And here we come to COMP staking. What is COMP staking? Well, in our case, it’s basically just a Compound\nmarket with only Supply side present. But it wouldn’t earn any interest, you’ll tell me. Yea, and it’s not supposed to. But it will earn COMP distribution. And it will give that distribution for those who hold COMP. And that is, a way to\nimprove COMP distribution. AND because same address hold the same amount of cCOMP, it’s theoretically possible for COMP staker to participate in governance still.\nAnd of course, we should never forget that existing COMP market is a SHAME and DISGRACE of all DeFi. Example of market manipulation via artifitially setting CAP and rewarding select portion of users for months, at the expense of everybody else.\nEvery single person holding more than 65k COMP should feel personally responsible in that mismanagement. Because they actually are, by being aware, having power to create a proposal to fix that and yet taking no action. And if regulation eventually comes to crypto, that would be example of inability of DAO to properly self-regulate. Because, they would say look, they created special privileged group of users, who are earning extra rewards. And that isn’t a market condition, it’s a manipulation.\nIt’s worth reminding that the problem is still there: COMP market, as it is now is NOT fine.\nBut let’s go back on topic of improving COMP distribution. It could be somewhat fixed for users who LP COMP in v2 type of AMM. It’s a bit more complicated because of oracle for LP tokens, but it can be done in same way as COMP staking, as a one-\nsided Compound market for LP tokens. Voting portion is more complicated as amount of COMP in LP varies, maybe somebody could find technical solution for that. But so far aside of Snapshoting i don’t see anybody made a way to give LP voting power.\nCreating such tools not only improve distribution itself: Part of COMP distribution could be diverted to COMP staking and to LP staking. But that also will improve liquidity. Users, who provide liquidity for COMP tokens in various DEXes, are in some\nway doing more for protocol, then users just supplying or borrowing funds to Compound.\nAnd actually big portion of users will be able to actually execute voting powers which they DO already have, but actually unable to use, because it have financial costs to actually pull their tokens either from collateral, or from LP to wallet to participate.\nNot like i’m strongly against rewarding voters, even though i believe it’s useless spending of COMP. But unless things mentioned above are solved, yeah, i might pull 1 COMP or whatever minimum needed to get distribution and do signing. But i’d\nsurely not going to pull all tokens from where it makes financial reason for me to keep to just cast a vote, which wouldn’t change anything. (Yeah, as a very long time user, i do hold some very minor voting power, and yes, i pretty much never participate for the reasons i outlined. I’m aware of proposals and could be voting still, if technical options would exist even without any specific rewards for doing so)\nI want to shortly mention airdrop. It’s not really that much bad idea. And it doesn’t need to be 200m airdrop. It’s just dragged for so very long time while it should be closed long ago. There is nothing really to discuss, basically only founders and early investors have the voting power to pass it or not. They should just voted on that long ago instead of collecting opinions from those whose opinion doesn’t matter in decision making. Idea itself isn’t that bad, yes. It indeed somewhat improve distribution, because right now COMP isn’t really going to users pretty much at all. It goes to big capital in vast\nmajority, not to the broad community: small users are diluted by massive capital.\nAs for expanding grants and separating supply and borrow markets for rewards being able to be fine tuned, it’s both great ideas, enough was said on those and i very much agree with support of that, voiced by others.\\nI address only 1.\nDepending on how it’s done, rewarding people for voting could be a lite version of raiding the treasury:\n\nWhoever votes ‘yes’ on this proposal will split the treasury with me in proportion to voting power.\n\nCompare it to:\n\nThose who vote will receive funds from the treasury in proportion to voting power.\n\nTo avoid abuse, I see two problems to solve.\n\nSybil attacks - create many wallets to vote and receive a flat Comp reward.\nWhales draining the treasury at the expense of small holders through a scaling Comp reward.\n\nTo address these, Getty suggests a scaling reward up to a limit. (@allthecolors suggests logarithmic scaling, but I address linear.)\nThis limits 1. because the reward scales up to a point (say 10,000 votes). Basically empty wallets cannot game the system.\nThis tries to limit 2. to some debatable level. However, this fails to protect against Sybil attacks by any user able to command greater than 10,000 votes during the duration of the program. A user able to command 40,000 votes could split the COMP between multiple wallets during the voting rewards program. Maybe current large COMP holders would be honorable and not Sybil, but there are people who might buy a lot of COMP to farm voting rewards.\nHow many COMP would someone have to hold to get 10,000 votes in during a duration?\nWhat would they do to artificially increase this number? Make lots of pointless proposals to vote on?\nWhat limits this? If there are 10,000 proposals to vote on today, someone with 1 COMP could receive the maximum reward by voting 10,000 times today.\nIs the goal more people thinking about governance?\nIf so, another way of doing that might be COMP rewards from the grants program to people contributing thoughtful comments on comp.xyz.\nIf ‘more people thinking about governance’ is not the goal, what is? And why?\\nThis is a great list, and is sure to start a well-needed discussion in the community.\nThe highest priority, IMO, should be the optimization of the 2,312 /day distributed to each market, since small changes can have huge rampifications for how COMP is distributed, held, and used, and this currently represents almost the entirety of the distribution.\nThis is achieved by:\n\n\nSeparating the compSpeed into separate supply & borrow incentives 2, work which is almost complete (any / all additional eyes, and auditing welcome; as a community we should bring this work to the finish line)\nModifying the COMP distribution in ways which put more tokens into the hands of users; this likely means tying COMP more directly to “natural” use-cases, and less to “farmed” use-cases; collateral assets having COMP rewards only on the supply-side, and borrowing assets (stablecoins) configured with reserveFactors to make “folding” the asset un-economical (no “negative spreads”), but attractive to normal use. If done correctly, more COMP can get into the hands of users looking to participate in governance.\n\nIn terms of additional approaches / programs, as a guiding principle:\n\n\n\n getty:\n\nRobert Leshner posted in the Discord, “my personal view is that any change should be methodical, not a huge step. And, any reduction, should be paired with new ways of distributing COMP to users to increase the community, grow the protocol, and improve the protocol.”\n\n\nExpanding the grants team to encompass a full-time team has large upside–to encourage, and coordinate improvements to the protocol that are sustainable (long-term). Protocol improvements – whether on market selection, parameter selection, rate models, new features, and integrations are all results that compound. This is a great direction to explore, especially with the benefit of hindsight after months of operating the grants committee.\nAnother direction that the community can look, would be increasing the reserves of the protocol directly; figuring out a way to reward users that directly add reserves to the protocol, in markets that have the lowest ratios of reserves:borrowing. This is an alternative to providing COMP to farming users, while capturing most of the value for the protocol (and all users). This could increase safety & liquidity, while testing a new distribution model. It could be tied to vesting, etc.\nRewards for voting on proposals are interesting, but could create perverse behavior (e.g. making hundreds of “dummy” proposals, simply to capture more COMP). If this path is explored, it should be a set quantity of COMP per month, regardless of how many proposals there are, multiplied by participation rate. Now that there is an “abstain” option, it could bring participation close to 100%. This could even include a “quadratic” type reward, where smaller addresses earn a larger relative reward.\\n#2: Compound is a lending protocol… revenue is generated from borrowers, so if anything, borrowing should be rewarded. Rewarding supply just brings in dead TVL.\nCurrently as far as I’m aware, the only benefits of COMP are minimal voting rights and dumping. So adding value to holding COMP/locking up COMP, from the minting and/or from converting some of the part of the revenue(now going 100% to reserves?) might be useful.\nIf you really want to make this a community project, maybe use some of the treasury to hire a dedicated team to work on the protocol? Just because it’s decentralized doesn’t mean there should be no leadership…\nAnyway, just my 0.02 COMP \\nI believe Curve.finance 1 does the following: you lock for some years and you get on the pool an high interest rates on what you locked. I think it would be good to lock away some tokens while bringing more attention to the protocol and distribute some of the treasury, another interesting thing would be to increase borrow APY on comp distribution and outright remove supply distribution.\\nGreat writeup @getty. My thoughts on each of the ideas:\n\n\nI’m not in favor of paying people to vote. In an ideal world, every tokenholder is up-to-date on protocol affairs, reads all of the proposals thoroughly, and votes according to her independent view. Unfortunately, we don’t live in that world: it takes a lot of time to get up to speed on governance, and as a practical matter, few tokenholders have the time to vote with an informed view. (I’m not the first to say this; there are always tradeoffs to various governance mechanisms). Paying people to vote may result in incentivizing the wrong behavior: we want people to get informed first and vote second. Paying people to vote for voting’s sake is unlikely to add any tangible benefit to the protocol.\n\n\nI’m very much in favor of separating supply and borrow subsidies. As a general matter, I think rewards/subsidies/liquidity mining programs should be thought of as growth programs (rather than token distribution mechanisms). As such, they should be managed scientifically. That is, if we’re designing a new rewards program, we should have a hypothesis of what will happen, run the experiment, and compare the test results against the hypothesis. We should do this thousands of times until we are able to understand how users interact with the protocol. Once we have a deep understanding, we can use the data to grow usage, reduce churn, and allow the protocol to use its COMP efficiently.\n\n\n@getty and I had a few brief conversations about how to turbocharge the Compound Grants Program. One way to do it that is by growing its budget, of course. In my mind, that’s necessary but not sufficient: having the money to fund grants is good, but what’s absolutely necessary is having a well-staffed team of full-time individuals who is able to quickly, efficiently, and dutifully provide grants. One learning we’ve had from the Compound Grants Program experiment is it’s simply not enough to have a part-time grants manager (that’s yours truly). We will share more thoughts on this topic in a separate post in the future.\n\n\nPresumably, the idea behind subsidizing fees is to grow usage of Compound (particularly with retail users). If that’s in fact the motivation behind the subsidy, then I’m very much in favor. That said, similar to my thoughts in point two, I believe the subsidy should be managed as a scientific experiment. For example, we may have a hypothesis that covering 80% of the fees will grow Compound’s user base by Y, and this user base will churn by Z% over 6 months. To test the hypothesis, we can run a test and compare it against the hypothesis. More importantly, we can run this test hundreds of times until we find the optimal subsidy percentage.\n\n\nSee my thoughts on point one. We want informed voters, not profit-maximizing voters.\n\n\nI’m glad we’re having this discussion. It’s an important one!\\nFirst off, I want to thank everyone for contributing to the topic. This kind of participation is what is so important to developing something the entire community is excited about.\nRecap of the ideas:\n\nReward voters for casting a vote.\nSeparate supply and borrow side COMP rewards.\nSupersizing the grants program.\nTransaction fee reimbursement program.\nExtra COMP rewards to protocol users who also vote.\nAirdrop\nBurn\n\nLet’s toss out incentivizing voting (1 & 5), doing an airdrop, and burn. That leaves us with 2, 3, and 4.\n\nSeparate supply and borrow side COMP rewards.\n\n@TylerEther is working on implementing separate supply and borrow side COMP rewards. See his post  3to learn more. In the interest of time, the community should review Tyler’s work and begin the conversation of what speeds to use.\n\nSupersizing the grants program.\n\nRegarding supersizing the grants program. @sukernik is going to work on a proposal for the next phase of the grants program. That should come later this month or in early September.\n\nTransaction fee reimbursement program.\n\nI’ll dig up some of the tools I made for this a while ago and put together a database of Compound transactions for July. I’ll create a new forum post and Google Sheet with that info once I have it, and then we can begin a conversation about what to reimburse.\\n6?\nlol!\nYou 4got 6!!! \\nIf we reduce COMP liquidity mining rewards notably (as has been discussed), but want to keep the rate of COMP distribution high, we have to come up with ways to distribute large amounts of COMP.\nIt seems like 1, 4, and 5 wouldn’t move the needle much. 3 (beefing up grants program), is the only larger-scale COMP distribution method here, outside of continued liquidity mining.\nGrowth / biz dev strategies seem underexplored.\n\nA referral program to ‘mine’ new users\nSome institutional business development schemes would be great to fund, maybe even an institutional liquidity mining program or something (to get more potential power users playing with the protocol)\ntoken swaps with other protocols and/or distributing COMP to protocols in exchange for parking treasury funds in COMP\n\nOne other idea (that admittedly is a bit half-baked right now)…\n“Governance Mining” — Governance participation is perhaps the most valuable activity that we want to reward right now. Right now, we’re funding 1 FTE contributor and a couple handfulls of part-time contributors who have formally applied for grants — however, there is a ton of really valuable ideation, feedback, small governance tasks, etc. that is crowdsourced from the community through the forum, in the governance chat, community calls, and other mediums. This activity overwhelmingly goes unrewarded. We could boost governance participation by “mining” valuable governance activity. For example, we could distribute COMP to people who do things like: identify problems in COMP, propose solutions, offer thoughtful commentary/analysis on proposed solutions, etc.\nThe positives: this would speed up problem identification and solution ideation, it would get more people meaningfully involved in governance, and it would distribute COMP to a group strongly aligned with the protocol.\nThe negatives: might dilute the signal of discussion (gov spam), it’s really hard to come up with a quality & scalable heuristic for assessing gov participation (requires a lot of human input)\nI think there’s something here. Maybe this just means the grant program (or some group managing the governance mining rewards) gives small monthly grants/rewards to those who it deems have contributed valuable activity in governance, and even if it’s not a perfect distribution method, it’s at least a step in the right direction.\nSourceCred is the only model I’m aware of for something like this, but curious if anyone knows of others.\\nI like the direction of @JacobPPhillips’ thinking around growth / biz dev strategies as untapped potential. That said, I also share the concerns raised about potentially incentivizing un/counterproductive behavior through governance mining. Compound is an innovator in on-chain governance, but I’m definitely not convinced that we’ve found the secret sauce that will make incentivized governance a seamlessly productive use of COMP.\nZooming out, there are currently three major ways for people and organizations to procure voting power in on-chain protocol governance:\n(A) Earn COMP directly from the protocol\n(i) Earn as a founding team member or early investor\n(ii) Earn through liquidity mining\n(iii) Earn through governance for an implemented protocol development or through the Compound Grants program\n(B) Acquire COMP tokens through secondary markets\n(i) Buy COMP on an exchange or via p2p trade\n(ii) Borrow COMP through a lending service (CeFi or DeFi)\n(iii) Receive COMP from someone as a gift (I’d lump Coinbase Earn in here)\n(iv) Steal COMP (obviously this is terrible and not condoned, but to leave it out would be to ignore the very real problem we have with scammers, e.g. on our own Discord server)\n(C) Others can delegate their COMP to you\nIn this thread we are exploring tuning and broadening category (A) in ways that benefit the safety, health, accessibility, and overall mission of the protocol. I’m framing it this way because I think it’s worth emphasizing that this discussion is focused on strategies for apportioning future governance rights that are currently under the full purview of governance.\nI support all of the tweaks to (A)(ii) and (A)(iii) that have been discussed (e.g. compSpeed split, supersizing the grants program).\nRegarding category (A)(i): the folks listed here were absolutely critical to the funding, development, and bootstrapping of the protocol, so it is fitting that they are entrusted with a significant role in protocol governance. Also critical to the bootstrapping stage was the community of early (pre-COMP) users, would-be stakeholders who were in most cases diluted to oblivion since COMP launch day by the deluge of capital that came to yield farm professionally. Early protocol users should hold a meaningful fraction of collective governance power: perhaps not the full 5% level initially floated by a16z, but at least 1% of collective governance power. What is the rationale for holding this number at 0%?\nI also want to signal-boost @rleshner’s suggestion of rewarding on-chain activity in an asset proportionally to its effect on increasing protocol reserves in the relevant asset, with greater rewards for boosting reserves of assets in which the protocol is least well-capitalized. I can’t think of anything more clearly aligned with the goal of promoting protocol health and safety. And for those who envision governance eventually rallying around a periodic disbursement of reserves deemed “excess” to COMP holders, well, this idea could put the feasibility of such a concept on a faster track (fwiw, I would not support such a proposal for quite some time to come, if ever).\nI would love to see this “COMP for contributing to protocol health via effects on reserves” idea explored further and ultimately formalized (ideally by someone with more of a risk management background than me!) so we could think about adding it to the RFPs.\\nAnother model to crowd-source incentives is Coordinape 3. (developed in the Yearn community)\\n\n\n\n getty:\n\nI am against burning COMP and doing an airdrop. I think a burn is the least efficient way to distribute funds because it does not attract new users. An airdrop to users who have historically used the protocol is better than burning but not much better, in my opinion\n\n\nI mostly agree that burn mechanism isnt best option in this moment but I would not reject it for the reason that a well-implemented burn function can make the tokenomics system more flexible and sustainable through longer period. As for airdrop I understand the views of users with a large amount of COMP tokens, the thing is quite subjective.\nCompound was the first to implement the farming function (relatively) much earlier than the first real airdrop (Uniswap) so it is not necessary to specifically filter early users because realistically no one knew that the DeFi sector would move in this direction. How can someone play a system that doesn’t exist?\n\n\n\n getty:\n\nReward voters with one or more votes for each proposal they cast a vote\n\n\nHigh gas fees and pointless impact given the current governance structure was discentivized small voters to participate in governance. If you analyze voters structure on past proposals, you will get it. Users who voted with a negligible amount of COMP tokens either “donated” protocol support or expected a reward for the move. If there is a third reason please state it, because I am blind.\n\n\n\n getty:\n\nIncentivizing borrowing doesn’t make a ton of sense\n\n\nI can’t understand this part. What is the purpose of the lending and borrowing protocol? How does Compound make revenue? Given the very low lending rate, why do USERS put funds into the protocol? If we omit the borrowing function, will the Compound business model be sustainable?\n\n\n\n getty:\n\nSupersizing the grants program\n\n\nSupersizing in terms of a horizontal hierarchy is a good idea\n\n\n\n getty:\n\nWe could start a program to help cover fees related to using Compound. Balancer did something like this where they covered ETH fees associated with trading\n\n\nGreat idea\n\n\n\n getty:\n\nWe could distribute additional COMP rewards to users who also participated in governance\n\n\nCOMP distribution goes in pockets of big holders. Who have and who have not incetives to participate in governance. Maybe we could reconcile this issue with the airdrop issue - users will vote if they have voting power. We can compare two types of users:\n\nLong-term users (2-3 years) with less capital without farming attention\nTwo weeks active stablecoin farmers with huge amount of capital\nGuess who has a bigger voting power?\nWho is “more important” for protocol growth?\n\n@getty I appreciate your work in the community, I just don’t understand what the point of some opinions and ideas above (1,2,5 + intro). Please accept my criticism as something constructive, as I maybe have the wrong view\\n\n\n\n allthecolors:\n\nMany of the largest COMP holders appear to be opposed to Idea 6 (airdrop to early users). Going back to my main argument that the protocol should seek strategies to incentivize users, not just capital, I think folks are underappreciating how powerful of a community-building opportunity we have before us to start the process of incentivizing real users with an acknowledgment of the protocol’s earliest adopters. These are our evangelists and educators, the folks who can share their experiences with the protocol with their friends, the folks who will actually bother to write letters to their legislators when a bill misunderstands what smart contracts are or what decentralization means. The protocol doesn’t owe them anything, but that doesn’t mean it’s not in the protocol’s best interest to afford them a meaningful opportunity to engage in governance that doesn’t require them to market buy (like many of them probably have) while whales quickly farmed enough COMP to sell to them or submit a CAP.\n\n\nWell written @allthecolors! Thank you for everything you have done and hope that you were compensated. I am just over everything with Compound for now. I LOVED Compound, told my friends about it, played around with a few borrows, used Compound for what it was for, created a distribution simulator for the community, applied to an RFP, was more active in the forum and was (and still will) listen to the dev calls when time permits.\nThis will not make any sense until the end. Then, those who give a chit may come back and read the rest.\nMy first interaction with Compound was just a test of a supply of 180 USDC (LOL!). That 20% APY was my hook. I had to try this out and even thought about draining my savings account out to earn like 200X the 0.1% APR I was getting at the time. Good thing I didn’t drain the savings account… ended up getting locked out of the mobile wallet App I used to create the Ethereum address and the one seed phrase I had not written down by mistake. Oh, well.\nSoon, I had built up a nice crypto portfolio from mining, trading and yield farming. Binance kicked me, took them long enough LOL, that is where I was yield farming all of the launchpad tokens like AVAX, KAVA, etc.\nSo, set up another wallet for interacting with Compound (Check it out 2 if you would like). Using Compound enabled me to earn some money, ended up making around $2k borrowing ETH, then selling at around $4,300. Repurchased between $2,100 and $2,400 and repaid the borrow! Didn’t work out as I had planned, but worked out.\nThat was my experience with Compound. Now though, after looking at the forum, I see so many topics I feel like I could give ideas to like I used to (maybe all my ideas suck, not really sure, but I’d like to think they are at least helpful), however, its just not really worth it. Lol! All the time I spent on here when I could have been doing ANYTHING else. Was fun, cause here I talked to people that had the same interests, I didn’t have a good selection of places to go to talk to people about cryptocurrencies and DeFi.\n@allthecolors, of course the large hodlers of COMP are not going to be 100% on-board with an early user airdrop, for multiple reasons. I just hope that the main reason is because the people/entities who abstained to such just flat-out don’t give a dam about the community of users. They care more about the DApps that interact with the protocol (the reason Compound was developed in the first place) and also care more about how it would look to other COMP whales if they voted on an issue that ended up flash crashing the COMP value. Some things could end up hurting the relationships of other whales.\nI feel like you should be aware of people who say one thing, but then when it comes down to when they need to make a decision, don’t follow through with their thoughts…\nOh, and @getty, my honest opinion of this thread? This thread SUCKS! Lol! Sorry, I had to, you asked  The only reason you created this thread was because you are against the early user airdrop. You want to divert COMP distribution from being more decentralized to becoming more centralized and gameable.\nCentralized ideas: 1,3,4,5,7\nGameable ideas: 1,3,4,5,6\n\n\n\n getty:\n\nThe original intention by Compound Labs was to distribute COMP tokens to users of the protocol…\n\n\nEarly users, between the time when Compound launched up until the launch of the COMP governance token have currently been distributed ZERO (0) COMP. Guess they weren’t users? Lol! What a joke, i’m done, peace"
  },
  {
    "number_of_comments": 23,
    "postid": "6dd418f0-7cb1-45f3-b29b-44dd8738d612",
    "posturl": "https://www.comp.xyz/t/community-feedback-request-compusd-a-safer-revenue-generating-stablecoin/4627",
    "combinedcontent": "Hello - its James Glasscock from Reserve Protocol.\nTL;DR:\nReserve Protocol would like to invite a conversation with the Compound community to explore the deployment of an ecosystem stablecoin (“compUSD”) that provides increased safety mechanisms and drives revenue to Compound DAO. The purpose of this post is to start a conversation and receive feedback from the community.\nThis proposal primarily focuses on the question of whether compUSD should exist. The detailed discussion regarding the establishment of a Compound v3 market for compUSD is not within the scope of this proposal. If there is enough community support and feedback for compUSD, it would be appropriate to address the topic in a separate forum post at a later date.\nAbout Reserve\nReserve is a free, permissionless platform on Ethereum mainnet to build, deploy and govern asset-backed currencies referred to as “RTokens.” RTokens are always 1:1 asset-backed, allowing for permissionless minting and redeeming on-chain by users without the need for any middlemen. Overcollateralization is provided by RSR governance token 4 stakers. Each RToken can have an entirely different governance system and is governed separately by ecosystem stakers. The Reserve Protocol launched on Ethereum mainnet in Oct 2022 and completed its fifth audit in Feb 2023. See audits at bottom of this post.\nThree of the RTokens already live on the protocol are High Yield USD (hyUSD) is a secure high yield savings dollar with up to 8% APY expected to outpace the rate of inflation in over 100 countries around the world. ETHPlus (ETH+) is a safety-first diversified ETH staking index with up to 4.5% APY. Electronic Dollar (eUSD) is a hyper-resilient stablecoin built to endure black swan events, recently proving itself during the run on Silicon Valley Bank 4.\nIn order to help bootstrap the ecosystem, Reserve has made a strategic investment 1 in the Convex Finance (CVX), Curve Finance (CRV) and Stake DAO (SDT) governance to incentivize deeper on-chain liquidity for RTokens.\nObservation\nCompound’s USDC v3 lending market could unnecessarily expose users to concentrated and centralized counterparty risk, as illustrated in the March 9th run on Silicon Valley Bank and its impact on USDC. Moreover, the borrowed assets leak opportunity and network effects, benefiting other ecosystems without reciprocating value for Compound. Additionally, the process of establishing new lending markets requires significant effort, and with a solitary issuer to represent the debt token, the market has limited flexibility to evolve with changes in demand. Although we acknowledge the deliberate precautionary measure of employing the single base asset design in v3, we propose an additive approach to further enhance risk mitigation.\nSolution: Introducing compUSD\ncompUSD is a dollar-denominated stablecoin concept that features 1:1 diverse asset-backing, emphasizing capital preservation and safety through umbrella overcollateralization to guard against depeg and black swan events. The compUSD (RToken) provides natively generated revenue to fund Compound ecosystem growth, with independent and flexible governance by the community, launched on the Reserve Protocol.\ncompUSD can provide increased safety through diversified and decentralized asset-backing, LP incentives attract liquidity providers to participate in the compUSD ecosystem, and the yield generated is used to provide affordable loans and facilitate capital activation within the Compound ecosystem. Lastly, the name “compUSD” is a placeholder for discussion purposes and we expect naming, like all RToken parameters, to be decided by the community.\nAsset Backing: Diversified, Capital Efficient, Yield Bearing\nThe proposed compUSD RToken has the flexibility to accept any ERC-20 token as collateral, which includes aTokens, cTokens, LP Tokens and tokenized RWAs. The value peg of compUSD remains stable through permissionless on-chain minting and redemption of the underlying collateral at a 1:1 ratio. This mechanism creates an opportunity for any market participant, even with no prior relationship to compUSD such as a debt position, to capitalize on arbitrage if the secondary market price of RToken deviates from the net asset value of its underlying collateral.\ncompUSD preserves capital through diversification. Security comes from hedging risks. This cannot be done from a single custodian or mechanism as those are points of concentrated risk, which can lead to complete collapse. We’ve entered an age which offers many choices on the issuer front, but few choices which diversify those risks. compUSD fills in the gap by drawing in coverage while remaining maximally capital efficient.\nTo minimize risks and ensure sufficient collateralization for compUSD while generating revenue for the ecosystem, the suggested initial basket may consist of assets external to the Compound ecosystem. This basket could include 33% aUSDT (Aave v2), 33% fUSDC (Flux Finance) 2, 33% sDAI (DAI Savings Rate).\n\nsDAI (Maker DSR) - sDAI is created when users deposit DAI into the Dai Savings Rate (DSR). The DSR yield comes from profit generated by MakerDAO. A primary source is the interest paid by those borrowing DAI against a variety of collateral. The interest earned on the loaned DAI is then distributed to the DSR depositors.\nfUSDC (Flux USDC) - fUSDC is created by depositing USDC into Flux Finance 1 (Compound v2 fork), a clever innovation that permissionlessly enables access to the risk-free Treasury rate by letting stablecoin holders lend to KYC whitelist investors who wish to get more exposure to US Treasuries via OUSG the Ondo Short-Term US Government Bond Fund 2.\naUSDT (Aave USDT) - aUSDT is created by depositing USDT into Aave v2 that allows stablecoin holders to lend permissionlessly throughout the Aave ecosystem.\nAs of this writing, the collective net yield of the proposed basket is generating approximately 3.3%, and varies according to market conditions.\nThe above referenced asset basket is for illustrative purposes of diversification and yield generation. Any persons launching compUSD or any RToken should carefully consider risk/reward tradeoffs in asset basket construction. Once launched, asset backing can be reconfigured and further diversified through governance at any time.\nRevenue Generation & Growth Flywheel\nWith yield bearing collateral asset backing, compUSD shares auto-compounding revenue (yield) with stakeholders. Revenue shares are programmable at the launch of compUSD and again anytime through a governance vote.\nThe trend of stablecoins utilizing revenue to incentivize usage is gaining momentum This trend is only accelerating as projects like DAI Savings Rate and USDC pass through revenue to those using its custody service. compUSD positions itself to capture the passthrough rewards of its underlying collaterals in order to power its own incentives layer.\nTable 1 illustrates a potential revenue share scenario to stimulate overcollateralization, independent governance and ecosystem growth through refilling Compounds treasury for incentives.\n\nTable 1: compUSD Revenue Share Projections\nAccording to the Table 1 example, 0% of revenue is allocated to compUSD holders, 20% is assigned to RSR stakers for governance and umbrella overcollateralization, and the remaining 80% is directed to Compound’s treasury (for incentives - discussed below).\nIf a percentage of compUSD’s yield were given to holders, it could reduce the attractiveness of borrowing debt tokens. This is because the actual cost of borrowing would additionally include the compUSD holder yield, on top of the interest charged on borrowing.\nInstead, we suggest compUSD structures its incentives to activate capital rather than to commit to holders during its earliest stages (this can be modified later through governance as market traction evolves). Compound has led innovation of supply and borrow rewards using COMP emissions from the treasury. COMP rewards/emissions are up to governance and the current annual COMP reward run rate is about 600k-700k COMP per year 4.\ncompUSD revenue can be directed to Compound treasury for use as v3 compUSD supplier or borrower incentives based on current utilization rate strategies and variables. This excerpt in the Compound Discord illustrates flexibility with incentives and outcomes:\nPer Paul | Gauntlet#1009 in referring to v2->v3 migration “we could allocate all the COMP to USDC borrows and none to suppliers, which would incentivize users to borrow USDC, thereby increasing utilization and supply APY, incentivizing users to supply USDC, and continuing until equilibrium is reached. But similarly, we could allocate all the COMP to USDC suppliers and none to borrowers, which would incentivize users to supply USDC, thereby decreasing borrow APY, incentivizing users to borrow USDC, and continuing until equilibrium is reached, at which point theoretically we’d have the same amount of USDC borrowed, except at a lower utilization rate. The question then becomes which utilization rate we expect to reach at each equilibrium, and which utilization rates yield the best reserve growth.”\nAs compUSD TVL grows, the native incentives generated create a sustainable lending market which is decreasingly reliant on traditional COMP incentives from the treasury.\nThe revenue generated by compUSD incentives can be converted into COMP before distribution or distributed directly as compUSD, aligning it with the currency used in the lending market. This approach can contribute to driving network effects for the new Compound v3 lending market.\nAt a $1 billion market capitalization, compUSD could generate revenue of $26.6 million for the Compound treasury, about 60% the current COMP emissions rate used for incentives 4.\nUmbrella Overcollateralization\nAlthough certain assets in the compUSD collateral basket may have their own safety mechanisms (e.g. Surplus Buffer Maker DAI, FDIC on some USDC), RTokens provide added umbrella overcollateralization covering the combined assets backing in the basket . The RSR governance token holders play a crucial role in providing umbrella overcollateralization for the compUSD RToken. This overcollateralization serves as a safeguard for compUSD holders in the unlikely event of a collateral token default. It is important to note that umbrella overcollateralization functions as first loss capital.\nRSR holders have the flexibility to choose whether to stake their tokens on a single RToken or divide their RSR tokens among multiple RTokens. By staking on a specific RToken, they can earn a portion of the revenue generated by that particular RToken.\nIf any of the assets in the compUSD basket were to default, the protocol would sell the failing collateral to purchase a predetermined emergency collateral basket. This process may temporarily result in the RToken being below its targeted peg. As a subsequent step, RSR stakers who have staked on compUSD would have their RSR seized, and those funds would be used to make up any shortfall. The ultimate outcome of compUSD redemptions can be reliably predicted due to the on-chain overcollateralization.\nSimilar to RTokens diversifying risk, the RSR overcollateralization mechanism is distributed and siloed across various RToken staking opportunities. As of this writing other RTokens eUSD, ETH+ and hyUSD have an overcollateralization buffer of 18%, 5% and 17% respectively. Comparatively, the FDIC provides about 1.3% coverage on deposits socialized across all member banks. Similarly, the Maker DAO Surplus Buffer provides 1.7% coverage socialized across all stakeholders.\nPermissionless Mint & Redeem\nAnyone can mint or redeem compUSD 24/7 on-chain at Register.app 2 or alternative user interfaces developed for the Reserve Protocol, as well as by directly interacting with the underlying smart contracts on-chain.\nAnyone can deposit the required collateral baskets to mint compUSD. In return for depositing the required collateral baskets, the depositor receives 1:1 equivalent amount of compUSD. Anyone can deposit the required compUSD to redeem their collateral. In return for depositing compUSD, the depositor receives a corresponding amount of collateral assets\nThe gas fees of minting and redeeming compUSD on Ethereum may discourage retail users, who have alternative options such as swapping compUSD from a CEX or DEX with minimal slippage or accessing it through a lending market.\nAdditionally, there is a question as to why collateral holders would be willing to deposit their assets to mint compUSD if it means giving up a percentage of their yield.\nThe process of minting and redeeming compUSD will mainly be driven by two groups:\n\n\nFirstly, market makers and yield farmers who have calculated that the combined yield from compUSD (which is zero in this proposal) and Curve LP yield is higher than the original collateral receipt token yield. Minters would be exchanging a 3.3% collateral yield for a much larger LP yield. For example: The eUSD, an RToken with around 2.3% yield-bearing collateral, directs its entire revenue towards overcollateralization and does not provide any yield to eUSD holders. However, the LP yield on Convex Finance currently ranges from 9% to 20%, meaning LPs are leveraging the 2.3% yield on the collateral for the higher LP yield.\n\n\nSecondly, arbitrageurs play a role in equalizing the price across markets. Since RTokens like compUSD will always mint and redeem on-chain at $1, arbitrageurs capture the difference between the mint price and the trading price. For example: consider a RToken that is pegged to $1.00. If its price falls to $0.98, arbitrageurs would buy it from the open market and redeem it at the protocol for $1.00 worth of collateral, earning $0.02 per bought RToken. On the other hand, if the price rises to $1.02, arbitrageurs would mint it at the protocol for $1.00 and sell it on the open market, earning $0.02 per sold RToken.\n\n\nIn summary, the minting and redeeming of compUSD will primarily be carried out by specialists, while the majority of compUSD users will acquire the token through CEX, DEX, or lending markets.\nGovernance and Censorship Resistance\nThe Reserve protocol, which underpins compUSD, operates autonomously to manage the asset backing and emergency collateral. The protocol’s functionality including asset-backing and behavior can only be modified through governance parameter changes, which are subject to the decision-making of a decentralized community of RSR stakers. This decentralized governance structure ensures that no single entity can unilaterally control or manipulate compUSD.\ncompUSD exhibits censorship resistance through multiple avenues. Firstly, compUSD is minted on the Ethereum blockchain in a permissionless manner, there are no intermediaries.\nFurthermore, compUSD’s asset backing is diversified and securely held in decentralized smart contracts. Any attempt to blacklist or restrict compUSD would necessitate blacklisting a significant portion of the DeFi ecosystem. This broad distribution of assets makes it challenging for any entity to censor or manipulate the value of compUSD.\nLiquidity Incentives\nStablecoins need liquidity to enable direct access to market opportunities. Loans need liquidity to reduce external costs of liquidations. In many cases, liquidity providers prefer isolating risk to a single side of a liquidity pair, but are open to more exposure to risk. As such many liquidity providers migrate to service the highest paying opportunities. Liquidity incentives play a crucial role in bootstrapping liquidity and ensuring it is available when you need it.\nReserve Protocol recently announced a $20m DeFi investment in Convex Finance (CVX), Curve Finance (CRV), and Stake DAO (SDT) governance ecosystems. By boosting its CVX, CRV and SDT holdings, Reserve can incentivize deeper on-chain liquidity by redirecting Curve ecosystem incentives to compUSD pools on Curve.\nA future endeavor may include governance electing to direct a portion of compUSD revenue towards additional liquidity incentives.\nComplexity Wrapper\nAs a Compound v3 debt token, compUSD can act as a complexity wrapper to reduce market fragmentation.\nThe market can service compUSD, which can rebalance internally as needed to reduce exposure and resolve defaults while maintaining a single unit of account for debt. This allows a simplified market experience while allowing the debt note to adapt to risk collectively. compUSD wraps asset-backing, diversification, overcollateralization and revenue generation into a single dynamic package, governed by the community.\nAdoption: Why Hold compUSD?\nThe incentives for adopting compUSD vary depending on the levels of compUSD supply and demand. This can be observed at different milestones such as 0 to 20 million, then to 100 million, then to 1 billion supply. As compUSD integrates different use cases, its flywheel and network effects start to take shape.\nIncreased safety. compUSD offers increased safety through its diverse and more decentralized asset-backing. This approach mitigates concentration and counterparty risks associated with a single collateral issuer. Additionally, compUSD implements umbrella overcollateralization provided by the RSR governance token holders, offering an added layer of protection against potential collateral defaults.\nLP incentives. Reserve has made a substantial investment in incentivizing on-chain liquidity for RTokens deemed safe with strong community backing. As one of the top holders of governance power in the Curve ecosystem, Reserve directs incentives towards liquidity providers, which encourages existing LPs supporting RTokens to mint and provide liquidity for compUSD. This strategy aims to attract a significant number of the 60+ LPs already engaged in supporting RTokens to participate in the compUSD ecosystem.\nAffordable loans. compUSD’s purpose is to expedite capital activation within the Compound ecosystem. To achieve this, 80% of compUSD’s revenue is allocated to the Compound DAO to be used as incentives for a compUSD v3 lending market (to be proposed separately), thereby enabling the lowest borrower interest rates in both DeFi and TradFi. Initially the TVL and revenue of compUSD will start from zero and be insufficient to drive organic incentives. Considering the long term benefits for the Compound DAO, we will propose separately (out of scope in the proposal) allocating COMP incentives during an initial bootstrapping of the compUSD v3 lending market.\nSavings, remittance, payments (future). We envision compUSD adoption extending beyond the crypto-native community, and Reserve have already demonstrated the potential in this regard. The first RToken, eUSD, initially deployed by MobileCoin for the MOBY payments app, was recently adopted by the RPay app in Latin America. RPay was recently recognized in a 2023 IMF Working Paper for its contribution to preserving savings and safeguarding livelihoods against volatility caused by hyperinflation. Since its 2020 inception, RPay has facilitated over $5.8 billion in cumulative stablecoin volume with non-crypto users across Latin America.\nHolder yield (future). As compUSD scales to higher supply and larger revenue generation, compUSD governors may elect to direct a portion of revenue back to the compUSD holders, accruing directly on-chain, accessible in any wallet, in effect creating a DeFi savings account.\nAdditionally, grants and hackathons can be directed towards the development and integration of new payment methods for compUSD, expanding its range of use cases.\nReserve Protocol Audits, Bug Bounty and Testing\nReserve Protocol has prioritized security of the platform and its users by undergoing multiple audits conducted by leading security organizations. These include:\n\nTrail of Bits: Report date: Aug 2022, review report: (link)\nSolidified: Report date: Oct 2022, review report: (link)\nAckee: Report date: Oct 2022, review report: (link)\nHalborn: Report date: Nov 2022, review report: (link)\nCode4rena: Report date: Mar 2023, review report: (link)\n\nReserve additionally offers a $5M Immunefi bug bounty (#3 in bounty size at the time of writing), and obtains ongoing audits with Code4rena for every new protocol release. This culminates in a smart-contract ecosystem tested and verified in real-world scenarios, and which is designed to prevent loss.\nThe first RToken, eUSD, was recently stress-tested with inclusion of USDC in its asset-backing basket at the time of the run on Silicon Valley Bank. Circle’s USDC reserves were held at the bank, which led to USDC depegging from $1 down to 88 cents. Through eUSD’s decentralized ‘self-healing’ capability, eUSD was able to autonomously recapitalize and return to $1 peg without the need for regulator or bank backstops.\nCall To Action: Request for Comment\nIn 2021, Compound had been developing Gateway - an initiative that combined a new stand-alone distributed ledger and a multi chain bridge with a new unit of account CASH and CDP like Maker. Our understanding is in 2022, Compound put Gateway on hold and pivoted to launch a Compound presence on EVM-compatible chains as fragmented markets, initially. This is Compound v3 Comet.\nHowever, now that v3 is launched and scaling, we believe the Compound community may have renewed interest in launching a native stablecoin, seeking a design that diversifies risk, amplifies Compound network effects, and provides revenue to the ecosystem.\nGiven the increasing availability of diverse real world assets, coupled with innovations in complexity wrappers for asset baskets, incentives and safety, this is a request for comment from the community to discuss the merits, risks, and opportunities of Compound deploying its own stablecoin and potentially utilizing the Reserve Protocol in the aforementioned design or a modified design.\nSuggested Development Approach\nThis proposal primarily centers around the existence of compUSD and the desired asset backing and revenue sharing configuration. The main objective of this discussion is to gather feedback from the community rather than providing an extensive development plan. However, for context, we outline three initial phases: Explore, Deploy, and Bootstrap Supply.\nExplore: The exploration phase focuses on determining the level of support from the Compound community and incorporating their feedback. Discussions revolve around the composition of the asset backing basket for compUSD, as well as identifying variables for a risk assessment with Gauntlet. Strategy, liquidity, and incentives for the associated compUSD v3 market are discussed and vetted. A detailed proposal is prepared, outlining the specified parameters for the deployment of the Compound v3 market for compUSD. If there is sufficient community support, an on-chain vote will be submitted to support the launch of compUSD and its associated Compound v3 market.\nDeploy: During the deployment phase, collaborators within the Reserve ecosystem work on developing any necessary collateral plugins. The Compound community takes the lead in evaluating and designing the branding for compUSD. The Compound community also configure the parameters and proceed with deploying the compUSD RToken, which can be done in just a few minutes at the only cost of Ethereum gas fees…\nBootstrap Supply: The goal in the Bootstrap Supply phase is to reach a supply of 50 million compUSD tokens. Efforts are made to assist with DeFi liquidity through collaboration within the Reserve ecosystem. COMP emissions allocation is specifically initiated for the compUSD lending market. Additional mechanisms and initiatives are explored to develop a growth strategy from 50 million to 500 million compUSD tokens.\nWe estimate the above development approach can be completed in 45 to 120 days at negligible external incremental costs to Compound DAO.\nIf you are still reading, THANK YOU. We would welcome your critical feedback or improvised ideas on this post.\\nHello & thanks for providing a space for thoughtful discussion around this topic.\nI believe Compound has aligned with USDC on purpose. Compound is if not the, one of the most conservative DeFi protocols in existence. I understand this thought has been passed on many years ago. Of course, it’s not a bad idea to supercharge growth CDP-style. That is why Torque 11 is building on Comet.\nMaker has Spark Protocol as a sub-DAO. I see Torque as an unofficial Compound sub-DAO, although we may not receive the same amount of co-marketing which is not the end of the world because our goal is to bring new liquidity to the ecosystem Of course, it would be appreciated. We’re still early though. This is my 2 cents & I wish only the best for Reserve Protocol.\nSincerely,\nCameron\\nHello, 0xJXMG. This proposal is very interesting.\n\n\n\n 0xJMG:\n\naUSDT (Aave USDT) - aUSDT is created by depositing USDT into Aave v2 that allows stablecoin holders to lend permissionlessly throughout the Aave ecosystem.\n\n\nI recommend aUSDT (AaveV3). The Aave community is currently in the process of migrating from Aave V2 to V3, and it is likely that V3 will be better in terms of liquidity, yield, etc. in the near future.\\nHi @0xlide,\nThe basket idea is a good one, but why should Compound feed a competitor? There are many better strategies. For example, GMX V2 or Uniswap V3 liquidity which we are working on dynamic vaults for.\nSincerely,\nCameron\\nHi @cmrn ,\nAdopting many CDP protocols (Maker, Aave, Compound, etc.) will ensure the diversity of compUSD’s collateral.I don’t know how 0xJMG chose the Protocol or why it’s not aUSDC.\\nPlease correct me where I’m wrong in my understanding.\n\nThe proposed CompUSD uses Compound’s direct competitors, and not Compound, as backing strategies. The only thing Compound about the actual product it is the name/branding and a revenue share?\nGovernance of the new token would be RSR holders, not COMP holders?\nCompound would then list CompUSD as an allowed collateral on the v3 USDC market, and provide COMP incentives to it?\n\\nimage710×142 5.95 KB\nAgree @cmrn Compound very conservative (which I love) but all eggs concentrated in any basket does not feel conservative. Will def check out Torque.\nimage758×67 3.04 KB\nExcellent note @0xlide on focusing on Aave v3.\nimage781×147 8.87 KB\nRegarding @cmrn 's question we thought long and hard on this. On one hand, a stablecoin backed by cTokens would drive more demand for cTokens and increased utility for the Compound ecosystem. On the other hand, it concentrates asset backing of the stablecoin as well as dilutes resilience and decentralization of the stablecoin. We decided to initiate this idea with emphasis on diversification and decentralization. But I am hopeful our friends in the Compound ecosystem with a more passionate pov will weigh in with more nuance.\nSidenote: with less than 10 million MAU in global crypto on a planet of 8 billion humans, Compound does not need to focus much on Aave. IMO much bigger opportunity to grow together, collaborate, and innovate far beyond TradFi.\nVery excited to look into your suggestions @cmrn on “GMX V2 or Uniswap V3 liquidity”, as I believe both will work in RToken asset backing.\nimage825×221 16.6 KB\nHi @dvf see above for the initial avoidance to back the RToken with cTokens. Its totally possible but I prioritized resilience and decentralization in the initial RFC. Revenue share to Compound IMO is a pretty big deal, with a compUSD at roughly $1 billion marketcap, the revenue can roughly either (a) replace current COMP emissions or (b) double down on COMP emissions for the Compound ecosystem to grow faster.\nRToken governance is very customizeable. Out of the box, RSR staker governance (“Alexios”) is tested and ready, read more here.\nRSR stakers also bring network effects. But there could be some creative ways to explore doing this with COMP governors, subject to some tradeoffs.\nOn your 3rd point, will be back to you on this soon.\\nThanks to the Reserve team for the proposal and for restarting the conversation around a Compound-specific stablecoin. The opportunity is really interesting and we’d love to see this happen. We wanted to highlight a few important points here that we still contend with and to propose some alternatives.\n\n\nYield to RToken (compUSD) holders - we think that to best incentivize usage of the stablecoin, the protocol should be redirecting most of the yield from the underlying tokens to the holders of compUSD. We propose that 60% of the underlying yield goes to compUSD holders, 20% to RSR stakers, and 20% to the Compound treasury.\n\n\nWe think that a fully collateralized stablecoin where the collateral is idle is capital-inefficient. The collateral itself could be lent out for a rate of interest (stablecoins currently yield 2-3%) that would significantly increase the yield attributed to compUSD holders. This could be designed similar to Compound v2, where users post volatile collateral and borrow stablecoins that collateralize compUSD. In the event of a default, liquidators would re-capitalize compUSD with the missing collateral and claim (a portion of) the defaulted user’s volatile collateral.\n\n\nWe were wondering why the basket of tokens used as collateral should be limited to 3. It seems that diversifying the risks would create a much more robust compUSD where a depeg in one of the tokens does not create a large enough hole in the balance sheet which RSR stakers would be unable to cover. We propose having 5 tokens, so as to strike a balance between sufficient diversification and excessive complexity.\n\n\ncompUSD should tap into the yield earned by collateral in Compound v2. By using cTokens as collateral, we could improve the composability of Compound as a money market and create a flywheel effect that raises the value locked in the protocol. Specifically, if we use cTokens of stablecoins in compUSD, it will unlock additional borrowing capacity in Compound v2 and reduce utilization rates on stablecoins.\n\n\nIn order to expand discussion, we wanted to propose a couple more stablecoin collateralization options for the community.\n\n\n\nOUSD: Origin Dollar smart contracts deploy underlying capital (USDT, USDC, and DAI) to a diversified set of yield-earning strategies, rebalancing over time to achieve great yields while diversifying risk. Earnings automatically accrue to tokenholders and compound continuously while they hold OUSD.\neUSD: Lybra Protocol’s eUSD is an interest-bearing, over-collateralized stablecoin that is backed by a basket of Liquid Staked Derivatives (although, currently, just Lido’s stETH). eUSD currently earns 8.83% in APY coming from the overcollateralization of stETH.\n\nLooking forward to hearing Reserve team’s and the community’s feedback and engaging in further discussions!\\nhere are some pros and con to @0xJMG proposal as a defi user i personally think reserve is the better choice.\nPros:**\n\nDiversified Asset Backing: The proposed compUSD stablecoin would have diversified asset backing, which can enhance its safety and resilience against single-point failures.\nUmbrella Overcollateralization: The umbrella overcollateralization from RSR governance token holders adds an extra layer of security and risk mitigation for compUSD holders.\nRevenue Generation: The revenue generated from compUSD’s collateral assets could be used to incentivize deeper on-chain liquidity and drive growth within the Compound ecosystem.\nIncentives for Liquidity Providers: The proposal includes incentives for liquidity providers, attracting them to participate in the compUSD ecosystem, which can enhance liquidity and market stability.\nComplexity Wrapper:** The concept of compUSD acting as a complexity wrapper simplifies the market experience while allowing the debt note to adapt to risk collectively.\nDecentralized Governance:** The decentralized governance structure ensures that decisions are made by a community of RSR stakers, preventing single-entity control.\nUse Case Expansion:** The proposal envisions potential use cases beyond the crypto-native community, including savings, remittance, and payments, which could broaden adoption.\n\nCons:\n\nComplexity: The proposal is highly complex, potentially making it difficult to implement and understand fully by the wider community.\nGas Fees: The high gas fees on the Ethereum network could discourage retail users from directly minting or redeeming compUSD on-chain.\nDependency on Liquidity Providers: The success of the compUSD ecosystem relies heavily on liquidity providers and market makers, which might limit its initial adoption.\nEarly Stage Adoption: The proposal acknowledges that early-stage adoption might be driven by specialists and arbitrageurs, which might not create a user-friendly experience for the broader community.\nRegulatory Challenges: Expanding use cases beyond the crypto-native community could introduce regulatory challenges in various jurisdictions, which could impact adoption.\nIntegration Challenges: Integrating with other DeFi platforms and achieving cooperation could be challenging, potentially delaying the realization of proposed liquidity incentives.\nRisk Mitigation: While the proposal outlines various risk mitigation strategies, no system is immune to unforeseen risks and potential vulnerabilities.\nEcosystem Fragmentation: The proposal suggests redirecting incentives from other DeFi ecosystems to compUSD, which could create competition among different protocols and potentially fragment liquidity.\nCommunity Adoption: Achieving community support and consensus for such a complex proposal might be challenging, especially given the need for a decentralized decision-making process.\n\nIn summary, the proposal presents a complex and innovative approach to creating a stablecoin backed by diversified assets, with a strong emphasis on decentralized governance and liquidity incentives. While it offers various benefits such as risk mitigation, revenue generation, and potential use case expansion, it also presents challenges related to complexity, gas fees, dependency on liquidity providers, and regulatory considerations. Ultimately, the success of the proposal would depend on the Compound community’s willingness to adopt and implement such a multifaceted system.\\n@0xJMG there are several areas where you could consider improving the proposal to make it more feasible, understandable, and attractive to the Compound community. Here are some suggestions:\n\nSimplicity and Clarity: While the proposal is detailed, it might be overwhelming for community members who are not deeply familiar with all the concepts. Simplify the language and structure to make it more accessible to a wider audience. Use clear headings, bullet points, and diagrams to break down complex concepts.\nTechnical Feasibility: Provide technical specifications and details about how the proposed mechanisms would work on the Ethereum blockchain. Include code examples or pseudo-code to illustrate key processes such as minting, redemption, and revenue distribution.\nRisk Assessment: Conduct a comprehensive risk assessment of the proposed stablecoin design. Address potential vulnerabilities, attack vectors, and risks associated with the new mechanisms introduced. Offer potential mitigation strategies for these risks.\nGas Fee Considerations: Given the current high gas fees on Ethereum, propose potential solutions for mitigating the impact of gas fees on users. Explore layer-2 scaling solutions or other networks that could offer more cost-effective interactions.\nMarket Adoption Strategy: Detail a clear adoption strategy for compUSD. How will you attract users, liquidity providers, and developers to the ecosystem? Highlight partnerships, marketing efforts, and community engagement plans.\nGovernance Implementation: Provide a step-by-step guide for how the decentralized governance process would work. Outline the voting mechanisms, quorum requirements, and decision-making processes in a clear and concise manner.\nDeveloper Ecosystem: Describe how developers can contribute to the compUSD ecosystem. Provide guidelines for creating collateral plugins, developing user interfaces, and building integrations with other DeFi platforms.\nTesting and Auditing: Outline your plan for thoroughly testing the smart contracts and auditing the proposed mechanisms. Highlight your approach to security and the measures you will take to ensure the safety of user funds.\nReal-World Use Cases: Expand on the potential real-world use cases for compUSD beyond the crypto-native community. Provide case studies or examples of how the stablecoin could be adopted by non-crypto users.\nCommunity Feedback: Before finalizing the proposal, consider seeking input and feedback from the broader Ethereum and Compound communities. This can help identify potential concerns and refinements before implementation.\nImplementation Roadmap: Develop a clear roadmap for implementing the proposal. Break down the process into distinct phases with milestones, estimated timelines, and goals for each phase.\nVisual Aids: Use diagrams, flowcharts, and illustrations to visually explain key concepts. Visual aids can help readers better understand complex systems and processes.\n\nBy addressing these areas and providing a well-structured, technically sound, and user-friendly proposal, you can increase the likelihood of gaining community support for your compUSD initiative within the Compound ecosystem.\\nis this a chatgpt response @defi_milli?\\nyes it is a chat gpt response\\nkilled the convo, which is unfortunate. @defi_milli\\nThanks @0xJMG for starting this interesting discussion! I have a few questions about the proposal:\n\n\nFrom the governance perspective, it seems strange that you are asking COMP holders and delegates to create something that they will then have no control over. Do RSR holders understand how Compound protocol works, and what aligns RSR holders’ interests with those of Compound? Is it possible for compUSD to be governed by COMP holders, while RSR stakers only get the financial rewards?\n\n\nWill there be a new risk manager for the parameter recommendations of compUSD? I believe Gauntlet works for Compound DAO, not RSR stakers.\n\n\nWhen users / arbitrageurs mint and redeem compUSD, do they get to choose which which collateral (out of the 3) to deposit / withdraw, or would it always be in the fixed ratio (1/3 sDAI, 1/3 aUSDT, 1/3 fUSDC)\n\n\n“If a percentage of compUSD’s yield were given to holders, it could reduce the attractiveness of borrowing debt tokens. This is because the actual cost of borrowing would additionally include the compUSD holder yield, on top of the interest charged on borrowing.” Could you unpack this quote? Are you referring to a Compound v3 market for compUSD? In that case, would the compUSD holder be the supplier in this market? If a percentage of compUSD’s yield were given to holders, then the supplier would still be able to get that yield, because he’s also the holder?\n\n\nI’ve always been confused about how Convex generate yield. Is it just minting the yield out of thin air?\n\n\\nHello @Guangye thanks so much for your questions. Will answer below.\n\n\nRegarding governance by COMP or RSR holders: compUSD as discussed is deployed on the Reserve Protocol and technically does not require the Compound protocol. If there were a deeper Compound lending market integration, this is where COMP governors would be most active. The proposed RSR governance (Governor Alexios) 1 is tested, ready and free to use out of the box. There may also be ways to align COMP and RSR incentives through a bi-directional token loan or swap.\n\n\nRegarding the risk manager: Here is a link to the default risk parameters  1for deploying RTokens. Subject to refining goals, the parameters could be modified before a deployment. If compUSD is deployed by the Compound community and integrated with a Compound lending market, we would expect Gauntlet to play a role in risk assessment. But open to alternative ideas.\n\n\nRegarding mint/redeem: When users / arbitrageurs mint and redeem compUSD, they have should have the choice to redeem to 100% of the available collateral of their choice via “zap” or redeem the compUSD Rtoken in its exact basket proportions of collateral. This will apply to minting as well. Minting Rtokens via Zaps is currently live on Register.app. Redeeming via Zaps is coming soon.\n\n\nRegarding “If a percentage of compUSD’s yield were given to holders, it could reduce the attractiveness of borrowing debt tokens. This is because the actual cost of borrowing would additionally include the compUSD holder yield, on top of the interest charged on borrowing.”\n\n\n\n\n(4a) The yield referred to in the quoted excerpt is the holder yield of an RToken just for any user holding it, in any wallet, which does not require any lending market, DEX or other instrument. RTokens are bearer instruments and when they have holder yield, the user does not need to take any action other than hold in order to accrue the yield. In the specific case of compUSD we are proposing a 0% holder yield since this yield would increase the cost of borrowing the asset, for example if compUSD had 2% holder yield and were supplied to a Compound v3 market, borrowers’ liabilities would grow by the 2% + the borrowing interest rate. By keeping the holder yield to 0%, this is Part 1 of a strategy to make Compound the most affordable borrowing platform in DeFi. Part 2 of the strategy involves taking the underlying yield of compUSD and directing it back towards the Compound Treasury to be used as incentives for compUSD on Compound v3 – furthering Compound as the most affordable borrowing platform in DeFi.\n\n\n(4b) One might wonder, why would anyone mint compUSD? We think DeFi yield farmers and market makers would mint compUSD on the Reserve protocol, then either supply compUSD to the Compound v3 market to earn yield, or LP compUSD on the Curve DEX to earn yield. The yields in either case could be materially better than just holding the underlying collateral, and this is demonstrated in current market example Electronic Dollar (eUSD)  1with LP opportunities on Curve/Convex. A sufficiently liquid compUSD market would also serve as a venue for interest rate arbitrage relative to Compound’s USDC market, so borrowing demand would generate yield, and thus incentive to mint compUSD.\n\n\n\nWhere does Convex yield come from? Convex Finance does not play a role in this proposal other than as an optional/additional LP opportunity for compUSD holders. So neither compUSD nor Compound will have any reliance on Convex. But to answer your question, yield on Convex and Curve come from the CRV emissions in the Curve ecosystem. Convex is a meta governance layer atop Curve and holding 50% CRV 1, reflected in the CVX meta governance token. CRV incentives to LP pools are directed by the owners of CRV, CVX and a few other assets, of which Reserve is one of the largest holders of this governance power in the Curve ecosystem. For more background I suggest reading this general overview on the Curve Wars 1 and Reserve’s recent Defi announcement. It is fairly dense material for new folks onboarding and I’ve found the Curve telegram group to be the best place to uplevel understanding.\n\\nThank you for the thoughtful questions, Guanye!\nTo add to @0xJMG’s answer on Question #1, it is quite possible for compUSD to be governed by COMP holders, with RSR stakers participating in only the financial aspects. It just requires a tiny tweak to Reserve’s default contracts.\\nMy post sounded to ChatGPT like for some people … i hope this fits and suits better … english is not my mother language and i only speak 4 languages … so if somebody uses GPT as a translation tool … doesnt mean the text  and his thoughts are not from him … here we go:\nI’m really intrigued by this idea and see a lot of potential for both protocols to complement each other in various ways. This could significantly bolster the security of Compound, the Reserve Protocol, Ethereum, and the wider DeFi ecosystem, ultimately adding more value and usefulness to them.\nIntroducing a distinct governance token like RSR for the Compound Dollar would bring several advantages for Compound:\n\n\nStability and Independence: Having a separate governance token would give Compound a higher level of stability, less susceptible to market fluctuations that could affect the original COMP token.\n\n\nClear Governance Focus: This move allows for a clear separation of governance functions from other economic aspects of the platform. This would improve efficiency and transparency in how governance operations are carried out.\n\n\nEncouraging Engagement: A dedicated governance token could offer specific incentives to motivate active community involvement in governance decisions, potentially leading to wider acceptance and increased participation.\n\n\nSmoother Regulatory Adaptations: A specialized token for governance could make it easier for Compound to respond to specific regulatory demands, making regulatory compliance more straightforward.\n\n\nOpportunities for Integration with Other Protocols: A separate governance token could potentially integrate more seamlessly with other DeFi protocols that use similar token systems. This would encourage interoperability between different platforms, contributing to a more robust DeFi ecosystem.\n\n\nI just wanted to share some positive aspects about considering a different governance token choice… I hope you all find it interesting!\ncheck my twitter if u still think Iam a bot …\nkamehamehaaaaaaa  \\nAdding a few additional resources:\nLink to Reserve Protocol Ecosystem Factsheet 2\nLink to Reserve Protocol Documentation\nHelpful videos on Reserve protocol and deploying RTokens:\n\n\nHow to deploy an RToken in 5 minutes - https://youtu.be/sf0PuYpVWRU\nWhat are RTokens? 1/7 - https://youtu.be/hk2v0s9wXEo\nSelecting your RToken’s primary basket 2/7 - https://youtu.be/-kf_LH8bQnk\nSelecting your RToken’s emergency basket 3/7 - https://youtu.be/V3N33AtLJCM\nRevenue distribution for RTokens 4/7 - https://youtu.be/KMSFHB_zPbU 1\nAdvanced parameters for RTokens 5/7 - https://youtu.be/tPWD7God8uM 1\nGovernance for RTokens 6/7 - https://youtu.be/L774cpAK_nU 1\nMinting RTokens & early liquidity 7/7 - https://youtu.be/Ppq2v2fBn0M 1\n\\nJames, thank you for your RFC and for speaking on the community call recently. I think this is one of the more creative protocol ideas I have seen.\nThe way that you have presented compUSD as a protocol feature which could grow protocol controlled value (PCV) makes it particularly interesting. I think strategies to grow PCV adds to the sustainibility therefore the longevity of the protocol. Agreements with best-in-class firms like OZ, Gauntlet, and the Compound Grants Program are not cheap. So one can see how implementing a feature like this could add to the protocol’s runway to continue to maintain its support systems.\n\nCan you describe how Reserve accounts for prices of assets in a hypothetical compUSD basket?\nCan you walk through any and all liquidation mechanisms that get initialized in the event collateral value drops such that the basket is under water? Not sure I followed the Umbrella Overcollateralization portion.\nCan you explain the Complexity Wrapper section? I am lost there.\nCan you clarify “COMP emissions allocation is specifically initiated for the compUSD lending market” in the bootstrapping section? What are you imagining exactly?\n\\n@adam thank you for thoughtful questions. a few answers below.\n\n\n\n adam:\n\nCan you describe how Reserve accounts for prices of assets in a hypothetical compUSD basket?\n\n\n\nLike with Compound’s assetConfigs, RTokens maintain an AssetRegistry which keeps track of individual Assets (contracts which inform the protocol how to treat and price every relevant ERC20 token). All tokens, including reward tokens, are priced through Chainlink price feeds - see the full list of deployed Assets here (which includes a COMP asset)\nFrom this, the RToken price is composed from the weighted average price of the backing collateral\n\n\n\n\n adam:\n\nCan you walk through any and all liquidation mechanisms that get initialized in the event collateral value drops such that the basket is under water? Not sure I followed the Umbrella Overcollateralization portion.\n\n\n\nWe have a Tweet that explains how all this happens in detail!\nIn short, if a shortfall/default is detected, the system will initiate an auction to sell defaulted collateral for pre-determined backup collateral. If a shortfall remains (likely at this point), RSR from the staking pool is seized and auctioned to return the RToken to 100% collateralization\n\n\n\n\n adam:\n\nCan you explain the Complexity Wrapper section? I am lost there.\n\n\n\nThe idea of the “complexity wrapper” section is that compUSD wraps broad market exposure to yield/risk and flows it back to the Compound ecosystem. To the user, this wrapping allows compUSD to function as a typical ERC20 that is easy to transact, while under the hood, it powers a growth flywheel with security features.\nAdditionally, Compound maintains a single unit of account while the RToken can manage changes in market demand by adjusting its underlying to reflect the dynamic stablecoin scene. For instance if this grows to a 1B asset and diversifies into a broader number of underlying opportunities, in compound units of debt are unaffected.\n\n\n\n\n adam:\n\nCan you clarify “COMP emissions allocation is specifically initiated for the compUSD lending market” in the bootstrapping section? What are you imagining exactly?\n\n\n\nThis assumes a Compound v3 market will be setup, subject to a future proposal. The intention here is that compUSD can generate $26.6 million for the Compound Treasury per year on $1 billion in compUSD TVL. While Compound could arguably do anything it wants with that revenue, our suggestion would be that Compound buy COMP and distribute it as either supply side or demand side incentives to drive compUSD borrowing in the v3 market (to be set up).\nAdditionally, Reserve would invite its network of Curve liquidity providers, to support additional incentives for compUSD LPs on Curve - creating additional demand for the compUSD. Here is a link to yield incentives for other RTokens on Curve as of this week, ranging from 11 to 22%: https://twitter.com/reserveprotocol/status/1709312443548377418\nIncentives on the Comp v3 market market and LP incentives on Curve are only a beginning step, not the end game. As compUSD reaches Lindy and sustained TVLs, we believe other wallets, apps, DAOs and institutions will take notice considering the compUSD value prop vs alternatives.\n\nHope this helps excited to keep the discussion going.\\nReally interesting conversation.\nThank you Reserve Protocol to put it forward.\nI do think that the risk-averse approach of Compound makes it incompatible with something like compUSD, and I do think that the plethora of CDP stablecoins will only see one or two winners over the long term. The other 20 will perish.\\nHi @gonemultichain here is a possibly different trajectory.\nI think there will be thousands of stablecoins of all types. Agree many will fail in the near and long term (typical in any growth markets at less than 1% adoption) but in the age of digital money with the advent of near zero production cost, near zero distribution cost and real time trading of any asset for any other asset, I would argue winners will be far more distributed than concentrated. I dont believe in the one stableoin to rule them all argument.\nThis has already played out (and continuing to do so) in industries where digital broke through earlier; video broadcasting, ecommerce, app store, etc. - we now live in a universe of millions of channels, stores and apps, with a longer tail of prosperity than during the industrial age. Stablecoins thus far, both in innovation and adoption are still mostly utilizing industrial age paradigms - I think the greater unlock is ahead of us over the next decade.\nI agree and respect the conservative nature of the Compound ecosystem could deter from having its own stablecoin. The flipside of this is compUSD (driving revenue to Compound ecosystem) could become an important winner of the next decade+ cementing itself across many applications in the global financial system.\\nPersonally I do not believe that Compound should facilitate the creation of its own branded stablecoin.\nv3 has shown that simplicity and security in protocol design is a great attractor of value and is itself a revenue generator if it wants to be.  Stablecoins are incredibly complex systems where keeping the peg becomes a monstrous task and a failure to do so can damage the protocol’s image.\nCompound should instead focus on improving its own protocol offering and expanding to new markets, making use of existing stablecoins such as DAI, USDT to enable further v3 adoption and others not yet utilised by Compound such as EURC.  These are stablecoins that various groups of people trust for varying reasons, and having to educate people on yet another DeFi stablecoin offering, how they are designed and how they are backed is becoming tiresome. What sets Compound apart is its end-user simplicity compared to other DeFi lending protocols.  By maintaining this image, should there be a further large growth in TVL across DeFi, Compounds will have set itself up in a great position to capture the growth in value locked.\\nRecircling up here on this discussion. All in all, we are in large support of something this this coming to Compound sooner than later and the team is in a great position to execute this strongly and it’s heartening to see the community engage thoughtfully on the proposal of compUSD as a revenue-generating stablecoin.\nI agree with Cameron’s observation about Compound’s conservative stance, which has positioned it as a trustworthy entity in the DeFi space. As mentioned, the idea to emulate successful models like Maker’s sub-DAO, Spark Protocol and possibly integrating with evolving platforms like AaveV3 as 0xlide suggested, holds promise for enhancing liquidity and fostering growth in our ecosystem.\nThe suggestion to align with Torque to bring in new liquidity is insightful and could be a step towards achieving the larger vision of compUSD. Moreover, adapting to the advancements in the Aave community, could potentially improve liquidity and yield, aligning with the aim of capital optimization within the Compound ecosystem.\nThis proposal indeed presents an opportunity to not only diversify risk but also to create a robust revenue stream for the ecosystem. Overall, this is something we’d love to see happen and are looking forward to further discussions."
  },
  {
    "number_of_comments": 543,
    "postid": "1d490d35-721b-43c2-b304-ade6032dab96",
    "posturl": "https://www.comp.xyz/t/should-compound-retroactively-airdrop-tokens-to-early-users/595",
    "combinedcontent": "The Story So Far\nCrypto has always been about building networks that are owned and operated by their respective communities. For example, when Compound launched the COMP governance token in April of this year, the goal was to place control of the Compound protocol in the hands of a broad, representative, and actively engaged community of users and developers that have a stake in Compound’s success. After all, that’s what decentralization means.\nWhile the COMP token launch has brought new users onto the platform, most of those users to date have been professional liquidity providers (e.g., crypto hedge funds, whales, and prop shops). This has been good for Compound because, as a money market, it needs liquidity to function.\nThe Problem\nThe problem is that protocols like Compound need more than just liquidity to succeed. They need developers who work on things like protocol upgrades and on integrations with the outside world (i.e. wallets, exchanges, dapps, other protocols, etc); they also need a set of long-term power users who take an interest and step up to participate in protocol governance.\nAn Idea for Discussion\nCompound owes much of its success to the thousands of community members that have joined its journey over the past two years. This is true particularly of the early users who provided liquidity (both as borrowers and lenders) before there was any monetary incentive to do so and of the early developers who opted for building on top of Compound before the launch of the COMP token.\nIf there is any group of people that is most likely to be aligned with Compound’s long term success, it’s that group. As Compound’s early adopters, they are the ones who are the most likely to become enduring superfans of the network; they are the ones who stand to be the best stewards of the protocol’s long-term direction.\nConcretely, the community can opt through governance to redirect 5% the total COMP supply to the ~5,000 addresses that belong to both users and developers who interacted with the Compound protocol (either v1 or v2) before June 8, 2020 — i.e. one week before the launch of the COMP token. To help ensure that the recipients of these grants remain aligned with the community in long term, these tokens would be distributed to those addresses gradually, over the course of four years.\nThe above would imply that each of the ~5,000 early adopters of the Compound protocol would receive 100 COMP tokens over the course of four years. At today’s prices, that implies a grant of $12.35k worth of COMP per address each year. As for where these tokens would come from: they would be redirected from the flow that currently goes to liquidity providers.\nWe would love feedback from the community on this idea.\nFor disclosures, please see https://a16z.com/disclosures/ 334\\n+1\nMakes sense to give equity to early stakeholders—Uniswap, Curve, and Hegic have done the same. Currently, unless these people were already wealthy and could acquire significant amounts of COMP through liquidity mining, these contributors have little stake in Compound’s future.\\nGenerally think this could be a good move, especially with the vesting discussed.\nWorth considering taking a page out of UNI’s playbook: some X% of rewards distributed as an address-based airdrop to all users & X% of rewards distributed based on volume (or some metric that weights relative contributions)\nPosting a link to a previous discussion on this same topic: Distribution of COMP token to early users, pre-COMP distribution period 122\\nI believe retroactive airdrop of COMP tokens to users is in general good idea. It makes sense to give some voice to those, who used protocol before launch of governance token.\nWhy i think so? Because at the current point, vast majority of governance tokens are distributed to liquidity farmers. Which, while are indeed providing some benefet for protocol, doesn’t really justify why they actually should capture almost all governance power eventually (if things will go same way for 4 years). While retroactive airdrop doesn’t completely resolve that issue, it will indeed add to decentralization and will reward some actual users of protocol, not those who just put capital on both sides of DAI market with sole intention to milk governance tokens.\nWhile i think most of community will support the idea of distribution the details are worth discussing:\nFirst, what is reasoning behind 5% of Supply? Why not 3%? Or maybe 10%? Especially considering vesting period.\nSecond, should it be socialism distribution, or meritocracy? Like should users recieve equal amount, regardless of their usage, or should it depend on for example how many fees they generated? Or maybe should it be a combination of those, like base amount of COMP per address + additional COMP, based on usage for that address (fees accrued on both supply and borrow side). I personally would rather not see pure socialism, though to distribute at least some portion of COMP based on if address ever interacted with Compound seems reasonable and responsible towards community.\nAdd: Another thing to keep in mind, that unlike UNI tokens, COMP tokens do not replenish, they have fixed supply, and thus, with doing retroactive airdrops we don’t want them to be lost by being distributed to “dead” addresses, which user lost keys to or forgot about them, so i think distribution should have some sort of “Claim” interaction, where tokens, which were never claimed going back to distribution pool after some reasonable time, a year maybe. Like if user never claimed them within a year, account is considered abandoned, and tokens reserved for that account released back to pool with account loosing their right to claim them.\\n\n\n\n alive:\n\n5% the total COMP supply to the ~5,000 addresses\n\n\nThis makes total sens.\\nGreat idea, as compound was/is very developer focused, and interoperable with smart contracts, I think we’ll want to make sure that airdrops to smart contract addresses are claimable by the deployer. Or at least consider this and have some mechanism to make sure smart contracts that automatically deposited/withdrew from Compound have their airdrop appropriately claimable. Love the idea!\\nI like money so I’m not going to be sad if someone gives me a bunch of COMP but idk if this is really the best way to be spending the COMP. As someone who would receive a good amount of COMP if this passed and owns some COMP currently I will vote NO if this gets to be a proposal.\\nSupportive. Perhaps a more formulaic approach based on time-weighted liquidity ($ supplied/borrowed) would be best. Thoughts?\\nOkay, here is my personal story…\nI’ve been using Compound for a long time – since December of 2018. I’ve been active in the community and even started working on a protocol built on top of Compound. I’ve received a lot of value from Compound and also created a lot of value for the protocol.\nSince COMP distribution started I’ve held everything I’ve received. I’ve also done some market buys to increase my COMP. Despite this, I still don’t have enough COMP to create an autonomas proposal (100 COMP). I do find this discouraging.\nI think the idea of getting early participants up to 100 COMP is a good one. Specifically because that is the threshold to meaningfully participate in governance.\nIn terms of distributing this, I would probably favor some sort of larger up-front airdrop with a slower distribution of the remainder. I think in some ways, if the distribution is slow it actually incentives just selling it. But if I get 50 COMP up front then I am half way to my goal of 100 COMP and I have incentive to hold on…\\n\n\n\n alive:\n\nThe above would imply that each of the ~5,000 early adopters of the Compound protocol would receive 100 COMP tokens over the course of four years. At today’s prices, that implies a grant of $12.35k worth of COMP per address each year. As for where these tokens would come from: they would be redirected from the flow that currently goes to liquidity providers.\n\n\nJust wanting to clear up a slight ambiguity on the numbers.\n5% of total supply over ~5000 addresses, making it 100COMP per address - So I reckon it’d be 25COMP per year over four years, making it ~$3000 worth of COMP per address each year.\nIs that the correct way of looking at it? Or did I misunderstood how it might work?\nCheers.\\nI totally agree understand why this a good idea. We should look at those first supporters and include them into the voting/ecosystem. So totally YES\\nI am generally neutral regarding this proposal. However, I would like to remind everyone about the whole Uniswap debacles where some users did not receive their share due to the use of contracts account. If we want to be fair, let’s discuss it here first about who (addresses) get it and who does not.\nSome projects, with users who directly use compound, that we might need to consider:\n\nInstaDapp\nDharma\nArgent\nplease add more…\n\nI believe there are also other projects that use compound in parts of their protocol, where users  indirectly interacted with compound, such as PoolTogether. We should discuss where the lines would be drawn. I suggest two-tier approach to this problem.\n\nProposal 1: Determine the (max) amount of comp that the community wants to reward compound users with. e.g. 5% of treasury or 2% of total comp supply or 100 comp per address. Only after proposal 1 has passed, we then proceed with the next proposal.\n\nProposal 2: Determine which users will receive the airdrop.\n\nThis way, we can be sure of what the community really decides on and avoid scenarios like:\n\nI agree with comp airdrop but x amount is too much/little so I vote NO.\nI agree with comp airdrop and x amount, but I disagree that y project users also receive it so I vote NO.\n\nIf this proposal indeed goes through the motion: I personally support the cutoff date of Jun 8th 2020, vesting schedule and minimum airdrop of 100 comp per user (considering that’s what you need to create an auto-proposal). I also like @Sirokko’s ideas on mechanism to distribute comp to active addresses, in the hope that those who receive airdrop will be active contributor to the protocol. One potential mechanism is deciding pre-determined “Claim Window”, such as “After 1 year of no-claim, the comp allocated to the addresses will be returned back to treasury”.\nDisclaimer: I am a fan of both Uniswap and Dharma teams (which worked on the additional UNI airdrop proposal)\\nNot in favour of the proposal even though I stand to receive upside from this. I think the analogies to Uni doesn’t really address the issue of voter apathy/splits nor is it entirely obvious that early adopters still have any form of engagement or assets on the protocol.\nWould be much more inclined to have this converted to systematic grants to new developers on Compound, as opposed to additional acquisition costs on people who are familiar with the Compound ecosystem - but may/may not be part of the system now.\nIf this does turn out to pass, would recommend holding off until we have vesting implemented.\\nAgree that voter apathy/splits might be an issue but it is also entirely obvious that will be a problem regardless of the existence of this specific proposal. I also agree that grants for new and future contributors would be net good for compound. However, I disagree with the characterization of this proposal as “acquisition” cost, since the users have already been acquired. This proposal, I believe, aims at realigning those who have provided important signal of early adoption for the protocol.\n\n\n\n Praneeth:\n\nIf this does turn out to pass, would recommend holding off until we have vesting implemented.\n\n\nI think most people here agree with implementing vesting schedule for the airdrop (IF it goes through). I am not sure what you mean by “vesting implemented”. Do you mean comp vesting for liquidity mining rewards? I don’t see how that these two are related. The source of this airdrop would be allocated directly from the treasury. Hence, the community can decide to distribute the airdrop separate from the current rewards logic.\nAfter some considerations I’m in favor of this proposal now. It will take some time but I believe this will be net positive for compound. Sure, giving incentives to present and future contributors is important (with potential gamification) but not sure if it’s actually any more positive than incentives for past contributions. Also, vesting it in up to four years actually might make the recipients to actively contribute to the protocol for that period, ensuring active compound community for these early years. More importantly, those recipients use compound before any liquidity incentives (that disproportionately benefit whale LPs and short term users i.e. mercenary farmers).\nI would bet that these early adopters will actually be more active in the development of compound than the mercenary farmers. One last point, 100 comp for 1000s of actors is a great incentive for a diverse community to emerge from while decreasing the impact of any potential plutocracy.\\nI would go even more drastic about retroactive rewards,\ncompound v1 started around Sep-26-2018 and COMP distribution started at Jun-15-2020 that means that early users used Compound for at least 2,5 year.\nif we would want to be totally fair and give equal rewards for the users before COMP distribution started then 30.7%~ from the total amount that the Reservior received to distribute should go to them, but also i would love to include some calculation about protocol usage to calculate what amount should receives individually.\\nAgreed.  Been a user since 2018.  Proper precedent to set in my estimation.  These folks, a small sliver, whom have chimed in above have been users who were incredibly valuable to its operation.  Ought to matter to current users and funds who liquidity provide.\nMath is simple on this one.\\nAccording to the problem statement:\n“The problem is that protocols like Compound need more than just liquidity to succeed. They need developers who work on things like protocol upgrades and on integrations with the outside world (i.e. wallets, exchanges, dapps, other protocols, etc); they also need a set of long-term power users who take an interest and step up to participate in protocol governance.” I think there are two separate issues that the author outlines:\n\nreward early users of Compound (borrowers and lenders) and\nreward early developers who built on Compound in the early days when DeFi was not even a thing.\n\nI personally contributed to Compound in both capacities: was an early user of liquidity pools and the first team who built a third party dapp on Compound – Bloqboard (now we are a mobile wallet Linen.app and are solely powered by Compound). Several other teams built on Compound back in 2018 and 2019: Zerion, InstaDapp, DeFi Pulse/LoanScan integrated analytics, Argent, Dharma. I am pretty sure there were more. All these teams helped the protocol to get exposure in the early days. Back in 2018 and 2019 it was unclear what use cases can be entertained and who borrowers and lenders were. In 2018 only few VCs understood DeFi - it was that early.\nTo my knowledge none of the users and developers were recognized for contributing resources and building on top. I do not even own a single COMP and can not vote. So, I strongly believe that it is fair to consider airdropping tokens to both of these groups: early users and early developers. Amount of the airdrop for users and developers are up for the community to decide. Probably makes sense to discuss these two groups of contributors separately.\\nHello, I’m allo, active in the Uniswap governance forum ; Compound user since 2019.\nI support the airdrop, but I would like to also encourage you to initially include addresses who interacted with the protocol through smartcontract wallets like Argent, Dharma, etc.\nOr at the very least, have this debate before the airdrop. Otherwise, it may cause unnecessary drama and waste precious governance time.\\nI have a lot to say on this topic. Hope I’ll get around to writing up something soon.\nTLDR (TLDW?) for now: I think it’s important to recognize the reason we as a community will be gifting people COMP and we should focus on advancing the protocol/community. The idea that Compound owes past users doesn’t sit well for me.\\nI agree that the way to move forward is to think about the future of the protocol. Compound does not owe anyone for its success other than its community of founders, developers, and other contributors.\nThe reason I think this idea is net good for compound is because I believe that we will have a better chance at having a more active community by rewarding past contributions vs designing a new mechanism that could be gamified and will definitely cost more.\\nUniswap made a minimum of 400, and for users that provided the most liquidity to the pools they got compensated with more tokens.\nAlso UNI token received a good resistance on the past bear markets and helped uniswap to recover most of their locked liquidity from the sushiswap split.\nSo overall people trust and have a great perspective about uniswap thanks to this small gesture.\nBesides, most of the comp tokens will eventually end up being farmed by big whales (like the one farming with 500M DAI).\\nAlso we shouldn’t add any conditions besides using the protocol before COMP.\nWe’re only going to create a big drama if we start to pick who deserves tokens and who doesn’t.\nEveryone who used the protocol regardless if they did as curiosity (testing) or with a development  purpose should be compensated.\nIt’s only 5K addresses and only 5% of total supply.\nAnd 100 COMP is the minimum to present proposals for the protocol, is better if is the community who receives those tokens.\nUNI was a great success and even helped uniswap to recover most of the locked liquidity they lost during the sushi swap split.\nLet’s be grateful.\\nI agree with you, Compound is first defi dapp which I used and I have remained consistent to this day but decision is on big players with more COMP tokens (Dharma, a16z etc.).\nAnd I like your distribution proposal - not to be purely socialist but over time tokenomics has the effect that in the open market (the invisible hand from Smith) tokens find their way to users who really believe in the project.\\nTotally makes sense to reward early Compound users. Many of them should have a lot of experience and expertise in DeFi and it’s crucial for the Compound community to keep such users interested to contribute to the ecosystem.\\n\n@alive\n\nMore than 1200 people on twitter voted for this question. (its closed now)\n69% is for a Yes.\nAbout 22% want to discuss the percentage (5% more or less)\n\nPeople want this. An example of these kind is UNI token. The retroactive UNI rewards should be rewarded with a nobel price of economic incentive. Student all around the world get rewarded with UNI just bc the invested TIME into the project. Without those students and early users there was no Uniswap. This is also true for compound. The early users/developers who invested TIME into this should be rewarded with governance power. The VC alone is not enough to level this project up. Those 5000 addresses are not all interested in governance, and probably sell COMP. But they all remind it to be not a project that only listen to big money. And that is good thing.\nAnyhow, hope to see the proposal soon. I will certainly vote for YES and I will talk to others also.\\nI will note No on this proposal if it ever sees the light of day.\\nWhen this proposal can be submitted?\\nI’m in favor of supporting the early users, as it rewards those who helped grow the protocol when it was not as well known as well as puts Compounds governance in the hands of those who likely know its interworkings and needs intimately, therefore making them highly qualified to make tough decisions about its future.\\nWhat is your argument about every opinion you write…\nSome persons cant access the community forum but you get instant access with one sentence per post.\nShame on you - community admin\\n\n\n\n alive:\n\nThe above would imply that each of the ~5,000 early adopters of the Compound protocol would receive 100 COMP tokens over the course of four years. At today’s prices, that implies a grant of $12.35k worth of COMP per address each year. As for where these tokens would come from: they would be redirected from the flow that currently goes to liquidity providers.\n\n\nIs this a typo? So not $12.35k per address each year, but rather over the course of 4 years?\\nIt’s a typo.\nIt’s 25 per year. ($2.7K)\nSo 100 in total or $11.1K at current price.\\nThis would make total sense and should be positive for the protocol in the long run. Uniswap gained many more active users and dedicated supporters after their UNI distribution to early adopters. (Would be a fun exercize to analyze those account’s activity and UNI balance before and some months after UNI distribution.) I could see the same same happening for Compound!\n4 year distribution span is also a great idea to keep people experimenting with protocol for an extended time.\\nWill this be put to a vote?\nBeen using compound since 07/2019, was also in the Dao. Currently coding an auto-deleveraging bot to mitigate my risk  (seeing as to what happened with DAI).\nI was actually was excited when comp announced governance.   But seems yield farming took a over and was an unforeseen consequence of comp issuance?\nPerhaps early adopters who to the risk with the platform back then, and who are still users now are fit to be given even slight recognition of their support?\\nCompound was able to use our early funds to grow into the mammoth it is today.  By recognizing and rewarding early participants you will reach a wide audience of crypto enthusiasts from new to old!  I joined compound for the thrill of seeing a Decentralized lending site utilizing the sophisticated Ethereum Blockchain.  I as an original user, helped bring 2 other people personally I know to your platform that have never used crypto before (and they loved the APY)!  So by airdropping original users you are reaching people that were brand new to crypto.  My two cents!!! Vive la Compound!!!\\nFinally got around to a more extensive response. Sorry for the delay.\nFirst of all, I don’t believe that the Compound protocol owes anyone anything. Early adopters chose the use Compound for the product that Compound offered when they used it. There is no status quo set up by other retroactive airdrops that we are required to follow. That being said, the Compound community should carefully allocate its available resources to grow the protocol as much as possible.\nShould There Be an Airdrop\nIt seems that many are pro a COMP airdrop because of personal gain. I don’t think we should seriously consider those who are encouraging an airdrop because they want “free money.” However, I still believe that an airdrop to early users could be advantageous to the Compound community for the reasons highlighted by @alive . The core Compound community still desperately needs growth—we need more “power users” and active governance contributors. Giving people a vested interest in Compound is a way to acquire the human capital we desperately need. Think of it as a multi-million dollar marketing campaign. Early adopters of Compound are a logical choice for the target of this campaign. They already have a connection to the protocol, and an airdrop may encourage them to become more involved and more connected.\nHow Much\nI don’t believe that 100 COMP is an appropriate amount to airdrop to all early users. The argument provided that 100 COMP is needed for an autonomous proposal, doesn’t make sense to me. The limits placed on proposals and autonomous proposals are there to prevent spam but also to encourage collaboration. Governance must be a collaborative process. Providing each early adopter with enough COMP to create an autonomous proposal is backward from this point of view. We want to encourage more collaboration, not less. Each user of Compound doesn’t need to be able to create an autonomous proposal on their own. They can collaborate with a few other Compound community members and together create an autonomous proposal. With that in mind, I’d be much happier giving ~20 COMP to each early user.\nHow\nI believe that an airdropped COMP should be staked initially. Our mission is to give more people a vested interest in Compound—if they can sell their COMP immediately, this defeats the whole purpose. I have been working on a staking mechanism for COMP with another Compound community member for a while now. It is an extensive project, so it will take some more time, but I implore the community to wait until I finish this project to execute any retroactive airdrop. My staking mechanism will allow users to maintain their COMP delegation rights and earn a yield on their staked COMP. It would be possible for an airdrop to go directly towards their staked balance and be withdrawn after a set amount of time.\nTLDR:\nThe Compound protocol does not owe anyone anything; however, an airdrop to early users could bolster involvement in the Compound community, which would be worthwhile. This airdropped COMP should be staked initially which requires new contracts that I am in the process of building.\\nThe amount of COMP received by the airdrop should be decided by the community.\nI strongly believe that 100 COMP for early users is still a good choice (the minimum to present a proposal), and it’s going to be in four years.\nThis is nothing compared to the amount of COMP the whales will farm and dump every year.\nAlso, I don’t agree in gate keeping the airdrop from early users. Everyone should be free to take their own decisions. This is a one time airdrop that could reflect the trust of the community with compound.\n5% in the hands of the community is hundred times better than in the hands of the whales.\\nI am also in for 100 COMP Instead of 20 COMP. Its not the personal gain, but more that these actions could only be executed ones. So do this good and strong and make impact instead of choosing to staying weak.\\nIf I remember correctly prominent eth community members we’re given 10,000 comp to get them to contribute to the development and governance of the protocol.(@rleshner pls correct me if Im wrong sir)  While some have been keeping updated and have regularly been voting (Some have seemed to stop caring, yes calling you out RSA), also it seems I have yet so see some of them create proposals or throw in their 10 cents in this discussion board. Some even haven’t signed up on here.\nAlso, I think some are hesitant in distributing comp, because market price is being attributed to those comp being distributed. Ofcourse it would be a lie to say those who are going to be recipients haven’t considered the financial aspect as well. But right now really, the only thing COMP’s value is for being a tool for governance. There’s no share of interest payments/there’s no burn/there’s no staking. (If I had 100comp I’d be proposing 25% share of the reserve factor, and 20-30% of liquidation discount be allocated for a weekly buyback + burn pool. Or maybe return it to reserves, Lol)\nBut if the point is to get those who are being airdropped to contribute and propose, a regular vesting plan seems to be counter intuitive. Token transfer vesting might be more suitable, by slowly allowing a set of tokens to be transferred eventually, airdop recipients can make proposals and contribute from the get go rather than waiting for the amount of comp to accrue until one has a significant impact. Although this would also entail upgrading the comp token contract.\nThese are just some of my few thoughts.\\n\n\n\n tonyotonio:\n\nIf I remember correctly prominent eth community members we’re given 10,000 comp to get them to contribute to the development and governance of the protocol.\n\n\nI do not believe that community members were given any COMP to encourage contribution. Some were delegated to (like me), but do not own the COMP.\nAs for a vested airdrop, the idea is that people would have immediate access to the entire governance value of the airdrop (voting and delegation), but not immediate access to the economic value of the airdrop.\\n@tonyotonio this is incorrect.\nMany early community members who volunteered to participate were delegated voting rights. The governance platform was rolled out in stages, and many folks on the leaderboard were delegated to from early COMP holders.\nThe rest of your points are good observations!\\nI’m a new member here and just signed up because of this proposal. I have been a long time Compound user and Ethereum enthusiast and stand to benefit from this airdrop.\nI like this idea because free money. But what’s more, I have never been interested in COMP governance because of my thoughts on the initial distribution and liquidity mining.\nBy giving me a real stake in Compound governance through this airdrop, it will automatically compel me to be more interested and help the protocol grow wherever I can. Especially if there is vesting whereby the future value of my ‘free money’ can be influenced through my support of Compound. Giving away stake in protocols like this is overwhelmingly net positive for the protocol, as the value from passionate early adopters exceeds the cost of diluting the COMP supply.\nIn regards to @arr00’s comments about the amount of distribution. 20 COMP tokens would be nice, but it wouldn’t really move the needle for me personally. 100 COMP tokens on the other hand would make me a lot more interested in governance and grab my attention.\nThis is just one person’s opinion on the air drop but needless to say I support it and think it’s a good idea that will benefit Compound in the long run.\\n\nThe Compound protocol does not owe anyone anything\n\nI/Grapefruit Trading was a relatively large user Compound V1 and continues to be a large user in V2. We use the protocol because it benefits us. Although it was fun to be apart of something new I don’t feel as though the protocol/team owes anything to us for being an adopter.\nAny proposal should be purely about further development and adoption imo.\\nIt’s interesting to read these comments. It seems there are two sides: one is totally for this airdrop and think they are early users or early developers intergrating Compound so they deserve it and the other thinks users also benefit from Compound when they use it so Compound doesn’t own them anything. Both sides are right but in different views. In my opinion arguing this is meaningless and lose the initial purpose for this post.\n\n\nI recomand using rewards for early users instead of the word ‘airdrop’. Airdrop gives people a view of this is getting totally for free but actually there are only a few early users. Without those early users Compound is nothing. Of course they use Compound for different purposes but the truth is it’s hard to recognise the motivations for using Compound. Saying this I just want to express that if we make a more detailed plan to distinguish early users(such as who is true long-term supporters, who contributes nothing etc) it will only result more debates and much time and energy. We should make things easy and quickly deploy so that after this rewards we hope some of them can make more contributions and keep supporting the community.\n\n\nAs for how much COMP should be allocated we can propose a vote. Give 5 options such as 20,40,60,80,100 and see which gets the most support.\n\n\nThis early users reward has more meaning to the Ethereum community than the Compound community. It shows that exploring new innovate application may get an incentiv although it is never guranteed by anyone. This will indeed encourage users experience new products. Since I am a big fan of Ethereum of course I support this reward.\n\n\nI suggest treating this proposal with a common attitude. I will continue to encourage innovations and explore interesting dapps without this reward. Money is important, so as the interests.\n\n\\nI made an account to comment on this idea. I am one of the early users that would benefit from that proposal.\nMany (Some?) of the early users are idealists that are intrigued by the possibilites of Defi and Compound. I feel if there would be an “airdrop” on those 5000 over the next 4 years. It would be probably the best possible group to give away tokens to.\nHaving to claim the coins every year, would also prevent from sending to inactive wallets. And a time limit would prevent people from claiming it that forgot about compound.\nAlso the point is that the people participate. I am not sure how the details could work but what if the people that send the Tokens away i.e balance dips under 25 in the first year under 50 in the second year and so on wont get to claim them in the 2nd/3rd/4th year. So that only people that hold for 4 years will get the full amount and the right to make proposals.\nThis also would help with people dumping. Everyone that sells straight away wont get to sell more than 25. And someone who is holding for 4 years and claims yearly will likely be more involved.\nI dont feel like Compound owes anything to the early users but I think this might be a net benefit for the protocol when all is said and done and it would definetly make some waves in the crypto community and increase the interest overall.\\nI think this makes a lot of sense and especially given how well it worked with Uniswap. This will not only drive a large amount of interest to the Compound protocol through media saying “free tokens”, but it would also decentralize compound governance by distributing COMP tokens over more holders, and incentivizing them to participate in governance. I really don’t see a reason not to do this to be honest.\\nI like this idea. Heres my take on it.\nI have been using comp ever since it came out. Knowing it was risky I still wanted to be apart of it. I would see this as a “thank you for trying comp knowing you could’ve lost everything” reward.\nI agree with the main post about taking 100 comp and splitting between 4 years. I recently got into coding and the 4 year wait would give me enough time to get my skills up and be ready to make proposals when the time comes.\nUsers should get a chance to make improvements to the protocol they love. Even if a lot of proposals were made its not like they instantly get approved and added to code. They will still need to meet the 100k vote minimum right? If the community supports it they will back it and if not someone can propose something better. Plus if too many proposals became a problem what’s wrong with just adding a delay to prevent that.\nI disagree with any token locking if people sell early. let’s say someone took some of the first 25 comp they received to pay for some bills while they were developing. should they be penalized for that? emergencies can happen. Aren’t we all in this space because we don’t want people telling us what we can and can’t do with our money? That would be like getting my bank account shut down because they didn’t like my purchases.\\nI would also very much be in favor of this. I see it has a huge net positive for all.  See what happened with UNI, i think the protocol gained alot of loyal users with their drop aswell as new users with all the attention it got.\nI think it will further decentralize the protocol aswell as get alot of more “loyal” users using it as their go to protocol if something like this went through. Additional it could potentially also get alot of valueable dev talent onto building with us.\\nThis is a phenomenal idea that beneficially skews “initial” distribution in a manner that will have minimal impact on economics. However, I think beyond the minimum reward amount, it would only be fair to scale up rewards to participants who supplied/borrowed large amounts of capital- the cumulative effect of these efforts resulted in increasing larger TVL and success of the platform. These are also the participants most likely to participate in governance and not immediately liquidate rewards.\\nI think this is a great idea. Particularly I think some more benefits should go to early users. I joined the community to post my opinion. I am an early user of Compound Finance and I will vote on this proposal.\nSeveral other dapp airdropped their early users for their contribution and I’ll be shocked if this proposal doesn’t pass.\\nI agree with blck and think people who dedicated their capital earlier should be rewarded at least as much (per % of the network), we agree that supplying 1000 usdc to compound at the day of the launch of the v2 protocol didn’t carry the same involvement as a % of the total network, rewards, and risks than providing it in june of this year …\nWe shouldn’t say to early users \"thanks for providing early traction, which allowed VC money and kept us building throught the crypto winter, however we won’t ditribute you any comp”\nI must say I am totally in favor of this proposition, and actually think it is in the interest of compound itself, I already posted related to this earlier this year in the forum and will repeat some of my ideas.\nThe current structure is really rewarding the future users, given the 4 year distribution horizon, but it does tell to the old users “thanks for providing early traction, which allowed VC money and kept us building throught the crypto winter, however we won’t ditribute you any comp”\nUni actually distributed at launch almost all the tokens to early users pre-uni distribution (400 per account), but also, the early liquidity providers were rewarded proportionally to the volume they provided, and with a higher weight given to older date, this was taking into account that for a given dollar at an older date, represent a higher % of the network, higher risks (newer protocol), and higher importance in the traction given to the protocol :\nThat involvement int he protocol should matter,\nSome seem to think that rewarding early users would both : be bad for comp investors, be unfair for current network participants, would prevent from applying dividends to voter participation or further development in comp distribution.\nFirstly you should get that getting COMP into the hands of dedicated early users who demonstrated strong interest in the protocol itself, rather than into the hands of whale farmers who don’t care about anything but yield, is great for comp and for comp investors. And I don’t agree that “it would be unfair for the investor” : these tokens are allocated to the community already. So it’s not reducing their holdings or inflating anything away.\nTo come back to the technical aspects of distribution of COMP token to early users, I am thinking we could propose an approach which both benefits the users, and the capital, we could allocate a part of the reward toward the users, and a part toward the capital, the same way UNI did actually. We will need later to come with fair and accurate ideas and propositions.\nThere could be a socialized allocation of « X comps » tokens per adress and a volume weighted allocation depending of the fees for example you genereated, or volume of deposits, time weighted with maybe linearly more importance given to older date, to take into account you represented at that time a higher % of the network, took higher risk, and your dollar gave more traction to the protocol, the way UNI rewarded its early liquidity providers.\\nHi…\nJust a quick post to remind you all that it would be nice to include Argent and Dharma (among others ?) users in the retrodrop proposal.\\nAlso it would be nice if we move to an actual proposal. As compound is now in momentum to move beyond ethereum it would be a great moment to stand still with the initial founders, students, pioniers and all the early users that saw compound as a game to try out and test out the protocol.\\nI am totally agree with the idea that the early users shall be airdropped. the reasons are as below:\n\nthe early users of Conpound are real fans of comp with loyalty and idealism . They used the protocal mostly because Comp is cool, which is extemely reflect the core value of DEFI. So they should be encouraged and be awarded.\nif  the proposed taken successfully. it will be helpful to the goverment on Compound----- some powers(comp)  would be given to the loyal users who align with comp’s long term grow.\n\\nWe should now go one step further and come with actual calculations and proposals, so would be nice if people shared their ideas on the actual proposal\nMy thought would be to make a vote\n-should there be an allocation of COMP for early users of the compound protocol(v1/v2) during the years pre-COMP distribution ?\nIf yes : to which extend should we decide it and split between the two type of distributions ?\n-Socialized distribution : X comp allocation per eth adress being an early user of the compound protocol (pre comp distribution), and make a vote for 20/40/60/80…etc\n-capital weighted distribution : proportional to the interest fees received/paid by the adress(similar to current comp distribution)\nAn idea for this second type of allocation is that we might want to linearly weight higher, the older the fees were incured, to take into account the protocol TVL is for example in january 2019 19 million, and is 100 millions in june 2020 pre-COMP period.\nCould maybe decide of a “boost” to earliest fees incured, 1.5x/2x/3x ?, linearly decreasing toward 1x at the end of the period taken, this type of setting would by the way recall the UNI distribution did to the early liquidity providers.\nFinally we must decide if the other app which interacted with compound should also be a part of this reward, it seems that the capital weighted distribution should be naturally included, however for the socialized, it has to be decided.\\nI agree. My thoughts would be\n\n\nAs a base reward early users (100 comp for proposal minimum)\n\n\n“Boost” for earlier fees (3x decreasing to 1x as you get closer to the end of the period taken as you mentioned)\n\n\nSocialized + capital distribution for direct compound users and capital weighted distribution for apps that interacted with the protocol\n\n\\nI agree on this. Need to move forward with a vote. New topic? Anyways, need to decide the questions. My suggestions:\nShould early users be awarded an airdrop?\n\nYes\nNo\n\nIf yes, should this include users by proxy, e.g. Dharma, Argent…?\n\nYes\nNo\nAs a separate proposal\n\nIf yes, how much should be allocated per address?\n\n20 COMP\n100 COMP\nProportional to the interest fees received/paid\nproportional to the interest fees received/paid, with 2x boost\n\nThese are of course just suggestions. For the second one I think a separate proposal for proxy might avoid any trouble passing an initial proposal, but might make it unlikely that proxy users will pass the second proposal.\nFor the last question there are just so many variations, I feel like you have to cut it down somehow. Maybe do an initial scan, and then tune afterwards? Not sure.\nEdit: After posting I realize a significant shortcoming of my poll is a question that addresses the distribution schedule of the allocated COMP. Anyone got a suggestion? I’m not sure I’ve got all the options covered, but at least “1/4 yearly, for 4 years” seemed to be a suggestion.\\nSounds good.\nHowever the proportional option would again give the COMP to the people that already have the most voting power.\nThere needs to be a socialized component to it plus a part based on fees/interest, capped at a certain point.\nThere should be some sort of vesting component.\nEdit: 4 years sounds like a good timeframe.\\nDisclaimer - I stand to benefit from a distribution as I myself was an early compound adopter.  I keenly remember @rleshner  responding to my silly questions via email (on weekends!), back on the discord and fewer than 100 people and the protocol had 7 digits in assets (vs the 10 it does now!)\nI am a distribution of a one time allocation of 100 COMP.  If 100 COMP is required to submit a proposal, why not instantly and power those early adopters?  This is provided of course that the protocol can “afford” this distribution (I will leave it to those that are more knowledgeable to me on this matter).\nI can’t tell you the number of people I offered to gift some DAI/ETH just to try compound back in 2019.  I was all but ignored despite my best effort‘s in the interactive interest rates.\nIf the goal of compound/DEFI to replace your digital banking and provide opportunities to the unbanked, I think this is an opportunity to go under some media attention.  Not only that, it helps encourage people to take risks and invest in new protocols which is necessary for the growth of DEFI.  Without the individuals who all took quite a bit of risk early on, we would not be having this discussion.\nWhile I understand the perspective of trying to keep individuals engaged over multi year distribution (similar to CRV), I am not in favor of it.  Empower, enable, and trust people to decide what to do with the distribution on their own.\nLook forward to continuing this discussion and hopefully moving towards a proposal by years end!\\nNot totally related, but just one great exchange is using retroactive reward users for using the protocol (1inch).\nThis is how it should be always. Rewarding those who are driven the success of a product. For those who traded there, congrats and merry Christmas. Hope we can also see this retro rewards to compound, makerdao and more of those early DeFi protocols (those protocols where launched when retroactive rewards where not broadly invented/uses yet). Hope to see the proposal soon. @arr00 @rleshner\\nAnd most users are not dumping their tokens. \nSo I think we have another good example that supporting early users is a great idea.\\nHaving little dev experience but a huge supporter of crypto, I totally agree early supporters should be rewarded for adding support in developing the infrastructure\\n100 to all early supporters equally.\\nSeems like you don’t want early supporters to benefit from their sacrifices.\\nHe mean each one, I think\\nMaybe tie the right to claim the airdrop to voting participation?\\nWhat sacrifice would that be? If you hadn’t benefitted from using the service you wouldn’t have used it.\\nThe same sacrifices made by any pioneer in any industry that is new or unproven. Benefits are only seen many years down the line if at all.those that lost thousands early on due to dramatic swings understand that. Early adoption required early sacrifice with the hope of that it would payoff down the life, with no immediate or guaranteed benefit.\\nWhat’s your specific “sacrifice”?\\nTime and Money…what reasons or rationale do you have for not supporting this? Assumption is your voting against this because 1. you do not benefit because you were not a early supporter or 2. You are one that will potentially loose control because you have a majority share of COMP…so how would you lose?\\nSo you used the platform for it’s intended purposes, received (or paid) interest for supplying or borrowing? Seems like you received plenty of benefit already! I’d vote against it because I don’t like people who think they’re owed something for just existing, which seems to be what you are.\\nOr just make it one of the calculations for the “boost” mentioned by @Andre1. People who used comp + voted will most likely use their airdrop to participate even more into governance and if it comes out to be 100 Comp they will for sure start making proposals.\\nDon’t believe I’m owed anything. Wasn’t owed anything by uni but was happy to receive my 400 because I supported the product or idea. A product (or writing code) is nothing to anyone without its supporters or customers getting it from point a to point b. On another note; Being an “active voter” is also a wasteful and meaningless  proposition to receive an airdrop with no actual meaningful difference tied to it. The idea of a decentralized economic tool or society gets lost when the greed clouds a persons vision.\\nDistribution of power and opinion is key to a healthy and growing sector. The ultimate downfall is corrupt crypto oligarchs that try to control and maintain the power.\\nYou’ve had the same opportunity as everyone else to earn COMP by using the product, no need to go retroactive. I’ve been using COMP for nearly a year, I derive benefit by having a place to lend and borrow that I can trust. The current implementation of COMP rewards is great but I didn’t start using Compound with any expectation of getting a reward, and if the distributions went away today I would still use Compound. (Frankly I wish the rewards would go away, interest rates on supplying are way too low.) We don’t need free stuff to “reward” us for using it before anyone else. The utility that the platform provided was the reward.\\nYou do not stand to benefit. Got it. That’s clear. Financial reasoning makes sense, but the current infrastructure is not the same when I started with Compound in 2018, the market was unknown and not developed. The benefits of the dapp at that time were not simple as they were outweighed by risk associated with the unknown, unproven and unreliable hence my stance for supporting the retro airdrop allowing us early supported to have a voice\\nDepends on how you look at it i guess. Comp is a governance token. I see this as giving the community more voting power. The token could literally be worth 0 but i will still want to participate in governance. It just has the extra benefit of having value.\\nAgree 100%,  currently only crypto hedge fund suppliers have enough cash/comp to have a say in the decentralized economy.\\nInstead of having the comments section in protocols filled with “plz do this plz do this” it will then look like “hey all this is an idea I have vote if you support this or not” So far the only ones that are doing that are the big players.\\n+1\nThe first time I used Compound was back in September of 2018; I was fascinated and willing to take a risk in this because I thought it had a lot of potential. I usually gloss over the Vote section of the app because the small amount of COMP I’ve received so far wouldn’t make a difference in the proposals, but a flat distribution of 100 COMP, whether it’s vested or not, would be great, and it’d motivate me to be more involved in the governance of Compound.\\nFurthermore, let’s not forget that deciding not to vote is also a legitimate option. Refraining from voting, for example hoping that the quorum of votes is not reached, is also a legitimate action and a democratic way of expressing one’s dissent\\nClearly there is enough interest to move forward for a proposal.  Is there anyone who would like to take the lead (who has 100 COMP) to submit a proposal?\\nI don’t think interest in this proposal has waned. But you can easily understand that this is not only somewhat controversial but furthermore deciding the terms of the same is somewhat complicated. If, as we have to do, we have to enclose the options in a yes or no, we have to make sure that the terms of the same are as much detailed as possible. In my opinion, a distribution diluted over time, as already proposed by someone, could be the best solution, for EVERYBODY.\\nYes.  The devil is in the details.\\nThis exactly why we are here. Everyone should know now, there is almost no way to participate in active governance as normal/regular user (that makes 80% of the users of compound). This is also for other early defi products like makerdao. We want more of less governance?\\nYes, we need to start writing a clear and fair proposal, with difference choices,\nI think it should be proposed to divide the reward between the two types as said in my previous post, and mentionned by @Blck and the voters will choose : - the socialized, and - the capital weighted, meaning an early user of compound who would allocate a million dollar in compound would get the same social reward as any other early user, the capital weighted reward would then be distributed depending on the total amount of fees paid/received since the inception of the protocol with a linear decreasing Y time-factor (from Y x to 1x), pick Y=2-3-4…\nthe proposal would need to decide what % would be allocated to the social part to which % would go to the capital weighted part. 50/50 ? and so on\nThe idea is to come with a more constructiv dialog and soon a ready built proposal which will give the opportunity for someone who own 100 comp to simply submit it as well as the choice for people to vote on the different aspects of this distribution\\nJust my thoughts, but I still feel like the current idea of airdropping a fixed amount seems too arbitrary. I think it should be thought out first what is the goal of airdropping the tokens. Supposing there are 5000 addresses which should receive an airdrop, should it be instead tied into some threshold that a quorum is able to be reached if X% of the early users agree? Say, if currently 400K COMP is required to achieve quorum, we could say that if, e.g., 75% of early users agree on the proposal, then the proposal should pass. Choosing a proper X is of course a problem in itself. Yet, I think this achieves the central idea of more decentralization better than a static reward of 100 COMP. So, in this scenario and by assuming even distribution, 107 COMP tokens should be distributed per user to achieve an effectual action. Further, I think some sort of boost function should tried to be achieved. I’d imagine should need to factor in a) how early the user was to use the protocol, and b) how much were they invested monetarily into the protocol. You’d end up with uneven token distribution for early users, but if such early users are able to coordinate (which is hard) then such coordination will eventually be rewarded with the ability to reach quorum even if VCs decide to refrain from voting.\\nWhat we have for better or worse, now that @alive has seen the community on comp.xyz demonstrate an obvious appetite for rewarding early users who muddled gleefully through a Beetlejuice interface at compounds birth, is an opportunity to solidify the appropriate acknowledgement for early user impact OR continue to keep the opacity of the early user annals effectively at zero.  Lets support the blocks that were placed at the start of the chain for they bear significant weight.  My gawd @borovan on 11/24 wrote perhaps the most humble account of real responsibility for fueling the COMP rocket ship payload that escaped him.\\nIf the fears about the distribution of the token concern a possible dump of the same (which, in my opinion, it’s highly unlikely), why not simply introduce a gradual release system, mixed for example, to a system that discourages dump, with the introduction of a commission (That would benefit ALL holders) ?\\nHaving said that I must admit that the uncertainty about the feasibility of the proposal certainly does not help my mental health, time passes and I see movements in this direction by all the other platforms. There seems to be an interest in the idea being transformed into a proposal. And if this has not happened already I start thinking that early-users don’t even own enough resources to be able to make the proposal, as has already been pointed out by others\\nI think it would be cool if the accounts immediately received delegation ability of the 100 comp while the trickle out distribution occurred over 4 years.\nThat would accomplish allowing early adopters a voice while also minimalizing any flood to market of comp\\nThe early users took the most risk, what if Compound had a severe bug? There’s no Comp at that time to even think about compensation for the lost fund.\\nRegarding this official proposal not being created yet, feel free to correct me if I’m wrong, but my understanding is that several proposals will need to pass first. For example, I think https://github.com/compound-finance/compound-protocol/pull/71 59 will be necessary…?\\nThis is true, but maybe you could use another mechanism, like the one it is used to reward gradually the lenders and borrowers.\\nI have heard that the proposal has been created but that the necessary quorum has not been reached to get it approved, is it true?\\nThere was no proposal created. You can check which proposals are being voted on and which passed and failed by going to https://compound.finance/governance/proposals 43.\\ntechnical noob here…can you ELI5 this?  is the protocol not established such that if a proposal were to be submitted, proposed, and passed it could not be executed?\\ngiving voting power to the community is a far better solution than the present situation where we have an oligo-governance, cartel structure (VC funds). VC funds don’t care about protocol, users, and community in general, they are interesting only in token price pumping to take profits\\nSo are we doing this or what? What’s the holdup?\\nNo one has submitted an official proposal.\nAnyone familiar with solidity could.\\n\nIf you have over 1% of total votes (100,000.00000001) you will see a “Create Proposal” Button that leads to a simple interface to create governance proposals.\n\n—-Unless the VC’s are feeling generous.Not likely going to happen and the original creators of COMP now work for them, so with no financial interest to create such proposal, this idea will never get proposed let alone passed.\nUniswap kept true to its roots and gave back.\\n\n  \n    \n    \n    Should Compound Retroactively Airdrop Tokens to Early Users? Ideas\n  \n  \n    Regarding this official proposal not being created yet, feel free to correct me if I’m wrong, but my understanding is that several proposals will need to pass first. For example, I think https://github.com/compound-finance/compound-protocol/pull/71 will be necessary…?\n  \n\n\nMaybe @alive can create the proposal once all the necessary proposals needed for this are passed?\\nThe following functionality has been proposed and passed. I think it’s a matter of “how much” and “for who”.\nAdd _grantComp as a way for admin to grant COMP individually.\\nI’m confused, I thought you only need 100 COMP to create a proposal…\\nNeed 1% of total COMP, at current roughly 25,000.\nCheck out the COMP leaderboard\\nBut if a single person receives enough delegated votes we may have a chance of having a voice.\\nThread starter is a VC.\nYou only need 100 comp to start a CAP. Once CAP is there, it can start obtaining delegated votes for it to convert it into a full proposal.\\nsorry, what does CAP mean?\\nCompound Autonomous Proposal.\nAnd is exactly why 100comp is being discussed here, as its the minimum needed to start a CAP.\nCan check this link for more details:\n  \n      \n      Medium – 10 Sep 20\n  \n  \n    \n\nCompound Autonomous Proposals 26\n\nCompound Autonomous Proposals allow anyone with 100 COMP to create and gather support for a future governance proposal.\n\n  Reading time: 3 min read\n    \n\n  \n  \n    \n    \n  \n  \n\n\\nThere’s no much interest in this proposal anymore, even when is mentioned in Discord is usually ignored in the chats.\nSomeone should create the proposal soon.\\nI think the issue is most of the interest in this proposal are from the small fish who supported comp early but do not have enough comp to submit a CAP.  Tis a conundrum.\\nwhat prevents us from making the proposal? it seems to be one of the most discussed and yet we are not making progress. Is there any other place where it is discussed?\\nI think someone would need 100 COMP to propose it.\nCouldnt we propose it maybe now with different choices, weight of social reward, weight of capital weighted reward, vesting period etc so that everyone will express his wishes throught his vote…\nAnd depending on the choice made by voters, it would be fully implement only once the protocol evolved in a way that would permit all the choices to be applied (vesting period etc)\nOr an other choice would be to just wait longer that vesting and some features get implemented\\nI think there will be no airdrop, unfortunately, so we go home\\nDon’t be pessimistic, I doubt allocation for early users will be 0.\nLook at the polls that were made (I didn’t even participate saw it too late) on twitter by the founder, or at the activity on this page, this is the one most viewed topic.\nI think if someone has the ability and has enough comp to make a proposal or enough delegations it would be great to do it currently and give everyone the chance to participate while we are “gathered” around this Idea webpage without having some early users losing a hope as Vityaba or imhuk as they might in the future\\nYes…it is ignored in Discord.\ngive us some news ok ?\\nI wouldn’t be against even a small $15-20 airdrop just a hey we thank you guys for the support kinda thing \\nI’m sorry, but I think the mention of airdrop by the Compound team is just one part of marketing.\nThere will be no airdrop for early users and the reason for that are early investors in the project. Early investors (VC Funds) want to protect their investment and sharing a COMP token just does the opposite effect.\nIf they wanted to give users COMP tokens they would have already done so. Uniswap is an exception, and unlike Compound they were not so dependent on large token holders whose interest is to pump up the price of the token rather than achieve decentralization in decision making.\\nThe comp pairing from coinbase was my first step into the crypto platform and I’ve still have the pairing I’m still stuck learning contracts and trying not to run out of eth lol\\nSame problem, only option is waiting for Layer2.\nWith Metamask you can offer lower price for gas fee, coinbase dont have that solution. Try to import seed from coinbase wallet to metamask\\nThe creator of this forum page is a VC I think, so you cannot say that, plus VC probably can realize the importance of not forgetting early users in the token allocation now. « compound » do not wanna be the one protocole with a thorn in the side for having forgotten the 2 first years early users in its governance token distribution. And it would above all benefit comp with building an engaged community of users.\nI suggest to read again the wisdom ideas the creator detailed in his opening post as for example:\n“Compound owes much of its success to the thousands of community members that have joined its journey over the past two years.”\n“If there is any group of people that is most likely to be aligned with Compound’s long term success”\nTo answer to bigfootgd, about the amounts of comp we could allocate to early users,\nI think the amount should depend and we cannot decide to grand 0,1 COMP to every early user.\nA very early users who decided to put at risk a large part of his porfolio to lend it on compound when it just launched, sometimes interacting with the smart contract directly since 2018 on compound v1 when the TVL was less than 10 millions before incentives came up, is not the same with someone who would send a few bucks on compound about a week before the launch of comp token distribution started. That’s why a socialized as well as a capital weighted allocation seems maybe more fair. (Uni alike)\nGetting back talking about numbers\nI am proposing we could use the part that was removed of the LPS by the Gauntlet proposal 22 (20% speed reduction) and which is currently accumulating in the Comptroller for 4 years to be used, fully or partly, for this purpose of granting an allocation to the early users\nThese are being linearly released to the Comptroller and would insure a linear distribution to the early users satisfying those who wished a vesting system for the early users grant.\nTo give an idea, this sum represents 20% of the COMP distribution after proposal 22 so more than the 500k COMP suggested by the creator of this page.\nIs it right or is there any other solution about where the comp should come from ?\\nI am also  early user with 3 accounts  and I am writing here only facts. If you dont like it (or VC and other “bosses”) I dont care.\nWe were witnessed event - voting for proposal 32 what are the interests of VC funds in terms of sharing COMP tokens (a fact that you can check if you read a few more posts on the forum).\nWe all need to accept criticism because by criticizing decisions and proposals we can make progress. I have never insulted or provoked anyone on a personal level.\nAnd to repeat, I don’t care who the founder of the forum is, the boss, the director, etc., I will say what I think on the forum, so feel free to censor me.\nI have 70% of my funds on Compound, and I give myself the right to criticize everyone, but any criticism to my statements is welcome - when it comes to the functioning of the protocol.\\n\n\n\n Andre1:\n\nA very early users who decided to put at risk a large part of his porfolio to lend it on compound when it just launched, sometimes interacting with the smart contract directly since 2018 on compound v1 when the TVL was less than 10 millions before incentives came up, is not the same with someone who would send a few bucks on compound about a week before the launch of comp token distribution started. That’s why a socialized as well as a capital weighted allocation seems maybe more fair. (Uni alike)\n\n\nI am agree with this, something more like 1inch airdrop is more realistic (but with some changes because they not same DeFi category).\\nHey sorry didn’t mean to censor you or anything, and early users have a convergent interest anyways !\nIt was very good from the founder to make this twitter poll, which had positiv results, and from the creator of this page to open this idea, but now not to stay stucked and avoid what we saw : even the most activ proposal page can die if we don’t go one step further !\nI think what we need now is someone from the founding/dev team or someone with enough comp and notoriety to submit the proposal and help for the cause of early users and compound community.\\nAs an early user myself. (march 2020) who cannot afford to buy comp  : (\nthis would be a fantastic gesture!\ni agree!\\nIs there any other place where conditions, terms and modalities are discussed?\\nNot really, people are regularly calling for action for early users on compound discord but until now no proposal were submitted :(, you can try your chance to discuss there as well, the founder is often there as well as some interesting people and developpers.\nI hope the founder and community delegates will hear it and continue the process, it’s not enough to just launch the twitter poll as early users hadn’t the allocation permitting them to open the proposal, again I think they should do the next step which is to submit a proposal\\nHopefully somebody proposes this. Understandably, there will always be folks who oppose anything that doesn’t personally benefit themselves- the “it’s not enough that I succeed, others must fail” type of sentimentality. But considering the asymmetric risk early users assumed and the momentum they provided the protocol, it’s only fair.\\neverybody seems to hoping for somebody to start work on code \nsuggestion would be for those who are in support of this to at least say they are willing to delegate the current comp they have if someone tries to do a CAP.\n*i would delegate my comp in support of any proposal towards this\\nI would delegate my comp in support of any proposal towards this\\nYes, I guess the vast majority of people who wrote here agree to delegate for a proposal related to an early user comp allocation including me.\nIn a first step, from what I read from you, we would need someone to code the proposal. Once it would be submitted and in case it would be accepted :\nIn a second step, we will need to have someone coding to implement later the actual proposal (depending what kind of reward there will be, socialized, capital weighted etc…), in case the founder and developpers wouldn’t help early users  there could be a solution to call for a SAI/dai reserve to remunerate/hire the coder as it was suggested in some  other proposals, thus making sure the proposal is feasable. Other projects have gotten a lot of credit for the work their founders and/or community did not hesitate to do to make this retroactive distribution to their early users fair and possible (studying the history of adresses which interacted with the protocol). If I knew how to code, I would really do it myself\\nAfter reading through discord if we do come up with a proposal it will need vesting for sure. Since there’s not enough comp there would be no other way to grant it other than a slow release.\\nYeah as I mentionned I do not see an other solution to that one : “I am proposing we could use the part that was removed of the LPS by the Gauntlet proposal 22 (20% speed reduction) and which is currently accumulating in the Comptroller for 4 years to be used, fully or partly, for this purpose of granting an allocation to the early users\nThese are being linearly released to the Comptroller and would insure a linear distribution to the early users satisfying those who wished a vesting system for the early users grant.\nTo give an idea, this sum represents 20% of the COMP distribution after proposal 22 so more than the 500k COMP suggested by the creator of this page”\nIf you think of any, please share your thoughts here, after watching the tokenomics of COMP I thought we could use a part of the funds which were immediately available to the community at launch (to give an idea it was representing I think 500+k comps) but I was told these had been sent to the Reserve.\\nwe also need to come up with a way to tell who were true users and who just popped up for a day and left. Was mentioned on discord as well how some only used comp for the stablecoin voting event then never used the protocol ever again.\\nYes, I heard there was a kind of sybil attack with people creating a hundred adress the day of the vote and send it 0.1 eth , well for the capital weighted reward this will not change anything as dust wouldnt be rewarded significantly and spreading it into several adress or gathering in one only wouldn’t change the result\nfor the socialized reward, I agree we should remove these adress, this should be easy, you can\n1)remove adresses that had collateral deposit on compound for a too short period (less than 2 days for example)\n2)and/or remove adresses that added only dust amounts to the protocol (only a few dollars)\nThere will be ways to remove these I think from the reward, we will find\\nok good. Would it make sense to have the comp distributed into Ctokens so while people are waiting they can earn interest on it? think mentioned above as well. Just trying to understand what we all agree on so far so we could at least come up with a base draft idea.\\nCtokens is a great idea!\\nI think some are missing the point here.\nI think the point of  @alive  was to get more community participation, specially from early adopters.\nHence the proposed 100comp, which is enough COMP to start a CAP. They can lock the token from  being able to be transferred  of it for 5yers for all I care.\nI as an early user just want a stake enough to allow me to initiate a proposal.\n\n\n\n alive:\n\nneed a set of long-term power users who take an interest and step up to participate in protocol governance.\n\n\nI’m also in support of this cut-off as, as this was pre recursive farming era, and “wallet creation” attack was still unheard of. If someone wants to move the cutoff to an earlier date to capture really early supporters, if it has a logical reasoning behind it, I’d gladly support that too.\n\n\n\n alive:\n\nConcretely, the community can opt through governance to redirect 5% the total COMP supply to the ~5,000 addresses that belong to both users and developers who interacted with the Compound protocol (either v1 or v2) before June 8, 2020\n\n\n,\\nAgree on this as well. I work from home and I wouldn’t mind taking the time off to learn how to improve my coding skills and start building on compound. it just needs to be worth it. Dont care if they lock it up either. Thats why I brought up the cTokens so while we wait we could at least get some interest on those coins.\\nToo late for this. Come up with something new.\\ntoo late for what. The proposal?\\nAs someone slightly out of the loop, am I right in assuming that currently the protocol does not support vesting? So an update would be required in advance of any proposal that includes vesting?\\n\nI think the point of @alive was to get more community participation, specially from early adopters.\nHence the proposed 100comp, which is enough COMP to start a CAP. They can lock the token from being able to be transferred of it for 5yers for all I care.\n\nHey it would be great but are there COMP currently available for such a instant distribution?\nIt could be a solution as you say also to grant all the allocation instantly to give the voting power to early users immediately but add a vesting to it, it would permit to get the voting power without waiting the slow emission of comp I proposed\\nSemi off topic reply- I think the main result of enacting this proposal would be an increased comp price. That might seem counterintuitive, but I’d wager that a much lower percentage of these earned tokens would be sold than the percentage of liquidity mined tokens that are sold. Effectively taking coins out of circulation. Doubly so with vesting. And it’s great PR for Compound.\\nI can’t agree with you more.\nbut when will see the proposal is a big problem\nsoundless and stirless\\nSo what is this ?\nOne of the funding vc members create a poll about this juice “retro rewards for early users” and than suddenly they are going into silent. Why ?  And Why is in Discord so ignored ?\n\nwhy is this post also been encouraged by the founding fathers of compound on twitter ?\n\nI found this very strange… it looks like manipulation of the desire of the community for a retro active reward.\nWe should investigate the timeline of this post, the tweet, the raising funding round (recently) of this new compound chain\n@arr00\\nI doubt it’s anything nefarious like that, folks are quite busy. But it is interesting wrt to that specific timeline. Hopefully Compound keeps up with the good standards of the industry to reward those who contributed time and capital towards the protocols early success. To not do so would be… disappointing. I mean the new Compund chain might not have even happened had earlier contributors not provided the initial momentum. Remember the days when TVL was only calculated using single millions.\\n\n  \n      twitter.com\n  \n  \n    \n\na16z (a16z) 26\n\n Crypto protocols are meant to be governed by a decentralized community. Here's how we think about crypto governance — and our role as stakeholders — at a16z, including delegated voting, protocol grant programs, and early adopter rewards, by @amico_jeffrey: https://t.co/jDz0zestXp\n\n\n  9:10 AM - 5 Feb 2021\n    \n      \n        \n       115\n    \n    \n      \n        \n       41\n    \n\n\n  \n  \n    \n    \n  \n  \n\n\nJust another post yesterday about supporting retro active rewards. a16z support this idea and believes in it. This vc firm hold a large position in compound.\\nFollowing up with users in here who believe they aren’t being heard.\nI check up on this thread once a week. Most of the comments are about getting COMP. Many include shots at the team and large holders. While I understand where you are coming from with those points, you aren’t doing yourself any favors.\nI would suggest coming up with a thought out plan of how many COMP should be given out, who should be getting the COMP, why it is a good idea to give out that COMP. If someone analyzes Compound V1 and its users, I think that would get a good amount of attention.\nI have personally spent time figuring out how much gas has been spent interacting with Compound V2 and specifically who spent it and when as part of a proposal I am thinking for gas-reimbursements. I think governance is more accepting than many of you give it credit for. However, it does require someone to take the initiative and build a robust proposal and be prepared to defend it.\\nHi,\nI’m a newbie to this forum and the way I understand the situation is that for early adopters of COMP to receive a retrospective airdrop is organise to get 100 COMP to present this proposal for voting.\nIs there a mechanism in the Defi world where you could “borrow” a 100 COMP for a day (or two) and present the original proposal for voting? Alternatively, can early adopters act on mass and buy COMP and delegate their votes to an entity to present the proposal for voting?\nIs this a feasible idea?\\nYes and yes. I think we’re currently in a situation where we’re lacking someone doing proper analysis and possibly the competence to create the proposal. I’m not so sure mustering the COMP is the problem.\nWe’ve got broad ideas (100 COMP), but I also think that for this to really hit home we might need vesting, which is waiting in the wings with other progress being made on the protocol.\\nIt seems to me to be an odd situation. The major holders are in favour of the proposal. It’s also interesting that the instigator of this thread/proposal  could easily make the proposal to be voted on; but they choose not to advance any proposal, in any form (vesting schedules etc).\nIn addition, I don’t understand why some comments refer to ‘coding’ being a prerequisite for any proposal being voted on (are we talking plain English here?). Any coding requires would be an on-cost to be absorbed by the community.\\n\nFinally, we believe that token distribution models that reward bonafide early users and contributors are likely to create more engaged communities, and therefore more sustainable protocols. These are the types of users and developers who add value before a protocol achieves real network effects, or even has much inherent utility. Examples include the early users 6 and liquidity providers on Uniswap and the early developers 2 who built applications on top of Compound. We believe that these are the types of users who are likely to be the best long-term stewards of the protocol. While any token distribution model must take into account regulatory considerations, we generally believe that efforts to reward these types of early adopters are likely to position a protocol for long-term success, and we look to support them wherever possible.\n\nProtocols that achieve meaningful decentralization are more likely to gain long-term adoption and sustainability than those that do not. While the potential design space of governance is massive, we remain guided by this basic principle, and support initiatives that we feel embody it. We’ve provided a few such examples here, and look forward to helping identify and develop others that may emerge over time.\"\n    \n\\nIn fact we are seeing an “army” of early users calling for action !\nSo what to do now ?\nThe best might be to call these who handle the protocol the best : @rleshner @blck , @alive (etc…) and ask them these questions :\nWhat do you think should be proposed as an allocation toward early users now ? What do you think should be the next step?\nWould you vote in favor of that or not ?\nThe still emerging community of early adopters which was not strengthened throught an initial allocation may still need some guidance, support and bootstrap from founders or some major protocol governance participants.\\nI agree with this proposal. Anyhow, is it possible to delegate comp ?\\nI have said in the early days when this idea came up that i would be fine even with higher percentage to be rewarded to the users, but there are technical questions that has to be solved to make this work,\nas some people expect why nobody press the “distribute 5% to early users button” this is not how it works, there are some issues like there is not enough COMP “ready” to do a 5% distribution at this time, then how it should be distributed? streamed/vested over 1 year till there is enough available.\nthese are all things that has to be developed to be able distribute to thousands of users.\nN\\nand the biggest question why nobody made a list of users yet ? demanding is easy\\nalso some discussion should be about minimal usage of the protocol to be rewarded to avoid reward spam accounts\\nfor us non technical people how does one generate that. i wouldn’t mind learning.\\nI think I have read some thing on discord about that,I found @grasponcrypto and @wario talking about a tool that already exist and said “it seems like it would also be easy enough to add a simple check for users to see if they’d be eligible for the discussed airdrop”, these guys might know how to\\nWell, that discussion was about a tool @wario created which was rather neat.  You can find that here: https://mariorz.github.io/compcharts/ 11\nHowever, that tool is not exactly what you are looking for here.  It could likely be forked or modified to perform the duties you’re wanting for a non-formal vista as to what the result set may look like.  Further, the community would first need to decide some rules on what constitutes valid early users.  For example, uniswap picked a snapshot date where it dropped to all users before said date, thus guaranteeing they were not airdropping to accounts who only participated in order to farm.  I’d recommend we start there at the very least, which means we discount any accounts which had no activity before June 8, 2020 as stated by OP.\nFrom there its further discussion.  The goal is to get participation from a larger crowd and thus benefit early users who cannot afford comp but are key to the community.  One thing i fear is that by discounting accounts that contributed little, we’d be discounting the very people this idea came to support.  Any user that contributed, no matter how much or how little, did so with much risk and rather little to no reward.  Most of the time tx fees chewing into any profit which may or may not have been made.  Thats just my 2 cents.\nUntil the ‘rules’ are discussed and decided, no tool can help because the tool has no way to know what to look for.\nEdit: for full disclosure I was interacting with comp since at least 2019.  Im no whale however and was dealing with amounts around 100.00 of what was then dai and is now sai\\nThe idea was not to remove early users who provided 100 dollar to the protocol from the socialized allocation, I think it was just meant to remove the sybil vote system attacker (who attacked the voting system) only, as an attack occured where the attackers created hundreds of adress and provided 0.1 eth to make a vote, I guess it should be easy and these will appear obviously in the list, not so many adresses voted at these time, I don’t doubt we will easily remove these from the list secondly. We will find easily later I have no doubt how to exclude them without excluding any legit user.\nIn fact the idea is an allocation for early users (pre comp distribution) 2018 2019 and begining of 2020 so the date taken should be JUNE 15 2020 i guess ?\nFor pushing the process faster, I would really agree to add a compensation (small and reasonable just to compensate for the time spent) in a following proposal for amazing people here (not myself) who would bring the technical tools permitting this to happen.\\nI would say the requirement should be more than 2 interaction to the protocol that contains 1 supply event and 1 withdrawal event, this would probably filter out the spam accounts\\nIf someone supplied a large part of his stable portfolio for 2 years for example he would have only one deposit so one interaction, despite could have a large role in the protocol liquidity provision, so I think we should find something else or add something later, this should be easy.\nadding to that the attackers might have withdrew their funds in a cheaper gas day so might not even remove him/them\nSo if I understood well rleshner described these sybil attacks as one user sending a very small amount of eth, 0.1eth (worth a few dollar at the time) to dozens or hundreds of adresses at the day of the vote and submitting his vote, maybe we could use several process to exclude these from the socialized part of the reward, and not only one process :\n1)exclude dust amounts (less than 0.15 eth could be considered and would probably let in legit users)\n2)exclude the adresses for which funds are locked for a too short period (less than 2 days of participation in the protocol)\n3)etc\nas a last solution eventually could try to sneak in the list and try to find manually obvious sybil attackers (as I said I guess not many people voted so should be clearly easy)\nfor example I guess these attacks happened on a specific day and would be very easy to see that if on average 1 new user is onboarded a day, (at early stage for example), the sudden spike of 600 new adresses in one day wouldn’t be natural\nI think these matters are to be studied in a latter stage thought.\nwell plenty of possibility to avoid to exclude any legit user\\nSince the talks are about distribution to users, who used BEFORE governance token was introduced, and Compound protocol was pretty much the one who started all that governance token distributions, amount of accounts created specifically for purpose of airdrop is likely negligble. I believe it’s not really a problem at all.\nBesides, since the goal isn’t really to give away free money to everybody, but rather reward longterm supporters, your Sybil worry is relatively easy to fix: As simple as adding a requirement to have equivalent amount of COMP in cCOMP for example to be able to claim that distribution. If user have less, he can claim proportionate amount. And of course, vesting for 3-4 years. \nAnd majority of newcomers asking about airdrop should realise, that discussion is about users who interacted like almost a year ago or more. So people who discovered Compound recently, or lately last year after Uniswap drops are not eligeble anyway.\nActually, if seriously thinking about it…\nMaybe it’s time to move over really. Maybe we should revamp that idea Completely. From airdropping to early users to something that everybody could benefit and what will benefit protocol at same time.\nFor example, we can introduce multiplier for early users, but let everybody participate. So anybody could buy COMP tokens, lock them in cComp for like 3 years and become eligible for claiming “airdrop” of COMP, which will be gradually distributed over same 3 years. Early supporters in that case can get either get double rewards or have half of the COMP locked capital requirements.\\nI have a friend who did exactly this. supplied a large amount during the early days and has just been watching it since. Would prob be best to go with option 2,3 so we dont leave out many who supplied once the same way.\n\n\n\n Andre1:\n\nIf someone supplied a large part of his stable portfolio for 2 years for example he would have only one deposit so one interaction, despite could have a large role in the protocol liquidity provision, so I think we should find something else or add something later, this should be easy.\n\n\\nSirokko, your post goes too deep, in my opinion.  The idea here is to keep it simple. The want here is to give back to early users, not generate a whole new game theory.  Im not against that idea, but lets isolate it and put it on its own merit in a separate post.  My 2 cents there.\n@Andre1 - I agree, it shouldnt really be about how much one contributed and for how long.  The idea should be that anyone who participated in good faith be included.  The main idea should be inclusive not exclusive as at the heart that is the entire point of this proposal.  You hit the nail on the head that the main goal would be discounting sybil attacks, and to simplify the proposal I think thats likely our best endgame. Something along the lines of:\n\nbefore comp announce date.\nnon-malicious.\n\nWe could go back and forth about who deserves what and how to figure all that out, but for simplicity sake and to be able to push forward here, the above is, in my opinion, a great starting point.  There may be a few accounts who did nothing major to warrant the benefit, and maybe they are not even aware of the proposal, but the other side of the double-edged knife would be attempting to be too exclusive and leave out those who were truly supportive in their own way and their own right.\nThus in my opinion the goal should be to include all users who acted to benefit comp in their own way with their own weight.  Followed closely by excluding any and all MALICIOUS actors.\nOnce we go beyond the above it truly complicates things.  There may be a handful of accounts who dont necessarily deserve it, there may be similar accounts who do; but in attempting to perfect, we may just bicker this proposal into attrition.\nCurrently I am spending many hours learning geth api calls to find a way to do some basic reports and i hope to get back within several days at least a high-level list of accounts.  I’ll document what I did so it can be duplicated, discussed, and tuned, but I will start by limiting to all accounts before June 15, 2020 and then we can move on to exclude malicious actors and so forth.\\nThat is June 15, 2020, right? Also, a GitHub repo for this would be great.\\nyes, i edited the post.\\nI think the retro airdrop should be as inclusive as possible to bring people to the Compound governance.\nDo it like UNI, don’t focus on whales or filter out small amounts. Have a substancial base airdrop amount and maybe reward more active users after that (like Uniswap rewarded LPs more than swap-only users)\nBut, unlike the UNI airdrop, include smart wallets (Argent, Dharma) from the start.\nAn airdrop is the one time where quadratic participation rewards can be emulated most accurately.\nDisclaimer : I didn’t have a lot when I started in DeFi, and I deposited only small amounts using a smart wallet.\\nFor Opspec, I used a fresh account for every withdrawl/deposit.  Was not much gas back then and only my account and anothers.\nI think 1 interaction per protocol, minimum 200$+.  Would be better, keeps the spam accounts down and forces a actual contribution that the threshold can be as such that it filters out most spam amounts.\\nthere is almost nothing spam accounts(cheat accoount) before June 15, 2020\nwho know it will airdrop that time.\\nI think the two best airdrops to look at for guidance on how to go about this are UNI and TORN 1. Both of them distributed their tokens based on a snapshot ending several days before the announcement of the token, not the release, which is an important detail to think about here. COMP was announced 1 on February 26, 2020, and to avoid giving to users who joined after the announcement merely in hopes of getting COMP, the snapshot should go up to around February 16, 2020.\nFurther, UNI had two streams of tokens given out to users (three if you count SOCKS tokens, but there’s no equivalent to that here): one amounting to 400 UNI per address that interacted with the protocol in any way, and another averaging 1,000 UNI per address that supplied liquidity to the protocol, allocated based on share of total liquidity on a per-second basis from the beginning of the protocol to the end of the snapshot.\nA system like UNI’s would need very little modification to fit our case. We could have one stream give a smaller amount to every address that lent or borrowed from the protocol, and a second stream that gives a larger amount based on share of total deposited/borrowed over time.\nThere are of course details to work out, which will be easier to do once we have a ballpark estimate of how many addresses will be eligible. This is actually rather simple, since it’s a list of address that borrowed and lent before the cutoff date, and since you must first lend in order to borrow, it’s really just a list of addresses that borrowed. Note that addresses which only redeemed cTokens are not counted, similarly to Uniswap not counting address that only received from a swap but did not call the contracts. I’m going to suggest that the snapshot end at block 9500000 7.\nSo, the first item of business is to get that list! After that:\n\n\nDetermine which addresses (if any) are to be blacklisted as malicious and subtract them from the list.\n\n\nDetermine what sizes the streams should be.\nIntuitively, I suggest 3.33% and 6.67% of total supply, respectively. This would come to 1,000,000 tokens total, or one-fifth of the total promised to the community.\n\n\nDetermine what the per-address amount allocated by the first stream will be. Something to consider: TORN had a multiplier based on age of deposit to allocate its tokens. Since our goal is to reward early participants, I think this is advisable here. The complete formula they used and a graph visualizing it can be seen at the announcement linked above.\n\n\nDetermine what the per-address amount allocated by the second stream will be.\nThis will require some intensive on-chain data querying. As far as I can see, we need to know the total amount supplied and borrowed on a per-second basis from the beginning of Compound to the end of the snapshot, the share of the total each user had in each second, the result of dividing the total Comp given by this stream by the number of seconds, and then divide that much between the addresses based on their share for each second.\n\n\nOnce all this is done, formalize it all into a format that comp holders can vote on.\nCan anyone commit to any of these items?\\n\n\n\n grasponcrypto:\n\nWell, that discussion was about a tool @wario created which was rather neat. You can find that here: https://mariorz.github.io/compcharts/\n\n\nI think this is a tool for identifying COMP farmers and assessing the risk of individual accounts. The tool does not provide data relevant to the airdrop.\nThis tool was used when the DAI liquidation took place. If I am wrong feel free to correct me.\nI would just like to add that if airdrop occurs, comp farmers should not be excluded (note: I do not belong to that group) because these same Comp farmers inflated TVL and thus the current price of the COMP token. Far from it that this action is not good for protocol and such actions should be reduced.\nAnd the duration of the Coinbase earn promotion should be avoided because it is spam in some way\\nthat is also my opinion unfortunately, but I hope I will be wrong.\nAirdrop would be good for everyone because it would equalize(or reduced the gap) governance power of early investors and users - the degree of decentralization would increase.\\nYou mentionned very good points here\n1)the announcement date could probably be the one we take for the reason you mentionned\n2)we certainly wanna submit to the vote a socialized, but ALSO a capital weighted reward, however heavily we would weight toward the socialized reward, a la UNISWAP\nFinally about your last point \" Determine what the per-address amount allocated by the second stream will be.\" we can use instead the total amount of fees paid+received, it would match the current comp distribution, just an idea here, might be easier than the one you mention\nthe question is now, how do we get someone to commit to all this work ? clearly it seems neither the founder neither some users rushed to do it, so we might have to find some alternativ, compensation, how do you think ? Maybe we should attach to the proposal a call of DAI reserve to compensate the people making that possible technically to happen\\nhave not heard from this so far.\\nI understand the concern of comp token being announced, but at that time airdrops were not something done, so I dont think that is much of a concern.  Thats just, like, my opinion, man.\nSo i did some digging on dune analytics, its the only solution i’ve seen so far, on how to get some queries on addresses.  There is not a way to export those, at least not without paying for a 400.00 pro account.  However, they are free to peruse on the site.  Here is a link to the dashboard, I have not done queries for all types, and included some baseline minimums just to keep out spam accounts - those are very small, like under 1 dollar or that was my intent anyway.\nhere is a link to those:\n  \n      \n      duneanalytics.com\n  \n  \n    \n\nComp Early Supporters 75\n\nEthereum analytics by and for the community. Explore and share Ethereum data for free.\n\n\n  \n  \n    \n    \n  \n  \n\n\nSo far its only wbtc, eth, bat and usdt, but that is mostly because I didnt feel like spending more time on something that may not be even be useful.  If you did use comp for one of those before June 15, 2020 and want to search for your address to see if it shows up, please do and let me know.\nif anyone has an idea of other solutions for running these reports, let me know.  I do not have 100 comp to open a CAP but I figure if we get all the data required, we might be that much closer to convincing someone who does to put one in!\\nI was minted ceth on Jun-05-2019 and cant find my address from COMP-ETH dash\\nThat’s not accurate.\nmy friends are also not found on it\\nI think solution is in this:\nhttps://www.comp.xyz/t/compound-grants-program/\nIf they want to work on something, let them work on something that users want but are not able to do (technical and capital limitation). However, this thread is the most numerous and has the most comments, and airdrop is needed to decentralize the protocol a bit.\\nIt would be nice if some of the vc founders/funders take a position into this retroactive airdrop/reward. Its seems that the majority of the DeFi space are concluding that rewarding early users could give a major boost into governance. Look only how Badger finance is having a gold era just because the community is so strong.\nAnd its true I may be biased bc I used the protocol in march 2019! But you know why I am in this ethereum space? Because I believe in the community and truly decentralization. I hope Compound dont make the same mistake by only looking to the big pocket / money.\\nI feel that raining a large sum of money on anyone that happened to use Compound in the past is not an efficient use of the protocol’s capital. The community should align itself with “what is best for ensuring the platform’s future of becoming the global community-owned lending platform?” There is a lot of work to be done and where we are now is just the very beginning. I’m not opposed to a modest degree of financial reward for early users, but I think the bulk of the benefit should mostly come in the form of giving them a powerful voice in the stewardship of Compound. That voice comes through voting power, but that voice doesn’t have to come in the form of unrestricted COMP tokens given away freely.\nI propose that the COMP token allocation to early adopters be locked in a voting delegation contract in perpetuity. This contract could allow the early user to vote directly or delegate their votes to others.\nThe contract could also have the feature to transfer ownership of who controls those votes, which could in the future allow an early adopter to extract value through private sales, if in the future the market deems voting rights on Compound to valuable in-and-of themselves.\nThis eliminates the  immediate financial gain and perverse incentive structure that’s causing a bunch of buzz on this forum from people that stand to gain from raining money. The true stewards of Compound’s future will be able to use this gift to help build its legacy. For those just seeking a short-term financial gain, this gift will be worthless and, if because of that, they choose not to use their votes then that just makes everyone else’s vote more powerful.\nYes, large retroactive airdrops have been popular in the defi space and they create a flurry of publicity within crypto-insider communities, but Compound is already so well established that the marginal benefit seems small. Airdrops aren’t effective beyond this insular community and the majority of the tokens awarded in past airdrops just end up being sold on the market. We need to think much bigger and longer term.\nYou can think of this perpetual lock as being a burn of Comp tokens, but not the voting right attached to them. The burn of value does enrich existing COMP holders to a marginal degree by decreasing the fully diluted supply, but that’s more than offset by increasing the attractiveness of future COMP emissions and grants. In my opinion, grants are a much more effective use of the protocol’s capital in pursuit of its long-term goals.\nLocking the Comp rewards those who came early and truly care about this project in the purest form.\\nHere’s one reason why a vested airdrop of 100 comp should take place, right now when VC’s vote it’s pretty much already set decision.\n\n  \n    \n    \n    ZRX,BAT, and WBTC Parameter Update \n  \n  \n    With a16z and Polychain support that looks like it’s a pass regardless. \nI guess now it becomes important to higlight upcoming  CF change after it pass as much visible as possible, so anybody potentially impacted could adjust their positions in time. \nAnd i would like to specifically mention, that whoever is having a collateral with volatility like WBTC, and utilising it to such extend, that is put at liquidation at just 10% CF change is doing it wrong. Such risk isn’t worth a potential reward, …\n  \n\n\\nIs any result on  retroactively airdrop ?\\nThe question is that normal users now have nearly no voting weight to change the result of a proposal.The protocol is controlled by a few citadels. Airdrop can distribute more comp into user’s hand but not these citadels and it will make the protocol more decentralised.\\nThat is the big problem for Compound, I think its more CeFi then DeFi currently. I hope that protocol will be more decentralized in future\\nI say yes to an initial airdrop of COMP tokens to early users but only through using some sort of “claim” mechanism.  Also, the COMP not claimed by those users/addresses could still be airdropped to users who interacted with Compound after the launch of the COMP token, but before the initial airdrop.  Those users would also have to “claim” their airdrop.\nMy ETH address here: Address 0xe84d25b1C4fe0E5A9bEe95934AB24C9867Aac2cc | Etherscan 10 was created using Coinbase Wallet, however, I have lost the keys to this wallet and the address will always be earning interest on $360 worth of USDC, LOL!  This is the reason I believe the early adopters should have to claim their airdrop and the amount of COMP not claimed be airdropped to addresses that are currently supporting the protocol.\nAirdropping COMP to early adopters and not current supporters could be a mistake.  As early adopters may not have any use for a governance token, besides monetary value, which means they’ll most likely dump the COMP as soon as they claim it.  Current supporters of the Compound protocol would most likely use the COMP for voting or supply it to Compound.\nJust my 2 cents.\\nhow did it go in this topic for retroactively airdrop ?\nthere are many defi platforms airdrop done …remain us.\\nIf compound/coinbase lasts for the next 500 years and you get an average of 5% on that 360 USDC. You will have 14 Trillion Dollar in the year 2521.\\nJustin Sun (founder of TRON) added 1.000.000.000 usd worth of ETH in Compound. He is now farming 80.000 COMP a day!!!\nWell… we could have done this job (give governance to early users) but yet the VCs and other whales waited until this governance attack happened. Justin is not going to use it for Compound or Ethereum, he have is own agenda focused around his true bad blockchain Tron.\nAnyhow… compound is moving slow, governance to early users with a vested/locked time could make this protocol more open.\nHappy easter, enjoy governance attack\n\n  \n      twitter.com\n  \n  \n    \n\n0xGavΞ\uD83D\uDC68\uD83C\uDFFB\u200D\uD83C\uDF3E\uD83C\uDF3E\uD83C\uDF63\uD83E\uDD99\uD83D\uDD49 (0xGav) 19\n\n Justin Sun added $1B worth of $ETH to compound. Farming $80k of $COMP per day with that. \n\nDamn that dude is reeeesh af\n\n0x701bd63938518d7db7e0f00945110c80c67df532\n\n\n  1:48 PM - 3 Apr 2021\n    \n      \n        \n       295\n    \n    \n      \n        \n       42\n    \n\n\n  \n  \n    \n    \n  \n  \n\n\\nThis is a serious problem\\nI was inspired by @grasponcrypto’s effort with the Dune Analytics data to see if I could put together a more flexible tool for us to gather detailed info on early users of the Compound protocol to help move this discussion forward. I’ve played around with the Compound Subgraph but haven’t found a way to extract all the data we need that way, so I’m using the web3 Python module to scan the blockchain for cToken interactions directly.\nA first attempt is posted here. 20\nThis tool has been spot-tested on a few randomly-selected blocks bearing cToken interactions but hasn’t been run over the full range of relevant blocks yet: I don’t have access to a local ethereum node or the funds for a remote/paid service for the remote procedure calls (RPCs), so I cannot loop over all relevant blocks to produce an exhaustive list of addresses.\nA possible next step would be to start a conversation in the grants channel on Discord and see if some funds to cover the RPCs, or an ethereum full node, could be made available to run the script and collect the data, which we could then analyze together to develop a robust proposal.\nI welcome comments, suggestions, and pull requests, especially any ideas for reducing the number of required RPCs. As currently implemented, the script needs to have a peek at every ethereum transaction that took place between the deployment of the first cToken contract and the deployment of the COMP token contract.\nLike many in the conversation here, I am a small-potatoes early user with a completely unrelated day job who appreciates the enormous amount of labor that has gone into the protocol’s development and believes that empowering early users with a greater voice in governance will be a net benefit to the project.\\nIs the proposal still alive ?\\nThanks to @blck for pointing me toward web3’s Contract.events, I no longer need to loop over all transactions and can obtain all the metadata on early users we should need with a very modest number of RPCs. @blck also pointed out that my first attempt ignored V1 users (!) I think I’ve rectified that with the new version.\nHere is the updated tool 10\nI’ve also posted the resulting information in csv format, separately for V1 and for V2:\nCompound V1 Early User List 51 (36942 txs by 28021 unique addresses)\nCompound V2 Early User List 44 (254436 txs by 218725 unique addresses)\nI think/hope this is moving us in the direction of what @getty had in mind by:\n\n\n\n getty:\n\nIf someone analyzes Compound V1 and its users, I think that would get a good amount of attention.\n\n\nI didn’t provide headers, but you’ll see in the lists that each row contains the block height, user address, type of interaction with the Compound protocol, amount of tokens, and underlying token ID (cashtag) for each transaction. Also, I didn’t remove duplicate addresses: if you interacted with the protocol several times, there should be a separate line for each  transaction.\nAs a first sanity check, it would be awesome if the folks who didn’t see their addresses in @grasponcrypto’s Dune Analytics results could check these outputs and see if they’re listed here.\nAs a next step, I would suggest we slice the data in a few of the different ways that have been suggested on this forum already. Here are a few of the ways I noticed:\n1. (UNI-like or socialized distribution style) equal distribution to all early user addresses\n\nwith or without a minimum value threshhold (what value?)\nwith or without a bonus multiplier for V1 users (what multiplier?)\n\n2. (COMP-like or capital-weighted distribution style) pro-rata distribution based on (value supplied+borrowed)*(time)\n\nhow should early liquidators be included in a pro-rata distribution, if at all? They lack the “time” axis that suppliers and borrowers have.\n\n3. Pro-rata distribution based only on total value supplied, borrowed, and liquidated, not based on time.\n\nfor either 2 or 3, should the pro-rata distribution be rescaled to elevate small early users relative to early whales? I have seen quadratic scaling suggested; or we could take an even more aggressive exponential scaling, where the amount received grows linearly with each order of magnitude of value supplied to, and/or borrowed from, the protocol.\n\nThe script does not currently access the price data that would be needed for distribution styles 2 and 3. If someone else wants to add that piece to the puzzle, feel free to submit a pull request! Otherwise I’ll take a stab at it, but again this isn’t really my wheelhouse so it might take me a while.\nWhat do folks think? Are there other ways of slicing the data you’d like to see? Can we narrow it down to one or two of these options before producing the actual list of addresses and distribution amounts for a proposal?\nLast but not least, will we be able to pull together enough COMP to submit a CAP once we’ve converged on the details? I can get us a few percent of the way there; maybe if there are a few dozen of us with a few COMP each…\\nThank for the awesome work to start his off.\nBut it seems a block id cutoff would be better than which comp protocol version was used?\nAs based on the list.\nComp V1 has tx till may 2020 (Feb-27-2020 , block 9567234)\nwhile Comp V2 first tx was (May-07-2019, block 7711104)\nI think this would narrow it down to these  3 choices:\n\nAirdrop to v1 users (cutoff before start of comp v2 - block 7711104 )\nAirdrop to v1 & v2 users  with June 8 cutoff one week before comp token launched, as was originally suggested by @alive\n\nNo airdrop at all\n\\nGreat effort! Thanks for putting in the time on this.\nI think you may be using the wrong decimals for USDC (6) and WBTC (8). The numbers in the report are mostly zeros.\\nI would favor a distribution like (2) that increases somewhat with larger value*time, but sub-linearly. So something like sqrt(value*time), but I wouldn’t worry too much about the exact form. This achieves several goals:\n\nwide distribution by rewarding small users\nrecognizing bigger players as contributing more to protocol success\nbeing more efficient in getting a significant quantity of COMP to small users without breaking the bank with huge rewards to big users\n\nJust to clarify how UNI was distributed: as far as trading users went, it was a simple constant value, but for liquidity providers it was more complicated. Something like a fixed amount of UNI per day, distributed pro rata among LPs according to their capital value. So early LPs got big rewards because they were suppling a large fraction of the total. I think the idea was to reward those who took the most risk and contributed to the early bootstrapping.\nYou can make a good argument for that approach here, but personally I’d support it either way.\\n\n\n\n pyggie:\n\nI think you may be using the wrong decimals for USDC (6) and WBTC (8). The numbers in the report are mostly zeros.\n\n\nYes I am, thank you! Will have that corrected next update.\\nI think you summed it well.\nFirstly thank you for the big work which is necessary to make this proposal go further.\nWill try to sum up my ideas, they are just ideas opened to discussions ofc\nI would agree with the others and think we should propose 1 and 2.\n1 : a socialized allocation, which would grant a fixed amount to each early user.\n2 : a capital weighted allocation, which would match the current COMP distribution in a way.\nWe could choose it in a way it is proportional to the total amount of capital lent+borrowed throught time.\nOr maybe better proportional to total amount of interest paid+received throught time.\nHowever as pyggie said, if you supplied in the earliest day on Compound would carry more % of the total liquidity supplied a week before comp distribution. Thus we would have two choices :\na)either try to perfectly match current COMP distribution and at each block, a dollar lent on compound would represent a certain % of the network depending on the total liquidity, so basically we would simulate a behaviour as if the comp distribution started for early users at earliest stage. But I am not even sure it is possible\nb) Or we would do it MUCH simplier, but still it would be fair, we would simply apply a (linear?) coefficient which would also as 1 be able to distinguish the importance of the network taken by a liquidity provider at a given time.\nFor example we could choose to apply a coefficient that would linearly decrease from 4 to 1 starting from the comp v1 launch, finishing at the cut off date. This might not follow the exponential liquidity growth totally but at least would partly be able to weight the oldest liquidity at their share of the network.\nI would be in favour to propose these features to the vote.\\nAwesome work.  I have a local geth node running so if you want some help there dm me in discord.\\n@allthecolors\nI have run the script against my geth node and received the output.  I have a pull request back to your repo to add the file. Let me know if anything is amiss and we can try again.\n\n  \n      github.com/0xA1176ec01045/CompEarlyUserAnalysis\n  \n  \n    \n  \n    \n  \n\n  \n    \n      add account type - GraspOnCrypto 18\n    \n\n    \n      0xA1176ec01045:main ← grasponcrypto:account-contract\n    \n\n    \n      \n        opened \n        \n          \n        \n        Apr 10, 2021\n      \n      \n\n      \n        \n          \n          grasponcrypto\n        \n      \n\n      \n        \n          +410815\n          -0\n        \n      \n    \n\n  \n\n\n  \n  \n    \n    \n  \n  \n\n\nThanks,\nGraspOnCrypto\\nOption 1 is the best and easier solution.\nJust as @alive suggested.\nIncluding the vesting for over 4 years, so it doesn’t matter if receivers decides to sell or hold, it will take years for most of those tokens to enter the market.\nAnd that also means less tokens to farm and dump by whales.\nI guess people forgot about the $500M DAI whale from October, that  single handedly kept the price at $100-110 by dumping tons of tokens every day.\nNow with the current popularity of crypto, there’s gonna be whales farming with even larger budgets, just like Justin is currently doing with $1.2B in ETH.\\nI know I have spoken my mind before, but I’d just like to raise it again.  In my opinion, if you want this idea to go through, it needs to be as simple and straight forward as possible.  The more complex you make it, the more you attempt to create little niches of ideas, the more likely this is to go nowhere.\nI’ve put forth a quite a bit of effort to move this along, more so than most who are just replying to this thread in an attempt to put their voice out there, and I dont want that all to be for nought because the community decided to spend all the effort deciding which address deserves more than another.\nIMO, benefiting the compound community by distributing to strong early supporters in any way is better than trying to knick pick many details.  KISS - Keep it simple.  Lets find progress, get the information required, find someone with enough COMP to put in proposal and get it moving forward.\nthank you for your listening to this rant\n-GraspOnCrypto\\nThanks everyone for your input! I want to start this update by harkening all the way back to the beginning of the forum thread:\n\n\n\n alive:\n\nThe above would imply that each of the ~5,000 early adopters of the Compound protocol would receive 100 COMP tokens over the course of four years.\n\n\nAs my earlier posts show, I found significantly more early users than estimated by @alive; my analysis gives ~30000 direct users, not including interactions mediated by contracts. So,\n\neither we adopt @blck’s rationale to increase the available pool of COMP tokens to reach 100 COMP per eligible address;\nor we maintain the initially suggested 500,000 COMP pool, which yields fewer than 20 COMP per eligible address.\n\nI anticipate it will be much harder to pass 100 COMP per early user through full governance if it means sextupling the early-user allocation beyond what was initially envisioned. So the proposal I share below is based on the 500,000 COMP pool originally envisioned, inflated only slightly (by about 9%) to bring the minimum COMP distribution from 18.305842454139047 COMP to an even 20 COMP.\nProposed distribution list\nHaving now sliced the data several ways (socialized, capital-weighted, sqrt-cap-weighted, log-cap-weighted…), the approach that strikes me as the most fair is a “95% socialized, 5% capital-weighted” distribution.\n    Here is the list 90   \n(updated 12 Apr '21 to exclude addresses associated with Sybil attack on early governance)\nHow does this specific proposal balance fairness and simplicity?\n\nThere are compelling arguments for both socialized and capital-weighted distributions. Including both strategies in the formula is a way to reach a compromise on this question.\nIt is sensible to reward users who supplied and borrowed more capital to the protocol, but even a 50-50 split between socialized and cap-weight distributions is so heavily whale-dominated that it fails to achieve the goal of empowering the early user community with a meaningful shot at a role in governance.\nA 95-5 split grants the largest whale (who has outed themselves already in this forum, @borovan) roughly ~100x the smallest grantee.\nMore than 9 out of 10 early users would receive between 20 and 21 COMP under this proposal; 45 users would receive > 100 COMP; 4 users would receive > 1000 COMP.\n\nFinally, to align this distribution with the goal of broadening early user participation in governance, I suggest we develop a separate proposal to lower the minimum COMP required to submit a CAP to 20 COMP.\nWhat about contract-mediated interactions?\nThe elephant in the room with early-user distributions is contract-driven interactions with the Compound protocol.\nContext: Uniswap’s genesis UNI airdrop excluded contract interactions, spurring an initiative from application integration and DEX aggregator projects to propose an airdrop for users who interacted with Uniswap via proxy. The proposers took the position that this exclusion was accidental, and while the proposal received enough support to go to a vote, it ultimately did not pass.\nNow that I’ve attempted to account for proxy interactions with Compound through on-chain data myself (thanks to @grasponcrypto for running part of the analysis!), I’ve come to a different conclusion about the Uniswap team’s motivations for excluding contract interactions: it’s simply way more labor-intensive to unwind these proxy interactions and find the initiating user on-chain 5 – like, by orders of magnitude. And that’s just for projects that drop cTokens in the user’s self-custodied wallet. If a project uses Compound in a less user-direct way, e.g. holding everyone’s cTokens in a contract and periodically distributing interest in the underlying token, then it would not be possible to determine users’ distributions without additional knowledge about the project’s contract structure.\nApplication integration projects are welcome to develop separate proposal(s) for their users who interacted with Compound. Most of those projects have the skills in-house to do this kind of analysis much more efficiently than me, and they’ve had a few months to take the initiative. I am just a user/fan of the protocol, not representing any company in the space, so I choose to focus on early users who interacted directly with the protocol.\nNext steps\n\nGive folks a few days to review the code behind this analysis 12 and make the case for any revisions.\nDevelop a CAP: Here, I think we can borrow @arr00’s merkle-distributor code 1 originally developed for Proposal 32 6 to address the DAI liquidation event. This is venturing even further out of my coding comfort zone, so I would love it if someone with experience submitting a CAP were willing to step in here and use this list to produce the merkle tree and contracts required to submit a CAP (or at least help walk me through it!)\nPersuade one of the ~400 community members holding > 100 COMP to submit the CAP.\n\\nThis is awesome work, thank you for the great effort! I’m slightly out of the loop, but does this list make any attempt at removing those “manipulating” votes? Or did it take the KISS approach?\\nThank you for the awesome work you put into this proposal,\nthe remerk about the proxy makes sense.\nAlso note that for the socialized reward it might be possible to remove many adresses which took part into the Sybil consensus attack (they supplied 0.1 eth and Voted and withdrew), so it would maybe mean less adresses included if we found a way to remove them and only them, of course in case this is technically possible, maybe it would be feasable as not many people were voting at that stage. Note this wouldn’t affect the capital weighted distribution.\nAbout the share that should be split between socialized and capital weighted, I would say that 95-5 split seems really neglecting the capital weighted part.\nLooking at the example of UNI as we talked about it, around 49 million UNI were allocated for early Liquidity providers as capital weighted allocation, while around 100 millions were allocated to early users for as an socialized allocation.\nSo this would make around 67%-33% socialized - cap-weight distribution in this example.\nEven if there is no rule that would force us to match what was done in the other project, we could balance these two type of reward in a more evenly way…  without really affect the socialized reward, or to propose that ratio to the vote.\nI would say that the capital weighted allocation wouldn’t be only here for distinguishing ONE whale but also some smaller early users who dedicated large part of their funds to the protocol since its earliest days, which gave traction, and permitted compound to develop, thus the importance also I would say to weight the oldest higher (for the capital weighted part of the allocation )taking into account the exponential increase of TVL, taking into account a dollar supplied on compound was supplying a larger fraction of the total in 2019 than it was at the cut off date and consider these who took the risk in the early days to supply to the new protocol which was compound when the tvl was peanut. Thats how UNI did, by the way, as pyggie mentionned.\nIn january 2019  TVL was 10 millions and in june 2020 TVL reached 60x increase to 600 millions.\\n@allthecolors has the following info saved in the files:\n6400580,0x13dcf605c12832359ca64768c2a0c246a48181167fbc271cf5cd281378ba9f31,0x29884627385E3F7bb763Eb72E3d5B9A98143452d,supply,0.0,WETH\nthats a random line pulled for an example.  I’m not sure what he first column represents but the second is the contract address, 3rd is the user’s address, 4th is the amount and 5th is the token.\nIf anyone has an idea on which information could be used to purge the sybil attacks I’d be glad to work with this data and create an output which excludes those. I could purge all accounts that added 0.1 eth, but i think maybe that would exclude valid accounts?\nAnyway, I’m not familiar with the sybil attack or what it looks like, so if anyone wants to dm me in discord or paste info here on how we could weed those out, I’d be happy to work on that.\nIs there not a list out there already of the addresses which participated in this attack?  I’d think it would be easier to generate a list of malicious addresses as a separate task, and then use that list to purge records out of the one currently being proposed.  Further, I’d suggest we use MEWs list of naughty addresses to purge any of those from this list as well.  ethereum-lists/addresses-darklist.json at master · MyEtherWallet/ethereum-lists · GitHub 3\nEdit: Dont have to worry about that, did a quick search and there were no matches!  However, if anyone has a list off addresses that should be filtered, maybe there are other blacklist address lists out there, or perhaps someone at compound has a list of malicious addresses used in the sybil attack, I have a basic script i can use to modify the current list.\\nI tried to watch a bit the code but I am not a programmer at all(\nWhat I was wondering was, which price did you set for the token which are not stablecoins ?\nDid you take the price of these token at each block to calculate the notional value lent on compound by the adress ? Or did you simply picked an average ?\nI tried to read SimpleCapitalWeights.py\ntoken_weights[token] = (0.5*(StartPrice+EndPrice))\nAs some coin price could be x30 or x100 it would make more sense not to take current price for a volume that was lent in 2018 for which the price was 1% of current worth, for example, or it totally distord the capital weighted reward, it would mean if I lent rep then withdrew and sold it, then rep would make x100, the notional would count 100 times instead of 1 for the capital weighted allocation\nJust saying it, as I’m not sure on the way which was picked\\nIt’s a good question @Andre1. StartPrice and EndPrice are the date the Compound V1 money market contract was deployed (Sept 26, 2018) and the date of the launch of the COMP governance token (June 15, 2020), respectively. These dates cover the eligibility window for the early-user distribution, not current prices. So the weights are just a “two-point average”, the simplest possible approximation for the average price over the eligibility window. Stablecoin weights were fixed at exactly 1 for simplicity.\nA daily TWAP (time-weighted average price) for each token would not be too much extra work if folks would like to see whether that has any significant impact on the proposed distribution…\nThe most surgical approach would be to use the Coingecko API to produce customized weights for each transaction by estimating the USD value of the supply/borrow at the relevant block. That is possible, but it seemed like a lot of work for something that is ultimately not likely in my opinion to change the final distribution amounts that much.\\nmy point of view but I am not sure if it will change something as I didn’t check price of all tokens\nwe could use integer(BTC(t)dt, t=26SEPT2018…08JUN2020(or 15 june))) the formula would then perfectly match for someone who deposit at the start date and let it throught the 15 june, without any distortion compared to other tokens, would only need to get this integer once for the few tokens available on compound v2.\nBut it seems there were no huge token price movement between these two dates so maybe this integer match your arithmetic average\\n@TragedyStruck good point, and @grasponcrypto thanks for collaborating with me on a solution. I prefer the “keep it simple” approach in general, but I also agree that if someone’s main interaction with the protocol was to spam an early governance vote with a bunch of small-value addresses that did nothing else, they should not receive a separate distribution for every address they used.\nI’ve posted a solution to the early user analysis Github repo 13 (see SybilAttackRemover.py, SybilAttackRemover.output.txt, and edits to EarlyUserProposal.weighted.py) and updated the early user list (link in previous post now returns the updated list).\nThere are probably other users with multiple addresses who will benefit disproportionately from the distribution, but I am satisfied that we have removed the most egregious (and hopefully the only malicious) example.\\nI like it.  I completely agree as well.  Keep it as simple as possible, but we need to do all we can to ensure malicious actors do not get rewarded for their misbehavior.  Especially considering this malicious actor would benefit exponentially due to the nature of their mischief.\nNice work.\\nI do believe there may be one issue, your solution calls for 500,000 COMP to be distributed\n  \n      \n      Ethereum (ETH) Blockchain Explorer\n  \n  \n    \n\nCompound: Comptroller | 0x3d9819210a31b4961b30ef54be2aed79b9c9cd3b 5\n\nThe Contract Address 0x3d9819210a31b4961b30ef54be2aed79b9c9cd3b page allows users to view the source code, transactions, balances, and analytics for the contract address. Users can also interact and make transactions to the contract directly on...\n\n\n  \n  \n    \n    \n  \n  \n\n\nThe Comptroller contract only has 228,000 COMP\\nMy interpretation of @alive’s post kicking off this forum discussion is that the proposal can request a fraction of COMP be redirected from the community token allocation, that is, the allocation currently being streamed at 2,312 COMP/day to active lenders and borrowers:\n\n\n\n alive:\n\nAs for where these tokens would come from: they would be redirected from the flow that currently goes to liquidity providers.\n\n\nThe Compound Governance announcement on Medium specifies:\n\n4,229,949 COMP are reserved for users of the protocol\n775,000 COMP are reserved for the community to advance governance through other means\n\nIt was later announced 3 that the 775K COMP have been apportioned to Compound’s participation in Coinbase Earn, with the rest sitting in the Reservoir contract.\nIn short, it seems reasonable for the proposal to include a transfer of COMP from the Reservoir to fund the early-user distribution.\nEdit: The background to Proposal 32 does an excellent job describing the relationship between the Reservoir and Comptroller and has helped me better understand how this would look in practice.\nThe Reservoir is immutable and drips 0.5 COMP/block to the Comptroller, of which ~0.352 COMP is distributed to suppliers and borrowers. Assuming we include four-year vesting in this proposal – which I support and which seems likely necessary to pass – linearly distributing 500K COMP over 4 years works out to about 0.052 COMP/block.\nA vesting-based proposal would “cost nothing” up front and would instead have the effect of changing the per-block distribution of COMP from “100% current users / 0% pre-COMP users” to “~85% (0.300 COMP) to current users / 15% (0.052 COMP) to pre-COMP users” while the early user distribution vests.\\nAlso, sorry for redundancy, just gonna think outloud as its the moment or never to submit our thoughts as we approach a proposal now that I analyze your 4 days ago post it seems that your program then went for the 1)socialized, and the 2) capital weighted without any account of the tvl which went from 0 to 239 millions.\nWithout going to a fully exhaustive pro rata ratio, which I am not sure will be technically easy, do you think it would be easy to simply apply a linear factor which would be a reduction of the curves representing TVL on compound throught time, knowing\n\non 18 June TVL was 239.737 Millions (according to DefiPulse)\nWe would make a linear reduction of the TVL curve as a line going from 30 SEPT2018 (2.779 millions) to 15 JUNE2020 (239.737 Millions) by multiplying the weight by a Pro rata factor taking into account the TVL each day, P(t) where t is the number of days elapsed since 30 sept 2018\n\nas P(t0=30sept2018)=239,737/2.779=86.26\nand P(15June2020)=1 aka p(655 days) = 1\nwe get p(t)=86.26 - 0.13016*t , a coefficient factor that we could apply per day  to calibrate a pro rata distribution taking into account a 86.26x increase of the TVL from the last day of the month of the launch to the 15 june cut off date.\nTaking into account such a factor or a more developped one would fit current comp distribution and would also be like UNI distributions., I also think it would permit a better distribution of the capital weighted allocation and spreading of the capital weighted part of the reward as well as the rebalance of the 95-5 to something more balanced.\nThe flaw in my reduction is the 30 september is taken arbitrary as well as that’s linear,extracting day to day the TVL datas would give an more accurate result\nThe only points that remain subjectiv are the ratio social/capital as well as the linear tvl reduction factors, let’s add it as a choice in the proposal ? Even thought I am not sure it’s the to go throught, as it mustn’t divide us on the main purpose all in all, but the stake of a right distribution is here too, I would at the end vouch for feasable solutions only\nWe could give several choice :\na-50/50\nb-55/45\nc-60/40\nd-65/35\ne-70/30\nf-75/25 … to 95/5 for the ratio socialized/weighted or even 100/0 meaning no capital weighted\nand several choices for the reduction factor from X to 1 where\na-X=80\nb-X=…\nc-X=1 (ignoring the TVL)\nd-extracting the exact daily TVL\nThis could be submitted as a preamble vote if feasable in this way “what should the early user allocation proposal contain, pick among these factors” once this vote would end the final proposal could be submitted with the factors which won, again I am only thinking out loud, this might not be the right way to go\\nI hear you @Andre1; the good news is that your “linear TVL” proposal is almost identical to the time multiplier in my proposal formula, except that in my proposal this multiplier only ranges from 1 to 2 (which was not chosen for any particularly good reason), while for the linear TVL formula it should range from 1 to 86.26 (i.e. by how many times TVL grew during the eligibility window). I think this is more rational than arbitrarily having it run from 1 to 2, so I think we should adopt it.\nI think it will also partially address your concern about the balance of social and capital weights. I agree that 95-5 appears imbalanced, but I don’t see a way for anything closer to 50-50 to keep the “floor” amount at 20 COMP without significantly inflating the 500,000 COMP request (we would already be asking for a 9% boost to bring the 95-5 distribution floor up to 20 COMP).\nFWIW a 50-50 distribution shifts the floor to about 10 COMP – with the other ~10 COMP per small user (and by small we’re talking anyone under millions of USD equivalent supplied/borrowed) going mostly to a handful of very deep-pocketed users, as any capital-weighted distribution would do.\\nAlright I would have several questions then as I am not able to understand this code easily, will be useful for others also who cannot read any code\n–1)Basic question, just to make sure I understood the choice you made, as I didn’t see the formula used, your system (for the capital weighted) is allocating a reward which is proportional to the number of blocks during which someone was lending (and possibly borrowing) a Volume on compound., for each block there is a factor (but thats question 2), in a way that your system is kinda similar to the current compound distribution ? I mean by that if someone came and lent 1 million for 1 block, he wouldn’t be elligible for example to a capital weighted reward right, as its proportional to the time he allocated his funds to the protocol?\n–2) So you wrote\n\" * There are compelling arguments for both socialized and capital-weighted distributions. Including both strategies in the formula is a way to reach a compromise on this question.\"\nDo you mean that you applied a “boost” which is linearly decreasing from the start toward the end date that went from 2 toward 1 ? (to take into account TVL change)\nI mean by that, providing 10 dollar for 1 block on september 2018, would be equivalent to provide 20 dollar for one block on june 2020 in the amount of capital weighted allocation your calculation generate ?\n–3)Were you able to take into account both compound v1 and v2 for the capital weighed allocation (including compound v1 users after the launch of v2)\\n@Andre1 I agree it would be helpful to share a simpler document laying out the current calculation and hope to add this to the repo; in the meantime, please take a look at the comments (specifically the lines starting with # in EarlyUserProposal.weighted.py). The formulas are written out and explained there.\n(1) No, the time contribution to the weight is not proportional to the number of blocks during which someone was lending (and possibly borrowing). It is based exclusively on the time of the address’s first interaction with the protocol. Disentangling the duration of supplies and borrows using the on-chain withdraw/repay data is much, much harder because of the way multiple interactions can layer on each other. If a user supplied 100 SAI at block A and withdrew 100 SAI plus interest at block B, that would be easy enough to track with the tx data. But many users executed multiple supplies, withdraws, borrows, and repays of differing amounts that would basically require tracking every address’s “state” at each block to achieve a duration-based weight factor. That would be a much heavier data forensics effort. The current approach is perhaps overly generous to folks who dipped their toes in for just a few blocks and quickly withdrew in the early days; but I’m okay with that, because the risk at the beginning was arguably concentrated in the choice to supply (and how much to supply) rather than in how long the user kept their assets in the protocol.\n(2) Yes, that’s correct – although as I was implementing your TVL suggestion, I found a mistake in the previous implementation which basically reversed the effect of the time multiplier, rewarding later users instead of early ones (!) This is now fixed in the repo and reflected in the updated list 16, which also implements your TVL suggestion.\n(3) Yes, interactions with V1 and V2 are treated in the same manner with respect to the weights for the capital-weighted distribution.\\nThanks for your answer\nNow that it’s clearer I can say\n(1)The problem in your method if I get it well is that if someone supplied 1 millions at launch and withdrew it instantly after a minut it would grant him exactly the same compared to if he locked this money for 2 years in the protocol ?\nI would argue this is not really fair as, take someone who supplied from 2019, for 2 years, consider he would be treated for the specifically capital weighted the same as someone who deposit for a year then left the protocol for an other, or his bank, after 1 week.\nIf talking about the risks, I would disagree with your assumption, the risks of oracle hack, liquidation failure, (which we saw in defi are the biggest) etc are proportional to the time you lent.\nDo you think there there is an easier way to obtain the total interest earnt+paid by an adress, that would make it easier and would match current comp distribution.\n(2) My tvl suggestion works only if we proceed block per block, as we cannot apply a x86 boost for someone who lent (and maybe withdrew btw) for the whole period, this boost would apply only for the first week on his deposit (as if we tried to retroactively distribute COMP as if they started to be distributed since day one)\n(3)\nDo you think the huge difference between alive number of adress and yours is due to the starting date ? (you picked 15 June and he choose 8 June)\\nRegarding (1), I don’t disagree; however, I think the practical difference between your\n“integral over the eligibility window of capital supplied+borrowed times TVL weight factor, int[C(t)w(t),dt]”\nand my\n“sum of capital supplies/borrows times TVL weight factor at time of the interaction, sum_i[C(t_i]]*w(t_i)”\nwill be negligible for all but a handful of addresses that supplied and borrowed multi-million USD worth of assets. Both formulas will deliver between 20 and ~21 COMP per user unless the user is in the multimillionaire early user club. But I can’t prove this hunch without actually implementing it, which is a fair amount of extra work.\nFor the same reasons, I follow your argument (2) but also believe that the impact of replacing the sum involving w(t_i) with the full integral involving w(t) will be minor except at the very, very top of the list.\nFor (3), the June 15 date is only used as the date at which EndPrice for each underlying token was determined. The EarlyUserCutoffBlock everywhere else (specifically the address list generated by CompoundEarlyUsers.py and the distribution list generated by EarlyUserProposal.weighted.py) is block 10228172 1, the last block produced on June 8, 2020.\nI can’t speak for @alive but would guess that the 5,000 user estimate was either a back-of-the-envelope calculation (and honestly not a bad one either, off by less than a full order of magnitude) or based on higher thresholds for what’s considered a small-value (dust) transaction and therefore not counted as a bona fide user.\nEdited: previously I stated that my formula exclusively used the TVL at time of first interaction, w(t_0), but actually I am using w(t_i), that is, each transaction’s capital is weighted by the TVL factor at the time of the transaction.\\nOkay, I think it’s time for a poll!\nSummary of recent developments for folks just joining or returning to this discussion:\n\nThere appears to be consensus in this forum toward some kind of airdrop to early users with vesting.\nMany good ideas about the distribution model have been shared, but the conversation stalled due to the absence of a curated early user list with sufficient data to inform a capital-weighted distribution.\nI was inspired by @grasponcrypto’s efforts with Dune Analytics data to try my hand at curating a list of early-user interactions by directly querying on-chain data.\nThe community has helped refine this list, e.g. @grasponcrypto and @rleshner provided the guidance I needed to isolate and remove addresses that participated in an early Sybil attack on governance.\n\n\n\n\n allthecolors:\n\n   Here is the list    \n\n\n\nThe code to reproduce this list is available on Github 27\n\nFinally, a written summary 25 is now available that explains how the proposed distribution is computed.\n\nI am hearing two main objections to this proposal as posted:\n\nObjection 1: the calculation of capital input should be an integral over C(t)m(t), rather than a sum over supply/borrow events of C(t_i)m(t_i), where C is the capital borrowed supplied and m is a multiplier that favors interactions that occurred earlier in the eligibility window (when Compound had a smaller TVL).\nObjection 2: the 95% weight to the social distribution is too high.\n\nThe first objection is addressable but would require considerable additional effort to implement. Given the level of effort required, I would seek support from the Compound grants program to do this.\nThe second objection is easily addressable; I just happen to disagree with it. As evidence, here are plots of the distributions for the 95%-5% case and the 50%-50% case.\n\n95-5 distribution800×400 17.9 KB\n\n\n50-50 distribution800×400 18.1 KB\n\nThese distributions look almost identical, except the 100 or so addresses that could afford to plow multimillions (or more) USD equivalent into the protocol receive ~10x the rewards in the 50%-50% case. All that extra COMP comes from smaller users, who each end up with only half of the 20 COMP available in the 95%-5% case. I don’t see how that helps achieve the goal of promoting participation in governance by early users.\nHowever, we should hear from more voices than @Andre1 and myself to understand where a consensus lies, if any.\nWhat do you think?\nDo you support the current proposed early user distribution list?Yes, I support the currently posted list as-is, not interested in seeing how the more accurate block-by-block capital weights would change thingsYes, but I support @allthecolors applying for a Compound grant to implement the more accurate block-by-block capital weights and produce a report for the communityNo, but I support @allthecolors applying for a Compound grant to implement the more accurate block-by-block capital weights and produce a report for the communityNo, I don’t support the currently posted list, and the block-by-block capital weights wouldn’t change my mind30voters30total votes\n                    \n                    Closed Apr '21\n                   \\nAbout the \" * Objection 1: the calculation of capital input should be an integral over C(t)m(t), rather than a sum over supply/borrow events of C(t_i)m(t_i), where C is the capital borrowed supplied and m is a multiplier that favors interactions that occurred earlier in the eligibility window (when Compound had a smaller TVL).\"\nAre you making a sum for one given adress of all the capital that was supplied even in case it was repaid, resupplied, repaid (the same money) ? meaning the whale who withdrew and supplied again next minut the same amount has his supply amount counting twice, three times, or ten times in your calculations for the capital weighted reward ? If yes it’s yet an other flaw of this approach.\nIf I get it right, let’s say Franklin : he deposited 100kusd on compound in 2018 which he locked for years.\nNow you look at Anton : he would deposit 100kusd on compound too in 2019, but then he would play with aave and a dozen of other protocol back and forth, in a way he effectively played with only 100kusd, but deposited and withdrew it often from compound, with effectively dedicating its funds on compound a much smaller time than Anton. Assuming Anton would have done it a hundred times.\nWe end up with Anton received 100x the allocation that Franklin would get ? Despite Franklin was locking his funds to compound (the same amount) for a far larger time ?\nAlso I can tell you that the top 100-300 people in your ranking are far from millioneers, we are talking here about some who dedicated a dozen k\nI would say there could be intermediate solution like 80-20, but even if keeping 95-5 most of all having that capital weighted allocation in opposition of the socialized, really capital weighted and less arbitrary as possible matching successful distribution as UNI\n.I hope a compound grant can already be obtained for all the work you are providing, anyways I will support the proposition choosed by community and which is feasable, it seems people are voting for the current distribution\nquestion : Would there be any easy way to extract that number per adress : Notional value of total interest paid + received ? Would then be a much fairer approach if that’s an easy path, but I don’t know if that number can be obtained\\nWhen the COMP Distribution began, it was expected that Governance would alter, improve, and rethink the program–its fantastic to see the community organizing, and doing the research to distribute COMP to early users.\nThere are a few questions to answer, and build consensus around:\n\nWhat is the purpose of distributing COMP to early users? Why is this a better use of COMP, than distributing it to current users, or to protocol improvements through development grants?\nHow many COMP should be distributed to early users?\nWhat should the vesting schedule, mechanics, and implementation be?\nWhich addresses should be eligible, and how should the quantity of COMP per user be calculated?\n\nA lot of recent discussion has gone in to question 4, and I wanted to document my own views, and some of the historical issues surrounding this question.\nThe addresses that receive COMP should be early users that risked their time and capital to establish the protocol. This means excluding addresses that were deliberately created to manipulate the protocol; one example of this is the first community vote, which was “sybil” attacked with hundreds of addresses supplying ~0.01 Ether each.\nSecond, early should be defined as anybody that tested the protocol before the COMP token & distribution was announced 33 on 2/26/20.\nThird, users should be measured by usage of the protocol. For an interest rate market, usage is a function of capital over time; how other protocols have distributed tokens to users is irrelevant to Compound. This approach was piloted during the second community vote 13, and formed the basis of the COMP Distribution when it began for users. Luckily, in Compound’s case, this can be easily measured by interest earned, and interest paid.\nLastly, several members of the community feel that the distribution should exclude or limit “whales” in some way, even though these addresses took the most risk, and contributed the most value to the protocol. This is a political question – but could be implemented in a simple way, by allocating through the square root (or another power) of interest earned+paid, which would partially “flatten” the wealth effect.\nI’m excited about the effort occurring here – and wanted to plug the Compound Grants 28 program as a tool to help organize the research, smart contract development, auditing, and proposal work that this distribution will require.\\nAgree with Robert on all points here. To penalize large users for supplying big capital to a risky protocol (at the time) seems not just unfair, but borderline malicious. If the protocol had been compromised and they lost everything, would the community coalesce and chip in to make them whole? Successful risk taking should be rewarded, not punished.\\nHow about smart contract wallets like Argent and Dharma? I prefer to use Argent to interact with Compound. However I did not see my address in the list.\\n@Compactivist see post 214 Should Compound Retroactively Airdrop Tokens to Early Users? - #214 by allthecolors\\nI am not sure if the definition of early meaning before the token release in Feb 20 is the best fit. I’d recommend setting the cut off date somewhere when DeFi started gaining traction. I think the COMP distribution update was more meaningul in this regard than the original announcement COMP Governance & Distribution Update | by Robert Leshner | Compound | Medium 18\\nMy first interaction with the protocol was on March 28th almost a month after the announcement and I wasn’t even aware of a COMP gov token.\nMe and many early users interacted with the platform for the first time out of curiosity.\nI discovered compound finance because of the integration with the Coinbase wallet.\nSo I made my first deposit with BAT then DAI then a monthly deposit and so on.\nDe-Fi wasn’t a thing until the summer, and I believe most early users weren’t aware of the future value of a government token.\nI think @alive suggested date is good enough as an starting point and in the case of the Sybil attack, we could filter the wallets that interacted less than 2 times with the protocol as @blck suggested.\nPersonally, I believe this airdrop could help Compound in reach a better decentralization, as long as it doesn’t discriminate the early users who genuinely believe in the protocol.\\n\nIf the protocol had been compromised and they lost everything, would the community coalesce and chip in to make them whole?\n\nAlready happened.\n\n  \n      \n      compound.finance\n  \n  \n    \n\nCompound | Proposal Detail #32 23\n\nDistribute COMP to Affected Users in the DAI Liquidations\n\n\n  \n  \n    \n    \n  \n  \n\n\nThe rejection of this proposal is one of the reasons of why a Pro-rate airdrop will not work for decentralization.\nIt’s either a fixed amount for everyone as the social distribution of @allthecolors suggests or a minimum amount as other protocols did.\nMaking this process more complex it will take longer and longer to achieve, it’s been months since someone decided to take real action and started to share relevant data to finish this proposal.\\n\n\n\n rleshner:\n\nWhat is the purpose of distributing COMP to early users? Why is this a better use of COMP, than distributing it to current users, or to protocol improvements through development grants?\n\n\nI think the purpose of COMP distribution to early users should be to increase the degree of decentralization, because it is a COMP governance token (at least that’s what we call it). Early users risked their capital and interacted with the protocol during the bear market without the possibility of liquidity mining. On the other hand, most existing users are attracted to the COMP distribution, so the assumption is that they already have enough governance tokens (if they have sold them then their intention is clear). My opinion is that snapshot need to be taken on token launch (not on announcement).\n\n\n\n rleshner:\n\n\nHow many COMP should be distributed to early users?\nWhat should the vesting schedule, mechanics, and implementation be?\nWhich addresses should be eligible, and how should the quantity of COMP per user be calculated?\n\n\n\nThe only fair distribution of COMP tokens is completely social (100%) so all eligible addreses get the same amount of COMP because it is a governance token after all. If we include capital distribution factor then the act itself has no purpose because the structure of the holders will remain the same (centralized).\nMeasured by usage is possible only before start of liquidity mining because last months only “whales” can afford to pay gas. Except that, usage metric is super relevant and it is a good idea.\nVesting period is necessary due to the possible DUMP on the market, + distributed and vested COMPs with some collateral feature (for accounts and platform stability, not for leverage).\n\n\n\n rleshner:\n\nI’m excited about the effort occurring here – and wanted to plug the Compound Grants  program as a tool to help organize the research, smart contract development, auditing, and proposal work that this distribution will require\n\n\nagree, process will be faster that way\\n\n\n\n borovan:\n\nIt’s fairly obvious how I’d like to see this resolved. I’m not the only person from this time with a bitter taste in my mouth about how Compound handled this.\n\n\nTime to reward those who deserve to be rewarded!\nThank you for your contribution Borovan\\n\n\n\n dabar90:\n\nI think the purpose of COMP distribution to early users should be to increase the degree of decentralization, because it is a COMP governance token (at least that’s what we call it).\n\n\nAnd/Or a “thank you” gesture to all those who took part in the early days of the protocol, borovan shared his experience and feedback very frankly and if others feel the same it might be a good way to make amends. But other early large contributors may feel otherwise.\\n\n\n\n idgm:\n\nThe rejection of this proposal is one of the reasons of why a Pro-rate airdrop will not work for decentralization\n\n\nI agree with you that Pro-rate airdrop doesnt change anything.\n\n\n\n YouCompDoIt:\n\nBut other early large contributors may feel otherwise\n\n\nYou mean early contributors or early investors? Early contributors were paid for that contribution and early investors obviously got their COMP share in which they invested. If I’m wrong, so the contributors worked for charity and the investors donated their funds to the protocol then the problem is in Etherscan data.\nThe purpose of COMP airdrop distribution should be protocol governance decentralization (in some degree) and treat each user equally regardless of the amount of capital he owns.\nThe fact that certain persons / funds treat the “DeFi” protocol as shares of the company creates a problem because “Defi” and “user governance” memes obviously serve as good marketing.\nBorovan’s situation maybe is unfair, but I don’t believe he didn’t make money from the aforementioned liquidations.\\nIt is clearly.\nwe hope see the proposal soon\\n\n\n\n dabar90:\n\nBorovan’s situation maybe is unfair, but I don’t believe he didn’t make money from the aforementioned liquidations.\n\n\nYou are totally right, it’s about Fairness. This means something totally different from one person to another.\nMaybe his expectations were too high on “how compound would retroactively reward him if ever super successful”.\nSomething to put more thought into maybe: milestones programmatically enforced for community contributors (i.e. community does xyz and if the project reaches xyz target then the relevant members get an airdrop). Never really heard of this but it may help getting this early community members/contributors more excited, more fairly rewarded and remove implicit hopeful expectations.\\nfwiw, UMA offers a solution that achieves exactly what @YouCompDolt is describing (so-called KPI options 7) with the “xyz target” being the total value locked in the protocol. It’s an interesting idea; at the same time, I see a lot of interest in this thread in keeping the early user distribution as simple and easy to understand as possible.\\nLooks like Sushiswap is now following UMA’s KPI Option model:\n\n  \n      twitter.com\n  \n  \n    \n\nUMA (UMAprotocol) 9\n\n The #SuperUmans have drafted a proposal for the @SushiSwap community to launch KPI Options to increase Kashi Protocol adoption for lending and borrowing.\n\nKPI Options can be designed to encourage certain outcomes and only pay out if they are met.\n\nhttps://t.co/8Sm9gJic9T\n\n\n  12:10 AM - 27 Apr 2021\n    \n      \n        \n       34\n    \n    \n      \n        \n       12\n    \n\n\n  \n  \n    \n    \n  \n  \n\n\nThis is not about retroactive changes so we are getting off topic. But seems like a logical trend we might want to follow in our next strategy moves.\\nIf the set kpi parameters are quantitative then we know what to expect - whale manipulation. I don’t know what qualitative indicators would be possible …\nIf the goal is to achieve greater protocol decentralization, then “KPI” is not exactly the best solution.\nThe UMA has targeted governance participation on its own and several other protocols. In the case of Compound, this is pointless because governance comes down to a few large accounts, and I don’t really believe it’s fair to any Compound user to reawrd accounts that didn’t use the protocol.\nI will not comment.SushiSwap because they are not relevant.\n@allthecolors I think the solution you offered (with 95% social weighted distribution) contributes the most to achieving the goal of airdrop. However, I assume that the goal is to decentralize governance and encourage early users to become more involved in working on protocol.\\nItamar from Argent here.\n@allthecolors, it seems you’re excluding all smart contract interactions including smart wallet users: Argent, Gnosis, etc…\nAny rational for that? We could easily provide a list of Argent contract factories, or identify Argent wallets within your initial early users list if that helps.\nWe integrated Compound natively in Argent extremely early, in summer 2019, our users interacted with Compound before there was even 100m$ locked into the protocol so it wouldn’t be fair to exclude those and it would be a pretty quick fix to include them, we’re happy to help.\nItamar\\n@Itamar Yes, I provided a rationale in this post sharing the proposal list:\n\n\n\n allthecolors:\n\nsee post 214 Should Compound Retroactively Airdrop Tokens to Early Users? - #214 by allthecolors\n\n\nPlease see the full post, but I’ll highlight this concluding section:\n\n\n\n allthecolors:\n\nApplication integration projects are welcome to develop separate proposal(s) for their users who interacted with Compound. Most of those projects have the skills in-house to do this kind of analysis much more efficiently than me, and they’ve had a few months to take the initiative. I am just a user/fan of the protocol, not representing any company in the space, so I choose to focus on early users who interacted directly with the protocol.\n\n\nIf integration teams want to advocate for their users to be included in this specific proposal, the way to make it practical for me to implement is make a pull request to the Github repo 11 with a script that allows anyone to extract – exclusively from on-chain data – a full history (through block 10228172) of supply/withdraw/borrow/repay interactions with Compound for every EOA associated with your users, in the format shown in this csv file 18\nIt’s my goal not to press too heavily on the scales going forward, but I will reiterate my point in post #214 that projects whose users interacted with Compound in ways more complex than a simple proxy may not be able to provide a mapping like the one I described above, which is why I felt it is politically cleaner to separate contract-mediated and direct interactions into separate proposals.\\nThanks for bringing this up, Itamar.\nSame issue applies for DeFi Saver users who have been interacting with Compound via dsproxies (or Smart Wallets as we call them in the UI).\nCompound was initially integrated in DeFi Saver in June 2019 and we’d hate to see a great number of early Compound adopters disregard in this initiative.\nJust like Itamar mentioned for Argent, we too could (very quickly, in a matter of days) provide a specific list of dsproxies that interacted with Compound, in line with the eligibility clauses outlined in this summary 9.\nThe argument shared by @allthecolors was:\n\nNow that I’ve attempted to account for proxy interactions with Compound through on-chain data myself (thanks to @grasponcrypto for running part of the analysis!), I’ve come to a different conclusion about the Uniswap team’s motivations for excluding contract interactions: it’s simply way more labor-intensive to unwind these proxy interactions and find the initiating user on-chain 3\n\nWhich I would hope will not be the ending argument of this discussion.\nReaching out to teams like Argent, Dharma, InstaDapp and DeFi Saver would have been extremely simple and can still be done with minimum additional time needed.\\nDont mix too platform\nBe simple, the early users of compound platform proposal first, and other platform on other proposal\\nI would strongly argue that which frontend was used makese no difference as long as it was very clear to users that they were using Compound - which definitely was the case in all the apps I’ve mentioned.\nOne of the main points of DeFi is decentralization, it is in the name after all. Which frontend was used by any users should make zero difference.\\nI’m proud member of DeFi ecosystem and I was using Compound with my Argent Wallet before it was cool \nIf the aim is to democratize the governance and increase participation with early users, it doesn’t matter which app they have used to test the protocol. This is against composability of the DeFi.\nEarly user should be defined as; a wallet which have interacted with Compound regardless of the way before a defined deadline.\nDont pursuit on filling your bag quickly, let us pursuit more democratic solution possible.\\nThe poll is now closed and we see a quasi unanimity in favour of an allocation for early users.\nTo answer the post of rleshner I think the first question on the reason why obtaining an allocation for early users was widely discussed before we came into the technical aspects with the amazing work of allthecolors, who permitted to unblock the situation.\n2. How many COMP should be distributed to early users?\nIt would be interesting to consider some elements as :“as many COMP as if the distribution of COMP token would have started from the day 1 of the protocol”, I think that then the number of 500k comp would seem very reasonable and little compared to the result obtained like it.\n3. What should the vesting schedule, mechanics, and implementation be?\n4. Which addresses should be eligible, and how should the quantity of COMP per user be calculated?\nI find the approach of both a socialized and capital weighted reward an interesting on, as long as they are balanced, I also agree it would be  certainly better to choose as a cut off date the date when COMP was announced, (as other protocols did), for the reason mentionned, but it is debatable (avoid adresses who came in only once comp was advertised and reward only early user here before incentives were announced)\nAbout people mentionning proxy protocols. The social weighted reward will not be granted in the way  allthecolors  programmed the distribution but I want to note that these users will be eligible (at the discretion of the protocol of course) to the capital weighted reward. the proxy protocols will be able to get the capital weighted reward and distribute it to their users the way they wishes it as the proxy adress will aggregate the capital weighted reward if I am not mistaking\\nNew user here, full disclosure I’m on the preliminary list and stand to benefit if this were to pass.\nAfter reading all the discussion I’m in favour of trying to reach as many addresses as possible (minus manipulators). ETH community is built on decentralisation, this would help with awareness of the project and keeping users engaged. I actively follow all tokens with actual governance votes.\nI tested compound but as a small user it did not make sense to leave my funds on the platform for a long time. But it was my first step in trusting code in Defi smart contracts, admin key risk and the devs, I think it’s great that we reward this in the ETH community. Really should be verify not trust, maybe one day I will learn to read code.\nI also received UNI airdrop which I’m hodling, the only airdrop I have dumped some of so far is 1inch, I didn’t like how they have partnered with Binanace and how all their marketing changed. Investing with a long time horizon, I’m sure there will be others who see the long term vision instead of dumping.\nSupport a long vested time for the economic reward, but the ability to vote and make a difference I would be excited for short term. Also making the rewards expire if not claimed seems like a good decision.\nThere was lots of complaints with the UNI retroactive airdrop and wallets like Argent etc I think this needs to be addressed clearly in any announcement and vote if this moves forward.\nBest of luck to the Compound community and dev’s on this big decision and with the platform moving forward!\nEdit: One other thing I keep raising in all the projects is the ability to ping users with notifications that a vote is taking place, I think the whole ecosystem could move forward faster if we got more engagement. I think these guys might be working on it but I can’t see why there can’t be more solutions for this or if there is more awareness of them. The more active voters the more value I see in these governance tokens.\nhttps://twitter.com/epnsproject 11\\nAs a new comer investing in compound with a sub-midget sized investment, I would hate to see dilution. If you create dilution how will you recover it?\nVesting periods are good but are you really going to make people wait 4 years before they get voting rights if that is what you intend.\n(just thinking out loud) If the issue is voting rights and creating proposals I would wonder if creating some static pools of 100 comp each that could be used to sponsor proposals.\\n\n\n\n allthecolors:\n\nI choose to focus on early users who interacted directly with the protocol.\n\n\nit’s reasonable totally.\\nI wanted to lend my voice to the support of this proposal. I had invested quite heavily into both borrowing and lending on Compound from mid-2019 to about April 2020, at which point I had to pull out all my funds to deal with personal fallout from the global pandemic. While I have the most respect for the Compound development team and their eagerness to help on their Discord, ultimately like in all things crypto, not having a real say on matters doesn’t give one the warm and fuzzy “decentralized” feeling. I am back in the system now but it will take me quite some time to accumulate a usable amount of COMP. A retroactive distribution would be a very welcome development by this user, and would be in line with the retroactive distributions conducted by others such as 1inch\nAs for how to accomplish the distribution, I support @allthecolors proposal of a roughly even 20 COMP for the majority of early users to cover the largest amount of early users with the least amount of overall inflation of the original pool.\\nMy first interaction with compound was v1 of the protocol (feb.2019). I did this while trying out the protocol and with the believe Ethereum with be a good safe haven. So I supplied Ethereum but I never though it will give us any token. Stronger: the narratives at that time was: make a protocol without a token, tokens are not doing any good. We where in the middle of bear market and we all lost.\nI than borrowed DAI and I found it extremely simple to do this. I promoted the protocol among friends and many other people. Supply Ethereum and borrow usd.\nI than used out the protocol and followed other crypto project. But than suddenly last year the governance token COMP was announced. There was not even a chance to opt in, it was all ment for the big suppliers. I think that’s a mistake. We should have a guardians launch or and airdrop that rewarded all those first users.\nI, and many others, where here in the deep red bear market and we I supported and used the protocol heavily. I, and many others, would like to discuss and think about governance of Compound. Don’t airdrop small, airdrop at least 100 comp (vested) to all users, small, big, supplier, borrower, and filter out bots.\\nValuable members are scarce in the crypto community. The early users of Compound were pioneers in helping to support a platform without any belief they would be rewarded monetarily. It would only make sense to reward them with 100 Compound tokens which is life changing for many of them and would highly incentivize them to become long term committed active members.\nEarly Compound users have a different mentality. The tokens rewarded would create another batch of highly loyal and enthusiastic supporters that never had a monetary desire in using Compound. This will solidify Compound as it continues to grow and help strengthen the governance.\nThe real discussion is not whether early users should be reward, but whether Compound wants those early users to be loyal and committed to Compound. The community has the ability to do this, but only if they reward them truly and not with just small reward, but one that makes a splash in the crypto space.\\nCould the COMP be distributed by using a placeholder token? The placeholder token would give users the ability to “claim” their COMP for being an early user. The UI could have a button on the upper-right hand side, close to where the user’s address and their COMP balance is displayed. This method of distribution would decrease the chances of the COMP being ‘burned’ because of distributing COMP to addresses which are no longer in use or inaccessible.\nAlso, why does the distribution need to be an even amount of COMP? Why not divvy out what COMP is available then dividing it by the number of early users?\nGive users 4 years to claim their COMP. The leftover balance, well, we have 4 years to determine what happens to it.\\nThanks @allthecolors, hugely appreciate all the effort here, we’ll make a pull request to the repo, it should be pretty straightforward\\nI struggle to understand why a user/address that interacted with Argent, for example, should be included in a COMP retro-active airdrop. If a user wanted to support the COMP protocol, it should have been a direct use case and not via a proxy front end.\nIf the shoe was on the other foot e.g. if Argent was to have an airdrop for early users, would they reciprocate in kind and airdrop tokens to COMP addresses?\\nWhats wrong with that tho. they contributed as much as we did going directly to compound. What other way do you know to where a user can simply download an app and add funds easily to compound with a few taps. they are just as important.\\nThe other way to do it is to interact directly with the COMP app\\nnot as easy as argent on a mobile phone. on desktop yea compound app for sure.\\nI agree it’s not as easy as Argent on a mobile. The users of Argent benefitted from the convenience provided, and the Argent platform benefited by drawing users to their platform.\nAs a side note, I sometimes think that it would be easier/simpler if the treasury for the airdrop was apportioned to the early unique addresses so that each would receive 100 COMP each so at least these addresses could make proposal outright i.e. a simple cut-off of early addresses each receiving 100 COMP voting rights at least\\n100 to the direct users would be good. I think it was changed due to the amount of addresses found (30k vs 5k mentioned in the first post). correct me if im wrong.\\nYep, you’re correct on the 30k v 5k. So, if there’s 500k COMP available for the airdrop, for example, may as well apportion it to the first 5K unique addresses that interacted with the protocol. Not perfect, but at least they’d have 100 COMP to make a proposal.\nTo me, the 20 COMP distribution doesn’t really achieve the aim of engagement - if I was to receive 20 COMP, I’d just sell it because the utility, in the most part, is lost\\n\n\n\n Mr_Drongo:\n\nIf a user wanted to support the COMP protocol, it should have been a direct use case and not via a proxy front end.\n\n\nits very reasonable… it should use to compound directly and not via other platform.\\n\n\n\n Andre1:\n\nI find the approach of both a socialized and capital weighted reward an interesting on\n\n\nI agree with you, obviously no one has anything against retroactive distribution (airdrop). I think a good question is in what social: capital weighted ratio would the potential distribution be?\nMy view is that retroactive distribution should be more user-oriented than capital-intensive. A large share of capital weighted distribution potentially leads to even greater governance cartelization.\n\n\n\n Mr_Drongo:\n\nI struggle to understand why a user/address that interacted with Argent, for example, should be included in a COMP retro-active airdrop. If a user wanted to support the COMP protocol, it should have been a direct use case and not via a proxy front end.\n\n\nI think it doesn’t matter with what type of wallet the users interacted to protocol. I’ve never used SC wallets so I don’t know if there is any big difference but EVM also treats external and contract accounts? If the distribution is social in any proportion, exclude the SC wallet is nonsense.\\n@allthecolors we made a pull request to update the DetectContracts.py script\nsee: Include Argent wallets 26\nWe didn’t resubmit all the CSV files as we assumed you’d run all the scripts once again but we can do it if needed. Let me know if you need anything else.\\n@Mr_Drongo you don’t interact via a Proxy in Argent, using Compound within the Argent app or using wallet connect in Argent would be exactly the same. So I don’t see any rationale to exclude them.\nThis would be like excluding Windows users, or Firefox users. Users were interacting with Compound directly, with Compound branding all over the place and getting ctoken in their wallet, no intermediary.\nI think a lot of people seem confused about what Compound is. Compound is a decentralised protocol, not a centralised front-end.\\nExcluding smart contracts is a dangerous approach that disenfranchises most users, and I will actively fight against it. Any address (msg.sender) that supplied assets or borrowed from the protocol is a user.\nFrom day 0, Compound was designed as a protocol for developers to build interfaces and applications on top of. The interface 1 built by Compound Labs was one of many ways to access Compound.\nTons of different users & wallets use Compound with a smart contract (Gnosis multi-sigs, InstaDapp, Argent, Dharma, etc). Rather than excluding smart contract users, and forcing applications to “whitelist” themselves, all addresses (msg.sender) should be included; excluding them is an arbitrary, political, and incorrect decision.\\n@rleshner I understand and support the philosophy behind an approach that’s agnostic to how the user interacted with the protocol.\nMy approach to identifying early users picks up EOAs and contract addresses from the account (V1) and minter/redeemer/borrower/payer (V2) arguments from the event associated with the transaction. So while I agree with you, @Itamar, and others that there ought not to be a difference in principle, in practice there simply is a difference in the event data that requires contracts be handled differently, lest COMP be allocated to contracts from which, in some cases (though not Argent, for example), it may not be retrievable.\nI’ve gone ahead and added my prior effort at unwinding contract interactions to the Github repo. FindContractCallers.py 19 examines early interactions with the protocol by smart contracts and recursively replaces the contract’s address with the from field of the transaction receipt. If there is a better way to get the originating msg.sender for contract-based early interactions with the protocol, I would appreciate a link or resource suggestions for how to do this.\nOne question I have for @Itamar (thank you for the pull request, it’s merged!): is there a general solution for the isArgentWallet() function that would work for any team’s smart wallets? This is the piece I was missing to handle the case where the contract is the user’s wallet, and I don’t know how to generalize it without input from every affected project and contract – which is functionally equivalent to “whitelisting” projects in my view. Perhaps I’m missing something, but I don’t see how it can be avoided unless all contracts are treated as users, which will result in some of the COMP being allocated to contracts that can’t do anything with it, and those contracts’ users being left out.\nI will reiterate my concern that the recursive address solution can only properly account for contracts that function as one-to-one proxies (or are their users, as in Argent’s case). If a project does any meta-commingling of user funds, for example by issuing their own IOU tokens and pooling users’ collateral, then it seems to me like a truly agnostic approach relies on trusting those third parties to distribute their allocations properly. Is this not the concern that I think it is?\nIt would be harsh but fair to characterize my rationale for leaving smart contract interactions off prior draft lists as ignorance, incompetence, or laziness. Calling the decision arbitrary and political, though, that really stung. I hope it’s clear now that it wasn’t arbitrary, though it may feel that way at a high level.\\nI appreciate the efforts to including Argent addresses in the early users list (is there an updated full list @allthecolors?), and I fully support your arguments for limiting support to one-to-one proxies (where the only difference in interaction is truly through the method of linking a wallet (e.g. using the wallet interface vs compound interface) when further transactions are involved I support the opportunity for an individual allocation being assigned to these proxy addresses to be distributed by the projects themselves as they see fit.\nAn important point is regarding the intent of the airdrop, and depending on the response I see different models as most relevant:\n\nThe intent is to improve decentralization and allow early users to submit proposals → result is cut-off number of accounts to ensure every address can receive 100 COMP (no need for social weighting etc)\nThe intent is to reward early users → result is cut-off accounts by specific date and split rewards across that list of addresses whether that’s a minimum of 20 or 18.57373 doesn’t matter, all that matters is that a clear cut-off date is aligned on.\n\nJust my 2 cents, I fully support an airdrop, but mixing the two intents doesn’t work, there is a need to clearly pick a direction.\\nUnfortunately it’s not a generic solution for all smart wallets, it’s custom to Argent, I don’t think there is a generic way to differentiate a smart wallet from a smart contract.\nMaybe we could accept all addresses but adapt the formula so that someone depositing 1bn$ doesn’t get much more than someone depositing 1m$ or 100k$, so some sort of cap on the allocation or limit the impact of deposit size.\\nOK - I see what your saying (and others) and agree that’s there no reason to exclude them.\\nTo back track a little, surely those addresses that interacted natively with the Comp protocol, no matter what wallet was used e.g Argent, is already listed?\\nendless topic\nThere s a time for everything\nconduct early airdop via official funtion user first and then open a other proposal on other proxy airdrop\\nI agree on this. Seems like we will be on this forever. Whats wrong with turning this into a 2 part proposal. Reward the direct users since it was the easiest to find. Then coordinate with all the proxy wallets so they can make sure all of their users get included as well.\\nIt seems easier to simply not censor any user.\n@allthecolors query already includes every DIRECT interaction with Compound. Argent, Gnosis and probably many other users are all included. Just don’t censor smart contract interactions and it will be fair.\nBut arbitrarily deciding to censor some users doesn’t seem to make much sense.\nI’d suggest caping the reward per address to avoid having 1 contract collecting most of the reward.\\nThese discussions going nowhere, there should be concrete decisions to be made. I strongly support for   airdrop to early users which used the protocol directly.\nAfter the airdrop to direct protocol users are completed, There could be proposal for other proxies(argent, etc).\\nHang tight everyone, I know this feels circular but we really are making progress. @Itamar is correct that the early user interactions script already has the capability to include all smart-contract interactions. I filtered out contracts in my initial proposal because I’ve seen so many situations wherein tokens get sent to contracts from which they cannot be easily retrieved (including on Compound, where depending on what went wrong, the funds are either irretrievable or have to go through a somewhat onerous governance process to be retrieved). To me, this possibility seemed like a worse outcome than tallying EOAs (simple addresses) first and worrying about contracts later. Imagine the negative impact it would have on a team that integrated Compound early if it can’t distribute early user COMP to its own users. Again, maybe this isn’t the problem I think it is; if you represent a team that integrated Compound early and this would describe the situation if COMP were to be airdropped to your contract, please speak up! \nClearly we don’t have a broad consensus on this question. I think we need more information: specifically, let’s go ahead and see what a distribution would look like if we include all EOAs and contracts. That will help us better understand the extent to which contract-mediated interactions delivered early user capital to the protocol. If the largest-capital-weight contracts can be associated with the major integrations, as we anticipate they will be, it stands to reason that it is in those integrations’ best interest as a business (or DAO, etc) to forward distributions proportionally to their users’ interactions with the protocol.\nThis approach will break any socially-weighted component of a distribution, in the sense that a single contract that served 1000 users would get the same weight as any direct user. Maybe that knowledge can help reassure folks who are concerned about bringing contracts into the picture.\nI’m happy to do this analysis, but one temporary hitch is that I don’t have a local node to run the analysis on. @grasponcrypto helped me out on the first analysis by running the scripts on a local node. I have a pending application with the grants program that includes a request for a Quiknode or Infura subscription to make the necessary calls. If someone else with a local node want to keep things moving and is willing to piece together the steps to generate a distribution list from the scripts I posted, you can download them from Github and run them after removing the step that removes contract.interactions (i.e. skip DetectContracts.py and IsolateDirectUsers.sh) The repo needs some more automation/cleanup, which will certainly be part of the effort if the grant is supported.\\nI get FileNotFoundError: [Errno 2] No such file or directory: ‘CompoundV1.abi.json’ when.I try to run it. I dont see ‘CompoundV1.abi.json’ at all. All other json files are there. Thoughts?\\nWeird, sorry about that – it should be there now, thanks!\\nThank you @allthecolors . I have my node (geth) in a ubuntu server 20.04 (192.168.0.55). I can access it using local macbook pro (192.168.0.15). If you wouldnt mind me asking, thoughts on what my local ethereum IPC would be? Also, my ubuntu machine has username password, I assume I will have to pass it somehow when running the py file (from mac command line)? Sorry if these are silly questions. Trying to learn while I try to help as well : /\\nSure, I’ll share what I think should work and refer you here 14  for more details.\nFor a local node you should just need to change the line\nw3 = web3.Web3(web3.Web3.HTTPProvider('https://mainnet.infura.io/v3/YOUR-PROJECT-ID'))\nusing IPCProvider instead of HTTPProvider. Instead of a URL, you’ll just give the full path to the geth.ipc for your node.\\nI’m a bit confused on the actual announcement date of COMP. Was it Feb 26 when the medium article was published or Feb 20 as written in this python script from @allthecolors?\n\n  \n      github.com\n  \n  \n    0xA1176ec01045/CompEarlyUserAnalysis/blob/55ebb3c5a33a74e150b84b814f1229e05b6fc9cf/EarlyUserProposal.weighted.py#L57 35\n\n    FilterTxData = FilterTxData[FilterTxData['amount'] >= dust[token]]\n    nondustDataByToken.append(FilterTxData)\naddressData = pd.concat(nondustDataByToken)\n#print(\"after removing dust-scale transactions:\")\n#print(addressData)\n\n\nCompV1deployBlock      = 6400278\n# If we use 1 week before COMP distribution (June 8, 2020):\n#EarlyUserCutoffBlock   = 10228172\n# If we use the date of the COMP token announcement (Feb 20, 2020):\nEarlyUserCutoffBlock = 9516777\n\n\n# ...sort address data by address and then by block\naddressData = addressData.sort_values(['address','block'])\n\n\ncap_weights = { 'ZRX' : 0.506366856012202,\n               'BAT' : 0.19272837890186123,\n               'REP' : 14.271295633796559,\n               'WETH': 225.80843211718437,\n               'SAI' : 1.0,\n               'ETH' : 225.80843211718437,\n\n\n\n  \n  \n    \n    \n  \n  \n\n\nThanks for all of the efforts!\\nIIRC it’s my comment that’s in error, it should say “one week before the COMP token announcement” (similar to one week before the COMP distribution in the commented out one), based on conversation here and in the Discord. This is still an adjustable parameter at this stage of the discussion.\\nIt should be before the COMP distribution.\\nOne week before the COMP announcement doesn’t make much sense to me. The users that used compound before feb 19 and those that used it between feb 20 and feb 26 had the same lack of knowledge of the compound governance announcement.\nI encourage the group to consider block 9555730 at Feb-25-2020 11:59:09 PM +UTC as the latest cut off before compound governance became public knowledge.\\nI agree, the first real airdrop happened half a year later (UNI), and the announcment is not relevant. The announcement could be compared to the claim that Coinbase has an oracle solution - and it was also an “announcement”\n\n\n\n allthecolors:\n\n“one week before the COMP token announcement”\n\n\nwhy 1 week? is there a reason?\\nLet’s keep @alive date.\nJune 8, 2020\nDon’t change it.\\ntoo many cooks spoil the broth\nit’s impossible to please every one\nI agree with all of you, but majority rule,or world without end.\npowerful person ,it’s time to make a decision\\nWe are running into analysis paralysis by tying in all forms of Compound interaction. It is likely that many of the more complex interactions will require one-off or custom solutions, if they can be resolved at all. I would vote to tackle each problem individually in their own proposals. Start with what is the “simplest”: rewards to individual users interacting with Compound as in @allthecolors current list. It has been seven months since this was first proposed and it can easily be that much more to sort out all the individual loose ends if we group them all into one mega proposal\\nI agree, this whole “one week before” thing doesn’t make any sense and seems completely arbitrary.\\nIt seems there is consensus enough to put the proposal to a vote?\\nI interacted as an ‘early user’ with the Compound protocol through my Argent wallet address because the gas fees were paid and that was attractive even at the time (way cheaper gas). I also loved (and still love very much) the idea and added security of an user-friendly proxy-type address (which Argent offers).\nNow my Argent address appears in the Early Users CSV files but it doesn’t appear in any of the CSV files in the ‘proposals’ folders. It’s not in the Sybil attack exclusion list either.\nI know these lists aren’t final, but I just want to say that I think the governance should propose the most inclusive proposal so that it doesn’t get defeated (for example Mr. Leshner stated he would fight proposals that exclude proxy users). There is no basis for excluding smart wallets users, as Compound is a protocol, not a centralized front-end. Now I do understand an inclusive proposal requires extra filtering work, but I am convinced it will be worth it in the end.\\nInteresting. Where can I find the early users csv. Would someone mind sharing it with me please.\nOkay I figured it from previous comments. Any ETA on the airdrop ?.\nThanks to everyone who came up with this great idea, which benefits the protocol a lot for sure once implemented.\\nNewbie here and I am not an early user of compound and do NOT benefit personally in any way with this proposal. Here’s my 2 cents.  It reflects extremely poorly on compound that a proposal that started in Nov 2020 does not any clarity/clear deadline in May 2021. So atleast let’s set a firm date (say June 15,2021) and work backwards and come to a resolution either way.\\nLastly, super minor feedback for @rleshner on the language used here “Excluding smart contracts is a dangerous approach that disenfranchises most users, and I will actively fight against it.” … Personally, I am 100% aligned with your logic here and would support this.\nBut you need to be very careful about how you express it. The entire purpose of decentralized governance is to provide confidence that every protocol user has a say in protocol governance decisions. As a newbie, If i see a protocol founder with heavy allocation of compound/voting rights take a stand using such strong language, it gives me little confidence that my opinions (when they differ) makes sense\nNothing personal against @rleshner and i am a big fan of your contributions in this space overall \\ni think the proposal already failed\\nohh okay. That’s cool then. Sorry, I am a newbie and did not realize this proposal already failed…and just assumed it was just dragging on for the last 7 months…\\nThis proposal has NOT failed.\nCurrently @allthecolors is attempting to include all smart contract users that indirectly interacted with Compound including his already 30,000 list of users.  He was attempting to get access to a Node to run his tests.\nIt takes time to do the airdrop correctly, we still have a few tasks to accomplish.\n\n\nWe probably want to have users get Vested tokens(a slow stream COMP of tokens over 4 years).  Since we cannot change the Governor Bravo contract because it will increase Gas usage, we should maybe look at a stand alone contract that will preform the functions of Vesting, Gas Less Voting, and allocating the appropriate amount of COMP.  This needs work anyone able to code this sort of contract please contact @allthecolors.\n\n\nWe need include Smart contract users that indirectly interacted with COMP in order to reward every user fairly. @allthecolors has attempted but could use help at his github: GitHub - 0xA1176ec01045/CompEarlyUserAnalysis 29\n\n\nI understand you want to rush things, but for as large of an airdrop as this will be we need to use caution.\nAnyone interested in seriously helping with coding a contract for vesting talk to gauntlet and @allthecolors about maybe getting their code for use in in a Vesting Contract.\nMaybe combine it with Comp.vote (gasless voting) in order to allow people to vote with the held funds, while they slowly get released quarterly for 4 years…\nI have read most responses, we are into the technical parts of the distribution and with Vesting and high gas costs we need to be responsible.  Please try to be up to date before posting.\n@allthecolors received a 10,000$ Compound Grant to work on this proposal last week!  Congratulations and well deserved.\\nSo 30k and including all smart contracts or wallets it could go up to 50k users. If the reward amount of 500k comp stays the same then it boils down to 10 comp per user correct ? That’s a big deal for Many I would think for sure but curious how these users can take part in governance since user would need at-least 100 comp to participate in governance if am not wrong. Thanks\\nThanks @ManTheWheelz for condensing the recent activity here! Longer threads like this really need it periodically, and it helps when the updates come from different folks along the way.\nYes, the Compound Grants program is supporting me to address finer details and questions related to the early user analysis. These include handling of early users interacting with the protocol via smart contracts (@allo, Argent addresses are coming!) as well as an update to the capital-weight model based on total interest rather than the more-complex-in-theory yet easier-to-implement model described in my earlier posts. I don’t expect the capital weight revisions to change most users’ weights much, but like @ManTheWheelz said, given the potential size of this proposal, it’s important to get those details right so that there are no surprises between what the proposal says and what it actually does.\n@cryptobuddy_1712, early users would still be able to use this COMP to vote or delegate (though maybe not immediately depending on how vesting is implemented). They would however not be able to submit their own Compound Autonomous Proposal (CAP) unless holding 100+ COMP. Earlier in the thread I floated the idea of a separate proposal to reduce the minimum COMP threshold for submitting a CAP to align it with whatever form this distribution might take. I’m not sure that idea has much traction so far, but it’s one of our options.\nI’ll have more to post in the coming days. For now I just want to say thanks everyone for the support and constructive criticisms to date. Looking forward to delivering the data needed to help shape these ideas into a proposal that the entire community, from the first depositors to those setting up Gateway validators, is excited to get behind.\\nThanks for the continued work. As a long time, but small time user, how does the “capital-weight model” work out? I realize it might not be determined, but is there to be a lower limit for received COMP, or could one end up with a very small fraction if the contribution didn’t include a significant amount of capital? Also, will the capital-weight be relative to the total capital at the time? Sorry for any “repeat questions”, and thanks for recap by @ManTheWheelz.\\nI still have my node up and running, and happy to help if you need\\nSo how has this proposal changed now during the dip, guessing its way cheaper for 100 Comp to get governance opportunity?\\n20 COMP, IMO, is perfectly fine for early users. Maybe even 5 or 10 COMP. I understand that COMP holders need 100 COMP to have increased utility, but I think less is better because COMP’s token value is quite substantial. Airdrop tokens to as many users as possible for decentralization purposes. I’m not sure exactly how much liquidity addresses provided to COMP during the early stages. I keep looking at the current COMP value and thinking, Compound is seriously talking about AIRDROP-ing over $40,000 worth of COMP tokens to early users?!?\nI believe that Compound has problems brewing on the horizon due to lack of decentralization of the COMP token itself. I’m for anything that can further decentralize the token.\nI’m saying it was AWESOME for those early users to get the protocol started, truly it is awesome. However, those people chose to use the Compound Protocol because it was developed nicely and made liquidity easy while providing them with interest on their crypto. Early users didn’t use Compound thinking “one day I will get an airdrop for using Compound”. And really, if they did expect an airdrop, those are gonna be the people who do EXACTLY what Compound does NOT want… Dump tokens.\\nI agree, it doesn’t matter so much whether the airdrop will be 5, 10 or 20 or 100 COMP tokens if the distribution is largely social weighted. Dump can be expected with higher capital weighted parameter\\nthank for your helping\nis any good news?\\nAs I have stated earlier in the thread, I have lost access to my wallet 0xe84d25b1C4fe0E5A9bEe95934AB24C9867Aac2cc 5 which was used to interact with Compound.\nI can prove that the address belongs to me by showing that I sent the USDC from my Coinbase account and then used to supply on Compound.\nSCREENSHOT of Coinbase account showing tx hash.\n\nCB Screenshot1197×645 141 KB\n\nSCREENSHOT of Etherscan showing incoming txns and supplying txns.\n\nEtherscan snapshot1600×900 122 KB\n\nI know it’s a lot to ask for, but is there any way that I request the airdrop (if it happens) go to my current Ethereum address 0x7d355f8b12d15213e3C6b187Cb5ca348EcD725f8 10? I love using Compound and would like to obtain more COMP to vote or delegate votes to be a part of Compound’s governance.\\nHi all, I’ve pushed a few updates to the project github and thought I’d accompany that with a progress report here.\nThe introduction of cTokens in V2 marked a significant shift in how interest is managed in Compound, so I’ve found it easiest to analyze early user interactions with V1 and V2 separately with intent to merge the results at the end.\nFor each version of the protocol, I’ve separated the process into four steps:\n(1) Pull all early-user transactions on-chain with relevant Compound contracts within the early-user window, whether EOA or contract-mediated\n(2) Determine the end user associated with proxy contracts so that the underlying user is credited, while ensuring credit for smart contract wallets remains with the contract which represents the user\n(3) Calculate each early user’s cumulative supply and borrow interest in each asset from raw transaction data\n(4) Use price history data to translate cumulative supply and borrow interest into a single USD- (or ETH-)equivalent interest.\nI believe it may eventually prove easier to perform steps (3) and (4) in tandem, but I’m separating them for now to make it easier to check the accuracy of each step independently.\nThe final cumulative interest serves as a “capital weight” for each user, replacing the cruder metric described in my earlier proposal for defining a capital-weighted component of an early-user COMP distribution.\nSteps (1) and (2) are currently in good shape, other than some clean-up and provision of a more automated workflow for others to reproduce the csv files.\nSteps (3) and (4) are on their way but still in-progress.\n\nFor step (3) in V1, CompoundV1.interest.py is a working implementation for V1 that passes my sanity checks and is available for public review. I’d be especially grateful for any feedback from devs familiar with the interest calculation for V1 to let me know of any potential edge cases I might have missed. I’m aware of one minor edge case involving liquidators that I have a solution in the works for.\nFor step (3) in V2, I’m troubleshooting an issue with CompoundV2.interest.py that appears to be contract-related, but a basic approach to computing V2 acculumated interest is implemented there and available for review as well.\nFor step (4) in V1, CompoundV1.USDvalue.py offers a proof-of-concept for appending USD price data to the raw transaction histories using the CoinGecko API for historical market data. If anyone has concerns with using daily CoinGecko price history data instead of using the on-chain Compound open price feed, please speak up! So far I am going with this approach because it is easier and more intuitive for me to implement while having negligible impact on accuracy, but I am willing to take the time to figure out how to do it with on-chain data if that’s what folks need to see in order to support an eventual proposal.\n\nI’ll report back on steps (3) and (4) as they come closer to fruition. In the meantime, I think it could be constructive to have some parallel effort on crafting a clear, organized, and concise rationale for governance to support an early user distribution. Anyone want to start collecting the relevant input from this thread and merging it into a collaboratively editable document?\\nHow will the amount of distributed COMP relate to your weights analysis?\\nI’ve created a repo 52 to create a rationale.md file collaboratively with the community.\\nNo issues with using coin gecko.\\nHere’s the latest on the early user analysis:\n\nAt each transaction in the early-user history, accrued interest is now converted into a USD equivalent based on the price of the asset on the date of the transaction from the CoinGecko API. This is now implemented for both V1 and V2.\nFor both V1 and V2, the most complicated step IMO is the reconciliation of interest accumulated in the period between the last transaction as an early user and the EarlyUserCutoffBlock. This step is essential for proper handling of users who deposited/borrowed very early on and simply held their position(s) through and beyond the EarlyUserCutoffBlock. The updated scripts now handle reconciliation of outstanding early-user supply and borrow interest for both V1 and V2.\nI’m continuing to sanity-check the output for addresses with increasingly complex transaction histories.\nFor V2 interest, I’ve traced back the contract-related issue from the previous update to whether to credit borrow interest on RepayBorrow() events to the payer or to the borrower in the event that these are different addresses. I had initially chosen to credit the payer, but I realize now that it needs to be credited to the borrower (sorry payer), otherwise we mess with the tracking of accountBorrows in a way that causes certain contracts to get showered with more accrued interest than they should.\n\nOnce the results are updated to account for this latest change, I’ll post output with a list of users and their accrued supply+borrow interest (in USD-equivalent at the time of accrual) across all V1 and V2 assets during the early-user window.\nIn the meantime, I should note that in cleaning/reorganizing the early user analysis Github repo 60, the location of the version 1 distribution lists has changed. The link posted earlier in this forum will no longer work. These obsolete files are still available in the version_1 folder in the Github repo in case you have reason to refer back to them.\\nWe have also posted an idea about the loyalty NFT badges for community members. Project Galaxy - Helping to build Compound's first NFT-based Loyalty Campaign! 18\nIt can be done along with the token airdrop to early adopters!\\n\n\n\n ken-galaxy:\n\nWe have also posted an idea about the loyalty NFT badges for community members. Project Galaxy - Helping to build Compound’s first NFT-based Loyalty Campaign! \nIt can be done along with the token airdrop to early adopters!\n\n\nI think that’s pretty cool.\\nHey everyone! The challenges and edge cases highlighted in my previous post are now all accounted for in the latest update 26 to the early user interest analysis.\nHere is the key result:\n Early-user addresses by total interest accrued 84 \nHere is the same information in histogram and pie chart formats:\n\nTotalUSDinterest.histogram1200×400 15.3 KB\n\n\nThe USD equivalence is based on the USD value of the asset in which interest was accrued, on the date of accrual, as reported by the CoinGecko API. The analysis includes outstanding interest from supplies/borrows still active at the early user cutoff block which would otherwise have been neglected since there’s no associated on-chain event in this case.\nA few notes about this new list:\n\nThis is not a proposal for how much COMP to distribute to early user addresses. Rather, this tally is intended to provide a more precise way of assessing early user capital deployed to the protocol during the early user window relative to the crude capital weights introduced in the April model.\nThe median address’ accrued interest over the early user period was approx. 35 cents (0.35 USD).\nFully 20% of early user addresses accrued interest that, rounded to the nearest cent, is zero. However, every address on this list interacted with the protocol during the early user window, resulting in formally non-zero accrued interest.\nMany of the addresses whose accrued interest rounds to 0.00 deposited hundreds of USD worth of value to the protocol for a few minutes or hours; others deposited smaller amounts for much longer periods of time. This may be useful information for the community in reflecting on whether to adopt any minimum on accrued interest for inclusion in a proposal.\nThe parent script runAnalysis.sh 4 includes a line implementing the request from @CryptoCraig regarding an inaccessible early-user address. Recognizing the potential for controversy over any direct modifications to the list, I felt it was important to explicitly point it out for the sake of transparency.\nBecause this address list no longer makes any distinction between contracts and EOAs, we can expect that the composition and weighting of addresses with the largest capital weights will look quite different from that of the previous model, and indeed it does. That said, several EOAs from the previous list remain in the top ten.\n\nThe tool now passes my sanity checks, so I’m hoping others will think of sanity checks orthogonal to mine to further boost confidence in the results and detect/fix any remaining issues. FWIW my checks have mostly taken the form of manual inspections of hand-picked addresses with specific types of transaction histories across V1 and/or V2, via etherscan. Examples: only supply/redeem interactions; only borrow/repay interactions; addresses that have done all four of these actions in just one or two assets; addresses that have been on one side, the other, or both, of a liquidation; addresses with outstanding supply or borrow balances at the EarlyUserCutoffBlock to check reconciliation of outstanding interest; and last but unfortunately not least, addresses that have traded cTokens on secondary markets, as those trades – to my chagrin – throw off internal balance tracking in ways that can falsely imply a negative interest if not accounted for.\nI’ll be here to field questions and revisions based on community feedback. Thanks again to the Compound Grants program for supporting the early user interest analysis!\nWhat comes next?\n\nCommunity review of this analysis and requests for follow-ups. What other summary or detailed early usage info, if any, should we be extracting from here? Identification of the largest contracts seems worth pursuing, to get a better idea of whether the proposal needs to contemplate handling these differently from simple addresses.\nCommunity development of a brief written rationale summarizing the case and value proposition for an early user distribution of COMP. I think it makes sense to build momentum on this before returning to consensus-building on the specific details of the proposed distribution, to avoid “putting the cart before the horse”: The rationale should inform the shape of the distribution, not the other way around.\n\n@ogamiitto set up a github repo for collaborating on a rationale (thanks!), but it hasn’t had any activity in the two weeks since it was set up. I’m guessing that’s in part because it’s a little more involved to make edits/comments and less accessible for many among us who aren’t using github on a regular basis. We could use a collaborative editor like a Google doc, or maybe CryptPad 3 if folks would prefer something more privacy-oriented?\nI think it is a good time to pick up on this thread’s previous discussion of a mechanism for realizing an early user distribution of COMP. Several good ideas have been floated, with key features including some kind of vesting. I have a couple of ideas here which I’ll save for a separate post after folks have a chance to digest this analysis.\n\\nThanks so much for this report. I am still wondering about the value for EarlyUserCutoffBlock. It seems to be Feb 20, 2020, but the COMP announcement wasn’t until Feb 26, 2020. I’ve left some feedback earlier in the thread about this to consider block 9555730 as the canonical “latest block before the information became public”. As we do not know the exact time the announcement was made, the oldest block prior to Feb 26, 2020 00:00:00 seems to be the most logical choice. Should Compound Retroactively Airdrop Tokens to Early Users? - #295 by prestonvanloon\nIs there any reason for Feb 20? If you could, link to the timestamped announcement that occurred on Feb 20.\\n\n\n\n prestonvanloon:\n\n9555730\n\n\nGood catch, sorry about that – I had meant for EarlyUserCutoffBlock to be 9555731 throughout, and it is in the USDinterest scripts, but the first script of the pipeline is still using the earlier block which cuts out interactions in the Feb 20-26 window. I’ll update the source and link above once those are back in, thanks!\\nAn additional issue with the reconciliation step was brought to my attention by a Compound Discord member that should only affect addresses that only received cTokens via direct ERC-20 transfer (i.e. not mediated by Compound), and nothing else. If these are self-transfers, e.g. to/from cold storage, the issue only affects how your accrued interest is assigned across your accounts, but not the total interest. Still, it’s a relatively easy fix; I will update when this is corrected.\\nWhy hasn’t the cutoff date been set to:\n\nidgm\nLet’s keep @alive date.\nJune 8, 2020\nDon’t change it.\n\n? I though this date was kind of agreed on already.\nAnd thanks for the amazing work @allthecolors !\\nThank you allthecolors, appreciate the quality of the work you provided. This is a beautiful pie chart with a much more spread distribution.\nThese data that you gathered permits now to remove any arbitrary in the results. With this it become much easier to come up now with a discussion as they permit to measure the usage of the protocol for the interest rate market which is Compound. It will need then to be decided the importance of the capital weighted part of the allocation.\nI sent a pm to avoid to fog this thread, about the preparation you mentionned, trying to gather all the ideas of this thread.\n(For what it’s worth :  I could not check the code used itself but tried to hand pick a dozen adresses, the biggests and randoms : the numbers seemed to match up with etherscan datas.)\\nspeaking of comp, Add markets: stETH 7 we would love feedback\\nThis proposal has turned out to be huge and very detailed. Thanks @allthecolors for your work on supplying the Compound Community the data needed to make an informed decision!\nThis is what the community needs to figure out from this data in order to submit the proposal: (my opinion)\n\nWeight distribution:\n\n\n\nPercentages based on social and capital provided? (90% social, 10% capital. With the cutoff for getting the capital distribution being addresses which earned greater than $100 interest. Just by eyeballing the list, those addresses represent about 10% of all accounts.)\n\n\nShould we handle contract addresses differently than individual accounts? (No, unless the capital distribution is diluted by not having a minimum amount of interest earned.)\n\n\nAny other metrics that need to be considered, based on this data? (Probably a lot of different metrics could be considered, but I feel these 2 metrics are the most important, and therefore, all that’s needed for Compound’s early user distribution.)\n\n\nThe Compound Community’s input is always welcome and encouraged!\nEDIT:\nMaybe since such a large number of addresses earned $0.00 interest we could use the median interest earned ($0.35). Addresses below this threshold will be distributed 30% of the social distribution. Addresses above would get 70% of the social distribution. This would effectively distribute more to users who risked more capital, without it being to skewed.\\nOldest block prior to public announcement is obvious choice block 9555730 as @prestonvanloon references above and @allthecolors cites from a couple days ago.  (Feb. 26th, 2020).  Awesome work @allthecolors !\\nI still don’t really get why the date was set to public announcement of COMP. Could someone elaborate a bit more why that is such a good candidate?\\n\n\nPercentages based on social and capital provided? ( 90% social, 10% capital. With the cutoff for getting the capital distribution being addresses which earned greater than $100 interest. Just by eyeballing the list, those addresses represent about 10% of all accounts. )\n\n—-My recommendation would be $50 in interest where those that may have not provided as much collateral but may have used the protocol longer and would be more inclined to continue supporting the protocol.\n\n\nShould we handle contract addresses differently than individual accounts? ( No, unless the capital distribution is diluted by not having a minimum amount of interest earned. )\n\n—Agreed\n\n\nAny other metrics that need to be considered, based on this data? ( Probably a lot of different metrics could be considered, but I feel these 2 metrics are the most important, and therefore, all that’s needed for Compound’s early user distribution. )\n\n—- Agreed that these two basic metrics provide the most universal way to apply for distribution.\\nThe proposed date of June 8th 2020 was quite widely publicised around the Ethereum community. It seems a shame to change it now. It would likely create a negative sentiment towards the Compound protocol amongst a number of early users, who are now going to miss out. I know many who have been following this thread closely for many months.\nI am one of the users who would stand to benefit from a June 8th date, so I am of course somewhat biased. But I am also conscious of the 23.1k views on this thread, and how widely it was shared on Reddit, Discord, and other crypto communities. So it wouldn’t just be me missing out! Many in the community checked their address against the previously posted distribution list. They would now be disappointed to be cut out.\nI should also note, that in a informal poll in this very same thread, the original distribution list was widely supported: Should Compound Retroactively Airdrop Tokens to Early Users? - #232 by allthecolors\nThis also creates a “black hole” between February 2020 - June 2020 where no Comp was distributed to users during that period (before distribution begun, but apparently too late for an early user airdrop).This seems strange to omit this one time period. The logic only appears to be on the basis of the announcement of an upcoming COMP token on Feb 26th. But there was absolutely no indication of a retrospective airdrop being on the cards at that time.\nSo, why now change the date, when the originally suggested date appeared to be widely supported? There is not really any clear indication in this thread as to why the date was changed since that poll was conducted back in April. A date change was suggested by one user in the thread following the poll, and it is in fact the suggested change of date which appears to be the main point of contention for some posters here. Nobody seemed to have a major issue with the originally suggested date.\nI totally support the work to include more accurate capital weights, and make sure anybody who used the contract is included. But it seems we have included those extra users, at the expense of others included in the previously proposed distribution list.\nIt is clear a lot of time and effort has gone into making this proposal work. Thanks @allthecolors for the great work. I am sure the governance process will come to a sensible decision on these matters, and get this proposal into motion. These are just my thoughts (as a biased person admittedly!)\n-Sku\\nIncreased COMP distribution to large funded groups/individuals that already received and continue to receive a large portion of COMP through interest generated payments would not achieve/drive the overall goal of a decentralized governance but rather lead to increased centralized control.\nWhich legally and from a conflict of interest perspective may be detrimental to the future of the protocol as we saw with EtherDelta and Uniswap respectively.\nI.E.\n“Parties with conflict of interest pushing through pro- posals (Uniswap-Dharma): A recent example of conflict of interest took place during Uniswap’s first proposal, made by Dharma, to reduce Uniswap’s existing governance thresholds. Dharma had 32% of the voting power at the time of the proposal while the second biggest voter had 30%. Dharma’s made two proposals. One was for a reduction of the amount of tokens needed, in order to submit a proposal, from 1% to 0.3%. The second one was for a reduction in quorum, or the percentage of the total supply which must vote on a proposal in order to pass, from 4% to 3%. Dharma claimed that these two proposals would give the ability to smaller token holders to make proposals and also it would be easier for important proposals to not get rejected since many token holders do not bother voting. Dharma might have had the right intentions but looking at the numbers many could infer that these proposals could easily give more power to Dharma itself on governing Uniswap’s protocol. Uniswap total (not circulating) supply is 1 billion. Under the initial protocol rules, 10 million tokens would be needed in order to submit a proposal and 40 million tokens for the proposal to pass. Dharma’s proposal was for a reduction of these two numbers to 3 million tokens and 30 million tokens, respectively.\nConsidering that Dharma’s voting power is 15 million tokens or 32% [8] of the total voting power, while the second biggest voter has 30% of the votes or 14 million votes, by reducing the quorum threshold to 30 million, Dharma and the second biggest voter (Gauntlet) could combine voting power to push through any proposal these two believe would be best for Uniswap or them.”\\nOn your proposal for comp distribution method to use:\n“All Compound protocol users will be eligible to participate in the community vote, weighted by usage of the protocol (measured by lifetime interest earned + interest paid) Third, users should be measured by usage of the protocol.\nFor an interest rate market, USAGEis a function of capital over time ; how other protocols have distributed tokens to users is irrelevant to Compound.\nThis approach was piloted during the second community vote  9, and formed the basis of the COMP Distribution when it began for users.\nLuckily, in Compound’s case, this can be easily measured by interest earned, and interest paid.”\nThis approach is specifically focused on financial contribution not necessarily a algorithm for “USAGE”. Wouldn’t a better approach for defining usage be\n(interest earned+interest paid)*50% + (interest earned/paid days)*50%\nThe number of days contributing to the protocol in my eyes is just as important as the financial contribution.\nGiven we are not assigning currency risk associated with the financial assets provided to the protocol.\nExample those contributing stable coins and borrowing stable coins, (interest earned+interest paid) had little risk but would be the most rewarded with COMP governance.\\nToo many opinions\nendless\\n@voodoorider, it must be before anyone knew of the COMP token’s existence. Once people knew of the token, know that it must be distributed somehow. I’m not sure, but I imagine after the announcement, digital asset users flocked to supply their assets to Compound. This was a new concept, to earn interest + rewards on deposited funds. Compound’s APY was already 10-50X from a traditional finance POV. At one point, I remember seeing +20% APY!\nSo, after the announcement is too late for ‘Early User’ distribution. Early users risked capital without being rewarded Compound Governance Token (COMP). You’ve heard of banks giving you $100 if you open an account by xx/xx/20xx, right? What if you already had an account? What if you open an account the day after? Same principle here.\\nIt’s been a couple of weeks since I mentioned two lingering issues with the interest analysis, particularly surrounding handling of cToken transfers. These issues proved more time-consuming to untangle than I’d anticipated – y’all early power-users sure gave the protocol a thorough testing but created quite the mess of transactions and transfers to untangle (good job!)\nI’ve updated the github repo with corrections that address those lingering issues. The early user interest list is also updated; the previously posted link now points to the new list, or you can use the quoted link below. For most users, there will be either no difference or a small relative difference in interest recorded. If you transferred cTokens among several addresses, you may see your accrued interest redistributed somewhat over those addresses.\n\n\n\n allthecolors:\n\n Early-user addresses by total interest accrued \n\n\nI have sanity-checked the results across a healthy cross-section of addresses with diverse transaction histories. Still, I invite everyone to review and flag any further edge cases that may still be lurking in the analysis.\nI’m excited to see the community use these data to move forward with a proposal that serves the protocol well.\nWith my work on the early-user interest analysis coming to a close, I am eager to re-enter the conversation about how the community should use these data to inform a proposal. However, to avoid any potential conflicts of interest, I will continue to refrain from opining on proposal formulation (beyond my earlier enumeration of logical next steps) until after the grant that supported this analysis is fully closed.\\nI am a bit afraid and share my concerns with @Fishbtc and others that we are missing the point of this airdrop with making “rich richer” and only rewarding super early users and moreover those who earned the most using the platform. The beauty of Uniswap airdrop (I agree,a bit communist but powerful  and its marketing impact (let’s face it - it became legendary) was the simplicity of just and even distribution. I think for Compound to extend its range and strenghten its position in Ethereum ecosystem would be to reach as much users as possible.\\nCan we also produce a list using the originally proposed cut off date of 08/06/2020?\nThis way a decision can be made by the community on how to distribute based based on the full data set.\\nLooking forward to see some progress in the airdrop soon :). Thanks everyone who helped to reach to this point.\\nthanks for your helping…\nwe hope the proposal can release to vote soon.\\nWhen does the grant expire?\\nMy understanding is that it will be closed out with the grant committee’s next disbursement from the committee multisig. I don’t have an exact timeline, but past precedent would suggest within 1-3 weeks.\nAlso to @Sku 's point, I agree with the spirit of providing all the data we can to inform discussion, with the gentle reminder that there is already strong resistance to cut-off dates beyond the COMP announcement from some of the largest token-holders documented in this thread. Folks who are interested can find the data for a June 8th cut-off date here 16. The major difference on the whale side of this list is the sudden rise of a smart contract controlled by Dharma, whose Compound integration grew dramatically during this Feb-Jun period. I should add a technical note that the script had to be modified to generate these data by capping the number of “buckets” for the market history data at 500 instead of sampling daily; the later date results a number exceeding the maximum number of buckets accepted by Compound API, but the impact of this adjustment on the estimated interest is negligibly small.\\nDistribution Method:\nI believe that time contributing to the protocol holds as much value/weight as any financial contribution and should be rewarded as such. Having said that, I suggest:\nComp Accrue Period = Time between first live completed compound block until announcement date block (2017 - 2020) Roughly 2.7 years\nComp Vest Date = First Comp Interaction/Contribution Date by Wallet - Comp Accrue Period (2.7 give or take)\nSo if you have been contributing to the protocol for 2.7 years you should vest before others that started contributing to the protocol for let’s say 1 year.\nTo me this provides the most value and support to both sides of the coin. Time and Money.\nLove to hear any thoughts on this recommendation.\\nSo what happens if the grant expires without concluding on airdrop? Just asking for information. Thanks\\nI can see the appeal in theory of vesting at different speeds depending on how early a user interacted with the protocol. I also think it is technically feasible with existing code in the Comptroller, specifically by borrowing the _setContributorCompSpeed() function.\nThat said, here are some of the significant challenges I would anticipate with time-weighted vesting schedules:\n\nMy hunch is that using the _setContributorCompSpeed() function to assign different vesting schedules for each early user would be a highly gas-intensive operation. Other major projects’ (e.g. UNI, FORTH) token distributions have used a Merkle root distributor approach that effectively requires the user to pay part of the gas associated with the distribution.\nThis use of setContributorCompSpeed() would be a dramatic deviation from its intended use, and a separate solution would be needed to finish vesting by resetting those speeds on a per-user basis in a way that doesn’t require further intervention by governance.\nThere’s potential for disagreement over whether someone who supplied capital and quickly withdrew it within a few blocks early in the protocol’s operations should vest at a faster rate than someone who supplied one day later and never redeemed / still holds those cTokens.\nTo the point about honoring “time and money” in the distribution, this is precisely the purpose of using total interest earned/paid as the weight factor for proposing a distribution. On a standard linear vesting schedule, if two users supplied the same amount of capital (and never withdrew) but one started much earlier, the early bird would accrue more vested COMP per hour than the newer one because of the greater interest they earned overall. So we could make the case that adjusting the vesting schedules by date of first contribution is “double-counting” the time element.\n\nOverall I think a constant vesting rate for every address eliminates this question and also simplifies the implementation, but welcome additional opinions on this.\\nWere the contributors paid or did they volunteer? If they volunteered then I agree with you.\nIn case you already being paid with a COMP token, do you think that this would lead to greater centralization? Or does the term “decentralized finance” serve solely as a narrative?\nPersonally, I think contributors deserve more than other users (and get more) but how much more?\nIf the goal of the planned airdrop is to increase decentralization then such a method of calculation does just the opposite\\nSo would see it as two distinct levels of engagement.\nContributor is anyone engaging/using/interacting with the protocol prior to the existence of the comp governance token. Without those users or contributors early engagement and support in the protocol, like many others it would have fizzled away.\nDevelopers or those contributing directly to the maintenance of the protocol  or “code” is different and would be associated with compound labs or have been provided a grant for development type work.\\n\n\n\n allthecolors:\n\nTo the point about honoring “time and money” in the distribution, this is precisely the purpose of using total interest earned/paid as the weight factor for proposing a distribution. On a standard linear vesting schedule, if two users supplied the same amount of capital (and never withdrew) but one started much earlier, the early bird would accrue more vested COMP per hour than the newer one because of the greater interest they earned overall. So we could make the case that adjusting the vesting schedules by date of first contribution is “double-counting” the time element.\n\n\n—-Agree with most of your points. (Gas price especially)\nBut with respect the above, I also see the flip side where large investor groups are accruing substantially more in a shorter period of time with less risk by providing a stable currency and borrowing another. Eliminating the risk many/most early users endured.\nTo me those early users that contributed and withdrew their c-coins a short period of time later would likely sell their comp but those that don’t will likely be encouraged to engage and contribute to governance. Just my 2 cents. (Fighting for the old school, small timers)\\nHi,\nI have created a spreadsheet containing the raw data (csv) that was generated by @allthecolors. Thanks again, btw! The spreadsheet contains a formula with variables in which the data can EASILY be manipulated to change the amount of COMP that is airdropped to each address (simulated airdrop).\nThe formula has multiple parts, which allows for a mathematical approach to determining the balance between a social and capital based distribution method:\nVariables:\n\ncZero = amount of COMP distributed to addys who earned 0.00 interest\niLowAmt = lower bound interest earned from addy\ncLowAmt = amount of COMP distributed to addys who earned less than the iLowAmt\ncReduceCapital = this is the number that divides the square root of the interest earned\ncAddSocial = excluding the lower bounds interest earned, this is the least amount of COMP each address should receive\n\nFormula:\nAn Addy with 0.00 interest gets cZero COMP\nAddy < iLowAmt interest gets cLowAmt COMP\n(Sqrt(interest) / cReduceCapital) + cAddSocial\nThe spreadsheet:\n  \n\n      docs.google.com\n  \n\n  \n    \n\nCOMP Airdrop Distribution Manipulation 55\n\nSheet1\n\nRAW DATA:,https://raw.githubusercontent.com/0xA1176ec01045/CompEarlyUserAnalysis/main/csv_results/TotalUSDInterestByAddress.csv\naddress,Total,COMP rewards\n0x58cCC12eB2bC42c41E6232547a694106e85cC4C2,0,2.5,The formula:,An Addy with 0.00...\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nThe spreadsheet contains global airdrop data (left side) including the total amount of COMP distributed in the simulated airdrop, the smallest and largest single address distributions and the amount left over from the proposed 500,000 COMP, which I have labeled as Community COMP airdrop. Which I think should be divided equally and airdropped to everyone who participated on this forum post.\nUsing this sheet to simulate airdrops:\n\n\ncZero variable\nThe cZero variable is included because of the multitude of addresses which interacted with Compound, but without even earning a penny’s worth of interest. This could have been due to several reasons including the possibility of flash loans being tested in the earlier days, hackers trying to find a way to break the protocol, Sybil attack, inside knowledge of a possible airdrop or many other reasons, none of which IMO deserves a substantial amount of COMP to be airdropped to each address. Also, these addresses which earned less interest have a greater chance of being abandoned, unless they are from malicious actors. I would even consider forcing those addresses with $0.00 earned interest to need to perform some type of claim, like signing a message.\n\n\n\niLowAmt and cLowAmt variables\nThese are other variables used to determine who and how much to airdrop/reward addresses which interacted with Compound in a way that couldn’t be due to flash loans, or other “same block” instances, but also didn’t risk as much capital. Also, would imagine that a greater than average percentage of these addresses are abandoned/lost. So, again, considering a claim function or message signing operation for these addresses may not be a bad idea.\n\n\n\ncCapital and cSocial variables\nThese variables are directly related to capital and social weighting distributions. Instead of using a percentage, I developed a mathematical equation in which can be modified by anyone on our forum to modify for easy and quick simulations of airdrops.\n\nPlease send me a request if you would like to manipulate the data within the spreadsheet. Actually, you can just copy it, if you have any problems or questions about how to use the spreadsheet, let me know. I have pre-filled the variables where I believe is a fair distribution model.\n\n\n\n Fishbtc:\n\nSo would see it as two distinct levels of engagement\n\n\n@Fishbtc, this is able to provide 2 levels.\n\n\n\n cryptobuddy_1712:\n\nSo what happens if the grant expires without concluding on airdrop? Just asking for information. Thanks\n\n\n@cryptobuddy_1712, lets not let it expire, take a gander.\n\n\n\n Fishbtc:\n\nI believe that time contributing to the protocol holds as much value/weight as any financial contribution and should be rewarded as such.\n\n\nReally, using the interest earned, is taking the amount of time in consideration. I mean, someone supplying $50,000 for 5 days should earn nearly equally the same interest as a person who supplied $10,000 for 25 days. If time wasn’t already a product of interest earned the person who supplied $50,000 would get 5 times the airdrop right?!?\n\n\n\n getty:\n\nI would suggest coming up with a thought out plan of how many COMP should be given out, who should be getting the COMP, why it is a good idea to give out that COMP. If someone analyzes Compound V1 and its users, I think that would get a good amount of attention.\n\n\nJust needed to ask @getty if this is helpful?\nAlso, would be much appreciated to hear feedback from @rleshner, @arr00, @Sirokko, @massnomis, @TylerEther and @alive. Thank you!\\nThis needs to be brought up at the next dev call, or should have been brought up a the last dev call! If an idea of this magnitude (both user and COMP) fails to reach the proposal phase, that is absolutely crap. It should be required to be brought up during the call, or really needs to be a calander somewhere that members can check for important dates.\nCan we do a simple proposal, just voting on that there will or will not be some type of airdrop? That should get plenty of votes. Also, I hope you already saw the spreadsheet that I created allowing everyone to be able to simulate airdrops, even without a single hint of developer knowledge.\\nGreat work @CryptoCraig . I really agree with you. Lot of people are following this thread since months hoping for an action to be taken but unfortunately there isn’t any. Hope there will be some conclusion drawn in next couple of weeks from all the contributions. Thank You.\\nThanks man, really I was just looking back at it and realized that @allthecolors data needs a way to enable the community to be able to manipulate the data in a way so that they can see the scope of the airdrop. My sheet is designed to allow anyone to be able to change the metrics simply by editing the 5 variables highlighted in yellow on the sheet.  Copy the sheet and then edit the variables to get a distribution that you see fit, then share it with us!\nUse my data as a guideline. Let me know if you need help creating your own airdrop simulations!\\nI’ve been following this discussion for a while, and I just created an account to say that the points and suggestions that @CryptoCraig raised are in my opinion valuable and really worth considering.\nTaking into account that the goal of the proposal is to improve governance decentralization (that is, wide distribution to active addresses and users) I find there is little downside to adding a lower bound that significantly reduces the airdrop to addresses with trace amounts of interest, and making that process “opt-in” instead of an actual airdrop for wallets that have a high likelihood of being lost.\nI personally think there is an argument to be made for filtering out accounts with accrued interest < 0.01$ completely, for the reasons mentioned in this previous post. FYI, the total amount awarded to accounts with accrued interest in the spreadsheet < 0.01$ is 21392.5 COMP.\\nAgree with that as well\\nThanks very much for sharing this tool to help put the interest data into the community’s hands! I went back and forth about including this step in my work. The reason I held back is my agreement with the grants committee to deliver the interest data and not to conflate it with the discussion of a distribution model. So I really appreciate your effort to fill in this gap (and tagging of key community members to continue this conversation!)\\nMy recent suggestions are more geared around distribution methodology and timeline than comp allocation. I’m onboard with the distribution allocation suggested by @allthecolors with your recommendation of minimum qualification interest of greater than .01.\nProvided we have the ground work for the comp governance distribution not sure if those in the position to bring this to vote are ready to do so. @rleshner ?\\nEven once a distribution and mechanism are decided on, there is still smart contract development work ahead of us before a proposal can be submitted. I’m optimistic that once we develop a clear specification for what needs to be implemented, we’ll be able to recruit someone experienced with COMP’s smart contracts to bring it to life.\\n@gabrocheleau, just make a copy of the Airdrop spreadsheet 23 and create simulated airdrops within google sheets or excel. This is a tool for the community, I just wanted to give everyone an idea as to what an airdrop could look like. On the spreadsheet is now a description of each variable making it easier than ever for community members to link to their distribution simulations.\nPlay around with the tool, it may take you changing every variable 2 and 3 times before you get to the point where you feel like “that’s it!”\nThe ONLY reason this was created was to make it as easy as possible for community members to visualize what an airdrop looks like with their ideas. If you need any help PLEASE let me know, I will be more than happy to help. There have now been 23 link clicks and I have yet to see another simulation ran with the tool, which is kind of upsetting to me because I want to get the community involved. After all, Compound is a DAO and what is a DAO without a community?\\n\n\n\n CryptoCraig:\n\nPlay around with the tool, it may take you changing every variable 2 and 3 times before you get to the point where you feel like “that’s it!”\n\n\nwe cant change variable\nthanks for your tools.\\nThanks, yes I actually made a copy yesterday and started playing with it. I’ll try to come up with something that makes sense to me and share it here.\n@bulajacky You can’t change variables on that document, you must first make a copy. To do that just log into your Google account and click File > Make a copy.\n\\n\n\n\n gabrocheleau:\n\nTo do that just log into your Google account and click File > Make a copy.\n\n\nok\nthank u\ni will try it later\\nHey @CryptoCraig!\nSo I’ve played a bit around with the numbers. The issues I was trying to address were the following:\n\nAs previously mentioned, I do not see value in distributing COMP to addresses who have earned less than 0.01$ on the protocol. For the reasons you mentioned (likely flashloans, attacks, tests, etc.)\nTo me, it also did not seem reasonable than an address right below the iLowAmt (e.g. 0.49 interest earned) would get 5 COMP, while an address right above (e.g. 0.50 interest earned) would get 5 times more: 25 COMP.\nI felt like the distribution curve could be “flattened” and the median received amount by lower interest addresses increased. I believe this to be consistent with the spirit of this airdrop.\n\nWith that in mind, and keeping the amount of COMP distributed to a max of 500k, I have done the following:\n\nSet the COMP airdrop to 0 for addresses with < 0.01 interest accrued\nWith this freed COMP, I increased the social rewards to 30 COMP\nI have increased the iLowAmt to 1.00\nInstead of giving a fixed amount of COMP to addresses below the iLowAmt, I have set it to Max(cLowAmt, cAddSocial * interest earned). Basically, this means that the rewards start at 5 COMP for 0.01 interest earned, but quickly increase to 30 COMP as one approached the iLowAmt. This removes the huge discrepancy described above and I believe makes much more sense.\nLastly, I have flattened the distribution by adjusting the cReduceCapital parameter to 12.5.\n\nSo in practice, this means that:\n\nAn address that earned trace amounts (< 0.01) of interest gets 0 COMP (vs. 2.5 COMP)\nAn address that earned between 0.01 and 0.15 of interest gets 5 COMP (same)\nAn address that earned 0.49 of interest gets 14.7 COMP (vs. 5 COMP)\nAn address that earned 0.50 of interest gets 15 COMP (vs. 25.28 COMP)\nAn address that earned 1.00 of interest gets 30 COMP (vs. 25.4 COMP)\nAn address that earned 200.00 of interest gets 31 COMP (vs. 30.60 COMP)\nThe address that earned the maximum interest (3271357.72) gets ~175 COMP (vs 748 COMP)\n\nHere is the document:\n  \n\n      docs.google.com\n  \n\n  \n    \n\nCopy of COMP Airdrop Distribution Manipulation 48\n\nSheet1\n\nRAW DATA:,https://raw.githubusercontent.com/0xA1176ec01045/CompEarlyUserAnalysis/main/csv_results/TotalUSDInterestByAddress.csv\naddress,Total,COMP rewards\n0x58cCC12eB2bC42c41E6232547a694106e85cC4C2,0,0,The formula:,An Addy with 0.00 interest...\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nLet me know your thoughts!\\nThanks for putting this together. This step is crucial in turning hypothetical discussion into reality.\\nI like this distribution model as it targets the \"middle \" more effectively which will achieve the goal of decentralizing the protocol. In my opinion getting voting rights to users who legitimately engaged with the protocol is top priority. Distributing 5 ETH to 150 addresses is much more effective than 1 address getting 750.\nOmitting addresses who contributed trace amounts also helps with this IMO for the reasons you mentioned.\\ni lend my eth at comp… but that time they only offer 0.01% interest… of course i eligible to get 0 comp LOL\\nI would argue that even at 0.01% interest (was it really that low for the whole duration of the loan?), one would have to lend 1000$ for ~40 days to reach 1 cent worth of accrued interest. A penny of interest is really not that high a treshold imho, but this is worthy of discussing.\\nEarly users should be defined as anyone that genuinely interacted with the protocol prior to the COMP announcement. For example, Uniswap gave UNI to anyone that made a trade on their platform prior to a certain date.\nExcluding genuine users that either did not deposit enough funds to earn an arbitrary amount before a certain date would disservice to those early users. I argue that if any user of compound deposited any non-dust amount of funds for any non-negigible amount of time prior to the announcement is an early user.\nExample: Alice deposits 1 ETH one week prior to the announcement, but doesn’t earn a full 0.01 during that time due to low interest rate. Alice still holds her Compound ETH today. Isn’t Alice an early user that took risk with her capital to engage with Compound?\nI urge the group to prioritize COMP distribution to all early genuine users, not just the extremely early or very wealthy ones.\\nI just want to add a thought to this discussion as a new-ish user of Compound.  The fact that these discussions have been on-going for so long without any real advocates from large comp holders or developers points to that this endeavor is a fools errand.  The Compound team did not think to air-drop early users when the token was launched and as such, we should be forward looking.  Why should I want to acquire Comp (either thru deposits/loans or on the open market) if I am just going to be diluted thru air-drops instead of spending that Comp on forward looking contributors  and protocol improvements.  We could also sell that Comp to capitalize protocol reserves.\nIt’s my opinion that we must be forward looking.  Early adopters were rewarded with a larger Comp rewards from their activities with the protocol due to number of assets on the protocol.  Anyone who felt they did work that they felt they should be paid for should submit a proposal outlining why they deserve it and what they would like to contribute on in the future (i.e. Getty).\nThis protocol has a lot of potential to be a real force for good and stability in the DeFi world and I think a large risk is us porting over too many bad ideals of the legacy finance system.  With all these talks about backward looking hand outs and bailouts for overleveraged parties (yes, sometimes Dai does trade at a strong premium because it’s exchangeable for real assets in the market), I worry that maybe this project will not be able to mitigate the effects of human greed and psychology through decentralization and code.  Anything we do now will set the precedents for the future of Compound.\\nFully agree with this. I think the cutoff separating “dust” from “non-dust” should not be the arbitrary 0.01, but likely something smaller than that. What would be clear examples of non-genuine users in your opinion? One that comes to mind is \" A wallet that has held a position for only one block\".\\nSince you are a new user I think you should first read the threads on the forum about the topics you are commenting on.\n\n\n\n eggbagels:\n\nEarly adopters were rewarded with a larger Comp rewards from their activities with the protocol due to number of assets on the protocol\n\n\nMost early users were not rewarded with “larger” Comp rewards. I don’t know how you came to that conclusion. Its probably just your personal assumption which can be dispprove by checking the data on Etherscan.\n\n\n\n eggbagels:\n\nWith all these talks about backward looking hand outs and bailouts for overleveraged parties (yes, sometimes Dai does trade at a strong premium because it’s exchangeable for real assets in the market)\n\n\nObviously you are not well informed about this topic, so you should read the complete thread and look at the situation objectively and maybe your “arguments” would make sense.\n\n\n\n eggbagels:\n\nAnything we do now will set the precedents for the future of Compound\n\n\nEarly users took the risk and used the protocol without any guarantee that they would be rewarded. These users are also one of the reasons why you can talk today about the future of the Compound protocol.\\nI’m not particulary motivated in discussion on that topic, as it feels to me like beating a dead hourse. It’s mostly year-long whining about how much airdrop is deserved, but fine, I’ll share my thoughts yet again.\nFirst of all, nice job with airdrop-emulator.\nLet’s start from the beginning, to see what we currently have and what we trying to achieve here. Let’s refresh memory on how it started:\n2,396,307 COMP have been distributed to shareholders of Compound Labs, Inc., which created the protocol\n2,226,037 COMP are allocated to our founders & team, and subject to 4-year vesting\n372,707 COMP are allocated to future team members\n4,229,949 COMP are reserved for users of the protocol\n775,000 COMP are reserved for the community to advance governance through other means — which will be announced at a future date\nAs you can see, while users of protocol have about 42% of total supply, all this COMP where scheduled for linear distribution over 4 year period. So vast majority of voting power stayed concentrated in hands of shareholders, founders and team.\nAnd this isn’t a bad thing by itself. Compound governance mainly controls important protocol parameters, which can’t really be adjusted by average user without good understanding of not just protocol functions, but often how economy, broad markets\nand even tokenomics work. And adjustments to protocol often require coding knowledge, ability to write code, test it, audit it prior than putting it into governance. That is the task many just can’t achieve. It really does make sense that Compound governance require substantial amount of voting power to be able to create proposals. (And i not even completely convinced that lowering it to 65k was such a great idea) And what is maybe more important, Compound governance works. It’s functional mechanism, not just some instrument of creating community polls.\nSo, that 4,229,949 COMP are those, which are currently disributing to users of protocol with aprox 3,500,000 remaining for distribution at the moment. Approach was quite innovative and not really far from being called groundbreaking at that time. Now, when many more projects followed it might not look as such, as there been some improvements to distribution model discovered by other projects. One of that improvements is so-called community airdrop. It’s something what Uniswap did later,\nand what Compound have not. One of the main purposes of community airdrop, wasn’t really to enrich masses, it’s more about creating engagement and strong community, aligned in long-term success of the project.\nAnd here we come to that community airdrop. First we are not in same position now, as if it was done when token launched. At that time you can call it zero-value governance token. But now it’s just not anymore. It’s traded on regulated exchanges and now is an asset which DO have monetary value, and is a taxable event as well. We not just talking about distribution of 500 000 voting tokens, but about distribution of approx 200 million of value. And much care should be taken.\nOk, so we are talking about distributing not just voting power, but a monetary value. But to whom? Well, since that tokens are taken from user distribution than it should be users of protocol. Then what is the problem with current distribution?\nWell, there are some.\nTo be clear, i’m big fan of capitalism. Not because it’s perfect (it’s not), but it’s pretty much best what we have now. So, there is nothing wrong with the fact that bigger capital have more gains. The problem is when it grabs vast majority, like almost all. And this is the problem with current distribution. If you are a user of Compound protocol, maybe even more or less active, using it for either personal or business purposes. Might be you have 5 or 6 digit value in protocol. But you still going to get scraps from that user distribution of governance tocens, as you massively diluted by big capital. And that is NOT a good thing.\nCommunity airdrop adresses that particular problem, decreasing that massive advantage big capital have to a degree. Not to the point of making everybody equal. But to shift some power back to community. That makes sense, as if we empower more users they eventually become stronger and more engaged players, they might be able to develop and pursue their own agenda, their values. WE need more of that both in broad society, and in particular communities, which share same interests and values.\nThat is what is about from my perspective, it’s about letting active users to collect fruits of their engagement. And the more you engage, the more it should scale.\nIt might be a bit long introduction, but it was kind of necessary, so we could look from same perspective. So, the goal of community airdrop, is to reinforce engagement primary. It’s not that much about decentralisation. In such a big project as\nCompound is, it’s not viable for everybody to be able to create a proposals left and right. We can’t achieve that no matter what. And it’s completely unnesessary, person who is complaining that he can’t create a proposal more often than not is not really able to propose anything meaningful. And actually work on implementing that. There are some people in community who do, and governance should be about delegation of power to that users, not about airdropping tokens to them anyway. (but that’s out of scope of current discussion)\nSo, our goal is to provide additional push for people who already pushing for something. I as well see no value for Compound to deliver anything to addresses with less than 0.01$ interest accrued. This are not users, they were never really used Compound, and i see no reason why should our power be wasted on anything like that. I would strongly advocate for zero, and if we feel charitable for some reason than 1 COMP is a max in my opinion what should be considered. It’s about 400$, which is very generous for doing literally nothing useful.\nSecond group is group from 0.01 to 0.5. Or might be even to 1$. These aren’t really an active users either. It took very little usage to get 1$ of interest, and that argument somebody presented, that he supplied ETH and iterest was low. Yeah, but\nyou not really supply collateral to earn 0.01% APR. That’s not even peanuts. You use that collateral to borrow something, and interest on stables was double digits for a very long time. So telling that somebody is active user of Compound and he not\nmanaged to earn 0.01$ in interest is damn LOL.\nBut ok, that’s so-so users, but still users. They kind of fit into description. I see airdrop of 5 COMP kind of appropriate to an extend (more about that later)\nAnd here we come to third, largest group earned from 0.5$ (or 1$) to whatever. That’s primary userbase, actual users who did used protocol. If we are talking about doing community airdrop, that is where most of COMP drop should go. And weighted by\ntheir interest accrued.\nThere is some nuance i want to suggest though. Last 2 addresses in the list earned amounts vastly bigger then anybody else. I suggest to exclude them from calculations. Not from airdrop itself, but from calculations. I suggest to distribute to them exactly same amount as third address from the end of the list recieves according to formula. And the reason is because it’s overall a community distribution. Interest earned is used as a measure of activity rather than size of capital. Big capital can still very much participate in ongoing distribution and isn’t diluted to same point as small users.\nNow, when we outlined main thoughts on every group and starting to talk about numbers i want to highlight that the idea to reward somebody for actions done in some distant past doesn’t really resonate in me much. And certanly it feels extremelly\nwrong to distribute 30 COMP to addresses which achieved only as much as 1$ of interest. Like 12k$? Are we even serious here?\nBut here is the concept of what possibly can be done with that list.\n\n\nFirst it shouldn’t be airdrop. It should be claim only, for any address, any amount. Big chunk of that addresses in list might be dead and not active anymore. We don’t want COMP to be send to dead address.\n\n\nI suggest to implement additional requirements for users to be eligible. For example, let’s say user address is eligible for 30 COMP according to the formula.\n\n\nThan, to be able to claim it, user is required to provide equivivalent value amount of LP tokens in Uniswap v2.\nIf it’s 30 COMP, then to claim it, user need to stake 30x400$=12k$ of COMP/ETH Uniswap V2 liquidity tokens, lock them in staking contract for 1 year, and than can claim his 30 COMP. Which, ideally, would become gradually unlocked and able to be\nwithdrawn linearly overly one year period.\nI hold very small amount of governance tokens personally. It’s not me who can decide how value, created by other people may be distributed. My main point is here: it’s not about charity, it’s not about giving away, and it’s not about decentralization for the sake of decentralization. It should be about empowering active userbase in doing something they already doing. Kind of fixing a little bit a disbalance in distribution, vastly diluted by massive capital holders. And i don’t believe that doing something in very distant past justifies giving away 10k$ of value.\nBut qustion is: Are we fine that vast majority of tokens dedicated for users going to go in to pocket of very few owners of big Capital? Because if nothing is done, that will how it will eventually end with current distribution mechanic.\\nSo I think as community we first need to take an approach that distributes COMP to all that have “used” the protocol. This way we get consensus from all.\nThe first step to me is to define a minimum COMP for the lowest usage and a maximum for the most usage, based on the Interest earned list provided by @allthecolors.\nRecommendation\nMinimum\nLowest Interest Earned = 1 Comp\nMaximum\nHighest Interest Owned = 500 Comp\nIf we can get consensus with these two starting and end points we can then discuss Point Allocation, Criteria on ratio estimates.\nBut if we can all agree on these two end points it would make the rest a little easier.\\n@gabrocheleau,\nVery nice! These numbers look good. When looking at some random addresses that earned $0.00 interest, it looks as though some of them are smart contracts in which distributed small amounts of cTokens to many other addresses. Some only kept the funds for a day, others for a week, but majority of them looks like something ‘fishy’ is going on… LOL!\nAlso, you’ll notice a lot of those addresses start with the same number:\n0x5D, 0x5E, 0x5F, 0x99, 0x98, 0x97, 0x96, etc.\nI wonder if there is a way to gather data on these addresses to weed out the addresses that deposited less than $5 or so. Or addresses that received cTokens via transfer, then were soon sent to another address. Maybe there is a way to search for contracts that are linked to malicious activity and we could select those accounts, like @allthecolors did with the Sybil attack.\nI’m thinking there are a lot more bad actors that made less than $0.10. Either way, I think that addresses in which earned less than $x.xx interest needs claim the airdrop. I hate to bring up how other protocols distributed their airdrops, but seems like the majority of them used a ‘Claim’ function.\\n@Sirokko, nice! This is what we need!\n\n\n\n Sirokko:\n\nAs you can see, while users of protocol have about 42% of total supply, all this COMP where scheduled for linear distribution over 4 year period.\n\n\nThe way the airdrop is distributed might be able to accommodate for that.\n\n\n\n Sirokko:\n\nIf it’s 30 COMP, then to claim it, user need to stake 30x400$=12k$ of COMP/ETH Uniswap V2 liquidity tokens, lock them in staking contract for 1 year, and than can claim his 30 COMP.\n\n\nThis idea has the possibility of either losing the amount of COMP that users should have received with the airdrop, or the chance of there being a surplus of COMP by the time it gets distributed. That depends on the ETH/COMP price being stable.\nI had thought of a distribution method that would really engage the early users of the protocol. Without simply handing out an airdrop to early users. That wouldn’t help Compound at all, especially if those users are no longer using the protocol (sell pressure/dumping) or those addresses can no longer be accessed (burning). We all agree, this is NOT what we want, right?\nSo, my idea is to distribute the COMP using the current reward mechanism, through earning COMP as interest.\nOnce the owner of an address who qualified for the airdrop chooses to claim the COMP:\n\n\n 1. Their address becomes blacklisted from earning the standard COMP rewards. \nThis should effectively increase rewards to Compound’s current users who didn’t receive the airdrop.\n\n\n\n 2. The 'airdrop' details need to be clearly displayed to users on their dashboard.\nCould appear on their dashboard where it usually shows their COMP rewards. Would display something like: “Earned 0.535 of 30.00 COMP”\n\n\n\n3. Now, we just have to figure out a rate of accumulation.\nThe following are just examples.\na. Match it to the current reward %, but in COMP value instead of USD value, it would be in COMP value – i.e. every $1.00 of earned rewards would be 1 COMP.\nb. Derive it from multiplying the current reward % by x.\nc. Use the interest that the user earned/owed, to get the airdrop. The airdropped COMP rewards will be unlocked as the user earns/owes interest and will be fully unlocked once they earn the same amount of interest as they earned as an early user.\nd. etc., etc.\n\n\n\n\n Sirokko:\n\nBut qustion is: Are we fine that vast majority of tokens dedicated for users going to go in to pocket of very few owners of big Capital? Because if nothing is done, that will how it will eventually end with current distribution mechanic.\n\n\nThat is the reason I created the spreadsheet, for community members like you to be able to change a few variables and provide your very own simulated airdrop. Maybe your simulation will be the one that gets the okay to go.\\nHow is 30 comp calculated ? I thought we have total of 50k addresses and 500k comp that puts about 10 comp per user ? Isn’t that true ? Thanks\\n\n\n\n gabrocheleau:\n\n\nAn address that earned trace amounts (< 0.01) of interest gets 0 COMP (vs. 2.5 COMP)\nAn address that earned between 0.01 and 0.15 of interest gets 5 COMP (same)\nAn address that earned 0.49 of interest gets 14.7 COMP (vs. 5 COMP)\nAn address that earned 0.50 of interest gets 15 COMP (vs. 25.28 COMP)\nAn address that earned 1.00 of interest gets 30 COMP (vs. 25.4 COMP)\nAn address that earned 200.00 of interest gets 31 COMP (vs. 30.60 COMP)\nThe address that earned the maximum interest (3271357.72) gets ~ 175 COMP (vs 748 COMP)\n\n\n\nthank u\nI agree with you\\nMy understanding (correct me if I’m wrong) is that it is based on @gabrocheleau copy of airdrop sheet, where his use of 30 COMP works out to around 500 000 COMP airdrop. It is around 32 560 addresses, but with his specific use, many of them get 0 COMP, and then there’s several values in between (under and over).\\nMakes sense. So we have the users who are eligible and emulator in place to play around and just a week span to prevent the grant expire. So what’s preventing someone to start a proposal and take this idea into fruition ? Thanks\\nFirst off, I’m guessing “whats the holdup” is just that I don’t think there’s reasonable consensus around the actual numbers. I mean, @gabrocheleau had some reasonable numbers, but I’m not sure it has matured enough yet.\nSecond, you’re talking about the grant expire. I’m not sure I see the issue here. The grant as I see it was to provide the data. The data is here. There’s no real urgency about it. Obviously this thread has been going on forever, but as long as there is progress I think there is no short term urgency.\nJust my 2 wei.\\nHey guys, are you available to listen to the developer call tomorrow? @gabrocheleau, @TragedyStruck, @bulajacky, @cryptobuddy_1712, @Fishbtc and anyone who I may have missed.\n\n  \n    \n    \n    Compound Developer Community Call – July 28, 9:30am PT Protocol Development\n  \n  \n    [CDCC July 28] \nOn July 28, at 9:30am PT, the Compound community will hold its next Developer Community Call. This will be an open discussion forum for individuals and organizations developing protocol improvement proposals or building applications and projects that integrate the protocol. \n\nHeld in the Compound Discord , with options for screen-share, voice, or text-only participation\nWednesday 7/28 at 9:30am PT, and bi-weekly after (Add the Compound Developers Calendar )\n\nBelow is the high-le…\n  \n\n\\nI’ll be listening in.\\nYes. I see no agenda item for this topic though…?\\n\n\n\n cryptobuddy_1712:\n\nwe have the users who are eligible and emulator in place to play around and just a week span to prevent the grant expire.\n\n\n@TradegyStruck is correct. I really appreciate @cryptobuddy_1712 's enthusiasm, but the grant expiry concern is a bit of a misunderstanding of how the grant is related to this broader initiative. The grant supported the early user interest analysis, and that work is complete; I will do my best to be responsive to any questions or concerns about it. This broader initiative continues beyond the expiration of that grant.\nThe only difference before v. after the grant is that until the grant is officially closed out, I won’t be weighing in on the present discussion of translating the data into a distribution. If this discussion seems to be converging on a consensus recommendation before that happens, that’s awesome; otherwise I might add my two cents to the discussion after the grant’s closed.\n\n\n\n TragedyStruck:\n\nYes. I see no agenda item for this topic though…?\n\n\nIt looks like a pretty full agenda, but it would be great to have someone involved in this discussion briefly jump in to update the dev community about the current state of the discussion, so that it can incorporate any questions or concerns from the broader dev community. I am not sure if I will be able to make it; if I can, I’m happy to bring it up, otherwise anyone else here is welcome to do so. I’ll post a note to this effect in the forum thread for the dev call agenda.\\nOh I see. Thanks for clarification on the grant. Just wanted to make sure all great efforts are not in vain. As some one pointed out as long as the progress is made probably we are good .\\njust do same amount for all eligible address, uniswap done with 400 each address, why not compound? much easy and take no more than week to completed\\nThat’s not how Uniswap worked. Uniswap had a capital weighting mechnasim based on amount and duration an address provided liquidity. What’s being discussed here is actually pretty similar to how the Uniswap airdrop was structured.\\n@samscalet @daver,\nIt is really besides the point of how Uniswap or any other project decided on how to do the airdrop. Those projects aren’t Compound. Those projects may or may not have had the huge difference in users, or the multitude of users who interacted in such a way to not really be users (Sybil attack/smart contracts designed to spread cTokens to make it look like there were more users/etc/etc). Over 50% or +20,000 “early users” had earned $0.00 interest.\\n\n\n\n CryptoCraig:\n\nThose projects aren’t Compound. Those projects may or may not have had the huge difference in\n\n\nI totally agree with you. Just pointing out a blatantly false claim.\\nUpdate re: today’s dev call. I summarized our progress and solicited input from the dev community. It’s important that we get this input periodically as plenty of key community members have limited bandwidth to follow this thread.\nMy sense is that the concerns about a distribution raised in the dev call – which are inclusive but not necessarily representative of the developer community as a whole – center on:\n\nSize of the distribution, both in terms of monetary value and in terms of COMP available to the comptroller for distribution\nOpportunity cost of distributing COMP to early users that could have been deployed for additional grants or other yet-to-be-determined incentives for driving adoption, hardening the safety and reliability of the protocol, etc.\n\nI hope we can continue to work hard toward incorporating these concerns in this discussion. We may be tempted to defend this initiative by going on offense against constructive criticism; but my hope is that any proposal that emerges will be responsive to the various concerns raised across the full spectrum of protocol users and COMP holders.\nOne suggestion raised was the idea of submitting an initial on-chain proposal that simply asks governance whether there is support for pursuing some flavor of early user distribution.\nI appreciate the suggestion and the accompanying good intention of avoiding unnecessary development work. I also recognize that there is an appetite in this forum for timely action toward an on-chain proposal. That said, I would advise against submitting a preliminary or temperature-check proposal through governance:\n\nEvery prior governance action has involved an on-chain task(s) for the protocol to execute. A proposal that just asks COMP holders whether to eventually hold a vote on this idea would deviate from that standard, treating this proposal initiative – which is probably at this point the most distributed, in terms of community participation and development, in Compound’s history – differently from initiatives coming from Compound Labs, its backers, and a small handful of all-star devs in our ranks (no shade intended, y’all are amazing!)\nWithout a specific distribution or mechanism laid out, never mind coded up, the proposal will be inherently more ambiguous than any prior governance action. Given the conservative approach governance has taken to protocol changes, the ambiguity of a temperature-check proposal makes it likely to fail irrespective of the merits or deficiencies of this initiative because a “yes” vote carries greater yet harder-to-measure uncertainty/risk than a “no” vote.\n\nIn short, I think the intent of the suggestion is in the right place, but for this idea to really get a fair shake, it should be presented to governance in a polished, well-reasoned, and executable format.\\n@allthecolors @CryptoCraig was unable to attend. Was there an indication of what volume of comp would be more widely accepted by governance?\\nThe discussion did not get that quantitative, so I don’t have a clear answer for you on that one. (@CryptoCraig was there fwiw, just a mic issue I think)\\nYeah, mic issues. Was using my mining rig, since the usual PC wouldn’t have been able to keep up with the other stuff I am working on currently.\\nWill also be voting no to this proposal. Like many have mentioned, many alternative more productive ways to utilize reserve funds.\nWhat you’d be creating is a possible divide between users going forward.\\n@getty @arr00 @allthecolors @CryptoCraig\nMaintaining comp distribution to the existing small group of comp holders creates financial and regulatory risk outlined in the SEC’s token safe harbor proposal.\nIf we don’t make a real and quantitative effort to decentralize governance we pose the risk of COMP being defined as a security and the protocol could become legally susceptible to laws under the security exchange act.\nNoting specific concerns to how network maturity is defined, quantitative measures and related persons.\nhttps://www.sec.gov/news/public-statement/peirce-statement-token-safe-harbor-proposal-2.0#_ftn2 4\\nAnother day in crypto. Dydx protocol released the airdrop and I must say the model is elegant. Dydx is also from the old day, an old giant.\nThan I really thinking again, why in earth is comp discussing an airdrop for almost a year. Yeah I know we lunched first the token and than do an airdrop. But I am tired to read everything. So can we conclude and move on?\\nBeen following the thread since a while and at this point am tired to check for an update . Hope there will be a final decision made soon to end the speculation. Just say yes or no and when if yes. Thanks for all the contributions.\\nGovernance in Compound is largely decentralized, hence the longer conversation. dYdX was not a DAO or foundation until just now, so the decision to airdrop required no building of community consensus. Interestingly, dYdX also chose not to recognize its earliest (V1) users at launch.\\nI would encourage everyone engaged in this discussion to also review the recent conversation on the future of COMP rewards and chime in about which ideas there they support, whether or not that includes an early user distribution. If you think this is a good idea, there is an opportunity right now to make your case on that thread.\\nI have some more thoughts on the distribution proportions, and while i still have a very little confidence that that would pass governance at all, but if we talk about formula, that my suggestion would be something like that:\nUsers, who earned 0 : 0 COMP (not a user, so no distribution)\nUsers, who earned between 0.01$ and <1$ : COMP = MAX ( (Interest earned),0.25)\nUsers, who earned >=1$ : COMP = 1 + MIN ( SQRT(Interest earned), A)\nWhere A is up to discussion, i’d suggest something like 10, because at the core it isn’t a capital distribution, but rather a community one, and capital here is just a measure of weighting participation. That would require much less COMP to spend, give noticeble rewards for majority of early users, but nothing ridiculous.\nCOMP which were initially suggested for that distribution, but not needed i suggest to spread via different community initiatives.\\nThe question on opportunity cost of distributing comp to early users vs using it for other incentives in concerning. If you are using comp as an incentive for other activities it calls into question if COMP really intended to be a governance token. It implies that you are using the perceived value of comp to fund activities rather than using it to actually govern the protocol. How would regulators would view conversations surrounding this issue if it were ever brought to court?\\nUnless I’m mistaken, Compound didn’t promise any rewards for early users, so in my opinion, there should be no airdrop. Early users chose to use Compound exactly as is and for whatever reason; they found the idea intriguing, they wanted to borrow, they wanted to make money using the protocol, etc. Maybe they helped the protocol and were valuable community members, but they chose to do so without a promise of compensation.\nNow, early users may not have received compensation for the early days, but that’s not to say they haven’t benefited from being an early user. Being able to shape the protocol and community, knowing about Compound before many newcomers (I’m one of them) - being able to purchase and accrue COMP at better rates, learning from the project, making friends, having a chill time, etc.\nFurthermore, there are still many opportunities to earn COMP (the grants program is more than generous!) and also to buy the token before DeFi goes mainstream.\nIf this goes to a vote, for these reasons, I will vote no to the airdrop.\\n\n\n\n Tyler Loewen:\n\nIf this goes to a vote, for these reasons, I will vote no to the airdrop .\n\n\nThe thing is it REALLY doesn’t matter how you and all users (early or not) together combined will vote. What do you think it’s democracy or something like that? All combined voting power distributed to users for all the time is about 1 mil COMP tokens. Founders, team and early investors control 4+ Million of COMP voting power. It can be discussed as much as everybody want, but the only people who can make decision about any sort of airdrop or distribution  aren’t users.\nEverybody is welcomed to tell their opinion and advocate for it as much as they want, but when it comes to decision making, that’s irrelevant. There is just not enough voting weight for it to matter.\nBut don’t worry, anything which is dragged and dragged over year, unlikely ever bear fruits.  We will probably will see second annivesary of that topic without any result. \\nI think I understand where you’re coming from @oniru, but I also think your argument would actually land more squarely against uses like the grants program than against a retroactive distribution. There is no question in my mind that the grants program uses “the perceived value of COMP to fund activities” other than “using it to actually govern the protocol”. Yet:\n\nThe grants program’s launch included funding for legal counsel to review regulatory questions associated with the program, and since it has moved forward, presumably no fatal issues were identified\nthere seems to be broad agreement in the discussion on the future of COMP distribution that the grants program is a good use of funds.\n\nAn early user distribution would, by definition, empower early users with a stake in governance corresponding to their early-user time and capital, so unless I’m missing something, I don’t think this is a convincing argument against a retroactive distribution.\n@TylerEther’s case is understandable to me and one I think we can agree to disagree on. I agree fully that early users are not owed anything from the protocol. At the same time, I think that COMP’s distribution model to date has nearly guaranteed that ordinary users will never have a meaningful say in Compound governance, and to me it’s a bit surprising how unconcerned most folks seem to be about this (I can hear @Sirokko making fun of my naivety here ). So @oniru 's concerns resonate with me, albeit with us arriving at different conclusions.\\nI dont understand this protocol sometimes. last time i checked comp was a governance token. The best way to decentralize the governance is getting it more into the hands of the people. Which group of people would be best receive it? The early users who helped shape the protocol.\nIf the whales are gonna just call all the shots why even launch a token. why even try to be decentralized. Should just go back to the team managing this whole thing then.\nOf course comp didnt promise any awards. But when you talk about you want more community engagement theres no better way to do it other than giving your day 1 supporters a voice. even if its a small voice at least its there. Wasn’t that the whole point of the token?\n\nCompound (COMP) is an ERC-20 asset that empowers community governance of the Compound protocol; COMP token-holders and their delegates debate, propose, and vote on all changes to the protocol.\n\n\nBy placing COMP directly into the hands of users and applications, an increasingly large ecosystem will be able to upgrade the protocol, and will be incentivized to collectively steward the protocol into the future with good governance.\n\nEvery other protocol doesn’t have a problem recognizing the community. Go look at dydx new token for example. direct quote from the blog\n\nThe success of the dYdX Layer 2 protocol is the result of thousands of community members who have been trading on the dYdX Layer 2 protocol and its predecessors over the past three years.\n\nUniswap\n\nUniswap owes its success to the thousands of community members that have joined its journey over the past two years. These early community members will naturally serve as responsible stewards of Uniswap.\n\\nComp grants were not available in the early days but those that are getting them now are benefiting greatly as is the protocol. Yet limited to those with code development skills.Those that wrote the initial code got paid in comp.\nDecentralized governance is required in order to keep this protocol alive otherwise the SEC will come knocking soon. Creating further comp centralization protocols and making decentralized governance proposals  harder to submit  will not help.\nBottom line you don’t want or currently need to truly decentralized governance, as you would not benefit from a decentralized governance.\nTreasury is the driver here. Big business taking over the defi sector. Sad day\\nComp going down the path of DINO (decentralized in name only).\\nI agree with everyone that more COMP should be distributed to users, but I don’t think an airdrop is the best way of going about it.\nIs an airdrop an effective way of promoting governance through the users? How do we know users still have the private keys of those addresses? What about the contract addresses? What about contracts that would be unable to use airdropped tokens? How do we know recipients will actively participate in governance? How do we know recipients won’t just sell the COMP as soon as they get it? What about the recipients who don’t have the time, energy, or money to pay for gas to be [educated] voters or to delegate their votes to delegates they support? Do recipients even want the responsibility of governing?\nWe need to know the answers to those questions (or at least a reasonable estimate) if we want to be effective at further decentralizing the protocol and give more power to users. If ineffective, the result will be that Compound will have less COMP to distribute to users and to grow the protocol and everything stays approximately the same as if no airdrop were performed.\nI’m very skeptical that an airdrop will accomplish what everyone is seeking here. I really don’t want to see this drag out for who knows how long to end up not being effective at accomplishing the goal of further decentralizing the protocol. Hence why I support building for our future rather than focusing on our past.\\n[quote=“TylerEther, post:417, topic:595”]\nIs an airdrop an effective way of promoting governance through the users? How do we know users still have the private keys of those addresses?\n\nmy thought is that the COMP distribution should be claimable function by wallet not a unilateral airdrop.\n\nWhat about the contract addresses? What about contracts that would be unable to use airdropped tokens? How do we know recipients will actively participate in governance? How do we know recipients won’t just sell the COMP as soon as they get it?\nMaking the COMP distribution a weighted time schedule based distribution with those actively claiming/vesting the allocated comp but willing to prolong comp withdraw, in my opinion does two things puts more COMP into the hands that actively participate and allows the protocol to reclaim unclaimed COMP.\nWhat about the recipients who don’t have the time, energy, or money to pay for gas to be [educated] voters or to delegate their votes to delegates they support? Do recipients even want the responsibility of governing?\nComp voter pools I think are key to enabling voter participation and/or voter delegation.\\nYou won’t ever know this unless they KYC or make themselves known. People in crypto love to be anon\n\n\n\n Tyler Loewen:\n\nHow do we know recipients will actively participate in governance?\n\n\nHave users claim it and if not it gets returned to the vault within a specific amount of time\n\n\n\n Tyler Loewen:\n\nHow do we know users still have the private keys of those addresses?\n\n\nthen just don’t send to known contract addresses would be my guess. I think we had some wallet makers in the chat earlier saying it will be possible for their users to receive the drop.\n\n\n\n Tyler Loewen:\n\nWhat about the contract addresses? What about contracts that would be unable to use airdropped tokens?\n\n\nummmmmmmmmmm vest it?\n\n\n\n Tyler Loewen:\n\nDo recipients even want the responsibility of governing?\n\n\nthen like I said earlier. have users who do claim it and participate. Vest it for a period of time but still give them voting rights.\n\n\n\n Tyler Loewen:\n\nIf ineffective, the result will be that Compound will have less COMP to distribute to users and to grow the protocol and everything stays approximately the same as if no airdrop were performed.\n\n\nIs it even going to users? 30% of it is going to yearn and getting dumped. That comp could go into the hands of users who want to participate.\n\n\n\n Tyler Loewen:\n\nHow do we know recipients won’t just sell the COMP as soon as they get it?\n\n\\nIsn’t a16z a big investor in comp? even they believe rewarding early users is the right thing to do in any project.\n\nFinally, we believe that token distribution models that reward bonafide early users and contributors are likely to create more engaged communities, and therefore more sustainable protocols. These are the types of users and developers who add value before a protocol achieves real network effects, or even has much inherent utility. Examples include the early users 5 and liquidity providers on Uniswap and the early developers 3 who built applications on top of Compound. We believe that these are the types of users who are likely to be the best long-term stewards of the protocol. While any token distribution model must take into account regulatory considerations, we generally believe that efforts to reward these types of early adopters are likely to position a protocol for long-term success, and we look to support them wherever possible.\n\nOn Crypto Governance - Andreessen Horowitz (a16z.com) 5\\nFeels like we are going 1 step forward and 2 steps back.\nPick a date same amount for all who interacted with the protocol(based on Should Compound Retroactively Airdrop Tokens to Early Users? - #232 by allthecolors)\nWe are coming up on 1 year with this back and forth, and too many chefs involved.\\nToo many people have too many opinions…\nfinal makeing nothing\nI dont know why who open the topic never reply and support it\\nThis is reflecting really bad to other communities. Compound is a great protocol and pioneered lot of ideas and still top 5 DeFI in the space. But not being able to materialize a plan since an year is really bad.  Hope some decision is made soon. No point in Continuing this thread forever. Not efficient utilization of time. It already raised expectations and curiosity amongst the community and from the recent developments it’s turning out to be a nothing burger which is disappointing. However, am still hopeful. Thanks\\nIf Uni,1Inch, Pooltoghter, Tornado and now DYDX can do it without issue.i cant for the life of me understand why Comp cant do the same…\\nLet’s be a bit more biased towards action. What can we do next for this proposal?\nCan we do a temperature check on snapshot for it?\nShould we do one for the cutoff date, based on @allthecolors 's work?\nShould we do another one for the distribution model: simple claiming, vesting, streaming, options.\nI guess with such a big community, the simpler approach should win - but let’s put it up for a vote and make sure this has the support of the COMP audience.\\n\n\n\n axevenor:\n\nCan we do a temperature check on snapshot for it?\n\n\nWe could, but I don’t think this would be beneficial.\n\n\n\n axevenor:\n\nShould we do one for the cutoff date, based on @allthecolors 's work?\nShould we do another one for the distribution model: simple claiming, vesting, streaming, options.\n\n\nAll of this has also been discussed…\\n*****  We need someone who can reach out to the larger COMP hodlers in the hopes that one or more of these hodlers supports any kind of airdrop.\n\n\nIf a COMP hodler likes the idea of an airdrop, then we should consult with them on what that airdrop should look like. After we work out the details, we will need to get them or someone else to create the proposal. Then the large COMP hodler could initiate the proposal for review and then voting.\nI would imagine that many smaller hodlers will probably delegate their COMP tokens to the person who made the proposal possible (and hopefully before the proposal gets to the vote stage). The majority of the smaller COMP hodlers will more than likely need to either vote or delegate their votes for this to not be a huge waste of time. I also think that every early user address should have a single vote as well. But my experience so far, is saying, why did you even just write this? LOL!\nI keep thinking about the COMP that will be coming back to Compound’s Reservoir contract in November from Coinbase as possibly the best option to use for the airdrop. Since the 500,000 COMP sent to Coinbase Earn could have been drained anyhow, but it appears it isn’t. During a dev call, @rleshner had mentioned how many COMP Coinbase has yet to give out, I wanna say maybe around 180,000 COMP have yet to be used by Coinbase’s Earn program.\n\n\n\nOn the otherside of the road, we cannot find a large COMP hodler who supports any type of airdrop, I think the chances of an airdrop happening REALLY falls off.\nThis would not only be like “awe man” for the early users, but could end up being a HUGE blow to Compound in the near term and in the long term. Just look at this thread! Majority of people on this thread is in support of an airdrop, so word is gonna get out. This would be news. It could potentially reach crypto news outlets like Cointelegraph, The Block, CoinDesk or even Bloomberg, Forbes or CNBC. Or those of us who are already content creators for media outlets could just publish a write-up explaining the situation.\n\nIn case you didn’t know… Quotes from the COMP Distribution Update a while back:\n\n“In total, 5,004,949 COMP will be distributed to the community — just over half of the total supply.”\n\n\n“In June, we designated up to 500,000 COMP for distribution through Coinbase Earn. COMP tokens will be available through Coinbase Earn until November 2021, at which point Coinbase will transfer all remaining tokens to the Compound protocol’s Reservoir contract.”\n\\nno any result at all\\nI feel more sad each day passing by…\nI mean… has anyone give to proper attention to this !?!\n\n\n  \n    \n    \n    Project Galaxy - Helping to build Compound's first NFT-based Loyalty Campaign! Ideas\n  \n  \n    Hey guys! This is Pokka from Project Galaxy. Project Galaxy is a NaaS (NFT-as-a-service) infrastructure that empowers communities with gamified loyalty systems. We believe that community management is key to decentralized organizations and NFTs can be a perfect medium to power on-chain achievements and credentials. After talking with Calvin Liu, we want to share our plan here with all of you. \n\nSorry for being wordy because this proposal might take a minute for you guys to go through. Therefore,…\n  \n\n\nI don’t even care anymore about $comp… i just wanted the NFTs !!! \\nI kinda knew that the only way of pulling this off is to do a simple equal distribution to everyone that interacted with COMP contract before a given date. Period. Uniswap style - simple & easy.\\nAnother day another airdrop. Dydx airdrop. No progress with comp airdrop though.\\nGiven recent sentiment among some larger COMP holders around this idea, currently I think the momentum on this initiative is best framed as part of the ongoing conversation about evolving the protocol’s strategies for distributing COMP 53 to support its health, safety, and growth. I’ll circle back here with my thoughts/suggestions on next steps once that thread has had some more time to develop.\\nThank you allthecolors for the help you provided which permitted to catalyze the wilingness of thousands to decide of an allocation to early users closer to the actual proposal. Since we are reaching close to the endof the more technical discussion over how to balance the allocations (number, vesting, etc), which I would be glad to discuss in an other message, I would like to come back to the core of the approach and recall the most relevant arguments that I have read in and around this discussion.\n1)Compound own much of it’s success to it’s early users\n« Compound owes much of its success to the thousands of community members that have joined its journey over the past two years. This is true particularly of the early users who provided liquidity (both as borrowers and lenders) before there was any monetary incentive to do so and of the early developers who opted for building on top of Compound before the launch of the COMP token. » by Alive\n«-I would go even more drastic about retroactive rewards,\ncompound v1 started around Sep-26-2018 and COMP distribution started at Jun-15-2020 that means that early users used Compound for at least 2,5 year.\nif we would want to be totally fair and give equal rewards for the users before COMP distribution started then 30.7% » by blck\n2)The allocation toward early users will permits to strengthen a community and will participate to Compound long term success\n« Crypto has always been about building networks that are owned and operated by their respective communities.-Crypto has always been about building networks that are owned and operated by their respective communities. » by Alive\n« -If there is any group of people that is most likely to be aligned with Compound’s long term success, it’s that group. » by Alive\n«-Giving people a vested interest in Compound is a way to acquire the human capital we desperately need. Think of it as a multi-million dollar marketing campaign. Early adopters of Compound are a logical choice for the target of this campaign. They already have a connection to the protocol, and an airdrop may encourage them to become more involved and more connected. » by arr00\n« Firstly you should get that getting COMP into the hands of dedicated early users who demonstrated strong interest in the protocol itself, rather than into the hands of whale farmers who don’t care about anything but yield, is great for comp and for comp investors. » by Andre1\n« -Finally, we believe that token distribution models that reward bonafide early users and contributors are likely to create more engaged communities, and therefore more sustainable protocols. These are the types of users and developers who add value before a protocol achieves real network effects, or even has much inherent utility. Examples include [the early users ] » by Pollon\n« -We believe that these are the types of users who are likely to be the best long-term stewards of the protocol. While any token distribution model must take into account regulatory considerations, we generally believe that efforts to reward these types of early adopters are likely to position a protocol for long-term success, and we look to support them wherever possible. » by Alive\n« -Protocols that achieve meaningful decentralization are more likely to gain long-term adoption and sustainability than those that do not. While the potential design space of governance is massive, we remain guided by this basic principle, and support initiatives that we feel embody it » by Pollon\n« -The addresses that receive COMP should be early users that risked their time and capital to establish the protocol. » by rleshner\n3) There is an extreme excitement and support around this proposal\nI am impressed that we are here in the most active discussion, despite we are approaching a year of discussion and not much support from the core developpers, 29k views. However the support of the founder himself helped to obtain the compound grant which acted as a successful tool to help organize the research making it soon possible to apply for a proposal. In the same way the twitter polls launched by the founder with 1202 participants showed a 69% of voters in favour of an allocation of Comp to early users.\nThere is an obvious wish from a large part of the community to, when we would move toward a proposal, vote in favor of a non-0 Comp allocation for early users. Even if the interest of a few holders might seem to diverge in the very short term (which I do not agree with), we can appreciate that a16z, one of these holders and one of the biggests Vcs in Defi, is pushing throught this topic in favor of allocating a part of the Comp toward the early users, forgotten by the current distribution system until now. The longer term view should win against some shorter term interests of a few.\nI think the question of an allocation for early users is independant from the other topics and it would be wise to move toward a vote now that we got all the datas provided by allthecolours throught the grant.\nWe could for example start by voting over the total number of COMP which should be allocated toward early users, starting from 0 (current distribution without allocation for early users)\nAnd secondly we could vote over the specific way this allocation should be granted, in a balanced way between user and capital which will have to make consensus to permit it to proceed.\\n@allthecolors - I feel so sorry for all the efforts you put together for this activity.\nBased on how things went so far (in last 10 months), this is a clear no-go. Inspite of clear support from majority of the community, opposition from 2 or 3 influential leaders has destroyed this. So lets atleast put everyone out of their misery and get closure on this topic\n@alive  - Curious to know what you think of this entire failed exercise. It was nice and fancy to see lofty ideas being discussed in a16z marketing materials (Crypto Startup School 3), and wake up to this reality.\\n\n\n\n allthecolors:\n\nGiven recent sentiment among some larger COMP holders around this idea\n\n\nThis entire idea was put to discussion, so that the protocol is decentralized and a few large COMP holders wont even let this go to a snapshot vote…what an irony !!!\\nSo the entire proposal and optimism from whole community is going into vain just because of few large comp holders ? Then how is it decentralized? Doesn’t SEC be after comp then ?\\nof course, it’s a constant problem when deciding on something. Currently, Compound acts more as a centralized financial institution than as a decentralized protocol (they use marketing term: “user-owned protocol”).\nI don’t understand why users who use the protocol for 2-3 years wouldn’t get a “governance” token? Probably because it’s worth $ 400, if it happened to be worth a few cents, wallets would be as full for everyone as on the BSC network.\nIf nothing else, at least it would be a nice gesture if some VC representative would present his views on forum.\nTheses they present in their articles and what they talk about in podcasts is just one big promotional campaign.\n@allthecolors I appreciate your work, you really put a lot of effort into this thing\\n\n\n\n dabar90:\n\nIf nothing else, at least it would be a nice gesture if some VC representative would present his views on forum.\n\n\nI tagged @alive in an earlier post to atleast comment, given the idea originated from him. I don’t really care if this proposal passes or not, but i will be pissed if a16z continues to market themselves as supporting decentralized communities, after this shitshow…\\nReally sad to know about the exploit. Wish they rewarded the comp to retroactive early adopters instead of loosing it through exploits. Really bad reputation on these 2 events. Hope they correct it and bounce back soon.\\nI believe you owe it to those early adopters for where you are today and im proud to be one of them. 5% seems generous but if you really look at the amount of money that those early adopters added to the current market growth in terms of % growth you would see that compound has grown tremendously because of these users.\nI propose 10% to for these 5000 wallets so 200 each as an incentive as they have stayed from the beginning and its because of them compound finance managed to claim its current position today.\\nAgree, nice airdrop for lucky winners. eligible for aidrop are users who supplied “0 COMP distribution” tokens on protocol?\\n@rleshner  - When there is a f*** hack, compound is a decentralized governance community. But when majority of early adopters were planned to be rewarded - its 2-3 centralized individuals who can say NO to a retroactive airdrop for a proposal fully supported by community without it even going to a vote. What an irony !!!\\nA bunch of opinions for retroactively airdop… dont want to do.\nWhat should be done? What should not be done?\nnow everything is sxxt\nits really disappointed at all\\nBecause Compound protocol isnt decentralized governance community. It may become one day, but right now it’s just permissionless. We need to wait for the decision of the “big players” on the last problem, so we will see what other decentralized component protocol has.\\nAgreed. It can only be decentralized some day in future, if COMP is better distributed to community and one of the easiest tools we have is to airdrop tokens to early users.\nIt’s not like big holders are against this, as proposal was started by @alive.\n@allthecolors  - could you do us a favor and tag those 2/3 individuals who opposed  and literally shut this down without even a vote. I (and the community) are very curious to hear their viewpoints\\nThe one who has the biggest voting power in COMP is the one who create this thread, the founder himself several times voiced in favour of an allocation.\nI am asking, after all the work that was provided and the board, what is really blocking us back from making the proposal right now ?\n@blck said the community should come at least with the full adresses history of early users, done with the work of allthecolors\n@rleshner asked one a vote the opinion over an allocation for early users, which was widely in favour\n@alive asked the feedback of the community over an allocation for early users, I think a lot of feedback have been given by now…\nSo, we only need one, one person with enough comp, who either think the earliest users should have a non-0 Comp allocation, either, at least, think that this deserve to go on a vote due to the community support.\\nThe only way comp can restore some confidence and credibility after such an exploit and debacle is by rewarding its early adopters and proving its decentralized roots and ethics.\\nAgreed. This proposal should be passed. I was an early candidate to work at Compound before it was even developed. I’m sad to see early adopters such as myself ignored in favor of whales.  The best thing for Compound to do to mitigate their current press disaster is airdrop to historical users. I suggest doing the airdrop in vested tokens that can be unlocked via providing liquidity.\\nOur other option is to appeal/request newly minted COMP exploit addresses to help create a vote for us.\nOnce we have a vote, I am pretty confident all the work @allthecolors put together will pass due to community support, inspite of opposition from 2-3 individuals.\\nwhy creater dont do anything?\\n@allthecolors  - Can you share a little more light on the 2-3 individuals who are blocking this effort? i am genuinely curious to hear their view…and we all hate to see all your hard work go waste…\\nIf you search for the word “individuals” in this thread, you’ll find that there is only one poster repeatedly pushing this idea / rumor that there are “2-3 individuals blocking this effort”. It’s not true, and in my opinion, persistently promoting the idea of naming and shaming individuals for their perspective on a governance issue is misaligned with the community’s values.\nNo on-chain vote can happen until we have executable code for the protocol to implement, so it doesn’t really make sense to imply that there is some kind of vote suppression when there aren’t formal protocol actions to vote on yet!\nFolks who would like to see this idea move forward to a proposal can help in any of the following ways. I’m sure there are others, but these are the ones I’m aware of:\n(1a) If you are a solidity developer: implement a merkle root distributor that allocates testnet COMP to accounts using your preferred distribution model (see spreadsheet from @Cryptocraig posted earlier), deploy it on Kovan or Ropsten, and share it with the community for review and feedback.\n(1b) If you are not a solidity developer, you could do some outreach/networking to see if we can find someone interested in taking on the merkle root distributor project, either as a learning opportunity, or as a proposal for future grant support 1.\n(2) Read closely @getty 's thread on the future of COMP distribution 4 and the forum activity since the bug in proposal 62. It is a difficult period to discuss the protocol’s token allocation after losing control of a single-digit percentage of the supply, but those discussions will need to carry on, and if you believe that an allocation to early users should be part of the conversation, then your voice is needed in those discussions.\nBy the way, in his recent remarks at the Compound Grants Summit at Eth Global 4, @rleshner commented that the initial token distribution is among the handful of things he would do differently if he could go back in time and build Compound from scratch again. Folks who have expressed a disinterest or opposition to an early user airdrop understand the value of empowering early users. I guess I just want to emphasize that the world is not black-and-white and that there is common ground to be found among the different ideas out there on how to effectively allocate protocol-controlled COMP.\nIf we can get a merkle root distributor implemented and thoroughly tested, I think there will be a willingness among the broader community to discuss what non-zero fraction of the available COMP could be justified for this initiative. What do you say?\\nHey @allthecolors  - Big fan of your work on this. You don’t have to beat around the bush to point out out that a poster is claiming 2-3 individuals are blocking effort. That is me.\nI got this idea from your post from Sept 9\n\n“Given recent sentiment among some larger COMP holders around this idea, currently I think the momentum on this initiative is best framed as …blah blah”\n\nUntil this date, the community was engaging on dynamics of airdrop - what wallets or smart contracts to include or exclude in the criterion, and all of a sudden you get into a meeting with a grants committee and come back with this post that a few large COMP holders dont like this idea and kill this entirely. While i am a fan of your work, you have no right to stop this in its tracks and kill it completely, leaving us in blind on who these “large COMP holders” are. There is no conspiracy here…\nComing back to me, ironically i am not even an early user and will gain nothing if this succeeds. But my agenda is to see this put to a formal vote, and let everyone see how current COMP holders voted for/against it.\nThere are so many users like @TylerEther who already said they will vote “No” to airdrop and justified their response. I have amazing respect for folks like them. All i want to see is this being put to vote and coming to any reasonable conclusion (fail/pass)\nBased on your most recent post, if all it takes to put this to vote is some sort of merkle root distributer, let’s all focus our attention on it, and plan next steps.\\nAgree all we need is yes or no instead of dragging this topic for years.\\nwhen do the proposal\\nIn this context, it’s worth reading A16Z’s just released policy framework 4 for web3, which fervently makes the case for DAOs as a new and superior org structure.\nA16Z lays out a vision in which DAOs can perform basic functions of traditional organizations, including “pay taxes”. It also means that, in my view, to unlock their full potential, DAOs need to adopt and tailor the high-growth strategies espoused by Reid Hoffman in his book “Blitzscaling”, where capital efficiency is not the immediate goal.\nTo me, it’s a bit of a surprise that many DAOs (including Compound, as exemplified by this topic) are too focused on capital efficiency and not moving fast enough to capture the full potential of their platforms and protocols.\nIn the big scheme of things, awarding 100 COMP to each of the early users (5% of the total COMP supply) seems very reasonable, if it can even marginally create a stronger community. With decentralized protocols, there is no protection for intellectual property. What builds the moat is a strong community.\nAt a minimum, we should take a vote ASAP, in the true spirit of DAOs. Regardless of the outcome, this topic doesn’t deserve so much time and energy, as to be in the top-trending list for over 10 months now.\\nIt is really nice to hear rleshner talking about historical users, and that the first thing he would have done differently with COMP would have been not to totally forget them in the governance allocation schedule. However, no, I do not see why it would be too late to revise it, and it is still time allocate a non-0 amount to historical users. I would say even more, just watch that most of the governance activity of the forum is directed toward this topic, this will have to be solved once for all at a point, and this is by progressing toward a proposal, or would remain an anvil around the neck of compound for the next decades.\nThank you again allthecolors for your suggestions, your participations has been a huge help toward the creation of the proposal.\nI was studying the spreadsheet of Cryptocraig earlier.\nI wanted to try the parameters which would induce the simplest, least arbitrary distribution, a distribution which would fit the current way comp is allocated, however applying it toward historical users (capital weighted), with an adding to that a socialized allocation.\nHowever it was not possible because it seems that it is already applying a square root that can be only amplified, not removed.\nDon’t forget the allocation to historical users would already be socialized compared to the current distribution : the mere existence of a % allocated toward the socialized distribution (per address) already « penalize » larger users and decentralize the distribution. Let’s keep it simple and the least arbitrary to be able to reach a consensus, and try to follow what became highest standard in the communities.\nApplying some sort of square root in a way to socialize the allocation, would seem arbitrary, unwelcome by dividing the community (as suggest the replies on this page), and a duplicate(due to the fact part of the allocation proposed will be already socialized, per address)\nRleshner got a point there :\n\n« Third, users should be measured by usage of the protocol. For an interest rate market, usage is a function of capital over time; how other protocols have distributed tokens to users is irrelevant to Compound. This approach was piloted during the second community vote 13, and formed the basis of the COMP Distribution when it began for users. Luckily, in Compound’s case, this can be easily measured by interest earned, and interest paid. »\n\nSo what I wanted to study is the following : it would be actually quite simple, around 16 millions interests have been paid+earned in the period we take into account for historical users, 24000 contributed for a non-0 interest paid/earned.\nAll we would need to decide is the total COMP : T,that compound should allocate toward its historical users\nT=0…500k comp…1M comp etc\nOnce we get this T the canonical and non arbitrary way will be to split this number between the two types of allocations, firstly the capital weighted, measuring actual usage of the protocol, and secondly the socialized, per user. And these in a balanced way which needs to make consensus. (50%/50%) would seem the most natural choice, however pushing it to 60% socialized, 40% capital weighted, or the other way, would remain around the standards of the most successful governance distributions to historical users made in the industry. (but why not voting on it ?)\nSo for the example, let’s try it with 50/50.\nT : total allocation to historical users\nI : interest earned + paid by USER 1 during the historical period\nSo finally, the calculation of the allocation to the USER 1 is very simple and canonical since we now got all the data,\nFor the capital weighted allocation\nthe allocation of USER 1 would be\n\n(T/2)*I/[16 Millions]\n\nFor the socialized allocation, the allocation would not depend on the User, by definition\nand the allocation of USER 1 would be :\n\n(T/2)/24000\n\nwhere 24000 is the total number of addresses who interacted with comp (with a non -0 interest paid/received, note that maybe we need to add a few proxy users to that)\nTo conclude, however important is this possible distribution, I would agree with the idea of RogerS, we should go into a vote rapidly once we get the Merkle root distributor and call for help from all these who believe the historical users should not be allocated 0 governance tokens. The energy of historical users as well as of these who believed in the well founded of this future proposal and helped should at a point be preserved.\nAll the time and energy around this proposal would have been used toward other development if we could have get a bit more help to solve it before,\nThe initial allocation which didn’t grant any governance power of the protocol to historical users made/make the support of the founder, developers, Vcs and these detaining the largest voting power essential to pass this proposal.\nBut let’s not blame it only on others, maybe we could/can ourselves have done better to move faster… anyways it’s already amazing to realize that have now some good hope to submit and pass this proposal despite the initial flawed distribution which forgot us.\nSo lets focus on the next steps and try to progress as fast as possible to realize this future proposal.\nFor the non-devs, as allthecolors mentioned in (1b) I think what we can do to help is to ask estimate to blockchain companies for building this Merkle comp and submit it to the Comp grant, as this is the natural next step of the amazing and quality work provided by allthecolors, I will try this myself.\\nThis topic has been in the back of my mind for quite some time now, and my stance has changed over the past few weeks.\nDecentralization is important and the distribution of our token matters significantly. Talking to smaller voters and being a small voter myself, I understand how many users feel like their stance on proposals doesn’t matter when voting power is so centralized between a few entities.\nNot only this but there are also regulatory risks with voting power being so centralized. What happens when government agencies make demands of our largest voters? It’s a real risk we must mitigate.\nI think we could see more community and voter engagement if we improve our token distribution. Our community and users are ever so vital to our success!\nI will support this initiative and will help direct it so that this topic doesn’t get dragged on for many more months/years.\nI applaud everyone for all the hard work done on this matter! Especially @allthecolors!\nTo get this initiative moving forward, I think we need to utilize temperature checks. What does everyone think about me posting a temperature check proposal within the next few days - asking the question “Should Compound retroactively distribute tokens to early users?”\\nPlease do @TylerEther. Really appreciate it\\nReally appreciate you doing that. Please do it asap. Let us all make comp great again.\\n\n\n\n tgmed:\n\n2-3 individuals are blocking effort\n\n\nagree with you, maybe 2,3,4,5…but with enough COMP power. We will not pretend not to know who these “individuals” are, it is enough to read a few threads where the interests of users and early investors conflict.\nNo one needs to be called out, but the question is whether “DAO” is going in the right direction given that venture capital funds control the situation?\nSimply to protect their investment they need control and I don’t think they will let it go.\nAs for articles, shitposts and polls - we need to understand that this is marketing content (fairy tales).\n@TylerEther I wonder when you changed your mind, before or after the comptroller leak?\\nLooks like legal concerns is now swaying you opinion.\\nReally appreciate you\\nAny progress yet. Thanks\\n    \n    \n      \n      \n    \n    \n      \n        \n          AMA - Questions from the Community with Robert Leshner\n        \n      \n    \n\n\n\nRelevant info starts from 9:16, but i recommend to watch full AMA because you will get clear picture.\nMaybe @rleshner does not know that a time-machine is not necessary because he has enough COMP tokens to run the proposal\\nCould you summarize for benefit of others. Thanks\\n@TylerEther when you find some time, can you put this through a temperature check? Thanks\\nTemperature check proposal writeup: https://hackmd.io/@TylerEther/Skum_saIF 105\nHow does this look?\\nLooks good. Let’s hope for best\\nLooks good.\nwith a flow of easy words and in a deep\\nGood stuff! Time to get some wheels in motion and a proper indication of COMP-hodlers opinion with a vote.\\nLooks great. A temp check should be as thin as possible, while acknowledging that the thick will be found in the proposal. Nicely done. Thx\\nHow many people has Uniswap hired as a direct or indirect result of their retroactive airdrop?\\nLooks awesome. Thanks Tyler\\nHere’s my personal story.\nI started participating in the community by doing research during the DAI liquidation event of last year. I would summarize my contributions thus:\n\n\nFiguring out a considerable amount of the capital liquidated was concentrated on very few accounts that were also doing recursive farming Compensation Proposal: Distribute COMP to Affected Users in the DAI Liquidations - #23 by wario 4\n\n\nA compensation proposal that attempted to compensate regular users liquidated without compensating “recursive farmers”\nCompensation Proposal: Distribute COMP to Affected Users in the DAI Liquidations - #73 by wario 5\n\n\nDuring the course of the research I developed a simple public dapp that anyone can use to get an idea of what arbitrary accounts have been doing on Compound https://mariorz.github.io/compcharts/ 5\n\n\nThe research was done because I thought it interesting and useful, not considering there might a grant possibility. But members of the community approached me about receiving a retroactive grant when the grant programs was first launched. This was unexpected but well received on my side of course. However I did not hear about this again and in the end grants where awarded retroactively to other contributors.\nAfter this I asked on the #grants channel on Discord if I should apply for a retroactive grant for this work. And was told by @sukernik that retroactive grants were something to be avoided in his opinion (even though they had been awarded previously and will probably be awarded in this instance as well).\n\nTo make matters worse, recently passed proposal #59 9 ended up bailing out the industrial recursive farming operations investigated, only 2 of these operations receiving the great majority of the compensation when most of the community was not paying attention.\nLooking back I think that perhaps some important interests were not aligned with the investigation into what the bailed out industrial farmers were doing. Needless to say, the whole affair has been a disincentive to participation in the protocol. Just to be clear, I will not receive any rewards given the dates proposed, but still think the idea of retroactive rewards is something to support.\\n\n\n\n wario:\n\nLooking back I think that perhaps some important interests were not aligned with the investigation into what the bailed out industrial farmers were doing\n\n\nWhen you looking at the initial COMP distribution and the structure of further COMP distribution to “users” where the parameters are set in favor of the account with more capital, it actually becomes clear whose interests was aligned. Initial 50% voting power and distribution system setup which favors recursive farming puts VC funds and related individuals in a position that they simply cannot lose control of the protocol.\nAlso, although DAI liquidation event and comptroller leak bug are  clear errors of protocol, comparing the reactions of “majority protocol owners” after these unfortunate events can be inferred whose interests are a priority.\nAfter the error in the DAI liquidation, the liquidated users were blamed for poor risk management (tactical move), while after code bug event we could see crying and threats.\n\n\n\n wario:\n\nrecently passed proposal #59  ended up bailing out the industrial recursive farming operations investigated\n\n\nOf course, they have to cover their losses and due to the loss in stablecoin this case could be resolved for a year (stablecoin is stable after year), users whose liquidated positions were in ETH this delay made huge losses (tactical move).\nI am very sorry for your case because I followed your work and I think you definitely deserve reward.\nI would say that Compound has entered in governance crisis, but realistically governance has been illusion from the beginning.\\nLooks like voting started\n\n  \n      \n\n      compound.finance\n  \n\n  \n    \n\nCompound 24\n\n  Compound is an algorithmic, autonomous interest rate protocol built for developers, to unlock a universe of open financial applications.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nIf anyone has contacts with groups like ‘blockchain at berkeley’ who are voting ‘No’ to this. Please invite them into this thread. Maybe we can understand their reasons and clarify why this is good for protocol.\nBut ya, like i said before, even if this vote fails, i will be glad this sage is over for good. It will be a terrible look for compound governance, but it cannot get any worse than it already is.\\n\n\n\n tgmed:\n\nbut it cannot get any worse than it already is\n\n\nwith unaligned interest between users and early investors can be worse. i.e. with this gas fees, who can afford to vote on proposal? Regardless of the airdrop prop result, a solution needs to be found to the current governance problem.\n\n\n\n tgmed:\n\neven if this vote fails, i will be glad this sage is over for good\n\n\nagree\\nDo these temperature checks have a minimum quorum? how do we know if this passes/fails?\nIn terms of gas fees, top 30 voting addresses are all know entities who can afford to vote (inspite of gas fees). We should request all 30 of them to vote yes/no as this clearly shows which way they lean\\n\n  \n\n      twitter.com\n  \n\n  \n    \nLeighton \uD83C\uDF0A\uD83C\uDFC6 12\n@lay2000lbs\n\n\n  User control and ownership is a core principal of web3. \n\nThe COMP token launched PRIOR to any retroactive airdrops happening. Meaning, early Compound users were true organic community. This is an excellent user group to distribute control to.\n\n\n\n  10:57 AM - 5 Nov 2021\n\n    \n      \n        \n      \n      2\n    \n\n    \n      \n        \n      \n      1\n    \n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nPool together stepped up for the yes\nI agree, the writeup from Tyler is very open : “in at least some amount and/or way”.\nConsidering the community enthusiasm around this proposal, the polls, the work and research done, I hope we see every major governance actor step up FOR, thinking about the author of this thread who own more than 10% of the voting power of the protocol, and voiced in his initial post in favour of a non-0 governance allocation toward early users, the founder who participated in this thread as well. Now seems to be the moment or never\\n\n\n\n tgmed:\n\nhow do we know if this passes/fails?\n\n\n\n  \n      \n\n      compound.finance\n  \n\n  \n    \n\nCompound | Governance 45\n\n  Compound is managed by a decentralized community of COMP token-holders and their delegates, who propose and vote on upgrades to the protocol.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nSaludos.-\\nThanks to pool together and others for stepping up the game. We are close but lower than votes for against. Really not sure who is against this proposal which only does good.\\nIf you want to vote with out gas you can use https://comp.vote/ 37 it alows you to vote by signature\\nanyone knows who ‘blockchain at berkeley’ is represented by? 50k votes to ‘No’. Did they make their position clear on why they are voting No?\\nhttps://twitter.com/CFrogE1/status/1457049632811401216 56 Finally some momentum on twitter with influencers bringing attention to this\\nwow so much against this proposal… should ask first guys to vote since he voice his support for this airdrop\\nRemember majority of community is for proposal.\nThere are 3 voters who voted ‘no’ so far.\nOne of them is @getty . He responded to someone on twitter with thoughts on why he voted ‘No’. He does not believe users who used protocol early deserve to get an airdrop. He is building his own https://gfxlabs.io/ 13 and if he sticks to his philosophy, he will not airdrop and figure out better ways to drive value in ecosystem. We need to respect him for stating his opinion and judging him if he fails to follow his ideals in his own ventures.\nThe other two - “Blockchain at Berkeley and columbia” are just plain bad actors in this space. Bunch of 20 year old’s who have no respect for protocol and trying to safeguard their own positions. Multiple people who are on some of their telegram alumni groups mentioned this as a running joke. I don’t want to jump to conclusions, but it is what it is.I am not questioning their vote, just their lack of justifying their response the way getty did.\nBut honestly, the real people who are letting down this community is not those 3 individuals/groups who voted ‘NO’. Its those people who are refusing to vote on this protocol. This includes @alive who started this whole idea, and @rleshner who talks about this topic at every event but refuses to take action. Don’t even get me started with other VCs such as polychain capital who control 300k votes and voted on 0 proposals so far in history of governance.\nSummary: This thread has ~500 responses and ~$30k views (highest by any stretch in governance forum, and its not losing because a bunch of selfish university blockchain groups have voted ‘No’. It is failing because @rleshner and @alive and other VCs who control 70% of governance votes now fail to use their vote where it matters to decentralize this protocol. I will be extremely happy if these individuals come and vote ‘No’ instead of hiding behind the scenes. Shame on a16z and other VCs in this space.\\nI agree, I think as a community we have tried everything possible to get the larger wallets to voice an opinion.  I think we know why they are not voting and not voicing an opinion.\n\nIt shows all major COMP is held by VCs or Individuals\nIf they vote WITH the project they dilute their value and loose money\nIf they voted against the project it would send a even clearer message that they don’t want to dilute COMP at all and are happy with its current centralization.\n\nLet us remember COMP did a public launch of a Uniswap LP pool with a large amount of liquidity for sale at once the price rose to 100$ in hours, these whales didn’t earn their COMP through farming.  So they got an advantage early in the COMP token release.  I personally think this is bad OPTICS for these big actors.  They know we can’t pass a Vote because we have no COMP because we didn’t acquire it the first day it was trading.  You don’t let people get Grants and funding and actually put time and energy into something you know won’t pass.  That is just not the spirit of crypto in my opinion, the fact we know who the whales are and they are voiceless is the worst part of this.  Just say NO lock the thread and move on 9 months ago.  I don’t see any point in continuing this discussion after the TEMP check lack of voting.\nIf the Major Holders won’t even compensate people for protocol failures(dai liquidation, comp reward issue), I wouldn’t hold our breathe they will vote Yes on this.\\nSad to see we lose . But that is how it has been since decades. Rich get richer , poor get poorer . Top few % control majority and decentralized protocols are no exception unfortunately.  Probably Nothing. WAGMI some day. Thanks everyone for their sincere  effort.\\nwhy that guy been working and supportive on this idea before not voting at all? are they just making fun of us\\nHere is what we can do next\nEvery time @alive or a16z tries to market decentralization as a goal - let’s show this thread as a reminder on double standards.\nEvery time @rleshner pulls crap about how he feels guilty about not supporting early supporters and its only thing he could done better, let us show him this thread and ask him to **** up\nThis fight is just getting started. As i mentioned before, this fight is not against those who voted ‘No’. This fight is against those who refuse to vote, so that their positions are not diluted.\nThe next time asks for community support to justify to SEC this protocol is decentralized, lets us abstain from voting. Let only top 30 votes vote for themselves with 70% of votes.\nOn a positive note, lets not forget gems like @lay2000lbs (from pooltogether) who went out of his way to show his support. What he did was not a token gesture, that was pure respect to community. Let’s show our support to these guys. And of course @TylerEther for doing the right thing and getting this to a vote and @allthecolors for his hard work.\\nI agree with you, a mistake was made in the initial distribution of COMP tokens where early investors and the team got 50% of the max COMP supply and the other 50% they picked up by recursive farming. Given the strategically controlled governance structure, it is difficult to assume that a third party was allowed to access the “governance token” in an easy way (stablecoin farming would probably be disincentivized, but isnt, why?).\nCommunity members (individuals) who have expressed their views non-anon should not be called out or lynched on the basis of their decision, but the stated reason for that decision is worrying.\nAs for these two state institutions with 50,000 COMP, I would be surprised if they voted “for”, because they actually represent one of the reasons why we want a decentralized decision-making structure.\nGiven the apparent opportunistic behavior and such statements, what are the long-term goals of the protocol? On many examples so far we have seen that a community cannot be bought with $, only IPO.\\ni was right a year ago)\\nWe shared some of our thoughts here:\n\n  \n\n      twitter.com\n  \n\n  \n    \nBlockchain at Berkeley 14\n@CalBlockchain\n\n\n  1/ We have voted AGAINST Compound Proposal 67, which is a temperature check on whether Compound should airdrop COMP tokens to early users of the protocol. \uD83D\uDC47\n\n\n\n  2:33 PM - 7 Nov 2021\n\n    \n      \n        \n      \n      5\n    \n\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nIt’s not an easy decision for us as well. We as an organization don’t hold any significant allocations of COMP tokens - only voting power - and our goal is to make Compound as decentralized & open as possible. Efforts like https://comp.vote 1 and other community initiatives to increase access have been the best way to do so - and we hope to improve access in this way as well.\nHowever, our main issue with a one-time airdrop (in any form) is that it doesn’t incentivize medium to long-term engagement in the protocol - we need effective systems in place that directly reward holders who participate in governance and not just dump their tokens/not participate in governance. With those in place (i.e. governance reward mechanisms to those with “real contributions” - however that is defined), rewarding early users could make sense - but without them, we see little advantage to an airdrop in comparison to using that funding to decentralize the protocol more at a high level.\nRewarding governance participation isn’t easy, but it is a necessity to the future health & sustainability of Compound for us to work on the best way to do so.\\nCompletely agree with everything Ratan said, but also would love to instead investigate some kind of rewards system for successful proposals or finding bugs in currently active ones. Active governance is extremely important for keeping the proposal safe and innovative.\\nThank you for the response. While I (and most of the community) personally disagree with your response, we thank you for making your position clear on this and justifying it.\nIn 6 hours, when this proposal fails - the idea of compound decentralization will 100% fail. It is no longer a game of (a) do early users get airdrop or (b) do contributors get COMP token for rewards. It is about top 30 VC + influencer wallets covering 70% of COMP tokens vs rest of community. And at this time, the non-whale community is frustrated and we have 0 incentive to support any method of decentralization\nI will only urge you to reconsider your position and vote ‘Yes’ and when actual dynamics of airdrop are revealed, feel free to vote ‘No’ if you think that’s not a good use of COMP funds. But if you decide to stick to your ‘No’, we as a community will continue to respect that as well.\nThank you so much\\nand @ratankaliani  - Just to be clear, this airdrop will be unlocked over 4 years and we can set it up in a way that airdrop recipients contribute to protocol over next 4 years to be able to claim it. We can set this up however we like in a follow-on vote where exact dynamics are finalized.\nPlease remember that your ‘No’ vote will help retain governance power among 30 inactive VCs, and rest of community is likely to abstain from any further participation\\nAnd @ratankaliani  - just so everyone get’s a perspective of who these early holders are. Here is a story from @lay2000lbs who is founder of Pooltogether (built entirely on compound). He built pooltogether which is probably the best protocol to have come from compound community and he has been engaged every single day from 2018 and does not have 100 COMP to have his voice heard. We are talking about airdropping rewards to folks like this (locking these rewards for 4 years, if needed).\nI rest my case here.\n\nI’ve been using Compound for a long time – since December of 2018. I’ve been active in the community and even started working on a protocol built on top of Compound. I’ve received a lot of value from Compound and also created a lot of value for the protocol.\nSince COMP distribution started I’ve held everything I’ve received. I’ve also done some market buys to increase my COMP. Despite this, I still don’t have enough COMP to create an autonomas proposal (100 COMP). I do find this discouraging.\nI think the idea of getting early participants up to 100 COMP is a good one. Specifically because that is the threshold to meaningfully participate in governance.\nIn terms of distributing this, I would probably favor some sort of larger up-front airdrop with a slower distribution of the remainder. I think in some ways, if the distribution is slow it actually incentives just selling it. But if I get 50 COMP up front then I am half way to my goal of 100 COMP and I have incentive to hold on…\\nAgree with your thoughts but I don’t think the distribution will be 100 comp per user since there were revisions and lot of addresses were added from argent etc. I believe it boils down to 10 comp or less given about 20k - 40k addresses are currently eligible for this airdrop .\nTo clarify I am 100% in favor of this airdrop.\\nSo you don’t think early users deserve to be supported, but VCs should get even more tokens by voting.\nWhat a slap in the face.\nWe can’t get an airdrop but you want to design a system to enrich VCs even more when they “vote”.   What a sad day for decentralization\\nYou have math class on berkeley? For claim 0.48 COMP I need pay 0.3 ETH, with only impact on my wallet. So, you think that members with less funds are idiots? If I go to uniswap and buy COMP only for voting on proposal, is that good practice from community member?\n\n\n\n\n ratankaliani:\n\nRewarding governance participation isn’t easy, but it is a necessity to the future health & sustainability of Compound for us to work on the best way to do so.\n\n\nFrom first post on this thread we read that poor political definition (a16z - where they are?). Your group is calculated medium to long term impact of airdrop distribution? I often read forum, maybe I miss that? That amount of governance power requires perhaps some responsibility?\nOn COMP Genesis over 50% token was airdroped, and now you can joke with community.\nThis outcome of the vote is not surprising, the problem is that none of you didnt bring a good argument so far. The question is if you have thought about it at all.\\n\n  \n\n      twitter.com\n  \n\n  \n    \nkyoronut | きょろナッツ 5\n@kyoronut\n\n\n  https://t.co/KFQKLPjuuE\nINDEX meta governance: FOR\n\n~50k COMP impact, seems not included yet.\nhttps://t.co/pL7GaPjtBF\n    \n      \n        Index Coop @indexcoop\n      \n\n      A new meta-governance proposal for @compoundfinance is live! \n\nThis proposal is a [Temp. Check] to retroactively distribute COMP tokens to early users.\n \n$INDEX holders can vote with the ~$19.2M $COMP locked in $DPI\n\nVote on COMP-67 here \nhttps://snapshot.org/#/index-coop.eth/proposal/QmcLmFvPcHNuYuN8rXkQGMtAW4mxoucJ4ECfxtmKLobBvD …\n    \n\n\n\n  3:16 PM - 7 Nov 2021\n\n    \n      \n        \n      \n      2\n    \n\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\\nTonight comp will definitely stand out as an outlier with respect to rewarding early adopters as it didn’t stick to the ethos of decentralization unlike Uniswap, DyDx and now ENS which will be remembered in history for being sincere and helping the community.\\nAnd when I think its over with this look here: https://twitter.com/compoundfinance/status/1457485858500718593 31\n@rleshner you play chess? Some decision can be prepared in few hours, but we are in this thread almost year?\\nAs an early user of Compound, I’ve avoided commenting on this thread for several months. Of course I’d want an airdrop! But watching this unfold has left me worried about the compound community and the relationship between its leaders and smaller members, so I thought I’d share a few thoughts.\nI don’t feel like compound owes me anything, and I will continue to use the app regardless of the airdrop status. However, I find the shortsightedness of some of the arguments against airdropping — and the reluctance of community leaders to meaningfully participate in a divisive issue — concerning. Developing an active, decentralized community should be compound’s first priority, and it’s baffling that certain people would risk alienating early users for the “future health” of the project.\nOf course some early users would dump tokens if there was an airdrop, and many wouldn’t actively participate in governance. However, the outcome we’re headed to — one where community leaders appear to view small users with distrust and indifference — seems much worse.\\n\n\n\n madmax:\n\nI don’t feel like compound owes me anything, and I will continue to use the app regardless of the airdrop status\n\n\nIts not about airdrop value in $ denomination, its about disincetivising big part of community from participation in governance.\n\n\n\n madmax:\n\nOf course some early users would dump tokens if there was an airdrop\n\n\nvesting COMP no matter how long fix that\\nThank you so much @madmax. Extremely well said.\nI mentioned this multiple time in the past as well, i am NOT an early adopter, so even if this passes - i get 0 COMP airdropped for myself\nThis is (and has always been) against founder/VCs in alienating users and preventing decentralization to preserve their own selfish positions. Irony of it being, whenever there is a COMP hack or SEC comes calling - these same individuals come calling towards smaller voters and call this a decentralized community.\nThis is truly a black day in compound governance and is a million times worse than a hack or anything COMP has endured in the past.\\n@rleshner retweeted all past temperature checks/proposals (when there was a hack) from his personal twitter profile, but now he refuses to even retweet this particular temperature check.\nAll he had to do was say, i do not plan to vote on this topic and stand behind whatever community decides. That’s all it would have taken and he choses to stay silent.\\nHere we are, heading toward a NO. Very dark day for Compound. The smaller users are no competition to the great fish and VC out here. And we just beginning this DeFi experiment !  Let alone in 5 years ! I can see web 2.0 centralization all over again !\nAnyhow, who is controlling the Compound twitter account ? The she/he is playing 4d chess with us, by referring that compound already did this airdrop via Coinbase Earn. It looks like a preparation towards discussion about airdrop / rewards.\nSee here:\n  \n\n      twitter.com\n  \n\n  \n    \nCompound Labs 9\n@compoundfinance\n\n\n  In June 2020, Compound Labs designated 500,000 $COMP for distribution through Coinbase Earn, to increase education and decentralization of the protocol.\n\nMore than 5 million people learned about the Compound  protocol through these videos and a Coinbase Wallet onboarding lesson.\n\n\n\n  3:11 PM - 7 Nov 2021\n\n    \n      \n        \n      \n      44\n    \n\n    \n      \n        \n      \n      8\n    \n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nCiao\\nAnd immediately after proposal past, he put tweet with article (he/they prepared Twitter marketing move).\\nAgree with you, very sad day for Compound\\nwhy don’t we ask all the VCs to send back  their COMP allocations to treasury (so we make good use of it) and we can give all of them voting power.\nSo a16z will send back their 321k COMP token to treasury, but we can empower them with 321K worth of voting power on decisions.\nDoes it make sense?\nTo all initial supporters its same - do you think they are all idiots to yield voting power and contribute to protocol without any perceived expected benefit.\\nI am 100% you didn’t read more than 5 posts about this topic. When you read discussion you will realize that your comment doesn’t make sense. You even don’t know what is all about and you put few fast solutions just like that.\\nWhat the hell is this ? This comment does not make any sense.\\nAlso what this idiot votes by those universities ? Have they involved in this thread here ? Should a university not first discuss a proposal before voting ? Why do they have 50k votes each ? Are they somehow involved with some VCs ? I really don’t understand it.\\nI am wrong, he didn’t read 1 post about this topic.\nCheck initial COMP distribution, i think more than 50% was reserved, so protocol decentralization wasn’t possible from beginning.\\nObviously they are cronies of a16z. Multiple alumni in telegram groups also confirmed what a joke they are. They spent 0 effort trying to read what this topic is about.\nBut i don’t want to be disrespectful to them just because they voted ‘No’. I have immense respect to people voting ‘No’ than those shameless VCs who are hiding behind the scenes\nOn the extremely off-chance that they truly did not understand what this effort is about, I truly feel sad for our next generation students coming out of such reputed universities.\\nand i strongly urge the community not to attack @ratankaliani . He had courage to atleast come here and post something. Let’s leave this at that. I apologize as well if my previous posts implied something on those lines.\\nYes, thank you @tgmed. While I disagree with @ratankaliani, I respect him for actually articulating his position and reasoning, and taking an unpopular stand.\\nI didnt disrespect him, i bring fact. On member profile you can check stats and topics.\\nThank you @rleshner for being so active in the beginning and giving the feeling that like this proposal is the way to go and now you extremely silent and the main twitter account of compound is tweeting this cryptic message about these past airdrops via Coinbase Earn. What kind of voodoo is this.\\n@ratankaliani why did you delete your post? we are all being respectful to you, but when you got found out that you spent 0 mins reading any post on this thread, why are you so ashamed and deleted your post?\nIf you really think you made a mistake, reconsider your vote. Do NOT delete posts\\nPolychain is abstaining here — open to a retro airdrop, given that early users were ‘contributors’ in a sense, and very supportive of efforts to continue to decentralize the protocol in sensible ways. However, this is long overdue (hurting effectiveness) & not sure this is the best use of time/contributors right now.\nSeems that the lack of engagement on this temp check makes the conclusions unclear.\nThis should have been done a long time ago, but due to a lack of organization never found its way to the voting booth. Now, it’s in a really strange spot. Any prop to accomplish this will be complicated (the amount is debatable & vesting/terms to make this effective will require some cleverness). COMP has some bigger fish to fry. Perhaps this could be part of a larger effort to revamp how we reward contributors on an ongoing basis. There are plenty of COMP ‘contributors’ who probably deserve some retroactive grant for their efforts (of many forms).\\nThank you so much for finally taking time to post here. Really appreciate it.\nThis vote is purely on the first part - “open to a retro airdrop, given that early users were ‘contributors’ in a sense, and very supportive of efforts to continue to decentralize the protocol in sensible ways”\nIf you abstain/vote “No” to this - then you are contradicting this very point.\nThere will be a subsequent vote - that will come up with amounts, vesting schedule, additional criterion that needs to be met to claim COMP etc. We are 100% comfortable if you decide to vote ‘No’ to subsequent vote based on what has been put forward.\nRight thing to do if to vote ‘yes’ now if you believe you truly mean what you said when you were open to a retro airdrop and early users are contributors in a sense, and you believe decentralization is important to protocol. Please remember this community will refuse to engage in any token gestures of decentralization if this initial vote fails.\\n\n\n\n JacobPPhillips:\n\nSeems that the lack of engagement on this temp check makes the conclusions unclear.\n\n\nThis is a terrible point to make. The lack of engagement is from VCs such as yourself for this vote. This thread that has been 1 year in making has attracted 500+ comments and 35k+ views.\\nAnd as multiple people have called here, we would like polychain to show some spine here and vote ‘No’ instead of abstaining. Controlling 300k+ votes and abstaining is not a good outcome when the community is enraged\\nWhat bigger fish do you have to fry than decentralization?\nWithout that everything else is meaningless.  You just become the corrupted big web2.0 business we are trying to get away from.\nWhat a joke.\\nWanted to put out more clear & well-defined thoughts on our ideas soon, and recognized it was unfair to put that out without conferring with our entire team. We’ll be releasing our framework for where to move forward with this soon!\\nIn the spirit of transparency, I wanted to share an update (and my thoughts) in a personal chat in the last few hours.\nA voter who voted ‘No’ to this proposal reached out to me (via DM) with a question around why we should be airdropping tokens to early supporters when we could better distribute them via grants/development work for meaningful contributions. We spoke about many other topics, but here is response i shared with this individual, which i feel is important for everyone on this thread to also understand.\nCOMP: 70% of existing COMP token is controlled by VCs who don’t care about community and stop any meaningful work. As a new individual, if I can chose to make a meaningful contribution here to earn COMP, it still counts for nothing as my minimal COMP has no say.\nPOOL: On the other hand, consider a community like POOL. Pooltogether team (@lay2000lbs ) airdropped 80% of their token to community and early supporters, so if i chose to work with POOL community, whatever POOL token I earn for my work, i am 100% confident my voice is heard as token is not distributed in a few VC hands.\nSo how does COMP come out of this messy situation? The only way is to find new faces (early supporters) who had no clue about sybling airdrops and showed genuine love and enthusiasm to the protocol, and ensure they make meaningful contributions in future.\nI also told this individual that we could ensure airdrop can only be claimed (if initial holders make certain contributions or protocol reaches a target metric), so we know COMP is not going waste.\nI want entire community to know that we are not planning to airdrop valuable COMP to early supporters for fun. We are doing this to come out of shit we are in as a protocol.\nP.S: After this proposal fails, i plan to meaningfully work with this individual (and others) to continue to make the protocol decentralized.\\nAnyone want to create a quick DAO to try and swing this towards vote towards yes?\\nYou do not need a DAO to do that. Every compound user can vote using comp.vote (gasless votes), so if anyone wants to have their voice heard - they already can.\nBut as everyone knows, top 30 VCs control 70% of vote and as you saw from polychain response, they need not vote ‘No’ for this to fail, they can just abstain and achieve the same outcome. Makes them continue to look good, while killing idea of decentralization for the protocol\\nYes, but not everyone still has enough COMP to sway this vote.\nThis was more of a reference to the tweet from @lay2000lbs that started pleasrDAO.\\nso if DAO members do not have enough COMP, how can we sway the vote?\\nbuy enough comp to make the difference.\\n@allthecolors @TylerEther thanks for your hard work and efforts.\ndisappointed @rleshner hasn’t voted, but there is still time…\\nEven if we miraculously win vote, that will only lead to all of us (especially folks like @allthecolors ) work hard on actual dynamics and to be stuck down at a later date.\nUnless, we see real and meaningful contribution from top 30 VCs/whales and votes are delegated to trustworthy individuals (unlike 3 blockchain groups who spent 0 mins reading through this read) - nothing will change.\nThe airdrop that you may potentially get is worth nothing, if protocol fails for lack of decentralization\\nI agree. the amount of work @allthecolors did on his own free time is unbelievable. One of first things i will propose to grant committee is to ensure he is compensated for his time he put towards this initiative, even if it was for nothing.\\nBye folks. Spent atleast hours here hoping for a positive outcome. But sad to see this outcome. Anyway one positive side is we are done and dusted . Just need to move on . Thanks everyone.\\nA16z opened this thread patently opting in favour of an non-0 allocation(he even mentionned a significant allocation) toward early users, wrote here and there articles promoting that, now that so much work has been done, and we succeeded to go toward an onchain proposal, he is abstaining from voting. Few days ago the temp check started, asking if early users should get an allocation of the comp governance token of « any amount of comp, in any way », yet a16z did not participate, by respect to all the work that was done following his proposal, thank allthecolors et al. We could have hoped a16z to at least participate to this governance process.\nGetty(the main official account which voted against any allocation toward historical users), and it is the main if not the only argument I could read by trying to find the ideas of the voters who voted against this proposal, justifies his vote on twitter by saying the protocol should keep enough capital to be able to pay for future development, and expenses.\nHowever, as I recalled, the temperature check do not refer to any amount : « in at least some amount and/or way »\nAssuming any allocation toward early users would bankrupt the protocol limiting itself from funding further developments would be very short sighted, mostly when the numbers we are talking about here are only a small fraction of the 2,4 millions COMP allocation toward Vcs airdrop.A fake reason, its how I would call it. I recall that until now the allocation toward early users is 0.\nIf you are so scared about the depleting treasury, why not opening now a grant where Vcs could give a part of their 2,4 millions Comp (worth close to a billion dollar)allocation or reallocating farmed comp toward historical organic users.\nFor these Vcs, despite of the beautiful talks we could read at the start of this thread, it seemed today that you (the historical user) are the product, not the community, the community is them, despite they certainly invested in Comp initially because of you, early historical users who commited time and capital throught the crypto winter. Gauntlet talked several times about the « cost of acquisition of users » which should be minimal when Gauntlet passed the 20% comp distribution speed cut proposal.\nI was sick lately but have been trying to follow the voting process on this temp. check, you can see that not many Vcs dared to vote against aka saying according to the Temp. Check paper : « You believe no amount of COMP tokens should be distributed to early users. », thus the record low quorum.\nI tried to follow, in live how the votes were broadcasted, and you could see anytime the temperature check was leading toward the FOR, a few minuts/hours later, one large voter has been added his vote AGAINST, including in the last half of the proposal live period.\nAlso, this proposal, despite its importance, has one of the lowest historical participation, I agree it can be explained by the process  ManTheWheelz  explained:\n-The compound DAO is not decentralized, there are a few Vcs and entities which rule the protocol and want at all cost to keep their priviledge, they can easily coordinate their strength and love to party together « I literally can’t believe that our D&D party full of VCs » Rleshner, twitter, they own shares of each others (a16z hold significant amount of shares of polychain capital)\n-The community, early users which did not get any allocation until now do not have much way to leverage their votes\n-Vcs, founder, are, in fact, voting against it by abstaining, but yet votes are being broadcasted whenever the FOR was winning and their position is threatened.\nIt is looking like they want this proposal to fail, aka not any allocation to early users, while, at the same time, tarnishing and sacrificing as little as possible their reputation score and remain able to promote their «decentralization washing » marketing outward show, as a16z has done in his initial post. They would vote for the «dirty against » only if they have to, to maintain their dominance over the protocol, just enough to block the early users initiative but not more. Thus the record low quorum reached against this proposal.\nSo, I wanna apologize, for believing in the honesty of a16z and rleshner in my previous posts.\nRleshner who said before we succeeded to go to onchain proposal without help, « -The addresses that receive COMP should be early users that risked their time and capital to establish the protocol. »\nsays «I’m excited about the effort occurring here »\nsays « When the COMP Distribution began, it was expected that Governance would alter, improve, and rethink the program–its fantastic to see the community organizing, and doing the research to distribute COMP to early users » (later in an interview he would say its too late to change it, and in fact we are seeing how this distribution is flexible today)\n-mention the first change he would have made to the distribution is an allocation toward historical users\n-make polls on twitter, which was widely in favor of an non-0 allocation toward early users\n-propose to reallocate the 20% comp speed cut toward early users last year when the Gauntlet cut took place last year throught a suggestion from blck\nYet, now that the community of early users which started from nothing arrived there, Credits allthecolors et al., since we succeeded to reach onchain proposal submission and the temps check started : he liked a twitter thread which mention we shouldn’t reward an early user according to capital allocation criterias, written by the now infamous Blockchain at Berkeley.Rleshner is now liking posts on discord which want to « put this [early users initiative] to bed », he was « very excited about the effort » but only up to a point, Rleshner is liking the tweet of Betty (the biggest voters against) against a non-0 allocation toward early users to keep enough for development, despite the temp check do not mention any amount, and is there only to ask if further developments around this distribution should be done. Rleshner also rushed to like and push any single tweet against this allocation but still did not vote, dodging ?.\nThe strange use (and timing ?) of the compound official account to play this was also not a very courageous move, check the message from devchain.\nSo, you guessed it, today is a bad day, a bad day for the smart early users who committed time and capital on this amazing protocol which was compound, for this organic community of pioneers which came before any incentive, and which hoped to take part in the governance process.\nBut above all a bad day for compound itself, which is showing itself as a highly centralized protocol where most of the voting power (due to the coordination abilities and the initial vcs airdrop) belongs to a few Vcs and entities which through this vote are refusing any allocation toward historical users, to maintain their dominance and avoid any dilution of their governance (and financial?) power. Treating it’s historical users this way is not a positiv thing on the long term for compound.\nA bad day for the reputation of a16z and Rleshner, and cie who like to promote the importance of an allocation toward these pioneers early users, and decentralisation, yet at the last moment, after these early users succeeded to bring up all the datas required, and the comp required to pass on chain vote without their help, when decision time comes they refuse to take any action which would dilute the current voting power monopoly.\nA bad day for Comp token, which seems to have behave more as a certificate of monopolistic voting power concentration with a perfect coordination between airdropped Vcs and entities, than a governance token, at the expense of historical users who until now had a 0 allocation. You will see that if it’s needed they would fund a research telling you how bad it is to airdrop early users, using their huge financial means.\nI understand the anger of tgmed, and of the historical users and community members against what just happened. Betrayed is a word probably not strong enough to characterize what the historicals users and community can feel.\nHowever I would like to finish this on a very positiv note, and from all that I studied last days, there is a still a solution, or maybe several solutions.\nFirstly, note please the ridiculously small quorum which make in appearance this governance vote not significant at all, among the last 20 proposal this vote won with the least number of comp commited against, for the winning side.\nSecondly, and it’s surprising, an idea for us seems to emerge from the biggest voters who blocked this proposal, Getty, in fact, is suggesting to airdrop votes to early historical users : https://twitter.com/getty_hill/status/1456727340260671500 7 “Good points, so maybe instead of an airdrop, we should airdrop votes w/out the coins?”\nIf we succeed to rebalance the voting system by weighting it toward historical users, who are an actually organic community of users as Pooltogether says that (which should be valued and not treated the way we saw today), it might repair the broken governance process and balance it, by permitting early users to participate in the governance process for which they have until now been allocated 0 governance power. It would also remove the main and only argument (which we know is only a mere outward show) the voters against used ( aka the « need to keep money for development » and suddenly need to reward others) as it wouldn’t distribute any financial value to early users, only weighting governance power toward them. It would become more complicated from a reputational point of view for Vcs/entities/founders to vote against that or abstain, as I recall this is being suggested by the biggest voter which blocked this proposal himself.\nDespite that I don’t wanna give too much optimism, as we know they can be full of creativity and means when it’s about serving their current interests, and it’s very possible they change of arguments and ideas, if it serves their interests at that moment, that they will throw you some random surveys telling you that their statistics show that it’s a bad idea, funded by their huge financial means, or propose more profitable ways for them of distribution (maybe the datas obtained by allthecolors did not satisfy ?).\nTo Blockchain at Berkeley, (could you disclose whose comp was used for the vote, who delegated you the comps, same question for the two other university which voted) and to these participants who seem not to have understood the basics : historical users have NOT received ANY COMP allocation, ever, since its birth, how can you say on twitter that they are already too much in profit from the price appreciation of COMP ? This doesn’t make sense and is almost disrespectful to this community of people who made of compound what it is now and in this time frame. Firstly we are talking here about a governance token, not a tool for paying services as “protocol cash” or a speculativ tool for making profits. I am sure most of the early users, including myself planned to hold this token and use it to participate in the governance. Of course a few would sell it, but do we have the numbers on how many tokens have been sold among the token which have been airdropped to Vcs since launch date and asked to reimburse that ? And why are you ignoring the vesting possiblities and using your creativity toward that ? A several years vesting period for example, adding conditions to remain activ on the governance process would align incentives and participation. (all this that was not required to any Vc who show us their involvement today in the governance process), the temperature check writeup was very open about this for these who read it carefully…\\n\n\n\n JacobPPhillips:\n\nopen to a retro airdrop, given that early users were ‘contributors’ in a sense, and very supportive of efforts to continue to decentralize the protocol in sensible ways.\n\n\ncan you define me “senzible way”? since a member’s contribution to the community in the case of the Compound protocol is undefined perhaps it should be standardized at all levels. Currently, community standards are set by a small group of people by tailoring it to their interests.\nThe notion of value in decentralized systems that the user brings to the protocol is not just capital and directly funded grants (high value).\n\n\n\n Comp100:\n\nbuy enough comp to make the difference.\n\n\neven if it were possible to do so (and it is not), what would be the difference between a protocol and an average joint stock company?\n\n\n\n Andre1:\n\nA several years vesting period for example, adding conditions to remain activ on the governance process would align incentives and participation\n\n\ngreat post.\nPretty pathetic by @rleshner and a16z by using cheap marketing tricks to gain control on community. In order to build a more decentralized and transparent system than the one we already have and which we have started to change, it is necessary to change the value system. Unfortunately, in this protocol, the initial capital allowed a small group of people to control every aspect of the protocol in their favor.\\nPost Mortem and next steps\nI know majority of the community is sad at the outcome, and there is a lot of frustration brewing against non-responsive VCs/Leshner for failing to act in the core interests of protocol by voting yes/no. Instead of delving in negativity, I want to channel all our frustration into 2 productive areas.\n\nAddress VC problem\n\nI really want to meet (and discuss) with our VCs/Leshner on the following topics\n\nHow they can use their vote effectively for future proposals.\nIf they do not plan to vote on a contentious topic, what is mechanism to abstain from voting and effectively communicate to community at an early stage\nIf they are busy, how can they effectively delegate their vote to responsible individuals (on their behalf)\nHow do we fix credibility issue of university groups with voting power. In addition to all criticism they got on this forum, important industry figures like preston von loom (and others) also posted on twitter on how this group did similar stuff on uniswap governance. So truly understand if this group is malicious (in intent) or if they need education,  awareness and better communication to voice their rightful opinions\n\nHow do we do this: We form a small core community (with me and interested/active community members) to work with VCs/Leshner and find a suitable middle-ground on above topics. This will improve perception of compound and everyone invested in long term success.\nFunding: Ideally hours put towards this will come out a grant budget. The actual compensation does not matter (could be minimum wage - $10 per hour worth of COMP) for core team spearheading this effort\nDesired outcome: Clear expectations on governance expectations from Top 30 VCs/holders.\n\nRewarding early community members\nWhile i personally do not agree, the main point of opposition to airdrop to early users was the fact that we could have a better use of funds for active community contributions. I plan to work with a few active community members (@getty @allthecolors) and do a better job at documenting set of activities technical and non-technical people can contribute to protocol in next few months. I plan to propose that if individual is an early user and contributes to an activity - he/she gets paid a multiplier (say 2x) of his proposed COMP rewards than a non-early user for certain tasks.\n\nWhat will this achieve - We get new faces interested in contributing, they know that they are getting a slight multiplier (say 2x) rewards for contributing to protocol, and grants committee is confident COMP rewards they are giving away is for a worthy cause.\nDesired outcome: I want to identify atleast 50 (new) early community members who contribute to protocol in next 6 months and get rewarded for their activity.\nFunding: I am happy to shape this effort (as well) and would request funding from grant committee (same as above - even if it is $10 per hour worth of COMP for my time)\nNote: After 3- 6 months, if these 2 initiatives fail, I will personally encourage all of you to stay away from participating in compound governance and do better things with your life.\nHappy to hear thoughts (especially from active and passionate community members like @getty @allthecolors @Andre1 @dabar90 @TylerEther etc).\\ngreat idea, maybe this proposal deserve new thread? For the reason that the word “airdrop” in the title of thread associates people with paying early users with COMP.\nAlso the intention created by this thread is pretty bad for the Compound community. I appreciate and support your move and intention to tackle with protocol governance problem.\n\n\n\n tgmed:\n\ncommunity is sad at the outcome\n\n\nI think it is not because of the outcome, I will rather say because already mentioned group of people which initiated “airdrop question” possesses control over the final decision. It is unclear to me personally what they wanted to achieve with that.\\nSure. Let me create a new thread on this topic\\nGood grief!! I get a GM position at a Poker Bar and Restaurant and a vote on the most important topic happens while I’m working 70 hours a week! Geez! This really upsets me that there is no way to message the community. Life happens and you miss out on something going on in the metaverse that is very important to you.\nI’m sorry guys… I don’t think we should have taken this to voting stage until after we have a way to get a hold of everyone. I mean, my vote wouldn’t have done much, but that doesn’t matter, I still would have voted and probably would have purchased more COMP as I have just recently migrated $12k worth of assets from the Ethereum blockchain. I would have gladly purchased a chunk of COMP just to vote/delegate to an address in support of this allocation.\nI am on Discord, the COMP dev calls (except the last 2 or 3 since I’ve been working 70+ hrs a week), this forum, yet somehow I missed this vote! There are probably a lot of community members in the same boat as I am. Is there any way we can get an address that is aligned with an early distribution and once we get “x” number of COMP delegated to that address, we try to vote on this again?\nThis will allow us to be more prepared, so we can advertise on our social media accounts, talk to industry leaders who might be able to throw COMP a mention (like VoskCoin YouTube/Telegram/Discord/etc) and basically be able to advertise to get people to understand that COMP is more than just a token with an attached value. That COMP’s value is/should be only derived from its ability to govern a DeFi protocol.\\n\n\n\n JacobPPhillips:\n\nPolychain is abstaining here — open to a retro airdrop, given that early users were ‘contributors’ in a sense, and very supportive of efforts to continue to decentralize the protocol in sensible ways.\n\n\nSo, your vote should have been “FOR”?!?\nCointelegraph, The Block, Coindesk, CNBC, Bloomberg, Forbes (Maybe):\nTHE ARTICLE:\nCompound Community Enraged as POLYCHAIN, other VCs, 'Abstain’\nHas been submitted.\\nAs compound is now in momentum to move beyond ethereum it would be a great moment to stand still with the initial founders, students, pioniers and all the early users that saw compound as a game to try out and test out the protocol.As compound is now in momentum to move beyond ethereum it would be a great moment to stand still with the initial founders, students, pioniers and all the early users that saw compound as a game to try out and test out the protocol.\\nCan we say we are done fighting? Or any hope left. Thanks\\nHi, it took you much faster to vote against this proposal than puting “clear & well-defined thoughts on our ideas soon” and answer here, did you already forget this thread ?\\nMan I was using this compounding in the dapps before it broke up into all these far out extra exchanges and things way way back in 2015 2016 into now so yeah I think you should airdrop me 100 plus comp and governance tokens\u200B:eyes:\\nI think the lack of decentralization is going to pose a problem for protocol security long term. Especially looking at US regulations and the statements of high ranking officials. It will be interesting to see if the SEC will go after the major token holders. I think the only reason why compound is not on their radar is that they have bigger fish to go after. The Ripple case outcome is going to directly influence my decision whether to divest my assets and sell my comp or not.\\nJust want to give my 2 cents as a new user:\nThis seems like an absolutely terrible idea, for the simple fact that the main thing holding COMP back is that it is generally seen (among the layman community) as a centralized, venture-based scam.\nRewarding early adopters is a terrible take, and is a huge red flag to newcomers like myself.\\nI just want to say It’s not a decentralized finance\\nIt was about allocating COMP to early users, as much as current users (substancially including whale recursiv farmers) got an allocation, as I recall early users who dedicated time and capital (first two years of the protocol development and life) had none allocation. This would have had a positiv effect on decentralisation, by granting voting power to a wide network of core pioneers early users, improving the current governance system which is only a mere outward show of governance, but in fact is fully centralized between the hands of a few Vcs, while early users have not their word. This thread and how this has been complicated to go throught proposal for a non-0 governance power allocation to early users, the refusal of Vcs to participate despite trying to praise early users governed protocol and decentralisation, to avoid any change on the status-quo and their domination of the Comp governance, being scared of dilution.\nThe initial distribution was 100% VC, 0% early users, (excluding the team distribution, neither talking about the ongoing distribution), it was about rebalancing this, we are still waiting for the comments of the so called “University” which voted there for now two months, and hopefully of A16z which did not participate, these universities more or less disappeared and it would be nice if we could know who delegated these 50k Voting power to them, we could have a better idea if they did not behave as a sole proxy to hide the Vcs vote.\\nThe reason why its so centralized is because compound unlike dozends of other protocols did not airdrop a governance token to early users but instead just dropped them to mostly whales after compound already got big.\nBasically a handful of people have all the power now which makes compound centralized. The problem is not an aidrop its the fact that there was no airdrop. Mistakes were made and not corrected later. Compound is basically a centralized company in the hands of very few. Its more like coinbase than uniswap or maker.\\n\n\n\n kenvo:\n\nRewarding early adopters is a terrible take, and is a huge red flag to newcomers like myself.\n\n\nas a newcommer you should first study the initial distribution of COMP tokens when you already catch “red flags” around. No one here supported “free money airdrop,” but long-term users and community members wanted some governance power.\\nOne possible middle ground here is giving some form of non-transferrable voting rights to early users. We  distribute power over the protocol to early users without reducing the amount of money in the treasury. It could also be done without changing the COMP token contract as well, by creating a separate token to account for these new voting rights, where this token is non-transferrable.\\nThis could have been done much earlier, but it is obvious that community expansion is not exactly a priority for the protocol. I have mentioned several times that it is not about monetary value and that early users at least deserved the right to participate in the governance process. In the end it turned out that Coinbase promotion is more important than community but maybe we are wrong and that is the right way\\nHi dear, it’s been near 5 months and we did not get any news from you and your promise to come back to usd with your “more clear & well-defined thoughts on our ideas soon” on your vote.\nThese universities appeared with 50k delegated votes… voted against an allocation to early users, promised to come back with their thought, and disapeared, it really look like they were a proxy vote not to tarnish the real voters who delegated their vote to them, aka top 3 VCS ?\\nWe were planning to suggest implementing a reward mechanism to significant users/contributors via a CGP-like program, but without further ops/legal support in implementing such a program, it’ll be difficult to do so. Happy to share more ideas here on this - but there’s a large need for a CGP program & working groups that are determining how much contributors should be getting rewarded on a continuous basis and I believe that until a broader conversation on that is developed, any airdrop mechanism will not yield the benefit to the protocol that we’re looking for.\nI think @tgmed has some great ideas on this front 14, and would love to jam on them to figure out steps forward. I want to push forward a proposal on this over the next few months especially with Compound III coming to fruition!\\nActually everything is fine. Working groups, plans, contributors, “we need to work”, bla,bla… Some of us who start using Compound 2019 know that empty story because we listen that to long. After 3 years of using this protocol I collect almost 2 comp tokens, and that’s sufficient for 1-2 vote events.\nCompound is the first and biggest fraud in this space\\nNow that you say scam. You could easily say that Compound falsely, inadvertently advertised being able to actually be rewarded more COMP than the borrow APR on the protocol’s native token.\nAlso, you could go further and say that the market cap of COMP was set in that fashion only to allow a few entities to reap the rewards of earning COMP by simply borrowing the asset. The cap made it IMPOSSIBLE for any other user to be able to borrow COMP while making it a very lucrative way to “pre-mine” a DeFi governance token.\nI feel cheated. Would be well worth the effort to see if any of the addresses that borrowed COMP have ANY affiliation with the addresses associated with VC firms, Founders, developers or any other person with an involvement with creating the COMP market as it was."
  },
  {
    "number_of_comments": 17,
    "postid": "d4be9f10-07a8-4a57-b973-50842c7fa298",
    "posturl": "https://www.comp.xyz/t/reserve-factor-standardization/608",
    "combinedcontent": "Background\nThe Reserve Factors 73 of supported markets are inconsistent, and have evolved through a variety of governance proposals and new market launches.\nReserves are taken out of interest paid by borrowers, and reduce the corresponding supply interest rate in the market. Reserves accumulate in each cToken contract, and can be deployed by the Governance process for any variety of use-cases.\nCurrent Reserve Factors\nUSDC: 5%\nUSDT: 20%\nDAI: 5%\nETH: 10%\nWBTC: 10%\nBAT: 50%\nZRX: 50%\nUNI: 20%\nCOMP: 20%\nPhilosophy\nReserve Factors have been a long-standing discussion in Discord, and there are a variety of lenses through which to view them. Historically, discussion has centered around:\n\nReserve factors are the protocol’s fee to offset risk (which is incurred when there is borrowing outstanding)\nReserve factors incentivize behavior, by acting a component of interest rate models\nWhen reserves are large relative to borrowing demand, reserves functionally add liquidity to a market\n\nChanging Reserve Factors does not immediately change the risk of the protocol (compared to changing a Collateral Factor), which should make them flexible levers to consider adjusting.\nPotential Approach for Discussion\nReserve Factors should be standardized in a consistent format–just as interest rate models have begun becoming standardized across markets. By using the risk associated with borrowing the asset as a parameter (e.g. volatility of the asset), we have a starting point to frame new reserve factors.\nI would suggest (and love everyone’s feedback on), the following Reserve Factors:\nUSDC: 5% -> 7.5%\nUSDT: 20% -> 7.5%\nDAI: 5% -> 10%\nETH: 10% -> 20%\nWBTC: 10% -> 20%\nBAT: 50% -> 25%\nZRX: 50% -> 25%\nUNI: 20% -> 25%\nCOMP: 20% -> 25%\nThese create sequentially higher Reserves from borrowing stablecoins, to semi-volatile stablecoins, to large liquid collateral, to more volatile collateral assets. Going forward, as new assets are added to the protocol, they could follow this format as well.\\nNumbers above look pretty good to me!\nOne potential modification, I think it would be interesting to add a surcharge onto the reserve factor when total reserve quantity for a market is below x% of total assets borrowed. If this is feasible on a technical level, it may be an effective way to discourage excessive leverage and help bootstrap reserves for new markets.\\nThat’s a cool idea.\nThe cToken contracts accept Reserve Factor as a Governance function; DAI, USDT, UNI, and COMP (the “gen-2” upgradable cTokens) could evolve to use more complex logic, but the other markets (ETH, USDC, WBTC, BAT, ZRX) need to be set manually. For simplicity, all of these could apply a “layer” in the manual Governance process.\nThe other perspective could be that Reserves are somewhat fungible; there’s nothing to stop Governance from using the Reserves from an old market, to support a new market somehow (this was actually done a long time ago, to transfer the SAI reserves from v1 to DAI in v2).\\nI also suggest having 2-step approach to Seting Reserve factor. I believe we should determine target reserve pool for every market. For example, it could be something like 5% of supplied assets. So, when reserves for market are lower than target (5%) reserve factor is set into accumulation mode and set to higher rate, to stimulate reserves grow.\nWhen reserves for market are at target level or above it, reserve factor is set to normal mode, which is lower.\nUSDC: 5% -> 10%\nDAI: 5% -> 10%\nReserves for both this stable coins are very low in comparison with size of market, i personally would like to see even higher reserve factor than 10% in accumulation phase.\nUSDT: 20% -> 20%\nI suggest to stay alert towards that particular stable coin. Despite of it being widely used, there is no evidence whatsoever that it’s backed 1to1 by USD, as there are no audits provided by issuer. It might be, it might be not, but risk is definitely elevated with that coin, and reserves for market are miserable (126 000)\nETH: 10% -> 20%\nWBTC: 10% -> 20%\nI’m not against accumulating reserves for that markets, as they are very small, but it’s worth mentioning that even such increase might be futile. There’s very low demand on borrowing side, and interest is very low also. Actually it’s so low accumulation, that might be it even worth consideration to subsidize these markets from reserves accrued from other markets (aka primary stablecoins)\nBAT: 50% -> 25%\nThis one is special, as being the one heavily used in initial farming of COMP, which in combination with initial high reserve factor let this market to accrue considerable reserves. This pool i beleive doesn’t really need to grow reserves and can benefit from lower reserve factor indeed.\nUNI: 20% -> 25%\nCOMP: 20% -> 25%\nWe might consider having higher reserve factor for these tokens, as these markets are new and reserves are still low.\\nTo address USDT specifically; I (and the majority of the community) completely agree that there is considerable risk of USDT  declining in value. It’s not fully backed, it’s not transparent, it’s printed whimsically, etc etc.\nThis risk is reflect in USDT’s 0% Collateral Factor, e.g. if it collapses in value, the protocol doesn’t suffer. But, it doesn’t have any more upside volatility than USDC – borrowing USDT doesn’t jeopardize USDT suppliers more than borrowing USDC jeopardizes USDC suppliers. IMO, they warrant the same Reserve Factor.\nOn the other hand, DAI is a somewhat more volatile stablecoin–we saw considerable up-side price volatility in March 2020, which poses slightly more risk that DAI borrows aren’t properly liquidated–leading to losses for DAI suppliers.\nThis is why, IMO, the framework for Reserve Factors should look at volatility as a starting point.\nSecond, your perspective for “seed the reserves” is a fair argument for having slightly larger Reserve Factors across the board. \\nOkay! My thoughts:\n\n\nRegardless of what our philosophy on reserves are, I believe we all would agree having larger reserves in 5 years is better than having smaller ones.\n\n\nTokens with the lowest reserve rates pay the greatest amount of interest into the reserves. Tokens with high reserve rates pay the smallest amount into reserves. Yes, I know correlation does not equal causation but this is a factual observation\n\n\nWe are very early as protocol, in absolute numbers these reserve rates will not significantly de-risk the protocol. (i.e moving wBTC from 10% - 20% will change reserve growth from $20 a day to $40 day).\n\n\nA primary value proposition of decentralized finance is dis-intermediating rent seeking third parties. The protocol taking 25% of all interest paid to lenders is goes against this ethos.\n\n\nThe primary value proposition of the protocol is earning yield, optimizing for maximum yield rates will lead to faster growth.\n\n\nGiven this, I would propose the following rates:\nUSDC: 5%\nUSDT: 5%\nDAI: 5%\nETH: 10%\nWBTC: 10%\nBAT: 10%\nZRX: 10%\nUNI: 10%\nCOMP: 10%\nStablecoins 5%, non-stable coins 10%.\nQuestions\n\n\nMy main assumption here is that higher reserve rates right now will not meaningfully de-risk the protocol when we look at the actual numbers. We should look at this question more closely\n\n\nThe counter argument I see to this is that right now borrowing and lending is being incentived with COMP and so therefore it’s a good time for the protocol to be more extractive. This argument makes sense to me but as total borrowing and supplying grows the relative subsidy of the COMP will shrink\n\n\\n\n\n\n lay2000lbs:\n\nWe are very early as protocol, in absolute numbers these reserve rates will not significantly de-risk the protocol. (i.e moving wBTC from 10% - 20% will change reserve growth from $20 a day to $40 day).\n\n\nYour observation is correct, but i do completely disagree with your colnclusions. Actually there’s much more sense to do exactly opposite.\nLet’s take a look at WBTC market:\nCurrent Reserve factor: 10%\nCurrent Reserves: 1.2383 WBTC\nInterest Paid/ day: $192.15 (10% of that will go to grow reserves)\nCurrent WBTC Supply: $489 Million ( 90% of interst Paid/ day are up for distribution between suppliers, which is $172.8 )\nIt’s pretty much safe to assume that interest accrued on Supplied WBTC is pretty much negligeble and close to zero. It doesn’t really impact Suppliers if they get their 0.01$ per month or not at all.\nNow let’s take closer look at Eth market in similar way:\nCurrent Reserve factor: 10%\nCurrent Reserves: 100.02 Eth\nInterest Paid/ day: $1315.15 (10% of that will go to grow reserves)\nCurrent Eth Supply: $747 Million ( 90% of interst Paid/ day are up for distribution between suppliers, which is $1183.5 per day )\nIn similar way, we can observe that interest gained by Suppliers is negligeble.\nConclusion: Interest is not a primary if not at all motive in Supplying Eth or WBTC to Compound, but rather serving as a Collateral.\nProposal: Set Reserve factor for ETH and WBTC markets at 100% for next 3 month and evaluate after that. Revisit Reserve factor for these markets in 3 month and decide on if it should continue as is, or be adjusted to lower Reserve Factor.\nArgument: For every single WBTC or ETH supplier interest accrued is a dust amount, however for Reserves while small, but still overtime noticeble amount, which would support seeding of Reserves for that markets. I don’t see a noticeble downside for suppliers as they were not actually gaining any substaitional amount to begin with. By forfeiting their pennies suppliers will help to create some reserves for protocol, which ensures more stability and thus financial security for Suppliers in future.\\nOne thought I was considering (along with an earlier proposal) is to essentially enable COMP holders to use their COMP in creating/adding to reserve pools - potentially across the board in terms of serving all markets.\nGenerally in agreement with @rleshner’s proposal to have a uniform notion of reserve factors - and also driving a better understanding of the risk framework associated with borrow demand. However - an increase in market driven rates, as pointed out by @lay2000lbs, wouldn’t really shift the needle in terms of enabling building up larger reserves in the short/mid-term + it has the effect of dampening supply rates and thus the enthusiasm of backers to participate in a market that reduces their capability to earn interest for taking the risk of participation.\nTo make this fair to people who are just using the platform to borrow/lend assets and may not be fully incentivised to be long-term participants/contributers to the growth of Compound, we should consider one of two alternatives:\ni) Increase the COMP distribution (in conjunction with vesting) in order to offset some of the losses they’d be facing + also potentially incentivise them to be more engaged in Compound\nii) I don’t understand the technical feasibility of this, but I’d really like to see the emergence of a common COMP reserve pool, with the capability for COMP holders to add liquidity to the same. The idea would be very similar to the notion of CET in the Basel III spec 1, with the notion that this reserve could serve as the minimum requirements needed to serve the risk parameters of a given market, in addition to serving as a function to signal the appetite to seed current/new markets.\nAdditionally, this would mean that COMP holders who are incentivised towards the growth of the protocol would have a very meaningful way of doing the same by chipping in liquidity to this reserve pool - with the understanding that this pool would be subject to liquidation risks as well as a potential to earn some part of the interest in the form of further COMP incentives/a few basis points off the supply rates. The additional reserve factor (as it is structured today) could then take the additional form of a market-driven assessment of risk in the underlying asset - and be used to contribute rewards to the COMP reserve pool holders.\nIf this were truly possible to implement, this would be a much neater way of classifying risk ratios in terms of risk-weighted assets, outflows and net exposure. Basel III specifies seeding markets with 4.5% of RWA’s - if the community is interested, could pull up some numbers on loans on the book for the past month/3 and use that as a basis to set out to find a number that makes sense, as opposed to a finger-in-the-air estimation of 1% to start out with.\nAny thoughts?\\nHmm. This is interesting. It’s a great distinction to make between assets people are depositing to earn interest vs. assets people are depositing for collateral.\nHowever, I still would push that reserve rates to be minimized.\\nI like these ideas! Overall I like the idea of COMP being able to serve as a reserve of last resort / insurance for the protocol in some fashion.\\nBased on COMP distribution of 2,312 per day and current price of ~$125, the protocol is distributing ~$290,000 per day in incentives. With current daily interest and reserve factors, the protocol is taking in roughly ~$17,000 per day to the reserves, or ~17 times less than COMP outflows.\\nHey, monet appreciate your work in trying to get Uniswap governance moving anywhere. \nBut i think you have a wrong way of thinking about COMP. Protocol isn’t distributing 290000$. COMP isn’t money, it have zero backing and cost zero to protocol to create and distribute. Same as with UNI. Basically that $290000 per day isn’t distributed by protocol, it’s actually taken from market, protocol have zero cost of making that.\nIt is a combination of distribution of tokens AND buying demand, which makes it’s value. But for protocol itself it’s not money. It could potentially be money if that COMP would be converted to Eth, or btc, or stable coin. But that is IF. Maybe market would be able to absorb it, if it was to be sold, but highly unlikely without price impact. It’s wrong to percieve COMP reserves as big money bag sitting there. Because today it might look as 125$ per comp and in 1 month it may very well be 10$ per COMP. It’s extremelly dangerous to treat something with highly variable value as a reserves.\nReserves on the other hand, especially in stable coins and maybe in eth or btc to certain extend are money already.\nHowever, that brings us to another interesting option for protocol. For example, portion of COMP could potentially be auctioned (solld) to create some reserves buffer for Compound pools rather than distributed to liquidity providers. And that might be another viable option for making reserves aside of Reserve Factor.\\nOverall, I agree with @rleshner assessing the level of reserves for each market for the time being.\nThis has been an ongoing tangential discussion, but never really been addressed due to lack of research. I think extensive research should be done in the future to choose optimal rates beyond the finger in the air, but for now, synchronizing them as Robert proposed makes the most sense to me.\nI like @Sirokko idea on the 100% reserves for ETH and BTC, but I do think that the competitive disadvantage would outweigh the potential reserve gains. I think the idea of 100% reserves will not sit well with the end user.\\nMost of the liquidity provided in compound, comes with the idea of farming COMP.\nThis is not going to be the case in maybe 4-5 years when all the COMP became available in the market, but if we start to look at COMP as a valueless asset it will affect the overall liquidity in the protocol.\nCOMP has value and we should consider it a liquid asset. Maybe we can work in fixing the inflation of the token, so whales stop dumping it as soon as they collect it.\nIncreasing the reserve is a good way to do this.\\nI agree with you, the borrowing side (DAI) needs to be strengthened a bit and ETH and BTC as basic protocols are oriented towards long-term holding and the most logical solution as collateral and I am of the opinion that no need for rising that reserves.\nFor UNI  and and especially COMP we need large reserves because their tokenomics are still in development and it is still quite uncertain where the value capture will be.\\nI generally like the numbers that Robert suggested.  I might also suggest using a formula for the reserve factor that’s a function of price volatility (standard deviation calculated from the relevant asset price feed).  The reserve factor could be the max of the Robert value vs the price volatility based model.\\nI think the best way to create value for Comp token holders in the future (only once the platform is stable with enough operational history) is to distribute reserves that accumulate above a certain threshold to holders.\\n-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA256\nThis seems like a reasonable first pass. Ideally, these parameters should vary function of the default probability, but that requires more analysis. Given the issues with Dai on 11/26 and the need to have higher reserves for assets with weaker liquidity, we will support this proposal.\n-----BEGIN PGP SIGNATURE-----\niQIzBAEBCAAdFiEEEJAuhmVduSEVN8uPKIPJySxksQ8FAl/QGBwACgkQKIPJySxk\nsQ9yJg//ffpZI9oga2HHUcK5Bo+IDNHzmNR6gnnC0GQnSYZHB1Eb0BWkle49XV+k\nekdV0GZ9oC7s0Hbt2zRKFXnTed9otQmRAzuDIaS9zx6RU9pcwPmzoj61cEGpQ9l9\n3lrPRS+a8is5c9jAsgw1YSDIgTLC83uNjz7XsTpFAmg+i9uTCcpHbtbl244XoaJg\nzfNco03XK+C78a+IxU8uF+yXE22QyVeGCJek0IJ+2seLYmvEkXI0DPva4mJesD1o\n7LfC6zAmYelUWmfklldAKnvmRq+4pWLVQPpveZheGfLnFGf+AzbanJed6odSQxMF\nyp0cfO+HS1wgBjDRlMcs0UQa4LNTA1Ks1aH7r/jFffGt5gaP1g0ERq10kBoCOvav\njigQjrMuOhrqu2porGRz8LjEQtztexnSMxznf9BEuUnu/AvuQwoYcR75SYumMoJB\nLwnZVBh5tD93YwFZ9N7svztJiCsFxeMPzbauDzYgaVaLKgHpCtjKlKGaSSnv8uq1\nch5Bg5HnG4BvVvQreBm42V4Znx1/t5nWJ/z1w+N45R5AxG0t6iSw1kx9k5ThX3Dy\nBr8n4dDStsjzDgeWp+m3YbVYES6Enx3SMaTPr0PS5Ra2vnp0f+wT4LJqWh8EnvNJ\nUnwFKvXetK7TmECpA1AkkCFK6J/88KKI9GIXjVyf3W3/nV/74Co=\n=bcN0\n-----END PGP SIGNATURE-----"
  },
  {
    "number_of_comments": 10,
    "postid": "9779cb4e-47a1-427f-9d74-36231970a8f1",
    "posturl": "https://www.comp.xyz/t/comp-rewards-adjustments-paid-work-proposal/3029",
    "combinedcontent": "Building off Getty’s discussion of COMP distribution speeds, I think we should be more intentional about how COMP is distributed.\nThe objective of the COMP rewards program was initially to distribute the token to our users. The sad truth is that an overwhelming amount of the COMP rewards are being farmed and instantly sold off, making the rewards program ineffective in achieving the initial goal. I believe there’s a clear need to re-evaluate how best to distribute the token to our community. This is something that will evolve over time, so it doesn’t mean we have to completely figure this out before changing the existing program.\nRather than distributing massive amounts of tokens arbitrarily to markets, I propose changing the existing rewards program (as just one of our rewards programs) to utilize COMP rewards to:\n\nStrategically incentivize growth while maximizing profits (minimizing losses), and\nMaintain minimum market sizes\n\nWhen a market first launches, we don’t see much activity until the market receives COMP rewards. There’s little incentive to deposit into it when there’s no borrowing demand (or rewards), and there’s no borrowing demand because there aren’t enough tokens to borrow to make the cost of the borrow transaction worth it.\nA market needs to have great enough incentives for depositors to provide enough liquidity to make the cost of borrowing worth it.\nAssuming this is the direction the majority of COMP holders want to take, it begs the question of how to get there. Here’s my action plan as a series of proposals.\n\nAction plan\n\n1. Normalize existing rewards across all markets\nBefore reducing COMP rewards across all markets, it makes sense for each market to start off with about equal (weighted) footing.\n\nExisting rates\nWe start off by looking at the existing at existing rates:\n\n\n\n\nMarket\nSupply Speed (COMP per block)\nBorrow Speed (COMP per block)\n\n\n\n\nStables\n-\n-\n\n\nDAI\n0.067\n0.067\n\n\nUSDC\n0.067\n0.067\n\n\nUSDT\n0.00965\n0.00965\n\n\nTUSD\n0\n0\n\n\nUSDP\n0\n0\n\n\nFEI\n0\n0\n\n\nTotal (Stables)\n0.14365\n0.14365\n\n\nNon-stables\n-\n-\n\n\nETH\n0.01075\n0.01075\n\n\nWBTC\n0.01075\n0.01075\n\n\nUNI\n0.0014625\n0.0014625\n\n\nBAT\n0.0014625\n0.0014625\n\n\nCOMP\n0.005\n0\n\n\nZRX\n0.0014625\n0.0014625\n\n\nLINK\n0.0014625\n0.0014625\n\n\nMKR\n0\n0\n\n\nAAVE\n0\n0\n\n\nYFI\n0\n0\n\n\nSUSHI\n0\n0\n\n\nTotal (Non-stables)\n0.03235\n0.02735\n\n\nTotal\n0.176\n0.171\n\n\n\nAppendix A. Current COMP distribution across active markets\n\nAsset classes\nActive markets have been split up into two different classes: stables and non-stables. Most borrowing activity on Compound is with stablecoins. They’re great to borrow because their value is stable and slowly decreases over time. The use-case of non-stablecoins is different; borrowing activity is presumed to be mainly for shorting, staking, and strategic governance.\nTherefore, this proposal will normalize rewards specific to their asset class.\n\nNormalization variables\nNow, the big question is how to normalize rates for every asset class. Let’s consider three different variables:\n\nMarket cap\n\nShould we apply the same level of rewards if, say, one coin’s market cap is 10x bigger than another?\n\n\nCurrent market size\n\nShould we scale rewards based on the current market size?\n\n\nRisk\n\nShould we offer fewer rewards for riskier assets as another avenue of risk management?\n\n\n\nMy intuition tells me we should scale rewards using these three variables; market cap, current market size, and risk all matter.\nIn addition to these three variables, I believe there should be a base rate for all assets to ensure that all markets will get at least some rewards.\n\nNormalization variable weighting\nI propose the following weights for calculating rewards:\n\nBase weight: 25%\nMarket cap weight: 25%\nCurrent market size weight: 25%\nRisk weight: 25%\n\n\nNormalization formula\nThen to derive the coin’s COMP speeds, the formula (for each class) would be:\n\nbase_rate = (total_rate * 0.25) / num_assets\nmc_rate = (total_rate * 0.25) * (coin_market_cap / total_market_cap)\ncms_rate = (total_rate * 0.25) * (coin_total_supply_or_borrow / total_supply_or_borrow)\nr_rate = (total_rate * 0.25) * (coin_collateral_factor / total_collateral_factor)\ncomp_speed = base_rate + mc_rate + cms_rate + r_rate\n\nAppendix B. Proposed COMP reward rate formula\nWe use collateral factor as the risk weight for simplicity.\n\nNon-stable rewards changes\nAdditionally, I propose we move all non-stable borrow rewards over to supply rewards. When looking at the borrow-side for these markets, we see most of the markets with borrow rewards having a negative net rate. We are paying users more than the interest they are accruing, and for what purpose? Doing so incentivizes a decrease in our liquidity and adds insolvency risks. By moving these rewards over to the supply side, we should see an increase in liquidity and decrease in insolvency risk.\n\nProposal changes\nFinally, here’s what the normalization and non-stable distribution changes would look like for supply speeds:\n\n\n\n\nMarket\nCurrent Supply Speed (COMP per block)\nProposed Supply Speed (COMP per block)\n\n\n\n\nStables\n-\n-\n\n\nDAI\n0.067\n~0.04256\n\n\nUSDC\n0.067\n~0.05182\n\n\nUSDT\n0.00965\n~0.03011\n\n\nTUSD\n0\n~0.00684\n\n\nUSDP\n0\n~0.00622\n\n\nFEI\n0\n~0.00609\n\n\nTotal (Stables)\n0.14365\n0.14365\n\n\nNon-stables\n-\n-\n\n\nETH\n0.01075\n~0.01633\n\n\nWBTC\n0.01075\n~0.01573\n\n\nUNI\n0.0014625\n~0.00325\n\n\nBAT\n0.0014625\n~0.00308\n\n\nCOMP\n0.005\n0.005\n\n\nZRX\n0.0014625\n~0.00285\n\n\nLINK\n0.0014625\n~0.00303\n\n\nMKR\n0\n~0.00256\n\n\nAAVE\n0\n~0.00267\n\n\nYFI\n0\n~0.00265\n\n\nSUSHI\n0\n~0.00254\n\n\nTotal (Non-stables)\n0.03235\n0.0597\n\n\nTotal\n0.176\n0.20335\n\n\n\nAppendix C. Proposed COMP supply rate changes\nAnd for borrow speeds:\n\n\n\n\nMarket\nCurrent Borrow Speed (COMP per block)\nProposed Borrow Speed (COMP per block)\n\n\n\n\nStables\n-\n-\n\n\nDAI\n0.067\n~0.04414\n\n\nUSDC\n0.067\n~0.05057\n\n\nUSDT\n0.00965\n~0.02986\n\n\nTUSD\n0\n~0.00677\n\n\nUSDP\n0\n~0.00622\n\n\nFEI\n0\n~0.00609\n\n\nTotal (Stables)\n0.14365\n0.14365\n\n\nNon-stables\n-\n-\n\n\nETH\n0.01075\n0\n\n\nWBTC\n0.01075\n0\n\n\nUNI\n0.0014625\n0\n\n\nBAT\n0.0014625\n0\n\n\nCOMP\n0\n0\n\n\nZRX\n0.0014625\n0\n\n\nLINK\n0.0014625\n0\n\n\nMKR\n0\n0\n\n\nAAVE\n0\n0\n\n\nYFI\n0\n0\n\n\nSUSHI\n0\n0\n\n\nTotal (Non-stables)\n0.02735\n0\n\n\nTotal\n0.171\n0.14365\n\n\n\nAppendix D. Proposed COMP borrow rate changes\nNote: The net COMP rewards rate (per block) remains unchanged at 0.347.\n\nDiscussion of changes\n\nStablecoin reward rate changes\nWe see reward rates for DAI and USDC decreasing - the rewards for the other stablecoins have to come from somewhere. The reward rate for DAI decreases more than that of USDC as USDC’s market cap is about 5x than that of DAI. I expect Compound’s market share of the USDC market to increase because of this.\nWe see the reward rate for USDT increase significantly. While Tether may lack transparency of their reserves, there’s still demand for it. Moreso, the risk of it losing its peg actually increases its borrowing demand. I expect Compound’s USDT market to increase dramatically with this significant change.\nWe see rewards rates for the other stablecoins becoming non-zero. This should promote competition among stablecoins while simultaneously decreasing centralization of the stablecoins. I expect these markets to increase significantly with these changes.\n\nNon-stablecoin reward rate changes\nNote: This discussion excludes the COMP market which remains unchanged - the COMP supply reward rate acts as a defacto staking rewards mechanism.\nWe see all supply rewards rates increasing since the borrow rewards were moved over. We should see an increase in deposits across all of these markets after this change. I expect Compound’s market share of users who borrow stablecoins against their non-stablecoin collateral to increase.\nWe see all borrow rewards rates being cut to 0 - no more net negative rates for borrowing these assets. We’ll see more realistic borrowing activities after this change.\n\nConclusion\nThis proposal aims to normalize all rewards rates in a clear and consistent manner, providing a formula for calculating rewards rates.\nWe address the problem of non-stablecoin borrowing rates being net-negative by moving these rates to the supply side.\nI expect Compound to become more competitive in the decentralized borrowing and lending sector through this proposal.\n\nSuccess metrics\nImportant metrics in determining the success of this proposal’s ability to increase competitiveness are (+) revenue and (+) TVL (relative to the overall sector).\n\n2. Rewards decay schedule\nAbout a month after the first proposal to normalize rewards rates, giving time to analyze and interpret results, we begin the rewards decay schedule.\nThere are a number of ways of going about addressing the COMP farming issue, such as:\n\nImmediately dropping rewards to zero\nGradually reducing rewards to zero\nGradually reducing rewards to an equilibrium where profits are maximized\n\nIf I’ve learned anything from managing a product with 8000-12000 monthly active users for a number of years, it’s that the slightest change will almost always impact at least one user. So the first approach is totally out of question - it’s too big of a change.\nThe second approach is an improvement - it gives time for market participants to adjust their financial planning and/or business logic. But what if users migrate to other platforms in such an amount that revenue loss exceeds the value of COMP being distributed to users? This seems counterproductive.\nFinally, we have the third approach - finding the equilibrium where profits are maximized. In this approach, rewards will continue to decay (for a market) until profits (for that market) decrease. At that point, we will be near equilibrium for that market, and the COMP rewards for that market will be frozen at a halfway point between the newly set rate and the previous rate for that market. Rates will be frozen for 3 months, then will be re-tested.\nThis proposal, or rather series of proposals, uses the third approach.\nThe question still remains of how often to decay rewards, and by how much.\nI propose stablecoin and non-stablecoins rewards decay at 6% and 1%, respectively, every 28 days.\nThis gives us the following exponential decay formulas.\n\nstableRewardsRate(t) = 0.14(1-0.06)^t\nWhere t is delta time, measured in 28 day intervals\n\nAppendix K. Proposed stablecoin rewards rate decay formula\n\nnonstableRewardsRate(t) = 0.0547(1-0.01)^t\nWhere t is delta time, measured in 28 day intervals\n\nAppendix L. Proposed non-stablecoin rewards rate decay formula\n\nDiscussion\n\nStablecoin rewards decay\n\n1185×340 8.67 KB\n\nAppendix E. Proposed stable supply rewards rates w/ decay rate of 6% every 28 days\n\n1185×340 8.71 KB\n\nAppendix F. Proposed stable borrow rewards rates w/ decay rate of 6% every 28 days\nWe see stablecoin rewards rates rapidly decline over the next 1.5 years, followed by a slower decline, hitting a total of ~0.001 COMP/per block after about 6 years on both the supply and borrow sides.\nStablecoin rewards are starting from a high point, and many of the rewards today are being farmed out - a behavior we want to discourage. Compound already has large stablecoin markets with stable interest rates, so rewards for them are less useful w/ great costs.\n\nNon-stablecoin rewards decay\n\n1185×340 8.9 KB\n\nAppendix G. Proposed non-stable supply rewards rates w/ decay rate of 1% every 28 days\nOn the other hand, rewards for non-stablecoins have a much more gradual decay rate. First of all, the non-stablecoin reward rates are starting at a much lower point, and that gives us more legroom. Second, the web3 ecosystem is rapidly growing, with many more coins being introduced. As Compound grows, more and more non-stablecoin markets will be added, and rewards will continually be spread across them. Hence the need for a much lower reward decay rate.\n\nOverall rewards overview\n\n1185×340 8.46 KB\n\nAppendix H. Proposed total rewards rates decay\nOverall, we see a rapid decay of rates for about the first 2 years, followed by a slower decay leading to a total rewards rate of about 0.02 COMP per block after 10 years.\n\n1185×340 20.4 KB\n\nAppendix I. COMP distributed since start of proposed decay schedule\nAfter about 10 years, we’ll see about 1,755,000 COMP distributed through this rewards program.\n\n1185×340 20.9 KB\n\nAppendix J. COMP remaining since start of proposed decay schedule\nThis leaves us with about 910,000 COMP remaining (excl. Compound payroll), and about 590,000 COMP remaining taking into account the current Compound payroll streams. This allows us ample room for other rewards programs and work contracts to fuel the growth of Compound over the next decade.\nA 28 day decay interval is used to give us about 3 weeks to analyze results before submitting a subsequent proposal.\n\nConclusion\nWe address the COMP farming problem by gradually reducing rewards rates in a predictable, transparent, and well-defined manner. Rather than dropping rewards to zero, we instead use an exponential decay formula with freezing conditions to maximize profits.\nStablecoin rewards rates decay at 6% every 28 days since they’re starting at a much higher rate and they are the main target of COMP farming.\nNon-stablecoin rewards rates decay at 1% every 28 days since they’re starting at a much lower rate and rewards will continually be spread across a growing list of markets.\nFinancial planning has been done on a 10-year time frame to ensure the long-term success of the protocol with an ample amount of COMP left over for other rewards programs and work contracts.\n\nSuccess metric\nThe success metric of each of these proposals is (+) gross profit, that is, revenue minus cost of rewards.\n\nSummary of action plan\nThe first step of this action plan is to normalize all rewards rates in a simple, transparent, and well-defined manner. Normalization is weighted by market cap, current market size, and risk, on top of a base weight; all of which are weighted at 25%.\nAbout a month after normalization, we start decaying rewards rates of the two asset classes - stablecoins and non-stablecoins. Stablecoin rewards rates decay by 6% every 28 days given the high starting point and being the main target of COMP farming. Non-stablecoin rewards rates decay by 1% every 28 days given the lower starting point and with these rewards spread across a larger and growing list of markets.\nNormalization will occur with each of these decay schedule proposals to rebalance rewards as global and local markets change.\nAfter about 10 years, we’ll see approximately 1,755,000 COMP distributed through this rewards program with this decay schedule. With this and the current protocol payroll streams, there will be about 590,000 COMP left to be distributed. This allows us ample room for other rewards programs and work contracts to fuel the growth of Compound over the next decade.\n\nServices and compensation\n\nWho is TylerEther\nEver since a young boy, I’ve always had a passion for math and science. I remember when I was in the first grade, I once failed a math quiz and was required to stay in during recess to improve my math. The embarrassment; I told myself never again would I be prevented from enjoying recess with my friends. Shortly after, I managed to convince my mother to buy me math workbooks so that I could practice my math with her help in grading my work. I felt so powerful with the knowledge to perform addition, subtraction, and even basic multiplication and division by the end of second grade. I knew then and there that I’d never miss recess again.\nI later started my coding journey when I was 10 years old, making my first website - featuring a flaming cursor! I was 12 years old when I created my own custom RuneScape private server to play amongst my brothers. I was 13 years old when I got into bot development at SRL 2. It was through SRL that I learned of Bitcoin in 2010. After becoming a community developer there at 14 years old, I eventually wanted to create something of my own.\nIt was then at almost 15 years old when I decided to create my own RuneScape bot - TRiBot 1. It took just over 3 years of development, community management, and many failed attempts at gaining traction before TRiBot took off. Shortly after going from about 10 active users to about 10,000 in a matter of months, I founded TRILEZ SOFTWARE INC. at the age of 18. I operated the company while attending university to later graduate with a BSc. in computer science with a minor in mathematics.\n\nWho is TRILEZ SOFTWARE INC.\nTRILEZ SOFTWARE INC. (“Trilez”) was initially founded in 2013 as a RuneScape botting company, serving over 400,000 unique users with over 30 all-remote independent contractors contributing to a vibrant script marketplace.\nThe goal of our flagship product, TRiBot, was to enhance the gameplay of all players. Rather than mindlessly grinding in the game, our users were able to instead enjoy life more. We all loved the game, so a lot of focus was put on ensuring our bots did not harm the game. Hence we put a lot of effort into making our bots as human-like as possible, mainly through tuning the bots’ reaction times to be the same as players (so bots didn’t have a competitive advantage), encouraging limited use of our bots, and having an AI chat component.\nSince we were operating in such a niche market, our growth was limited and I ended up taking on responsibilities in many different areas. Through Trilez I learned many different skills - product management, people management, community management, business strategy, research and innovation, marketing, dev-ops, cloud computing, Java/JVM, C++, networking, security, the list goes on.\nWe flash forward to 2021 when the rights to TRiBot were transferred to new ownership, pivoting Trilez to be a web3 company. It’s just me at the company for now while we find our footing in web3.\nSince pivoting to web3, Trilez has:\n\nAdded a LINK market 2\n\nAdded support for splitting COMP rewards between suppliers and borrowers 2 and promptly fixed the newly introduced proposal 62 bug [1] 1[2] 1\n\nPerformed a temperature check for retroactive COMP token distribution to early users\nAdded a USDP market\n\nAdded a FEI market\n\nPrepared a RAI market listing proposal\n\nBeen working with Getty & GFX Labs to introduce various other assets including stETH and RAI\nBeen working to create a market listing framework; the template for which has been used in the RAI proposal\n\nStarted working on a multi-chain evaluation framework and has started writing automated deployment and configuration scripts\nCreated Pythia 1 - a decentralized, fully on-chain price and liquidity oracle providing manipulation resistant price feeds (still a work-in-progress), built with Compound in mind\n\nThe intention of v1 is to be used with Compound’s L2 deployments since it’s too costly in gas to be used on mainnet\nA v2 has been roughly planned out to consume much less gas with some trade-offs\n\n\n\n\nServices offered\nThe following services offered are exclusively in relation to COMP rewards program(s), in no particular order.\n\n1. Development and maintenance of the rewards normalization model(s)\nThe rewards normalization model created and shared above will continue to be maintained, and improved upon or changed if need be.\nOne possible improvement may be the following:\n\nTrading volume normalization parameter\n\nInvestigate whether rewarding markets partially based on [logarithmic] trading volume can increase profit and TVL\n\n\n\n\n2. Regular rewards normalization\nProposals will be submitted every 28 days to normalize rewards rates as per the normalization formula and methodology as outlined above. These proposals will be in tandem with rewards rates decay.\n\n3. Development and maintenance of the rewards decay models\nThe rewards decay models created and shared above will continue to be maintained, and improved upon or changed if need be.\n\n4. Regular rewards decay\nProposals will be submitted every 28 days to adjust rewards rates as per the decay schedule and methodology as outlined above. These proposals will be in tandem with rewards rates normalization.\n\n5. Establishment of a minimum market size\nDiscussion of a minimum market size will be facilitated to establish minimum market sizes. Rewards to establish and maintain such minimum market sizes will be evaluated over time.\n\n6. Establishment of the methodology for when to increase rewards for a market\nWhile rewards may be high at the moment, there will eventually be a time where increasing rewards for a market will lead to an increase in profit. When this occurs, a data science based methodology will be established for increasing rewards for a market. This will likely manifest into the creation of an optimization algorithm.\n\n7. Development and maintenance of reward chart dashboard(s)\nPublic dashboard(s) will be created to visualize and analyze the performance and effectiveness of Compound’s COMP rewards.\nThese dashboard(s) will be promptly created as they are key in analyzing the effectiveness and longevity of our rewards program(s) as well as driving decisions.\n\n8. Exploration of additional rewards programs\nThe following items in no particular order will be explored should they bring significant value to the protocol:\n\nReferral programs (with sybil resistance)\n\nIncentivizes user-growth\n\n\nCOMP lock-up rewards programs\n\nIncentivizes long-term commitment to the protocol\n\n\nCOMP-ETH DEX liquidity rewards programs (on Ethereum and more importantly, L2s)\n\nIncentivizes COMP price stability, lower slippage on trades, and greater availability\n\n\ncToken DEX liquidity programs (on Ethereum and/or L2s)\n\nIncentivizes cToken-underlying liquidity to save on gas vs. minting/redeeming and provides entry/exit liquidity in cases where users are unable to normally do so (paused markets, transfer restrictions, etc.)\n\n\n\n\n9. General engineering services\nSoftware engineering services such as UI adjustments, smart contract development and testing, automation, and any other general engineering work needed to execute on the other services mentioned.\n\nThe following services which fall outside of the COMP rewards program(s) are included as well.\n\n10. Multi-chain deployment, research, and management\nResearch into the deployment of Compound to other chains will continue to be performed. Using the data gathered, we will work with the community to establish base criteria, processes, and anything else needed for successful deployment on other chains.\nDeployment scripts and configuration scripts will be finalized and made public, which will be used to deploy and configure Compound on other chains.\n\n11. Market listing, research, and management\nWe will continue to list new markets with continual and agile improvement of related processes and requirements, including updated deployment scripts and/or tools.\n\n12. Protocol development, research, and maintenance\nWe will continue developing and maintaining the protocol and its codebase. Research will be utilized and conducted to further improve the protocol.\n\nPriorities\nWith so many services listed, work priorities may not be clear. With that being said, here are our current priorities:\n\nScaling Compound to other chains\nAdding borrow factors and other tools such as velocity limits to allow the protocol to safely support many assets\nScaling supported assets\n\n\nDirection\nThe direction of the existing protocol with Compound Gateway in development remains unclear, so it begs the question of how useful will these services be?\nCompound Gateway is taking a different direction of the existing protocol, where only USDC (maybe other stablecoins as well) will be borrowable. While this may improve the efficiency of borrowing stablecoins, the use-cases of borrowing other assets are overlooked.\nOur vision is to have Compound Gateway and the existing Compound Protocol be competing products all under the same roof. In doing so, users more comfortable with the existng protocol can continue using it, and the existing protocol will continue to serve the use-cases of borrowing other assets, such as:\n\nStaking borrowed assets\n\nUsers who have stakeable assets but whom are unwilling to take on the risks and responsibilities of managing such stakes can lend out these assets to others who specialize in managing stakes\n\n\nQuick liquidity for OTC desks\n\nOTC desks can borrow assets to perform quick trades for their customers and can then subsequently slowly buy back the token to repay their debts while avoiding slippage\n\n\nStrategic governance\n\nSome users may be contributing to multiple projects simultaneously while not having the capital to hold meaningful amounts of tokens to participate in the governance of each and every project, so instead, they can strategically borrow against their bags to have more of a say\n\n\nShorting tokens\n\nWith that being said, Compound Protocol will continue to be developed to maximize the number of borrowable assets in a safe and capital efficient manner.\n\nPerformance review process\nTrilez’s performance will be reviewed once per quarter, taking place as a forums discussion.\nThe success metrics as outlined above will be analyzed and presented, and an analysis and discussion of the effectiveness and longevity of the COMP rewards program(s) will be presented.\nAdditionally, an outline of work conducted and our progress on existing initiatives will be included in our quarterly report.\n\nCompensation model\nTrilez is seeking compensation in the form of an initial fee with a base rate plus performance bonuses. This compensation package exclusively covers the services offered in this proposal.\nAll dollar figures are in USD.\n\nInitial fee\nThe requested initial fee is $500K in COMP at present value ($116 @ Feb 28) to cover existing and prior work relating to the COMP rewards program as well as the proposed normalization model and decay schedule.\n\nBase rate\nThe requested base rate is $4M/year, paid via stream, 75% in DAI and 25% in COMP. In being paid mostly in DAI, we mitigate the need to sell COMP to cover our expenses.\nThe value of COMP being paid will be determined using a moving 4-month average and will be updated quarterly.\n\nPerformance bonuses\nThe requested performance bonus is 10% of gross profit increases - directly or indirectly attributed to rewards program(s) management - paid quarterly in COMP and capped at $2M/quarter.\nThe value of COMP being paid will be determined using a moving 4-month average and will be updated quarterly.\nWith this performance bonus, Trilez is incentivized to increase the protocol’s gross profits by up to $20M/quarter.\nIt will be up to Trilez to provide sufficient proof of attribution of gross profit increases and to convince COMP holders of the validity of said proof.\n\nJustification\nFirst and foremost, I’d like to say that the industry is in a talent crunch, and Compound is in dire need of talent and of people bold enough to continue pushing ourselves past our current limits and assumptions.\nCompound has taken the courageous leap of redefining how we work - through a DAO with no single leader. This has presented many challenges; the grants program is paused indefinitely due to staffing issues 2; governance is disorganized 4; execution of brilliant ideas and needed changes takes time; and more. This makes it hard for Compound to retain leadership in the industry and to continue growing, but I believe we can make this work. Compound has come a long way, and we have much to be proud of, but we’re far from finished.\nBy approving this proposal and compensation model, I will be committed to growing Compound through Trilez and will have the necessary resources to do so. I’ll be able to hire top talent (which can be costly) to invest in Compound’s future. It’s my belief that by hiring top talent, we’ll have maximum impact with as little management overhead as possible, making us an efficient and effective powerhouse.\nFurthermore, I imagine myself as an active investor in the protocol. The more COMP Trilez holds, the more valuable it will be to continue investing resources into Compound - and this goes outside of the services being offered today.\nWith that being said, let’s get into the value that this proposal will bring.\nUtilizing rewards can be a very effective way to grow Compound, but the way we go about it is key. There are some problems with the current rewards system that are addressed in this proposal:\n\nCOMP being farmed through recursive stablecoin lending and borrowing\nThere’s no established methodology for COMP rewards rates across various markets\nThe perceived general consensus that reward rates are too high\nIt takes a while for new markets to receive COMP rewards, with 7 markets without rewards\n\nTackling the above problems will bring significant value to users and COMP token holders.\nIn addition to improving the existing COMP rewards program, Trilez will bring about further rewards programs, further fueling the growth of Compound.\nFor instance, incentivizing cToken-underlying DEX liquidity will allow for reduced gas costs of “depositing” and “redeeming” coins up to a certain degree, depending on liquidity. UI adjustments will be made to capitalize on this - choosing whichever is cheaper for the end-user (i.e. to mint cTokens, or to trade underlying for the cToken).\nBy approving this proposal, Trilez will be committed to the growth of Compound with laser focus.\nOver time, Trilez will slowly hire top performing employees to fulfill the various roles we create for the management of Compound’s rewards programs.\nLastly, compensation should cover any legal, regulatory, and/or lobbying costs accrued. After all, the regulatory uncertainty presents itself as a large barrier to entry.\\n\nConclusion\nThis proposal presents itself as a series of proposals with the goal of improving Compound’s rewards efficiency.\nWe first establish a normalization model for COMP rewards rates across various markets. With this model, we methodically distribute COMP in a clear and consistent manner, and we also setup structure for how new markets receive COMP rewards.\nFollowing an initial COMP rewards rate normalization, rewards rates begin their exponential decay. Rewards decay over a number of years in a clear and consistent manner, providing predictability and stability so as to not [significantly] disrupt our users’ operations.\nAdditional COMP rewards programs have been outlined for exploration to further fuel the growth of Compound:\n\nReferral programs (with sybil resistance)\nCOMP lock-up rewards programs\nCOMP-ETH DEX liquidity rewards programs (on Ethereum and more importantly, L2s)\ncToken DEX liquidity programs (on Ethereum and/or L2s)\n\nTRILEZ SOFTWARE INC. (“Trilez”) seeks between US$1M-3M a quarter to fulfill all of these high value services.\n\nAppendix\n\nA. Current COMP distribution across active markets\n\n\n\n\nMarket\nSupply Speed (COMP per block)\nBorrow Speed (COMP per block)\n\n\n\n\nStables\n-\n-\n\n\nDAI\n0.067\n0.067\n\n\nUSDC\n0.067\n0.067\n\n\nUSDT\n0.00965\n0.00965\n\n\nTUSD\n0\n0\n\n\nUSDP\n0\n0\n\n\nFEI\n0\n0\n\n\nTotal (Stables)\n0.14365\n0.14365\n\n\nNon-stables\n-\n-\n\n\nETH\n0.01075\n0.01075\n\n\nWBTC\n0.01075\n0.01075\n\n\nUNI\n0.0014625\n0.0014625\n\n\nBAT\n0.0014625\n0.0014625\n\n\nCOMP\n0.005\n0\n\n\nZRX\n0.0014625\n0.0014625\n\n\nLINK\n0.0014625\n0.0014625\n\n\nMKR\n0\n0\n\n\nAAVE\n0\n0\n\n\nYFI\n0\n0\n\n\nSUSHI\n0\n0\n\n\nTotal (Non-stables)\n0.03235\n0.02735\n\n\nTotal\n0.176\n0.171\n\n\n\n\nB. Proposed COMP reward rate formula\nFor each asset class,\n\nbase_rate = (total_rate * 0.25) / num_assets\nmc_rate = (total_rate * 0.25) * (coin_market_cap / total_market_cap)\ncms_rate = (total_rate * 0.25) * (coin_total_supply_or_borrow / total_supply_or_borrow)\nr_rate = (total_rate * 0.25) * (coin_collateral_factor / total_collateral_factor)\ncomp_speed = base_rate + mc_rate + cms_rate + r_rate\n\n\nC. Proposed COMP supply rate changes\n\n\n\n\nMarket\nCurrent Supply Speed (COMP per block)\nProposed Supply Speed (COMP per block)\n\n\n\n\nStables\n-\n-\n\n\nDAI\n0.067\n~0.04256\n\n\nUSDC\n0.067\n~0.05182\n\n\nUSDT\n0.00965\n~0.03011\n\n\nTUSD\n0\n~0.00684\n\n\nUSDP\n0\n~0.00622\n\n\nFEI\n0\n~0.00609\n\n\nTotal (Stables)\n0.14365\n0.14365\n\n\nNon-stables\n-\n-\n\n\nETH\n0.01075\n~0.01633\n\n\nWBTC\n0.01075\n~0.01573\n\n\nUNI\n0.0014625\n~0.00325\n\n\nBAT\n0.0014625\n~0.00308\n\n\nCOMP\n0.005\n0.005\n\n\nZRX\n0.0014625\n~0.00285\n\n\nLINK\n0.0014625\n~0.00303\n\n\nMKR\n0\n~0.00256\n\n\nAAVE\n0\n~0.00267\n\n\nYFI\n0\n~0.00265\n\n\nSUSHI\n0\n~0.00254\n\n\nTotal (Non-stables)\n0.03235\n0.0597\n\n\nTotal\n0.176\n0.20335\n\n\n\n\nD. Proposed COMP borrow rate changes\n\n\n\n\nMarket\nCurrent Borrow Speed (COMP per block)\nProposed Borrow Speed (COMP per block)\n\n\n\n\nStables\n-\n-\n\n\nDAI\n0.067\n~0.04414\n\n\nUSDC\n0.067\n~0.05057\n\n\nUSDT\n0.00965\n~0.02986\n\n\nTUSD\n0\n~0.00677\n\n\nUSDP\n0\n~0.00622\n\n\nFEI\n0\n~0.00609\n\n\nTotal (Stables)\n0.14365\n0.14365\n\n\nNon-stables\n-\n-\n\n\nETH\n0.01075\n0\n\n\nWBTC\n0.01075\n0\n\n\nUNI\n0.0014625\n0\n\n\nBAT\n0.0014625\n0\n\n\nCOMP\n0\n0\n\n\nZRX\n0.0014625\n0\n\n\nLINK\n0.0014625\n0\n\n\nMKR\n0\n0\n\n\nAAVE\n0\n0\n\n\nYFI\n0\n0\n\n\nSUSHI\n0\n0\n\n\nTotal (Non-stables)\n0.02735\n0\n\n\nTotal\n0.171\n0.14365\n\n\n\n\nE. Proposed stable supply rewards rates w/ decay rate of 6% every 28 days\n\n1185×340 8.67 KB\n\n\nF. Proposed stable borrow rewards rates w/ decay rate of 6% every 28 days\n\n1185×340 8.71 KB\n\n\nG. Proposed non-stable supply rewards rates w/ decay rate of 1% every 28 days\n\n1185×340 8.9 KB\n\n\nH. Proposed total rewards rates decay\n\n1185×340 8.46 KB\n\n\nI. COMP distributed since start of proposed decay schedule\n\n1185×340 20.4 KB\n\n\nJ. COMP remaining since start of proposed decay schedule\n\n1185×340 20.9 KB\n\n\nK. Proposed stablecoin rewards rate decay formula\n\n$stableRewardsRate(t) = 0.14(1-0.06)^t$\nWhere t is delta time, measured in 28 day intervals\n\n\nL. Proposed non-stablecoin rewards rate decay formula\n\n$nonstableRewardsRate(t) = 0.0547(1-0.01)^t$\nWhere t is delta time, measured in 28 day intervals\n\\nUpon talks with @getty, I’ve amended the work proposal to include services 10, 11, and 12 as well as a write-up on priorities and direction.\\nPlease vote on the following polls.\nNormalization Model Support Needs work Oppose6voters\n                  \n                  Votes are public.\n                 Results\nDecay Schedule Support Needs work Oppose5voters\n                  \n                  Votes are public.\n                 Results\nWork Proposal & Compensation Support Needs work Oppose5voters\n                  \n                  Votes are public.\n                 Results\\nYou are very competent and I can speak the quality and quantity of your work. I like the success metrics and agree that this should be laid forth.\\nThanks for the clear proposal and the nudge to provide feedback!\n\n\nThe normalization model seems solid to me; variations on this have been floated in other forum posts, but this captures the key features the community seems to agree on, including the need to start ramping down emissions and the inefficiency of rewarding non-stable borrows. I happen to disagree with the last point because it breaks COMP’s initial promise of “market participation yields governance rights”, but the community consensus on this seems clearly in favor.\n\n\nThe decay schedule itself looks good and is well-reasoned (disclaimer: I haven’t checked your formulas for accuracy but would do so before voting on-chain). My concern here is with the implementation as a series of governance proposals at 28-day intervals. Even though the proposals can be expected to be mostly copy-pastes of one another, running them monthly seems like an avoidable security risk, on top of not being an efficient use of community members’ gas to confirm or contest each monthly decay. Given the 10-year time horizon, wouldn’t quarterly or even twice-a-year adjustments be adequate? Finally, it’s tempting to wonder whether the decay schedule could be implemented by a change to the comptroller that uses epochs (based on timestamps or block numbers) to determine the correct compSpeeds; this would reduce the number of proposals to vote on at the cost of increased risk of smart contract bugs, so I can understand hesitancy around such an alternative.\n\n\nFor my critique on the compensation plan, you can pretty much take item (2) from my feedback on GFXLabs’ proposal 2 and replace each instance of USDC with DAI (while acknowledging that this proposal differs in that it seeks split compensation in DAI and COMP). I definitely want to see this ultimately funded, as with GFXLabs’ proposal. I also don’t want to see these two awesome proposals unintentionally drain the protocol’s stablecoin reserves or meaningfully limit our ability to recruit and retain other developers as the protocol’s user and dev communites evolve. It would help me get on board if you could share a back-of-the-envelope calculation showing what fraction of expected annualized DAI revenues coming into the protocol will be spent on this proposal at the requested $3M DAI / $1M COMP annual rate. Personally I would want to see this figure come out to less than 20% of annual DAI revenue but would support up to 50% of annual DAI revenue. For reference, by my math, the GFXLabs request comes out to about 100% of annualized USDC revenue, which makes me question whether it is a sustainable commitment for the protocol to make.\n\n\\nLike the three pronged approach of re-thinking the rewards. Thanks for the proposal!\\nThanks so much @allthecolors for the thorough and thoughtful feedback!\n\nGiven the 10-year time horizon, wouldn’t quarterly or even twice-a-year adjustments be adequate?\n\nThere are a number of reasons why I chose 28-day intervals:\n\nProvides more gradual reductions\nNormalization will occur with each proposal, increasing the efficacy of rewards as markets change\nNew markets will receive rewards without too much of a delay\n\n\nit’s tempting to wonder whether the decay schedule could be implemented by a change to the comptroller that uses epochs\n\nIt’s a great thing to consider down the line. I think at the moment, development and auditing resources are needed elsewhere. It’s also beneficial to start off by using governance proposals so that we can make adjustments, if necessary, quickly and without the delay of writing code, testing, and auditing.\n\nIt would help me get on board if you could share a back-of-the-envelope calculation showing what fraction of expected annualized DAI revenues coming into the protocol will be spent on this proposal at the requested $3M DAI / $1M COMP annual rate.\n\nYour critique of GFX Labs’ proposal in this regard directed me to make an informed decision on this matter, so thank you.\n\nMessari report on the DAI market1024×576 178 KB\n\nSource: https://messari.io/article/state-of-compound-q4-2021 4\nIn the past four quarters, net interest income for the DAI market was $22.3M, so my requested rate is about 13% of this.\nThanks again for the feedback! Please keep it coming.\\nAs per a comment in Discord about stablecoin decay rates being to slow, I’m presenting some alternative rates.\n\n8% decay every 28 days (for both supply and borrow)\n\nimage1185×340 8.55 KB\n\n\nimage1185×340 8.58 KB\n\n\nimage1185×340 8.38 KB\n\n\nimage1185×340 20 KB\n\n\nimage1185×340 20.8 KB\n\n\n10% decay every 28 days (for both supply and borrow)\n\nimage1185×340 8.44 KB\n\n\nimage1185×340 8.49 KB\n\n\nimage1185×340 8.3 KB\n\n\nimage1185×340 19.7 KB\n\n\nimage1185×340 20.7 KB\n\n\n15% decay every 28 days (for both supply and borrow)\n\nimage1185×340 8.31 KB\n\n\nimage1185×340 8.34 KB\n\n\nimage1185×340 8.2 KB\n\n\nimage1185×340 19.3 KB\n\n\nimage1185×340 20.6 KB\n\nPlease vote on these:\nStablecoin Supply Rewards Decay Preference 6% 8% 10% 15%2voters\n                  \n                  Votes are public.\n                 Results\nStablecoin Borrow Rewards Decay Preference 6% 8% 10% 15%2voters\n                  \n                  Votes are public.\n                 Results\\nI think Tyler is capable of adding significant value to the protocol. The protocol should incentivize Tyler massively (millions of dollars worth of compensation) to work on the protocol and grow it. A very small population understands the protocol intricately and cares about the protocol as much as he does.\nWith that said. I strongly feel that the COMP rewards program should be largely shut down and only used in specific instances to bootstrap new collateral markets for a short defined period. If Compound ends its liquidity incentives, Aave Polygon will follow and then Aave Mainnet as well. It is clear that the current liquidity incentives are not sustainable and have had a considerable cost.\\nThe action plan for COMP reward adjustments in this thread is being scrapped in favor of COMP Rewards Adjustments v2 44.\nI thank everyone for the feedback provided. I’ll be redoing my work contract proposal over the next week or two."
  },
  {
    "number_of_comments": 9,
    "postid": "4c8759e1-2cb0-4f5e-be58-6ca09561f7bb",
    "posturl": "https://www.comp.xyz/t/goodbye-and-thank-you/3306",
    "combinedcontent": "I’m sad to say GFX Labs and my time at Compound will be coming to a close. It has been a lot of fun to work with everyone on Compound over the last few years. I’ve found great friends and made some great memories.\nI joined the protocol back on v1 before Sai was listed. In hindsight, my usage of the protocol was probably a bit reckless as I definitely didn’t know enough about the protocol to use it the way I was at Grapefruit Trading. I remember being totally fascinated with the premise of the protocol.\nIt truly was the first financial tool I found amazing.\nI remember when COMP was introduced and the idea of protocol governance. Up until then, I just sat in Discord. COMP and Governor Alpha allowed me to take an active role in improving the protocol.\nIt feels like an eternity ago, but back when I proposed 24 to raise the WBTC collateral factor to 60%. I remember it was a huge deal (at least to me). I had managed to get Grapefruit to be the only OTC desk to support WBTC and knew there was a chicken and egg problem for WBTC. A recent Medium post by the Labs team introduced Compound Autonomous Proposals that allowed a token holder with 100 COMP to make a mini-proposal instead of the standard 100k proposal threshold. I bought a 100 COMP and made the mini-proposal. That began my lobbying effort to get people to delegate 100k votes to make it a full-fledged proposal.\nWhen proposal 24 passed, I was amazed that someone with such limited credentials could make such a massive change, and that became my catalyst to continue to work in protocol governance.\nIn November 2020, the Dai liquidation occurred. After a few months of little action by anyone, I decided to take it upon myself to improve the oracle system to avoid another tragedy from happening again. Everyone hanging out in Compound in early 2021 will remember the oracle improvement was a very long and controversial process, but after 6-months of work, it was implemented in proposal 47. That is when it became clear to me that improving protocols as a community member would eventually become a formal role in DAOs.\nIn a subsequent proposal, 51, I became the first individual to be paid a regular COMP payment at the protocol level. After 51, I went on to add AAVE, SUSHI, YFI, and MKR to the protocol. I started running regular community calls and supported many more initiates.\nThe Compound community has always been one of the friendliest groups in DeFi. Robert Leshner is still one of the few founders I know who would sit in Discord and personally answer people’s questions. The community used to be a vibrant and thoughtful group of like-minded individuals. I don’t want to tarnish the great memories of the protocol by talking about the current state of the protocol, so I’ll leave it at thank you…thank you, Compound, the community, and Labs, for what you all did and what you represented to DeFi.\nPS: We’ll be proposing UAV v3 once Chainlink finalizes the deployment in the coming week, and the proposal will also end my protocol pay.\\nThank you so much Getty and GFX! Your efforts to improve the protocol and build community were inspiring and appreciated by all. The precedents you set as grass-roots protocol stewards will pave the way for more contributors. You’ll always be welcomed in protocol discourse in the forums, community calls and chat. Best of luck to you in your endeavors!\\nGetty - thanks for all the hard work here and best of luck with what’s next for you and GFX!\\nGetty, best of luck in all your future endeavors!\\nQuick update regarding UAV v3. Chainlink ran into some internal infrastructure issues while deploying UAV v3 that pertain to gas usage. They should have them fixed and deployed by the end of next week. If, for whatever reason, they do not, I’ll make an individual proposal to end my contributor grant and circle back once the UAV v3 is ready.\\nHi everyone, sorry about the delay. I was hoping we’d be able to sort out the oracle upgrade by now, but we’ve run into a few deployment issues along the way. I think the upgrade proposal will come soon, but I don’t want to drag out this process any longer, so I will propose to end my contributor grant.\nOn that note, I want to introduce @CL_Michael to the community. Michael works at Chainlink Labs as a Solutions Architect. He’ll be the go-between for Compound and Chainlink moving forward and thus the best to talk with regarding assets listing and any oracle maintenance. I’ll be in the background helping with this transition and handing off all the relevant info. I think the community will continue to be in good hands with folks at Chainlink.\nGovernance Proposal 8\\nThank you, @getty!\nHi everyone, this is Michael Imperiale with Chainlink Labs.\nI’m a Solutions Architect where I help onboard new users into the Chainlink Ecosystem and assist existing users like Compound to optimize and grow their protocol with Chainlink. I’ve been in the Chainlink community since the early days and have recently begun working with Getty and CLL on Compound’s oracle needs. I am grateful to Getty and GFX Labs for being outstanding stewards of Chainlink and am excited to have the opportunity to continue our collaboration with DeFi’s foundational protocol.\nPlease feel free to DM me here or on Discord with any questions or comments about Chainlink.\\nGoodbye man!\nIt was a pleasure working with you on proposal 47 and the oracle upgrades that followed it.\nYou’ve been a pivotal player in governance here, a true role model for protocol community members.\nI’ve no doubt success is on the horizon. I hope we cross paths and build together again in future.\\nThank you for all your work you did great job:)\\nThank you for your contributions! Best of luck with the new endeavor, and wishing you success"
  },
  {
    "number_of_comments": 9,
    "postid": "5693dbe1-7d57-4111-a56e-3216cd83dfba",
    "posturl": "https://www.comp.xyz/t/rewarding-contributors/950",
    "combinedcontent": "I’d like to kick off a discussion on building some community norms on how to award people who contribute to the protocol. My opinion is that we should err towards being overly generous to contributors to help create and sustain interest in developing the protocol.\nBaseline community standard:\n\nI’d propose a baseline of 10 COMP to any address that submits a proposal that goes to vote\nAn additional 10 COMP to any address that submits a proposal that is approved\n\nI think this should be the floor of what someone receives even if the proposal is just for a simple parameter change.\nProposal specific bonuses\nOn top of this baseline there should be proposal specific bonuses for proposals that required unique work. This could be requested by the proposer or recommended by independent members of the community\nBonuses for work not in proposals\nMuch work on the protocol does not result proposals and should be recognized and rewarded through periodic true ups. Examples of this would be things like Comp.vote, Blck’s transaction bot, Warios help with the DAI compensation proposal, etc.\nBackdated rewards\nTo kick things off I’d like to submit a proposal to disburse rewards for work up until now. Doing that for the proposals is easy but to reward work that did not result in a proposal we will need to come up with a list. Here is my start, what else?\n\nComp.vote\nWario’s analysis of Dai liquidation event\nBlck’s transaction bot\n…\n\\nI like the idea for rewards of that not included in the vote, but for proposals, COMP/SAI can be granted in the post itself.\nWhat needs to be done is a clear layout of the work that needs to be done, processed through a grants committee.\\nI like this idea a lot, separate from the incentives, having a clear mechanism to determine what the community agrees are worthwhile contributions seems like a definite efficiency gain, which alone I think should speed up iteration cycles for possible contributors.\nOn the backdated rewards, all I can say that comp.vote is awesome, blck is always helpful on the discord and forums, and I feel very honored and surprised to be included along with them.\\n\n\n\n lay2000lbs:\n\nI’d propose a baseline of 10 COMP to any address that submits a proposal that goes to vote\n\n\nWhy? Things like parameter changes aren’t hard at all, take 5 clicks at most and take 10-20 usd. Yes they are needed but do they really need 10 comp?\n\n\n\n lay2000lbs:\n\nAn additional 10 COMP to any address that submits a proposal that is approved\n\n\nThis I think is very good, but 10comp is a bit excessive. something like 5 comp is better IMO.\n\n\n\n lay2000lbs:\n\nMuch work on the protocol does not result proposals and should be recognized and rewarded through periodic true ups. Examples of this would be things like Comp.vote, Blck’s transaction bot, Warios help with the DAI compensation proposal, etc\n\n\nYES!\n\n\n\n lay2000lbs:\n\nComp.vote\n\n\nYes, I think it should get >=50 comp for it.\n\n\n\n lay2000lbs:\n\nario’s analysis of Dai liquidation event\n\n\nY E S !  I think this deserves about the same thing as comp.vote if not more. @wario put in lots of work for these charts without expecting a reward.\n\n\n\n lay2000lbs:\n\nBlck’s transaction bot\n\n\nOld and useful, also deserves some comp.\n\n\n\n massnomis:\n\nI like the idea for rewards of that not included in the vote, but for proposals, COMP/SAI can be granted in the post itself.\n\n\nAs I have stated before, I don’t think rewarding sai is a good idea. First, sai is basically useless; you would need to use it as plain money rather than voting. Two, sai reserves are needed! Sai is still a money market, just has no supplying interest rate. Yes these reserves are growing since people are repaying their debt, but I don’t think that this should be used over comp, especially as one of the big “brandings” for lowering comp speeds in guantlet’s proposal was so it could be used for grants.\nAlso just for the record I wish that the community multisig not be used for granting comp.\\n\nMuch work on the protocol does not result proposals and should be recognized and rewarded through periodic true ups.\n\nI agree with this, and have experienced first hand why this may be valuable.\nBack in March last year, when governance had just been launched and didn’t yet have an interface, I quickly scrambled together a (very) basic implementation in a couple days. I figured this might serve as a good starting block in implementing a more legitimate interface, but quickly lost interest as there didn’t seem to be any incentive to continue working on it. Had it been standardized back then that community work may be rewarded, I could have seen myself continuing to iterate over it, eventually creating something with much more significant value.\nTo be clear, I’m not asking to be compensated for this project, but simply wanted to share why standardizing community work outside of proposals may be valuable in incentivizing community contributions.\\nI agree.\nWe think there should be incentives for activities that contribute to the Compound!\\nThe most expensive aspect of making a complex proposal as a dev is probably paying for audits. Even if the dev is fully reimbursed for the cost of the audit if their proposal passes, they are taking on an uncompensated risk in the case their proposal doesn’t pass. It might be help if there was a way for the protocol to pay a trusted auditor for changes which, e.g. reach the 100,000 COMP proposal threshold, without being contingent on it passing and being accepted by the protocol.\nRe: “community improvements” such as comp.vote, I don’t think the proposal system necessarily fits developments that are not direct changes to the protocol. It is expensive and slow-moving, which is fine for changes to code securing user assets, but raises the barrier for other developments. I think a system that drips COMP from a separate stream (separate from the yield farming one) based on community preference (possibly some kind of separate COMP delegation system) would be good at rewarding community developments.\\nWhile I believe that the SAI should be used to burn/buy comp or at least be changed in some respect.\nLets say that another community multisig with a grants committee shall receive at LEAST 0.05 COMP  a block to be distributed to contributors. I have looked around and seen many impressive projects that never made into reality because of the inadequate economic incentives.\\nFirstly, I think that the sole compensation for protocol efforts should now be COMP. SAI was used prior to the grantComp function, but should no longer be used. I’ll just share my point of view here as I have received multiple “bounties” from the protocol.\nWe should definitely have some sort of multisig that can distribute grants to non-protocol development efforts. This multisig should also have a part in spurring protocol development, maybe providing help with audits, gas deployment, and minimal compensation along the development process. I think that the major development bounty for protocol changes should still be part of the on chain proposals for the time being.\nIf anyone has experience running grants committees, maybe we could set one up (think like Uni grant committee). If not, maybe we can go more towards a democratic snapshot voting method? Didn’t put too much thought into this yet, but it is definitely important.\\n\n  \n    \n    \n    Grants Committee Governance Process\n  \n  \n    I have posted about this before so this will become a simplified version. \nFact: It is apparent that research must be done; economic and developmental to further the protocol’ \nFact: _streamcomp & _grantcomp have been enabled for governance to reward contributors with comp \nFact: a fat stack of COMP is ready, available and growing to fund development \nSolution: A team is needed to evaluate research proposals, guidelines for submissions, as well as maintaining a list of ideas and projects that ca…\n  \n\n"
  },
  {
    "number_of_comments": 9,
    "postid": "ce1d44a8-d2d5-4c4b-bc52-bdec6cb99e92",
    "posturl": "https://www.comp.xyz/t/managing-ctoken-versions/1220",
    "combinedcontent": "The Compound protocol (v2) has been evolving since its launch in 2019.\nInitially, all cToken contracts were immutable; this fit well with the protocol’s reliance on an administrator (the team at Compound Labs), to give users confidence that the functionality of the protocol wouldn’t change unnecessarily. As the admin powers were weakened, and then replaced entirely by  Governance, new markets were introduced with upgradable cToken contracts.\nThe later-generation cToken contracts have a few advantages over the immutable, original model; the ability to sweep assets into other protocols (e.g. DAI into MakerDAO’s Dai Savings Rate), vote governance tokens 13, recover orphaned funds 16, and support future/upcoming features (like supply caps).\nAs follows, a list of cToken models:\nETH: Immutable\nUSDC: Immutable\nWBTC: Immutable\nZRX: Immutable\nBAT: Immutable\nSAI: Immutable (deprecated)\nREP: Immutable (deprecated)\nDAI: Upgradable\nUSDT: Upgradable\nUNI: Upgradable\nCOMP: Upgradable\nThe community may want to consider migrating cTokens.\nMigrating a market from an immutable cToken contract, to an upgradable one, requires creating a new market for the asset. This would create a duplicate market (e.g., there are two ETH markets, with their own interest rates), which users would voluntarily migrate to. After a period of time, the original market could be deprecated.\nThe downside of duplicated markets include separate interest rates / incentives, chores for the ecosystem, potential user confusion, etc.\nSample Migration Process\nAs follows, is a sample migration process that could be used by the community, for an example asset (e.g. BAT):\n\nDeploy a second cBAT contract (“new”), using the latest upgradable contracts; potentially deploy an updated interest rate model\nDeploy an updated price feed proxy, to recognize new cBAT\nCreate a governance proposal, that adds support for new cBAT, and the new price feed proxy; set the new cBAT collateral factor and reserve factor equal to the legacy market;\nIncentivize the migration to the new market; set the reserve factor on the legacy market to 100%, disable the COMP distribution to the legacy market, and activate the COMP distribution for the new market\nAfter the ecosystem has had ample time to integrate the new market, deprecate the legacy market; disable supply/borrowing (but preserve repaying/withdrawing), and set the collateral factor for the legacy market to 0%\n\nEcosystem Considerations\nMigrating assets is cumbersome, and the community may want to approach this slowly, e.g. not at all, or one market at a time. Interfaces, dashboards, and tooling would have to be updated to reflect the new market.\nWould love others thoughts / considerations as part of this process.\\nFor the most part, everything seems good; however, I think this is a bit harsh.\n\n\n\n rleshner:\n\nset the reserve factor on the legacy market to 100%\n\n\nI believe that legacy markets should continue to function normally without COMP distribution for a while before bringing the RF to 100%.\\nThat’s a fair point; increasing the reserve factor could be staggered, and done in a follow-up proposal, while the COMP distribution could be switched at launch.\\nHow about to also incentivise the existing lenders with minor COMP distributions to shift to the new cTokens?\\nOne issue I see with migration is that currently gas prices are so high that moving funds is crazy expensive.\\nWouldn’t  upgradeability bring in more risk?\nPerhaps migration done gradually on a set schedule would make it a bit more manageable. (smallest market size first)\nWouldn’t migration also need positions to be wound down? So probably tooling to make migration of positions easier would really be welcome.\nI actually saw one cool tool, FlashPos, demoed during the ethglobal hackathon that probably would be similar to what we need. But it seems we would need to have a flashloan market available first, unless ofcourse we’re kay using our neighbor’s offering.\nhttps://www.youtube.com/watch?v=xXSUwmo0OXw&t=45s 4\nThe event was partly  sponsored by a competing lending platform.  Apologies, as they figure quite prominently in the video above.\\nAs a project that integrates with Compound, i want to bring the community attention to the fact that some integrated projects have cETH as an immutable parameter in their contract. As interacting with cETH requires different interface.\nAnd if a new cETH is deployed, then compatibility could be maintained if it would be cWETH instead of cETH.\nProbably also from other reasons, it could make sense to consider having the next generation of cETH token as cWETH, same way aave did.\\nLegacy market migration: WBTC, Proposal 41 8 passed today (3/17) with a unanimous vote and was queued. In a couple days the proposal changes will be executed for the upgraded WBTC market.\\n@yaronvel this is a creative and solid idea; and ETH could be handled in the interface with a proxy/forwarding contract. Thanks for raising this idea. It would be fun to architect this.\\n\n\n\n rleshner:\n\nThis would create a duplicate market (e.g., there are two ETH markets, with their own interest rates), which users would voluntarily migrate to.\n\n\nWhy not keep the user interface like it is? Only everytime a user supplies, it interacts with the new contract. Anytime users borrow, the old and new contracts, should be the exact same values as long as the price feed is the same. If you change the IRM, then the UI should show users a button to migrate to the new token if they want and have a “?” that will explain the upgraded cToken. Also, the UI could display 2 interest rates explaining that if supplied before X date APR is Old% on or after X date APR is New%.\nIf possible, everytime a user repays a debt, it uses the new contract and the new contract burns an equal amount of of the old contract’s cTokens from that user’s ETH address.\nAnother possibility would be to add both old and new cTokens supply and borrow to get the utilization percentage, after all, its the same underlying asset.\nEventually the old cTokens will all get recycled into the new cTokens."
  },
  {
    "number_of_comments": 11,
    "postid": "e67655e8-24a4-403e-acd8-c98735e714fc",
    "posturl": "https://www.comp.xyz/t/supporting-the-compound-ecosystem/1638",
    "combinedcontent": "Last week I wrote a post about an idea 9 for Compound Governance to support the growth of the PoolTogether protocol. A strong consensus emerged against that specific implementation but many seemed supportive of the general principle of helping support PoolTogether and thereby also help the Compound protocol grow.\nAfter talking to members of the community, I have a new proposal. This proposal achieves the same end goal of supporting growth in a way that doesn’t use a limited resource (COMP) and costs the Compound protocol much less.\nThe proposal is for the Compound protocol to “sponsor” the USDC prize savings pool on PoolTogether with $10 million of USDC.\n“Sponsorship” is money deposited into PoolTogether savings pools that contributes interest to the savings prizes but is NOT eligible to win. This raises the expected value for all depositors and helps kick-start a reflexive growth loop for PoolTogether (larger prizes create more deposits, more deposits create larger prizes). It’s important to note the sponsorship can be removed at any time! So the value is not locked into PoolTogether. At the end of the day, all this capital is going into Compound so it’s good for Compound as well.\nExecution\nThe actual proposal would do the following:\n\nSend COMP from the Comptroller to the Timelock\nMint cCOMP with the COMP\nBorrow $10 million USDC using cCOMP as collateral\nDeposit USDC into sponsorship of USDC savings pool\n\nRisks\n\n\nThe USDC deposited into the PoolTogether would be subject to the smart contract risk of the PoolTogether contracts. This risk is very low based on a few factors. 1) The PoolTogether smart contracts are non-upgradeable. 2) The USDC prize pool contract has been live on main net since January 11th 2021, securing over $80 million dollars with no exploits of any kind. Contracts using the same code but different asset types have been live for even longer. 3) the prize pool contract has been professionally audited by Consensus Diligence. 4) The prize pool has no external dependencies (no oracle risk). 5) The team has an excellent security track record and no smart contracts related to PoolTogether have ever been exploited.\n\n\nThe only additional risk is that the loan could be liquidated if COMP price majorly decreased. This risk can be well managed by ensuring the liquidation price is very low.\n\n\nBenefits\nReiterating the introduction. This is a low cost and low risk way for the Compound protocol to incentive growth via integration partners. As a protocol, making sure strong third party interfaces exist is crucial to the long term health of the Compound ecosystem.\\nI would like to highlight that the $10 million USDC will be redeposited back into Compound. I think this is a good proposal.\\nThis is a creative approach to supporting PoolTogether, and could become a template for supporting other applications or use cases with liquidity. As an experiment for the Compound protocol & stake-holders, I’m a huge fan of this approach; it puts the protocol/treasury/community resources to work directly.\nA few questions/considerations:\n\nThe “cost” of this proposal to COMP token-holders would include the USDC interest paid to the protocol; this isn’t free. For $10M, at a current interest rate of 4.6%, it would cost the protocol/token-holders 460k USDC/yr. Not that this is necessarily an issue, but should be paid attention to; the Timelock would have to repay additional USDC before being able to withdraw the COMP.\nHow would this impact the cCOMP market, or USDC market? Given the size, it’s not necessarily material, but should be paid attention to.\nWhen would the sponsorship USDC be returned to the protocol? Does the Compound protocol receive POOL? How does Compound enter into an agreement with another protocol? Who administers this? Is there any recourse, if the USDC isn’t returned?\nWhat proposal actions would need to occur? I believe the Timelock would need to ERC-20 approve COMP to cCOMP, etc.\nWhat alternatives exist, from the Compound protocol’s perspective? Selling COMP for USDC, to sponsor PT? Withdrawing USDC reserves, to sponsor PT? Are these more attractive?\n\nBootstrapping partner liquidity is certainly an interesting experiment to explore. I’m a fan of trying something along these lines.\\nI love this idea, and I think it could have many other awesome applications. You have my support!\\nThe deposited COMP as collateral would earn interest and COMP too, this might compensate the USDC interest paid in value.\nThe question is that should this process claim COMP after the deposited COMP or leave it.\nI think the idea is great.\\nAny COMP earned by the protocol effectively “lowers” the total COMP Distribution / Day.\\nI love the creative idea. Certainly needs and economic analysis. I can do it with a grant. (\nLet me know if I should apply for this or I can help out. Possibly needs also needs a legal look? I’m just thinking on the top of my head. But I love this creativity. Maybe a dedicated committee to deal with “investments” like this. I know governance is right for the self driving banks but we need “third party” due diligence.\\nGreat questions! Here are my thoughts:\n\nThe “cost” of this proposal to COMP token-holders would include the USDC interest paid to the protocol; this isn’t free. For $10M, at a current interest rate of 4.6%, it would cost the protocol/token-holders 460k USDC/yr. Not that this is necessarily an issue, but should be paid attention to; the Timelock would have to repay additional USDC before being able to withdraw the COMP.\n\nYes, good to acknowledge this isn’t free. As @blck noted it would be somewhat offset by yield on COMP and also high probability it would be completely covered by accrued POOL (more on that later).\n\nHow would this impact the cCOMP market, or USDC market? Given the size, it’s not necessarily material, but should be paid attention to.\n\nThe thing to pay attention to here is the supply side on the COMP market. That is currently $411 million and to do this would likely want to supply around ~30 million to have a nice buffer. So that is a decent size. On the USDC market it will be immaterial\n\nWhen would the sponsorship USDC be returned to the protocol? Does the Compound protocol receive POOL? How does Compound enter into an agreement with another protocol? Who administers this? Is there any recourse, if the USDC isn’t returned?\n\nTo clarify my proposal. Compound governance would maintain complete control of these funds and could unwind the position at anytime. I would advocate for a one year timeline as the assumed deposit length which could be unwind early or extended if desired.\nAlthough currently “sponsors” do not accrue POOL I do think that will change and Compound governance would accrue POOL. There has been a consensus in the PoolTogether community to start rewarding sponsors with POOL and although this hasn’t been implemented yet I expect it will be.\nI do not believe any type of agreement would be necessary between the protocols. We could do something but I think it would be unneeded overhead.\nFinally, in terms recourse if USDC isn’t returned. PoolTogether governance would have no way to stop the funds from being withdrawn so this would only happen if PoolTogether protocol was hacked and the funds were stolen. If this were to happen Compound governance should assume there is no recourse to get funds back.\n\nWhat alternatives exist, from the Compound protocol’s perspective? Selling COMP for USDC, to sponsor PT? Withdrawing USDC reserves, to sponsor PT? Are these more attractive?\n\nWithdrawing USDC reserves or selling COMP to USDC would both be viable options. Personally I don’t think withdrawing USDC reserves is the best though. We should keep those in the protocol at least for now. Selling COMP to diversify might be the right option but is a very different discussion.\nOverall, I would like to think about how we could get ~$250 million USDC “Compound Liquidity” fund and deploy it across multiple protocols building on Compound.\\nI’ll do a quick presentation on this at this week’s developer call. I don’t think it’s needs a grant at this stage, better to let it develop a bit more. Let’s wait for the call to determine next steps.\\nI think you killed it at the presentation. Compound liquiduty\\nAlso, sweep sai and rep reserves\\nOn the point around alternatives - would it be helpful to consider kicking off a model where Pooltogether (or any partner protocols) could work on setting up ‘limited/whitelisted’ native token pools as a basis for setting up borrowing markets?\nThe intention would be to allow for:\ni) Compound to run essentially a pilot program to kick-start new money-markets with partner protocols and build up reserve proceeds before opening these up to the broader market\nii) Enable partner protocols to tap into a line of diversification (beyond outright sales) and allow them to start thinking about filling in operational/working capital needs\niii) Potentially provide a good use for staked COMP to serve as a backstop for higher-risk/reward yields\nCould have limits imposed pertaining to collateral and reserve factors as well as limits on the money-market sizes for these."
  },
  {
    "number_of_comments": 17,
    "postid": "672da462-5860-44f7-8585-cabac332fea4",
    "posturl": "https://www.comp.xyz/t/project-galaxy-helping-to-build-compounds-first-nft-based-loyalty-campaign/1830",
    "combinedcontent": "Hey guys! This is Pokka from Project Galaxy. Project Galaxy 19 is a NaaS (NFT-as-a-service) infrastructure that empowers communities with gamified loyalty systems. We believe that community management is key to decentralized organizations and NFTs can be a perfect medium to power on-chain achievements and credentials. After talking with Calvin Liu, we want to share our plan here with all of you.\n\nSorry for being wordy because this proposal might take a minute for you guys to go through. Therefore, we have prepared some surprise gifts for you at the end of this proposal!**\n\nOur Motivation\n\nAll communities need a suite of tools like CRM to identify its long-term contributors and manage member relationships. This is especially true for DAO and NFT and therefore are perfect mediums for this.\nTraditional crypto marketing campaigns like airdrop don’t help much with user retention and engagement.\n\n\nHow Can Project Galaxy Help Compound Finance:\nGalaxy Protocol contains plug-and-play modules that allow communities to create customized NFT-powered campaigns based on their needs. With our V1 launch, we recommend three campaign templates for Compound Finance to kick off their NFT journey:\n\nBackground Story Ideas for the NFT Collection\nWe believe Compound Finance can build a new world of DeFi, so we recommend a brand story similar to creating your own SimCity. Community members can participate in different campaigns to collect architecture NFTs, Land NFTs of different dimensions and landscape NFTs. Eventually, users will be able to forge the buildings they have collected to build a city (City NFTs). Different NFTs would have different scarcity or power levels.\n\n1. Governance Campaign - Mission\nReward those who actively participate in the Compound governance with different kinds of architecture NFTs.\nNFT Collection Theme in this mission: For example, a skyscraper, a cathedral, Japanese garden…\nRules:\n\nEach vote you have cast will allow you to claim 2 Mystery Boxes\nEach Mystery Box will give you 1 random NFT out of the 6 different architectures.\n\n\n2. Supply (cToken) Campaign - Mission\nReward those who hold cTokens with landscape NFTs.\nNFT Collection Theme in this mission: Beach, mountain, rainforest, desert\nRules:\n\nHolding at least $500 worth of cTokens for 5/20/35/50/65/80 days gives you the chance to claim 1/2/3/4/5/6 additional Mystery Boxes. (e.g. holding for 5 days = 1 Mystery Box, holding for 20 days = 2 additional Mystery Boxes, etc.)\nEach Mystery Box will give you 1 random NFT out of the 4 different landscapes.\n\n\n3. Borrow Campaign - Mission\nRewards those who borrow $100 worth of assets for 1/5/10/15/20 days\nNFT Collection Theme in this mission: 2x2 land, 3x3 land, 4x4 land in the form of card\nRules:\n\nUsers who have borrowed $100 worth of assets for 1/5/10/15/20 days, will receive a chance to claim 1/2/3/4/5/6 Mystery Boxes.\nEach Mystery Box will give you 1 random NFT out of the 3 different sizes of land.\n\n\nForging Campaign\nAllow community members to forge collected NFTs from previous missions into Mega City NFTs.\nRules:\n\nThere will be 3 levels of the City NFTs (2x2 City, 3x3 City and 4x4 City), and each level has 3 different City designs.\nThe NFTs that the users claim from the borrow campaign will determine the level/size of the final product. The NFTs from Governance campaign and Supply campaign have different scarcity scores. To mint the final City NFTs, you will be required to provide at least 3 NFTs from those 2 campaigns. The total scarcity scores of the NFTs users supply will determine the final City in that certain level.\n\nThese are just some of the draft ideas. They are open for suggestions and discussions within the community so together we can make this NFT campaign for Compound the best in the blockchain space!\n\nSuch Achievement-based NFT Campaigns Can Help Compound with\n\nStronger branding through the issuance of customized NFTs\nBetter form of rewards that inspires community members’ ownership\nCreate something fun to increase community engagement and retention\nPromote governance and protocol usage through NFT gamification\nIdentify long-term and core contributors in the community\n\n\nFees Involved\n\n1. Minting cost\nWhat is this?\n\nThe minting fee charged is used to cover the transaction fee that Galaxy team pays to the network. This is not for Galaxy to gain any profits.\nNote that this is in addition to the transaction fee that users need to pay. When users initiate a claim transaction on-chain, our service listens to that event, checks user eligibility and initiates a minting transaction.\n\nHow is the minting cost determined?\n\nIt varies depending on gas price of the network.\nThe minting cost is calculated using the past 7 day avg gas price on Ethereum.\n\nWhy do we need this?\n\nThe reason that we don’t let the players claim and mint directly is that it’s expensive to have all the whitelist data on-chain (since this is dynamic and the list of addresses is huge). So we incorporated an oracle service involving subgraph queries to check user eligibility.\n\nAccess to control minting cost can be given to the Compound team if needed.\n\n2. Platform fee\nThe platform fee is charged when users claim the NFT. It is a $2 flat fee paid in equivalent $ETH or $BNB on top of the minting cost. And it will be charged along with the minting cost to users in one transaction.\nFees will be collected by the Galaxy community treasury and further support the development of the Galaxy infrastructure.\nThe fee can be waived if the developer chooses to compensate on behalf of users.\n\nWhat’s Next?\nWe will be launching Galaxy V1 and the first set of Compound NFT campaigns soon. While we gather initial feedback from community members through these campaigns, we will be preparing for the next set of Compound campaigns that aim to reward community members that contribute to different parts of the Compound ecosystem.\nIn the long run, we expect more community members can be more directly involved in the process of campaign/design creation. We are really excited to work alongside the Compound community on this.\nPlease don’t hesitate to comment under this post or on the Compound channel of Project Galaxy Discord 2. Looking forward to hearing your feedback!\n\nAn Easter Egg\n\nWe really appreciate all of your patience to stick with us to the end. If you are excited about this event as we do, please comment down below!\n\nThe first 100 Community members who commented will be eligible to own the ultra-cool exclusive limited edition Compound version Galaxy Genesis NFTs! (The comment has to be meaningful, don’t spam!)\n_____________________\n\nCase Study\nThough we cannot disclose the names yet, here are some upcoming partnership examples with other notable projects.\n\nCase #1 - Building a Mining Machine and Promote Staking\n\nBuild Your Mining Rig (Part 1 - Staking Campaign):\nWho is eligible: Everyone\nHow to Participate: Staking\nStaking requirement: 10-200 tokens\nParticipants will receive one of the following NFT\n\nUntitled1210×1126 180 KB\n\n\nBuild Your Mining Rig (Part 2 - Forging Campaign):\nWho is eligible: Everyone\nRequirements to forge: Stake 100 tokens\nNFTs collected from campaign part-1 will be able to forge into one of the three mining rig\n\nUntitled1200×556 108 KB\n\n\nCase #2 - Creating you own dishes and Promote the activities of DEXes\n\nCreate your own dishes (Part 1 - Collecting ingredients):\nWho is eligible: Everyone\nHow to Participate: Trade, Vote and Stake LP tokens on the DEX\nParticipants will receive one or more of the Ingredient NFTs\n\nCreate your own dishes (Part 2 - Cooking!):\nWho is eligible: Everyone\nParticipants will forge 3 of the ingredient NFTs they collect into a dish.\\nSounds very coool! Collecting NFT to build a city. But is the access of getting NFT too hard? And I’m wondering about the NFTs’ looking.\\nWhile this seems like a thorough concept, for me personally that’s too much gamification in an environment that doesn’t need it: finances.\nI’d rather support a loyalty campaign that links to real world environmental protection, preservation, restoration, etc. That would feel quite timely IMO.\\nIt seems like a fun idea and should help build the Compound community. I’m unclear what you need from Compound/community to make this success. Can you please clarify?\\nAny idea the art style on the city pieces? Really curious what they’ll look like.\\n\nTraditional crypto marketing campaigns like airdrop don’t help much with user retention and engagement.\n\nHow are NFTs different from airdropped tokens? They can also be traded away.\\nI am normally skeptical of gamification (more as a worry for what behavior is being promoted), but I don’t see any harm coming from this.\nCan you give estimates for minting cost? (I.e if gas is X then minting would be between $y-z)\\nWould be cool to provide NFTs to early users of the compound protocol. There is a retrospective airdrop being planned, that can accompany this NFT\\nWe are just gathering some ideas from the community:\n\nRules/eligibility to claim the achievement-based NFTs\nSome ideas about the design of the NFTs.\n\nBy the end of the next week if we can gather enough feedback, we will re-draft a proposal and apply for a small grant to start working on the design and development.\nThe program can be shipped in 1-2 weeks after that. When the campaign is live, we want to see if it’s possible to be integrated on Compound website. \\nWe welcome the community to join the discussion of the design/art style of the NFTs.\\n\nThe claiming process is simple. Just click the button and NFTs will be yours (if you are eligible ofc)!\nThe rules for the campaign can be up for discussion.\nWe’ll first and foremost make sure the NFT look amazing.\n\\nExactly, it can be a separate mission on top of what we have on the post. Retrospectively reward early adopters of Compound, etc.\\nWe can make NFT badges non-transferrable, depending on how community want. This is like a membership status, in the future it can actually be used for various growth hacking marketing for Compound.\\nThis is definitely an interesting idea, i just joined the discord group,and i hope to see some progress soon. I would suggest you check out a chargeparticle project as they are developing some features that allow people to “charge” their NFT with tokens inside! And I think these features would perfectly suit Project Galaxy !\\nThis would be really cool if the NFTs had something to do with Compound or finance. Instead of building cities, maybe users could build banks/banking systems. I understand that some of the NFTs will be created due to Galaxy’s partnerships and must be designed around such, but if this comes to fruition, I’d personally like to see some NFTs based on financial aspects first to see where it goes.\\nThis is interesting concept. DeFi + NFT - Why not DeFiNiTely !!!. I hope this will be a success.\\nOne thing is clear: Gamification is the future of NFT space. Especially when it is in a state of stagnation. I truly love this idea and I`ll be interested to watch your progress in the future. And I curious do you have a similar White-Lable solution 2?\\nI really like the idea of NFTs and awarding usage/community members, it’s a great way to get more people involved + get more exposure for Compound Finance.\nA few questions:\n\nAre you planning to support other (DeFi) projects?\nDo you plan to submit a proposal to Compound to actually reward NFT holders, as in (for example) discounts, beta access etc. Imo, this would even further improve the community involvement. Not sure if discounts etc. would be possible with Compound smart contracts atm, but it might be worth to check out the idea.\nMight be nice to retro-actively drop some NFTS to OG members? (note: I’m not an OG  )\n"
  },
  {
    "number_of_comments": 19,
    "postid": "cc79fc9c-0cb1-44c5-ab74-925faa0e38ea",
    "posturl": "https://www.comp.xyz/t/compound-grants-program-lessons-and-next-steps/2264",
    "combinedcontent": "We started the Compound Grants Program 375 with a simple idea in mind: let’s fund contributors that are working to make Compound better. Since we weren’t sure how many contributors there would be and how much money we would need to pay them to work on the protocol, we thought it best to start small.\nAccordingly, the first iteration of Compound Grants Program (“CGP 1.0”) was funded with a little over $2M 21 in COMP. After six months of operations, this program funded over 30 grantees with over $1M in funding. The CGP funded open-source dashboards, analytics, hackathons, and so much more. You can see a list of everything the CGP funded at this link 86.\nFrom the start, CGP 1.0 was an experiment to see what a grants program managed by independent community members could look like. We learned a lot from running the experiment, and we’d like to share some of the key learnings below.\nWhat Worked\n\n\nReasonable, but generous grant sizes. For most projects (Compound included), the bottleneck is not capital but contributors. That is, there is an abundant supply of capital but very few contributors that are willing (and able) to step up to the plate to do the work. Motivating contributors requires paying competitive salaries and grants.To that end, we have been generous (but reasonable) with grants, which has allowed us to attract incredibly talented contributors to work on improving Compound.\n\n\nAttach milestones to grants. Milestones serve two purposes: they protect us from giving an upfront grant to someone who doesn’t end up doing the work and they encourage the grantee to continue delivering since that’s the only way to receive the full grant. Milestones have worked exceptionally well for us, and we’ll continue using them going forward.\n\n\nBet on doers. When we started Compound Grants we weren’t sure who would be able to do the work and who wouldn’t. It turns out that there’s a very easy way to tell who can deliver: people who have already done good work are likely to do it again. Our most successful grants went to “repeat contributors” — they’ve done it before and they’ll do it again.\n\n\nWhat Didn’t Work\n\n\nLaunch and “they will come.” When we launched Compound Grants, we thought people would know what to work on. In retrospect, we should have known that it’s easy to boil the ocean when thinking about what to work on. What happened is applicants would apply for grants for all sorts of projects, putting us in the position of assigning priorities to applications after they were submitted. Instead, the right approach is for grants programs to understand the level of priorities for the protocol and put out RFP’s that community members can begin working on. We didn’t do that at first, but we learned quickly. Now, we have a list of RFP’s 69 that are a priority for the protocol to complete and encourage community members to work on them.\n\n\n2-week turnaround. Prior to launching Compound Grants, we did some research on how other crypto grants programs work. Across the board, the biggest “pain point” we heard was how slow grants programs operate: it would take months for applicants to hear back from the program, and by that point, they would not need money or they pivoted to working on something else. To solve this pain point, we decided to operate Compound Grants with a tight turnaround time: 2 weeks from application to grant. While we did a good job of being speedy, we didn’t hit our 2-week goal. There were several reason for that, with the biggest ones being (i) if you’re running grants in batches, the critical path is determined by the slowest applicants (which can take several weeks to process), (ii) larger grants can take significant up-front work to review and process, and (iii) yours truly is leading the grants program solo and part-time, which limits bandwidth for processing applications. What should we do to improve turn-around time? First, we should be more stringent with applicants: if applicants don’t submit the required information by a certain date, they forfeit the ability to receive a grant. Second, as the grant program matures from an experiment to something more sturdy, we recommend staffing the program with several full-time employees who can process grants in parallel. We’re not quite there yet, but this is something we’d like to actively work towards.\n\n\nWhat Now?\nNow that CGP 1.0 has concluded, we will put a pause on new grants. Over the next few weeks, we will finalize payouts to existing grantees and wrap up operations of the program. To provide us with adequate funding for essential expenses, we recommend keeping $250k worth of COMP in the existing CGP multisig. All unused COMP will be returned from the multisig back to the treasury. (To be more precise, we spent ~2.5k of the 5k COMP the program received in funding; we recommend keeping ~700 COMP  in the multisig as a reserve and sending the balance back to the treasury).\nCGP 1.0 was an enormously successful experiment. We believe it’s now time to double-down on grants and make the program better, faster, and bigger than ever before. Over the next few months, we will be designing CGP 2.0. Our hope for CGP 2.0 is to build the best community-led organization, ever. That means staffing CGP with a full-time team, developing a clear communications process so the community knows what’s being worked on and what needs to be worked on, and constructing incentives for contributors to work on Compound exclusively. We need all the help we can get; if you’re interested in getting involved, please comment here and/or reach out on Discord.\nDeveloping CGP 2.0 will take time, which is why we are pausing grants while we design the next iteration of the program. We can’t wait to share CGP 2.0 with you. Stay tuned!\\nJust wanted to give a huge shoutout to @sukernik and the rest of the CGP 1.0 Team; the depth and experience on the team paired with their vision for what the Compound Protocol will look like is incredible, and we’re especially excited to see what CGP 2.0 is able to accomplish! Would love to help out in any way we can, and I’m strongly in support of expanding the scope and size of CGP 2.0 given more direction and focus on how to incentivize clearly defined projects to apply to CGP and better the ecosystem\\nI think the next iteration of Grants should scale up and decentralize as much as possible. We need to target having a real DAO workforce & partnerships.\nSome ideas:\nMany-to-Many projects: In a DAO most people are not one-to-one on projects. Some of the most productive coders like Arroo and Tyler knock out multiple projects in one go and collaborate as needed. There needs to be a way to pay and track for multiple projects with multiple workers.\nStreaming payments via Sablier - Related to the above, this will simplify and automate payments for complex projects. Streams can be opened and filled up by milestone, and if necessary stopped to claw back remaining grants.\nProfessional grant and project management - grant workers should also have project management experience and be able to connect talent to existing projects. They should drive accountability for milestones and keep clear lines of communication open to the DAO.\nWorking document on DAO best practices -  Learn from Sushi, Aave, Yfi, and other protocols and publish the results in a living document so that everyone knows the rules\nLarry has done an amazing job taking grants to the million dollar idea factory it’s become. It’s been an honor to work with him and with Grants. Let’s drive the work forward and further cement its role in the community!\\nThese are some excellent insights @sukernik ! We’re working on our own grants program at Fei and experience a lot of the same things. Echoing @ratankaliani thanks for helping trailblaze and publicly document the learnings\\nStarting a new initiative is always challenging but you did an amazing job with the Grants!\n\n\n\n sukernik:\n\nNow, we have a list of RFP’s  that are a priority for the protocol to complete and encourage community members to work on them.\n\n\nIt’s great to see a list of priorities. Submitting a proposal without them is like shooting in the dark, especially when contributors has several ideas. It’s hard to narrow them down and then they end up not submitting anything or concentrating on the wrong thing.\nAre they listed in any particular order? E.g. is #1 more urgent/desired than #20?\\nThe RFP’s aren’t stack-ranked by priority. That’s something we have yet to do!\\nThank you very much for the operation of CGP!  I didn’t apply this time, but in CGP2.0, I would like to make a proposal to improve the compound.  Thank you again for your efforts!\\nIs there one place to see the project status of the grants that were funded?\\nRight here 48.\\nDo you mean the “links” column of each funded grant? I guess it wasn’t clear that is a link to the completed project or just a link for reference. btw it would be nice if that table could be full width so you don’t have to scroll to get to the other columns on larger monitors.\\nGiven the success of CGP1.0 with @sukernik at the helm, I think the community has understandably wanted to be generous in allowing ample time to develop and propose CGP2.0 as outlined in this forum thread.\nSince we haven’t heard any updates in the intervening five months, I think it is reasonable for the community to ask for a check-in. Is a proposal for CGP2.0 forthcoming in Q1 2022?\nIf a concept for the core structure is ready but certain details need attention, please do let the community know how we might help fill in the gaps! If bandwidth limitations are too much of a barrier, I would encourage us to consider other proposals for a CGP2.0 based on the lessons learned in Larry’s OP.\\nI have the same question. This would make things easier for everyone\\nHey\nI’m willing to do it.\\nHi, I would also be willing to help out the community in this process\\nThis is wonderful! Appreciate the approach to minimize the turn-around time and also bring more clarity to the applicants.\\nI think this is an important initiative and would be happy to contribute as well. I wonder if we are over-thinking things with the entity setup. It might be fun to think about the simplest possible grants program that allows governance to directly vote (on or off chain) on grants.\\nMy sense is having any sort of tokenholder vote for grants is overkill. Many grants, for example, are $5-$25k. Having tokenholders vote on such small grants is a burden for both the applicant (they need to wait for the vote to pass) and the voter (they need to spend time reading the applications).\\nparticipation = low\nprice = also low\nrisk = very low\nLet’s incentivize working code with a perpetual hackathon for 6 months.\nBravo gives 3k COMP to Micro DAO.\nMicro DAO ( simple majority comp voting ) awards 1k COMP to best bi monthly proposal.\nLet’s make it easy\\nI think it would be best to outsource this sort of work to stakeholders and beyond. Experiment with funding 2nd/3rd party grants coordinators with small amounts of COMP. If these parties do well, continually fund them with increasing amounts of COMP.\nTo get something like this started, it’s key for token holders to vote on an outline of items with priorities and values attached to each item. I can and will contribute to such an outline if someone wants to take the lead on this, or I can take the lead if I’m provided with enough incentives.\\nI’m very glad you resurfaced this post @allthecolors. I’ve been meaning to reply for a bit, but things kept coming up.\nHere’s the full update. After CGP 1.0 ran its course, we started working on CGP 2.0 right away. The basic idea was to take the parts of CGP 1.0 that worked and do them 10x better. As we were coming up with the detailed plan, however, it became clear that CGP 2.0 couldn’t just be one person. To operate well, it had to be a group of people working full-time on the initiative.\nSince we knew we needed a group of people, we started recruiting potential employees for CGP 2.0. While we had some great candidates, it proved challenging to get people to commit to a job before CGP 2.0 was fully funded by the treasury. As a result, we started thinking about creating a proposal to fund CGP 2.0 before the team was identified. Before we posted anything on the forums, we wanted to make sure the plan was sound legally too. We had our lawyers review the plan and give us advice on how to run with it.\nUnfortunately, this is where there were some hiccups. To launch CGP 2.0 effectively, our lawyers strongly suggested a legal entity of some type would need to be created to hold the funds. There are many workable entity types grants programs can use, but for programs of a significant size ($5M+), entities in tax-friendly jurisdictions were highly encouraged. Armed with all of this information, we realized that setting up the entities, recruiting the CGP 2.0 team, and managing the operations for CGP 2.0 would be a very expensive task, both financially and time-wise. And candidly, I myself did not have the time to do it.\nI strongly believe that Compound needs something like CGP 2.0. If there are any community members that are interested in picking up the mantle, now’s the time to speak up!"
  },
  {
    "number_of_comments": 13,
    "postid": "65d6c917-a04e-4945-896a-f6638919b2cf",
    "posturl": "https://www.comp.xyz/t/polygon-matic-starport/1772",
    "combinedcontent": "There’s been a lot of interest in building a Gateway Starport on Polygon 50. So let’s do it! I suggest we use this thread going forward to organize discussion around the topic.\nI’m trying to schedule a bi-weekly call to sync amongst those interested in contributing to this effort to coordinate ourselves, please DM me your email address in Compound Discord 39 if you’d like to be on this list.\nIn the meantime, Compound has started a living document on How to Build a Starport 142.\n\\nFor programmers following along, I have some PRs that are work in progress. Feel free to look over them\n\n  \n\n      github.com/compound-finance/gateway\n  \n\n  \n    \n  \n    \n  \n\n  \n    \n      wip polygon rust side 59\n    \n\n    \n      compound-finance:develop ← compound-finance:wayne/polygon-2\n    \n\n    \n      \n        opened \n        \n          \n        \n        May 18, 2021\n      \n      \n\n      \n        \n          \n          waynenilsen\n        \n      \n\n      \n        \n          +383\n          -38\n        \n      \n    \n  \n\n\n  \n    Changes to support Polygon/MATIC\n\nDone\n\n* Most practical work of actually ge…tting the rust side of the Polygon integration is done.\n\nOpen questions\n\n* Do we like the \"feature flag\"?\n* How are we going to handle in flight configuration of `matic_starport_address` and `matic_starport_parent_block`? We can't do it the same way that we do for Ethereum even if we wanted to because it would require us to change the chain spec file every time we add a new network which is not workable. *update* I put in into storage for now, I am actually quite happy with the way it came out.\n* I am somewhat uncomfortable with the way `pallets/cash/src/chains.rs` and `pallets/cash/src/events.rs` due to code duplication. Maybe `chains.rs` is what we want but I feel like the compiler should be able to write that code somehow. The same thing applies to `events.rs` but to a somewhat lesser extent. I can see how it is easy to reach for macros in rust at this point but I think we should resist that urge. If we are OK with doing this set of changes every time we add a chain I guess that is alright. The real issue comes in when you want to make a change to the structure and you have to change 27 places. Thankfully, the rust compiler is your friend. I did notice at least one explicit `panic` in that file due to a mismatch in enum variant though which in my mind is throwing up your hands on static type analysis.\n\nMore work to do probably before we merge this\n\n* Integration testing focused on regression testing?\n  \n\n  \n\n  \n    \n    \n  \n\n  \n\n\nand\n\n  \n\n      github.com/compound-finance/gateway\n  \n\n  \n    \n  \n    \n  \n\n  \n    \n      wip MATIC/Polygon integration testing 34\n    \n\n    \n      compound-finance:jflatow/const-compile ← compound-finance:wayne/polygon-testing\n    \n\n    \n      \n        opened \n        \n          \n        \n        May 20, 2021\n      \n      \n\n      \n        \n          \n          waynenilsen\n        \n      \n\n      \n        \n          +55\n          -7\n        \n      \n    \n  \n\n\n  \n    Note: change the base branch back to develop when we are ready. This is just to …show the diff better.\n\nThis testing approach uses the same ganache instance and the same contract code but we deploy two of everything. This allows us to simulate being on a different EVM compatible blockchain. The same actors / owners are used but this should not really cause any problem.  The one open question that I have is around how we want to handle token balances. Right now, the token balance is set on both matching tokens so given this code:\n\n```\nlet lock_scen_info = {\n    tokens: [\n        { token: 'usdc', balances: { ashley: 1000 } }\n    ],\n    validators: ['alice', 'bob']\n};\n```\n\nThe ashley address will have 1000 usdc on both chains. That is not ideal but it is now a question of how we want to paramaterize this configuration to say where the balance reside. We could do something like an optional dict with default as in what is above is valid and will default to the ethereum chain and if you want something on a different chain you have to specify the chain name as below\n\n```\nlet lock_scen_info = {\n    tokens: [\n        { token: 'usdc', balances: { ashley: {'matic': 1000} } }\n    ],\n    validators: ['alice', 'bob']\n};\n```\n\nI admit that this would be preferable \n\n\n```\nlet lock_scen_info = {\n    tokens: [\n        { chain: 'matic', token: 'usdc', balances: { ashley: {'matic': 1000} } }\n    ],\n    validators: ['alice', 'bob']\n};\n```\n\nbut that would somehow seem to indicate that the entire token should _only_ exist on that chain which I dont _think_ we want. Perhaps it is something like \n\n\n```\nlet lock_scen_info = {\n    tokens: [\n        { chains: ['eth','matic'], token: 'usdc', balances: { ashley: {'matic': 1000} } }\n    ],\n    validators: ['alice', 'bob']\n};\n```\n\nstill a work in progress\n  \n\n  \n\n  \n    \n    \n  \n\n  \n\n\nThese will end up being reviewed/merged after the next testnet release with some necessary rebase modifications.\\nWon’t this fragment the coin markets?\nFor example; If a user deposits ETH or any other token from Polygon does it become a different market than the original?\\nYes by default Polygon ETH will be different than Ethereum ETH, although governance could decide to handle it differently.\nThis is something that can be defined as part of the runtime changes that are proposed to Gateway, or changed later. It’s probably easier to start with distinct assets and think about combining them down the road.\\nLooking forward to collaborating with the Compound community and interacting more in future to kickstarting Polygon deployment.\\nFor those following the dev work I just merged https://github.com/compound-finance/gateway/pull/341 18 into develop.\nThis creates a failing integration test. Now, practical integration testing for wip polygon rust side by waynenilsen · Pull Request #330 · compound-finance/gateway · GitHub 4 will start.\\nUpdated wip polygon rust side by waynenilsen · Pull Request #330 · compound-finance/gateway · GitHub 6 with some code that actually works. I am happy to hear review from anyone that wants to comment.\nWe have a successful lock at this point in an integration test. There is a lot of work still to do but this is a first step.\nHere is a sketch of what I see as the roadmap after we merge this. Roadmap items marked with c denote those items that are not perhaps entirely necessary if we don’t need to launch with cash token.\n\nLock (soon  )\nUnlock\n\nc / LockCash\n\nc / UnlockCash\nFigure out governance\n\nc / Sync index / cash interest rate and process notices\nIntegration testing with multiple chains and scenarios\nUpgrade testing integration testing using governance on Ethereum to set the starport and enable matic.\nTestnet\nAudit\nMainnet\n\nThere is not necessarily an order here although some things are blocking others. Feel free to ask questions.\\n\nPolygon Starport Call Notes - Call #1 (June 1st, 2021)\nHello! Thanks everyone that participated in today’s call. I took some notes during the call. Please let me know if I misunderstood an explanation or have some info in here that is not accurate. I can make edits with corrections and clarifications. The next call will take place in 2 weeks from today, likely at a slightly earlier time. We will post future call details soon!\n\nWayne (Compound Labs) has made some progress on a Polygon Starport. The development process of the Starport so far has been to run 2 Ganache chains locally, one for Ethereum and one for Polygon. Also run an instance of a Gateway node locally that hooks up to those Ganache instances.\nWayne is deploying both a Polygon Starport contract and a CASH contract to the Ganache instance, and attempting to get the Lock operation to work locally.\nWayne is working on creating a test to run the operation; the test currently fails. Once this test passes, it will be the first milestone in development of the Polygon starport.\nThere is a forum thread to discuss the progress of the Starport\nWe went over introductions of everyone in the call. Included several members of Compound Labs, Polygon Team, and interested community members.\nJared (Compound Labs) took time to cover what a Starport is: Each peer chain to Gateway needs to have a Starport in order to enable collateral to be supplied to Gateway. It is required that assets be locked in the Starport of the peer chain and also an event must be emitted regarding the Lock, so that Gateway can read it, and recognize the account’s new balance. Once the collateral is Locked and the event is processed by Gateway, the user earns interest in CASH and can borrow another asset.\nGateway is the v3 of the Compound protocol. The current instance on Ethereum is v2 of the Compound protocol.\nJared asked about how block finality works on Polygon.\nJaynti (Polygon) says that it works similar to Ethereum, there is a checkpoint every 3 hours. Transactions go onto Ethereum every 3 hours and then they are considered final. Info can be read from the receipt on Ethereum at any time. A checkpoint is when all nodes on Polygon agree, then the data is verified and written to Ethereum.\nMihailo (Polygon) asked about other examples of Starports being built (EVM and non-EVM) so they can refer to those when building a Polygon Starport\nJared said that Polygon will likely be the first EVM based chain. Flow is being worked on, but it is not close to completion currently. The Polygon Starport might be the first completed besides the Ethereum Starport.\nAdam (Compound Labs) Asked what code changes must be made to Gateway in order to include a new Gateway Starport.\nJared said the runtime (Gateway code) must be modified and voted in through Compound governance (soft-fork). Gateway workers need to be made to fetch the events emitted by the new peer chain. Several enums need to be made in Gateway for the new chain, and then applied through a soft-fork code upgrade. Examples of some of the enums needed on Gateway are the addresses of the Starport and CASH token on the peer chain, and also the first block in which the Starport is recognized.\nToni (Compound Labs) Asked what is a good wait time for events on Polygon? For Ethereum, 6 blocks is a good measure. What is a good amount of blocks on Polygon?\nJaynti: 128 blocks is a good measure.\nMax (Compound Labs) asked do we need to wait for the checkpoint to be posted on Ethereum to be considered final?\nJaynti said yes, but this depends on how the Starport will work. Many applications are interacting with the chain directly. You can wait a certain number of blocks instead of waiting for the checkpoint. The process for re-orgs will be similar to Ethereum but we will need to do some more research on this implementation specifically before we know the answer.\nJay (community) asked: how can the community members help out? (for those that are not a part of Compound Labs or Polygon)\nJared: By reviewing the code that we publicly push to GitHub, like Wayne’s branch. This can be reviewed by the community which would be helpful.\nMihailo proposed for the group to come up with a roadmap for the project\nHamzah (Polygon) We should come up with the roadmap on the forums.\nMihailo asked: Are Gateway and Ethereum deployed to production (mainnet)?\nJared: No, we are using Ropsten testnet and we are awaiting audits. We should launch the Polygon Starport first on the Polygon testnet before a mainnet hookup.\n\\nAre there any permissions needed to comment/review on PRs?  my github account is kaishaku-ogami 1.  when trying to add a comment, it was not saved on PR #330 9.\\nI think its because you are a new GitHub user and your account was created 44m ago, we don’t have any settings in our repository limiting these kinds of interactions. Do you have another account you could use? Or perhaps just waiting a day will solve the problem.\\nOk, I will wait and see.  Makes sense.\\nSounds good. Thanks for putting this together.\n@jared - would love to work together to put a roadmap for the deployment and also how we can optimally engage community.\\ni hope comp can support matic layer\\nHello from Idle Finance 5, the longest-running DeFi yield aggregator \nCompound protocol represents one of the most important underlying yield sources used by Idle “Best-Yield” strategy on Ethereum, integrated since 2019.\nIn November '21, Idle deployed the Best-Yield strategy on Polygon 3 network, implementing Aave as the initial downstream lender.\nAs the Compound integration is ready and battle-tested for 2 years, we would be happy to add it as an additional yield source in Polygon.\nAs this post represents the main discussion about the deployment of Compound on Polygon, we are keen to know if there are some updates on this initiative, and how long will it take to complete the Polygon release."
  },
  {
    "number_of_comments": 34,
    "postid": "028096b3-eccf-4c40-be91-4d927ce59ca9",
    "posturl": "https://www.comp.xyz/t/add-markets-steth/1866",
    "combinedcontent": "so @TylerEther and are working on adding a few more assets to the Compound Protocol, starting with stETH. We are looking to engage community feedback for our proposal. All comments and reactions are welcome.\n\nProposal\nBackground\nstETH is pushing for listing on multiple lending markets\nAAVE 18\nCREAM 12\nCOMP 13\nLiquidity\nCurve 7\nDUNE 6\n\n  \n      \n\n      Lido – 26 May 21\n  \n\n  \n    \n\nConcerning stETH Liquidity | Lido Blog 12\n\n  Financial protocols need stETH to be liquid to be accepted as quality collateral. This article breaks down the various liquidity drivers of stETH.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nstETH Contract\n 3\n\n\nCollateral factor: 0% ?\n\n\nReserve factor: 25% ?\n\n\nComp speed: ???\n\n\nBorrow cap: ???\n\n\nSupply Cap\n\n\nPrice Reporter\n\nUniswap\nChainlink\nCurve\nPeg to 1 ETH\n\n\n\nSecurity\n\nInherited stETH risks: FAQ - Lido Finance 2\n\nSmart contract security\n\nOpen-sourced, audited, and covered by an extensive bug bounty program to minimize risk\n\n\nETH 2.0 - Technical risk\n\nETH 2.0 is still under development and there may be bugs in it which could affect this asset\n\n\nETH 2.0 - Adoption risk\n\nIf ETH 2.0 adoption is low, there may be significant fluctuations in the value of ETH and stETH\n\n\nDAO key management risk\n\nRisk of a DAO hack, rogue entities, or loss of access would result in funds becoming locked\n\n\nSlashing risk\n\nUp to 100% of staked ETH could be at risk. Multiple professional and reputable node operators with heterogeneous setups are used to mitigate this risk.\nInsurance paid from Lido fees can partially cover these losses.\nInsured by Unslashed Finance\n\n\nstETH price risk\n\nExchange price of stETH may be lower than its inherent value due to withdrawal restrictions on Lido.\n\n\n\n\n\n\n\nLIDO Incentives\n\nCurve 3\n\n\n\nInterest Rate Model\n\n(people will borrow eth to put into stETH)\n\n\n\nWill cETH IRM be able to handle this?\n\n\nWill comp distribution incentivize a move?\n\n\nIRM - ETH -switch to USDC 3\nIRM - stETH - Switch to ETH\ncETH Current IRM, New cstETH IRM\n\nChart1200×742 32.2 KB\n\nCurrent Stablecoin IRM, New cETH IRM\n\nChart1200×742 35 KB\n\nBorrowing/Lending activity between cETH and cstETH\nPeople will borrow ETH to convert to cstETH.\nSince the IRM for ETH is linear, the util will max out at around 50% as “arbitrage” disappears as ETH borrow rate meets cstETH supply rate.\nTo help this, we change IRM so that cETH can adapt to allow investing in cstETH\nWe will have cstETH IRM be close to what cETH is today, to anchorage long term holding in preparation for ETH 2.0 withdrawals.\n\nProposed stETH Risk Parameters\n\nLTV: 10%\nReserve Factor: 15%\n\nRelevant Links\n\nWebsite 1\nGitHub\nWhitepaper 1\nstETH Contract 1\nBlog\nPrimer 1\nAudits 1 1\nOperators\nLido Forum\nDiscord 1\n\\n\nOracle considerations\nWe have three approaches to how we’ll determine the price of stETH.\n\nPeg stETH to ETH at 1:1. When ETH 2.0 is merged, users will be able to redeem their stETH for EH at a 1:1 ratio. For now, Curve has the most stETH liquidity and the swap ratio is near 1:1.\nWait for ChainLink to create a stETH price feed.\nTake the price from Curve.\n\nSolution #1 would be the simplest and most efficient approach, but there is the risk of sETH’s price crashing due to any of the inherint risks of Lido.\\nSolution 1 might now work, as if the stETH discount increases beyond the liquidator discount it will not be possible to liquidate stETH backed loans. While stETH hasn’t had more than about 4% discount to date, it seems possible for the discount to grow larger than the 8% liquidator discount in stressed conditions.\\nGenerally, I think stETH would be a phenomenal asset to add.\nThe price feed questions could be reduced by using a 0% collateral factor to start, but would have to be answered. If users are only borrowing stETH (but not using it as collateral), a 1:1 peg is somewhat punitive to borrowers–but could be viable. Taking the price from Curve also works (by deploying a new View), but would need to be carefully researched/managed (there have been tons of issues using Curve from less secure protocols).\nLet’s make this happen! Thank you @massnomis and @TylerEther for kicking this off \\nYeah the price oracle question is going to be a fun one to explore. In the meantime a 0cf makes sense. Hopefully now ETH will be borrowed and put into steth, hopefully creating more borrowing power on the ETH market. @rleshner @monet-supply what do you think about the proposed IRM changes?\\n@massnomis, I am almost always wishing protocols/exchanges add additional assets. However, I think that adding an asset so closely tied in value to another (ETH, stETH) will open more doors for whales to game the system to obtain more COMP.\nAlso, once ETH 2.0 launches I see a slew of issues that may surface, not only with the redemption of stETH. Just look at Compound’s previously depreciated assets (still holding ~$130 million worth of assets). Thinking that Compound users will borrow ETH to deposit into Lido for stETH can and more than likely has been done. Just as users have also borrowed ETH to run ETH 2.0 nodes themselves.\nIf stETH does appear on Compound, I’d recommend the following:\n\nCollateral Factor: 0% -until ETH 2.0 launch\nReserve Factor: 25%\nCOMP Speed: 0 / day (unless COMP distribution increases)\nBorrow Cap: NONE\nSupply Cap: 10-20% of currently minted stETH\nPrice Reporter: Use all price reporters (for analytics) until the collateral factor is modified.\n\nUsing a 1:1 ratio to ETH price is a mistake, but as long as the CF remains at 0%, we really don’t have to worry about the price. If stETH price crashes significantly while using 1:1 ratio, then we could see stETH suppliers suddenly borrow up to their max to purposely get liquidated by liquidation bots. That would effectively enable those suppliers to exchange their lower valued stETH into any other asset on Compound. I could also see suppliers being able to leverage this by purchasing more stETH to supply for more collateral than they should have.\nBTW, how often does Compound’s price feed get updated? Compound may want to update stETH’s price feed more frequently.\\n@paraficapital stETH here\\nSo 0cf, 0 comp speeds. Supply cap is can be implemented by forking CREAM’s ERC20Capable and a comptroller patch, but for now, without incentives and CF, people are less likely to move over to farm, while CRV and LIDO are emitted over at curve.\nI talked to @Johann_Eid about chainlink bringing in stETH prices, and its on the way.\nI think for now, 1:1 ETH price, 0cf, 0 COMP, until chainlink can support it fully, and at that point we can migrate. This asset will need migration as ETH 2.0 Nears, so getting the ball running should be a good idea.\\nthe curve pool is massive, Possibly the deepest liquidity on chain out of all assets (not pegged to 1USD). I think that curve pricing alone ~can be enough, because to swing that price will be billions of capital, but possible with a “hack” of sETH over at Synthetix. Since there’s no CEX trading for stETH, curve’s prices are the most accurate. If stETH falls off par, keeping that peg could be an issue, but if its 0cf, then its minimal. with the current proposed conditions, I am all for pushing this forward.\\n@massnomis and I have discussed the highly likely scenario of users borrowing ETH, converting to stETH, and depositing again to farm COMP.\nIs this good or bad?\nA positive argument for this scenario is that Compound will encourage people to start contributing more and more ETH to validate the network, paving the way for a better ETH 2.0 launch.\nI’d love to hear more discussion of the pros and cons of this scenario; borrowing ETH, minting stETH, and depositing it back.\\nLet’s talk about it at the community call tonight?\\nThe biggest risk here is the discount could potentially get blown out if too many people do this trade. We’ve seen a similar dynamic play out with Grayscale products:\n\nGrayscale trusts like GBTC traded at a premium due to strong investor demand\nProp funds borrowed BTC, sold, and used proceeds to purchase GBTC, this allowed them to earn an implied yield from selling into the premium once the 6-12 month lockup ends\nToo many funds put on this same trade, causing the GBTC premium to collapse and even go negative\nFunds took a big loss, and apparently some of them defaulted on their BTC loans because the value of GBTC was no longer enough to pay back their debt\n\nThis is definitely a valuable opportunity for Compound to make cETH more productive with stable borrowing demand. But certainly not without risk.\nOnce stETH redemptions are enabled with the ETH2 merge upgrade, this price discount risk will go down considerably. But until then we should be a careful to set an appropriate collateral factor.\\nI certainly learned from that GBTC issue, very cool. Hopefully we will avoid that, we have no lockup and zero CF, I think good amounts of research should go into the CF, redemptions too.\\nFound this today:\n  \n    \n    \n    stETH Listing Proposal New Markets\n  \n  \n    This should be taken into consideration: \nA quote from a Coindesk article today… “* A rogue developer withdrew $500K worth of governance token SGT from decentralized Eth 2.0 staking service SharedStake. TAKEAWAY: A vulnerability in the timelock code that was meant to release SGT tokens gradually over time was exploited by one of the developers of SharedStake. The SGT tokens were subsequently dumped on the market, and the price of the tokens fell 96% from $1.60 to under $0.03. Furthermore, users …\n  \n\n\\nLDO tokens are the controllers for stETH, we should get some as part of the incentivization of LDO like they do on curve, and hold them in the governance wallet, or even a multisig.\\nTheir existing incentives on Curve (and maybe other DEX in the future) benefits lenders indirectly by supporting liquidity. No harm in asking if they’re interested in incentivizing users of the stETH market, but I’m sorta doubtful.\\nwe will get to that bridge later, but lets keep it in mind.\\n\nDeployment\nnpx saddle -n mainnet script token:deploy '{\n  \"underlying\": \"0xae7ab96520DE3A18E5e111B5EaAb095312D7fE84\",\n  \"comptroller\": \"$Comptroller\",\n  \"interestRateModel\": \"0xd956188795ca6F4A74092ddca33E0Ea4cA3a1395\",\n  \"initialExchangeRateMantissa\": \"2.0e26\",\n  \"name\": \"Compound stETH\",\n  \"symbol\": \"cStETH\",\n  \"decimals\": \"8\",\n  \"admin\": \"$Timelock\",\n  \"implementation\": \"0xa035b9e130F2B1AedC733eEFb1C67Ba4c503491F\",\n  \"becomeImplementationData\": \"0x\"\n}'\n\nInterest rate model: same as cLINK, cMKR, cSUSHI, cAAVE, and cYFI\nImplementation: same as cLINK, cMKR, cSUSHI, cAAVE, and cYFI\nThe interest rate model and implementation were chosen to be the same as standard ERC20 (non-stablecoin) because the utility of stETH is rather limited until the merge occurs and users are able to withdraw their ETH.\nThe interest rate model and possibly the implementation will have to be revisited around the time of the merge.\\nok this is epic.\n@getty the IRM look good for the other implementations?\\nLooks good. Do you want to use the ETH interest rate curve instead of the altcoin interest rate curve?\\nSince users can’t use stETH or the underlying ETH to pay for transaction fees, I think it will have a different market compared to ETH, so that’s why I think using the altcoin interest rate curve is best for now.\nAround the time of the merge, we can change the interest rate to one similar to ETH’s.\nThoughts?\\nChange ETH IRM to the stablecoin IRM, yes\\ncStETH deployed 7 to 0x6924c66ed77F5E45186b2E9a2b826ee061884624.\nnpx saddle match 0x6924c66ed77F5E45186b2E9a2b826ee061884624 CErc20Delegator 0xae7ab96520DE3A18E5e111B5EaAb095312D7fE84 0x3d9819210a31b4961b30ef54be2aed79b9c9cd3b 0xd956188795ca6f4a74092ddca33e0ea4ca3a1395 200000000000000000000000000 \"Compound stETH\" cStETH 8 0x6d903f6003cca6255d85cca4d3b5e5146dc33925 0xa035b9e130f2b1aedc733eefb1c67ba4c503491f \"0x\" -n mainnet\n\\nTyler you never cease to amaze me, thank you so much for your dedication and focus\\nI reached out in the discord Governance channel but figured I would do the same here for coverage.\nI am with Lido as part of business development and wanted to discuss the process to reengage this proposal?  We have a lot of updates since the last conversations here and it looks like there was already work done on the ctoken contracts.\nHappy to update data or provide additional insight that would be beneficial.\\nHello all\nI am just wondering what the current status is of the stETH integration in Compound. Would be a great addition.\\ncWstETH has been deployed 11 to 0xc7b9bE4b75280733655368F8a64645990944e3ca. Now we’re just waiting for Chainlink oracle support.\\nHi @TylerEther @massnomis has the plan to support stETH on Compound v2 been abandoned? I know it’s supported in Compound v3, but with $470M+ in ETH supplied and only $16M borrowed in v2, adding (w)stETH in v2 would be an easy way to boost the utilization rate (and fees).\nWould appreciate any relevant info here and feedback on the best way to get this proposal back on track.\\nHi @lucas, the initiative to add wstETH to Compound v2 was abandoned after over a year of effort. The primary issues were 1. adding enough liquidity to Uniswap v2 for the anchor to be reliable, or adding/changing the anchor source and 2. finding enough reporters to operate the custom feed.\nThe custom oracle contract was upgraded to use Uniswap v3 as an anchor quite a while ago, and I see that there’s sufficient liquidity for it to be reliable now. So the next step is to convince enough reporters to run the custom feed.\nI remember one reporter was willing to run the custom feed, although they were looking for around $30k/month to run it. One reporter is insufficient as they’d have sole control over the price +/- 15% of the Uniswap v3 1h TWAP. Any sort of reporting error can be detrimental.\nIf enough reporters can be convinced to run the feed, the next step is to convince Chainlink to deploy a new custom oracle (UAV) contract. Getty handled the interactions with Chainlink, so I can’t discuss this further.\nIf the above steps can be completed, then a series of proposals are needed. The first proposal is the UAV upgrade, and the second is the market listing (assuming the cWstETH contract I deployed doesn’t need any updates).\nI hope this helps!\\nHi @TylerEther appreciate your response.\nWill do some research on potential cost/benefit of paying for these custom feeds to understand if it is attractive enough for Compound to support.\nWill also reach out to a few Chainlink reporters to see if the cost could be lower than $30k/month since it sounds highly overpriced if that’s the case. Would welcome any feedback from @getty or other people with experience managing Chainlink support.\nHopefully we can make this work as the high amount of ETH available to be borrowed from Compound v2 seems like an untapped opportunity for protocol growth.\\nHi @TylerEther in order to support an asset within the Compound protocol, could you kindly provide guidance on the optimal number of operators or reporters required?\nIn addition it would also be beneficial to know Approximately how many requests (ie, price feed updates) per day would the protocol need. Furthermore will the price updates be time-based, or triggered by user actions?\\nHi @Pedro, it might be better to ask some of these questions to someone from Chainlink, but I’ll try my best to answer your questions.\nDeciding on an optimal number of reporters is difficult. Utilizing game theory, the value at stake by the set of reporters plus any additional costs (such as moving the anchor price) must be greater than the value that can be extracted or lost due to reporting problems. This is difficult to measure when it comes to a reporter’s reputation.\nWe can instead look at Chainlink’s stETH/USD price feed, where we see that it’s performed flawlessly with a requirement of at least 13 reporter responses. Past performance may not be a definite predictor, but it’s often the most reliable one. So I’d say 13 reporters is sufficient.\nChainlink’s feeds update based on either time or percent change. It looks like the stETH/USD feed updates at a minimum of once an hour, or when the price moves by 1%. In times of stability, one can expect 24 updates per day.\nLastly, if I recall correctly, the main difficulty is that Compound doesn’t support rebasing tokens, so wstETH must be used, but it doesn’t look like there’s a wstETH price feed, or a stETH-wstETH feed on mainnet, which makes it costly to start up a feed. I think rather than creating a new wstETH-USD feed, a cheaper wstETH-stETH feed could be created, and then the UAV could be modified to support that conversion.\\nGiven the community’s interest in deprecating the V2 market and the capital efficiency and risk management enhancements that Compound V3 offers compared to V2, if the community is interested in adding new markets and assets like stETH, we suggest proposing it for Compound V3 instead of V2.\\nGiven the lack of quality collateral in crypto in general, basically being btc, eth and stable coins indeed it makes sense to add wsteth to pretty much all comets. If somebody going to sit on eth collateral for a long term being exposed to price risk, it makes financial sense to do it in staked eth rather than in plain eth.  It is really strange to me that Compound do not have it still as collateral asset across all comets, while having something like matic, chainlink or uniswap, none of which is or going to be great collateral any time soon. Not that we should not be open to having some options for some “wanna be collateral” tokens, but seriously there is hardly any candidates to join btc, eth, their derivatives and stablecoins group in foreseeable future.\nSo i am as well for adding staked eth to v3. Which particular staked eth is open question, but lido would surely be one of the first to consider. Possibly coinbase and rocketpool versions might be another ones to consider as well.\\nI understand that the primary emphasis is currently on Compound V3. However, considering the substantial amount of idle liquidity in the Compound V2 ETH market and the fact that it’s been over a year since the V3 launch, there seems to be a noteworthy opportunity for the protocol to harness this unused liquidity.\nI reached out to several Chainlink node operators, and unfortunately they ended up offering support to cwstETH feeds for even higher costs than $30k/month. Nonetheless, we conducted an analysis of one scenario to assess how the reserve growth for the DAO would be impacted if the DAO were to bear this expense and introduce this market.\nOne of the key benefits of introducing stETH as collateral is that it should increase ETH borrows, and the protocol’s reserves along with it. For this proposal to make sense for Compound, the incremental reserve growth obtained from supporting stETH collateral should be greater than the cost of paying for the custom feeds.\nAs people take advantage of leverage staking, we can expect users to borrow ETH at most up to the point where borrowing costs equal the stETH APY. Given the interest rate curve currently in effect for ETH in Compound v2, the ETH borrow APY would reach 3.5% (slightly below the current stETH APY) at a utilization ratio of just 7%.\nUnder a scenario (option 1) where stETH collateral is added without any changes to the utilization ratio and the amount of ETH loans increases until the borrow APY reaches 3.5%, the projected reserve growth would be lower than the cost of the custom feeds. This clearly wouldn’t be beneficial for Compound.\nHowever, if stETH is integrated as collateral and the borrow curve for ETH is optimized, then the results are likely to be positive. In option 2, I explore what the reserve growth could be if the slope for ETH borrow rates were less steep, reaching a 3.5% borrow APY at 72% utilization rate. In this scenario, then the projected borrowing demand would be much larger.\n\n\n\n\nOption\nCost\nReserve Growth\nTotal\n\n\n\n\nOption 1\n($30k*12)= $360k\n(3.5%*$31M)*20%= $223K\n-$136K\n\n\nOption 2\n($30k*12)= $360k\n(3.5%*$323M)*20%= $2.26M\n$1.9M\n\n\n\nOption 2 would bring greater risks due to potentially larger ETH liquidations, but the potential reserve growth could be large enough to make this worth it.\nThese are simplified projections, but given the popularity of leverage staking and the hundreds of millions of idle ETH in v2, Compound is very likely to benefit from this scenario where wstETH is added as collateral and ETH borrow rates are optimized.\n@tylerEther would you think it is worth pushing this approach forward? It would also be possible to support stETH and then progressively improve ETH borrow rates in separate proposals, but I’d be curious to hear your thoughts."
  },
  {
    "number_of_comments": 11,
    "postid": "9c8d2eb8-af45-40a3-bb97-c2d6eb6e56f3",
    "posturl": "https://www.comp.xyz/t/remember-adding-mkr/299",
    "combinedcontent": "Hey everyone! Anyone remember the poll in 2019 for what two next tokens will be added? The winners were Tether and Maker. Tether has been added, and while we are talking about adding TUSD, shouldn’t we focus on adding what we said would be added earlier on?\\nI agree. I’d like to see another asset added soon. We would need a poster for the MKR price feed first.\\nYes but coinbase already supports it? https://www.coinbase.com/price/maker 1\\nTrue, but they aren’t posting for the open oracle. https://prices.compound.finance 5. @rleshner would you be able to request for them to add MKR? I’m sure you have better connections at Coinbase than me.\\nYes. I believe prices come from coinbase pro? what the difference? Maybe coinbase pro doesn’t support mkr, although I have no clue\\nCoinbase Pro does support MKR although it has low volume. Only 893 (~$530k) traded in the past 24 hours. Not sure if this is why they don’t post the price.\\nIf this is the case, then it is yet again another argument to make Chainlink our primary oracle. It is a shame to miss out on new collateral types because Coinbase cannot adequately support them.\\nI think adding MKR is a great idea, although I wouldn’t feel comfortable with the standard 60% CF given that the oracle (Coinbase) is not good quality. I suggest we vote MKR in with a <25% CF and increase the CF once a better oracle solution is in place.\\nCoinbase doesn’t support MKR at all through the open oracle. We would need to find an alternative oracle before adding it.\\ncould we do something like adding okex and taking the price from okex?\\nOKEx doesn’t report MKR price either.\\nI quite agree with you."
  },
  {
    "number_of_comments": 9,
    "postid": "276d3e91-3102-400d-819c-a8d25f9818bb",
    "posturl": "https://www.comp.xyz/t/new-listing-proposal-bilira-stablecoin-tryb/3654",
    "combinedcontent": "The BiLira team would like to submit a proposal to onboard TRYB. The purpose of this AIP is to list BiLira (TRYB), the only and 1:1 Turkish Lira-backed stablecoin, as a collateral asset on Compound.\nReferences\nLink to:\n\nProject 10\nWhitepaper 9\nToken contracts: Ethereum 9, Avalanche 6, Polygon 6, Solana 6, BNB 5,\nDune Analytics Dashboard 7\n\nParagraph Summary\nThe BiLira (TRYB) token is a full-reserve stable cryptocurrency that is built on the Ethereum blockchain and is available on 6 different blockchains, issued and managed by the BiLira organization, backed by the Turkish Lira and collateralized 1 : 1, secure and compatible with ERC-20 token standards. As emerging technologies continue to expand across the globe, the concept of open internet has allowed individuals all around the globe to instantly, securely and effortlessly share value and this created a need for non-USD stablecoins. We want to add TRYB as a collateral asset on Compound.\nMotivation\nBiLira offers every Turkish citizen a seamless connection to the decentralized internet. As emerging technologies continue to expand across the globe, the concept of the open internet - a fundamental network of information that is free and accessible to everyone regardless of financial motives - has allowed individuals all around the globe to instantly, securely, and effortlessly share information. The contributions to this advancement are immense and its impact is still unraveling around the world.\nToday, crypto assets and blockchain technology provide the means to transfer assets globally, securely, and at a low cost. Establishing an open internet based on value exchange can pave the way for a seamless, borderless and integrated world, which can eliminate barriers that enable the development of a global marketplace that is both economical and inclusive of everyone.\nSpecifications\nThe BiLira (TRYB) token is a full-reserve stable cryptocurrency that is built on 6 different blockchains, issued and managed by the BiLira organization, backed by the Turkish Lira and collateralized 1 : 1, secure and compatible with ERC-20 token standards\nBiLira goes through regular audits performed by independent 3rd parties which can be found here 1.\nTRYB is a Turkish Lira backed stablecoin. BiLira aims to bridge the gap between web 2.0 (monetized by fiat) and web 3.0 (fueled by crypto assets), to offer a convenient on-ramp and off-ramp solution for crypto traders to combat high volatility and to support the future applications of decentralized finance.\nThe legacy financial system is dependent on private organizations and third-party services which makes it challenging for individuals to connect to the open internet and transact with one another. The current infrastructure does not support a transition from Web 2.0 banking system to the Web 3.0 crypto-based native web currencies. Since mainstream cryptocurrencies such as Bitcoin and Ethereum are volatile, the public is looking for a better way that is more stable and safe to interact with digital assets and cryptocurrencies\nJust as information flows freely between web browsers, BiLira aims to make transferring value between wallets effortless, instant, and borderless for everyone.\nPositioning of the token in the Compound ecosystem.\nTRYB (BiLira) is a great fit both as a borrow and collateral asset for the Compound ecosystem.\nCompound will be onboarding a lot of new users from a very active Turkish crypto community by listing the reliable and only Turkish Lira backed stablecoin, TRYB (BiLira). Turkey was the world’s fourth biggest cryptocurrency market in 2020 and became the largest cryptocurrency market in the Middle East in 2021. By enabling this user base to be able to use their currency as collateral, Compound will welcome a lot of new users to enter into DeFi.\nThere are also multiple institutional and retail players that want to borrow TRYB to market make in Turkish Lira denominated crypto markets.\nBrief history of the project and the different components\nThe stablecoin TRYB(BiLira) was released in early 2020.\nAs it is a 1:1 fiat backed stablecoin doesn’t have any pegging issues due to its nature.\nAdvanced fiat on/off-ramp:\nBiLira offers a convenient on-ramp and off-ramp solution for crypto traders to combat high price volatility and to support the future applications of decentralized finance. BiLira builds a fiat <> crypto on/off ramp for users to interact with global exchanges and DEXs as it is the only Turkish Lira backed stablecoin.\nGlobal coverage:\nIt will allow Turkish Users to participate on Compound with only a domestic bank transfer. Compound will be onboarding a lot of new users from a very active Turkish crypto community. Turkey was the world’s fourth biggest cryptocurrency market in 2020 and became the largest cryptocurrency market in the Middle East in 2021. Turkey topped a million trades a day by the end of 2021. With over 40 exchanges operating in Turkey, the Middle East’s largest economy and home to around 85mn people, the adoption of cryptocurrencies is ever expanding. Turkish residents using cryptocurrencies were recorded at 16% to 20% between 2020 and 2022. Latest surveys show there are over 5 to 6 million Turkish people with cryptocurrency accounts. A cryptocurrency exchange in the country reported nearly 5 million users and had a trading volume of $203.5 million.\nTowards Decentralization:\nTurkish crypto users generally start their crypto journey through the local exchanges because they do not have to deal with the complexities of the blockchain technicalities, such as use of wallets and addresses. The BiLira team believes in the future of the Web3 decentralized ecosystem, where users control their assets via their own wallets, removing intermediaries and interacting with DeFi.\nToken (& Protocol) permissions (minting) and upgradability.\nTRYB tokens are backed 1:1 with TRY fiat reserves, BiLira (1 TRYB) is always supported by one unit of the reserve currency (1 TRY). There is no risk for users when our customers withdraw TRYB to Turkish Lira we burn that amount of TRYB.\nMasterMinter - adds and removes minters and increases their minting allowance (2 out of 3)\nMinters - create and destroy tokens (2 out of 3)\nPauser - pause the contract, which prevents all transfers, minting, and burning (2 out of 3)\nBlacklister - prevent all transfers to or from a particular address, and prevents that address from minting or burning (2 out of 3)\nOwner - re-assign any of the roles except for admin (2 out of 3)\nAdmin - upgrade the contract, and re-assign itself (2 out of 3)\nThe signers of the Multisig are the cofounders of the BiLira Company\nMarket data (Market Cap, 24h Volume, Volatility, Exchanges, Maturity)\n\nTotal Issuance: 3,782,105,670 TRY\nTotal Redemption: 3,642,521,623 TRY\nTRYB AUM: 139,584,047 TRY\nFiat Reserves: 139,584,047 TRY\nTotal on-chain transfer volume: 30,002,670,196 TRYB\nListed on FTX, MEXC, Bitget, Bittrex, Liquid, Pangolin and DFX\nOracle data for TRYB is available on Chainlink\n\nSocial channels data (Size of communities, activity on Github)\n\nTwitter 2\nTelegram\nLinkedIn\nYoutube\nInstagram\nNews: Crunchbase\nNews: Cointelegraph\nConferences: Converge22 1\n\nThe total size of the media community is over 50,000+ users across all social media platforms\n\nRisk Parameters\n\nLTV: 0%\nLiquidation Threshold: 70%\nLiquidation Bonus: 10%\nReserve Factor: 10%\n\n$10 Mil USD Debt Ceiling (Equivalent to 185 Mil TRY Debt Ceiling)\nContracts date of deployments, number of transactions, number of holders for tokens\nOn Ethereum:\n\nDate of Deployment: 2019-07-19\nToken Tracker 9\nNumber of Transactions: 19594 transfers\nNumber of holders for token: 829\n\nOn Avalanche C-Chain:\n\nDate of Deployment: 2021-05-27\nAddress 0x564a341df6c126f90cf3ecb92120fd7190acb401 | C Chain | Overview | AVAX Explorer | AVASCAN 6\nNumber of Transactions: 2308\nNumber of holders for token: 363\n\nOn Solana:\n\nDate of Deployment: June 20th 2022\nExplorer | Solana 6\nNumber of Transactions: 10003\nNumber of holders for token: 924\n\nOn BNB Chain:\n\nDate of Deployment: 2021-05-26\nBiLira (TRYB) Token Tracker | BscScan\nNumber of Transactions: 1154\nNumber of holders for token: 208\n\nOn Polygon:\n\nDate of Deployment: November 20th 2021\nBiLira (PoS) (TRYB) Token Tracker | PolygonScan 6\nNumber of Transactions: 4550\nNumber of holders for token: 102\n\nSecurity Considerations\nBesides possible smart-contract risks,TRYB stablecoins are backed by fiat TRY and therefore, are the subject to fiat currency risk.\nRisk Analysis\n\nThere is no regulation on stablecoins in Turkey. BiLira has been audited by the Central Bank of Turkey. Users need to go through the KYC and AML process.\nThere haven’t been any hacks or vulnerabilities on the project\nThe contract is verified on the explorers mentioned above in market statistics\nThe project can’t be considered a security as TRYB tokens are backed 1:1 with TRY fiat reserves, BiLira (1 TRYB) is always supported by one unit of the reserve currency (1 TRY). There is no risk for users when our customers withdraw TRYB to Turkish Lira we burn that amount of TRYB.\n\\nI have been waiting for this for a while. TRYB is super popular in Turkey. The currency itself constantly keeps losing value, but most users choose to use TRYB as a fiat on/off ramp solution.\nI think listing TRYB would allow Compound to onboard wide range of  Turkish DeFi users. BiLira would be the right address to reach the Turkish DeFi users. I support this proposal!\\nBizde bunu bekliyorduk. Başarılar.Desteklemeye devam …\\nGreat proposal. I have been using BiLira for 2 years now, one of the best web3 projects born out of Turkey. They provide wide accessibility thanks to its compatibility with 6 different chains. It is very easy to move money around different ecosystems. Considering Turkey is the one the biggest Crypto markets in the world, it would be wise to list it here to welcome more members to this beloved community. Cant wait for the outcome!!!\\nÇok sevindirici bir haber BiLira ekibini her zaman canı gönülden destekliyoruz \\nI’ve been following this project for two and a half years. It’s time to take this project to the place it deserves. I hope I see my TRYB’s on the Compound platform.\nLong live DeFi \\nbilira is one of the best web3 projects in turkey and actively working on defi platforms, it would be great to see tryb in compound\\nGreat proposal and opportunity for Turkish Compound users! \\nI have been using TRYB as stable coin for my transfer for a long time. It will be good to see TRYB at Compound.\\nEDIT:\nThe BiLira team would like to submit a proposal to onboard TRYB. The purpose of this AIP is to list BiLira (TRYB), the only and 1:1 Turkish Lira-backed stablecoin, as a collateral borrowable asset on Compound V3."
  },
  {
    "number_of_comments": 12,
    "postid": "7e993413-9c30-4e3c-8f91-2845287ebfe4",
    "posturl": "https://www.comp.xyz/t/deprecate-fei-market/3513",
    "combinedcontent": "\nOverview\nFei Protocol is planning to be wound down 22 in the near future. Based on current plans, all FEI will become redeemable for an equivalent amount of DAI, with surplus PCV used to reimburse losses from the Rari Capital hack and pay out TRIBE token holders.\nTo ensure an orderly wind down process for Compound’s cFEI market, I believe Compound should adopt the following changes:\n\nDisable new FEI borrowing and supplying\nSet reserve factor to 100%\n\n\nProposal Actions\ncomptroller -> _setBorrowPaused -> cFEI, 1 (True)\ncomptroller -> _setMintPaused -> cFEI, 1 (True)\ncFEI -> _setReserveFactor -> 1E18 (100%)\n\nNext Steps\nAfter allowing time for community discussion and possible objections, I hope to push forward an on chain proposal for the above actions in the coming weeks.\nIf within 1 month of proposal execution the amount of FEI borrows outstanding is still large, the community could consider further changes such as interest rate model change (increasing borrowing rates) to ensure all cFEI users are able to withdraw funds and participate in the TribeDAO organized redemption process promptly.\\nA few notes for deprecating a market;\n\n\nWhen a market has a 0% CF and 100% Reserve Factor, the protocol allows all borrowing positions to be immediately closed. This is intended behavior (and makes sense in this situation), but users should be warned ahead of time.\n\n\nIf there is a possibility that the asset is being globally deprecated, the community should decide whether to withdraw the reserves (which are no longer needed to cover bad debt in the market), and transfer them to another market. FEI, REP, and SAI all fit this framework (REP and SAI reserves were not offboarded either).\n\n\\n\n\n\n rleshner:\n\nWhen a market has a 0% CF and 100% Reserve Factor, the protocol allows all borrowing positions to be immediately closed. This is intended behavior (and makes sense in this situation), but users should be warned ahead of time.\n\n\nGood point. We would definitely want to give significant notice before putting FEI borrowers in a position where they could be liquidated.\nMaybe it would make more sense to split the proposal into 2 parts.\nPart 1:\n\nDisable FEI borrowing\nDisable FEI depositing\n(Optional) Set FEI reserve factor to high, but less than 100% value\n\nPart 2:\n\nWait at least 1 month from execution of part 1\nSet FEI reserve factor to 100% (which allows for liquidation of outstanding borrows)\n\\n@monet-supply @rleshner Sorry for the late answer, I missed this post. I wonder if it would make sense to disable supplying only in part 2.\nThe market is currently illiquid on Compound so users that would want to withdraw immidiately are not able too. On the other side, users that are ready to take a larger illiquidity risk but benefit from the current high APY could supply FEI for the little bit of time left, enabling the first class of users to withdraw asap. With time, the borrowers left will repay, enabling everyone to withdraw.\nMoreover, it would also simplify the FEI wind down on Morpho, the biggest Compound borrower (and second depositor) all markets combined this last quarter. Indeed, Morpho borrowers, to repay their positions, will sometimes trigger a supply on Compound.\nThis is not necessary for Morpho as there are workarounds for this specific case but it would simplify things. This is why, if there is any significant downside in disabling FEI depositing in part 2, I would suggest that we stick to the plan proposed by @monet-supply.\\nThanks for your comment. I see no issues with moving freeze on new supplying to part 2 of proposal. So tentatively:\nPart 1:\n\npause borrowing\n(optional) set reserve factor to high value, less than 100%\n\nPart 2:\n\npause supplying\nset reserve factor to 100% (enables closing all remaining borrows)\n\\nGiven the wind-down of Fei, Gauntlet is supportive of this proposal. We’d note that utilization of the market has reached 100%.\\nFEI proposal here:\n\n  \n      \n\n      compound.finance\n  \n\n  \n    \n\nCompound 25\n\n  Compound is an algorithmic, autonomous interest rate protocol built for developers, to unlock a universe of open financial applications.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\\n\n\n\n monet-supply:\n\nPart 2:\n\nWait at least 1 month from execution of part 1\nSet FEI reserve factor to 100% (which allows for liquidation of outstanding borrows)\n\n\n\ncc @monet-supply – is it feasible to move ahead with setting reserve factor to 100% so that lenders can exit the pool within the next few weeks? Volt Protocol is the largest lender left in the market and while it is a small portion of PCV and not impacting user liquidity, it will be good to consolidate lending activities into the DAI market.\\nProposal was executed around 10 days ago, would like to give a bit more time for notice before borrows get force closed (and face potential liquidation fee). Would love to hear community’s thoughts on if 30 days would be enough time to wait before submitting the final deprecation proposal (setting reserve factor to 100% and disabling supplying)?\\nI wanted to follow up here to confirm that there are no strong objections to submitting the next stage of the deprecation process 30 days after the initial proposal executed. That would be on October 10. As essentially the only lender left in the market from which Compound has both withdrawn the reserves and set the reserve factor to 99%, Volt Protocol would appreciate being able to clearly confirm our expected withdrawal date.\\nWith one week to go until the suggested next proposal submission date (October 10), wanted to check in and see if there are any obstacles, and also confirm who will be submitting the proposal.\nAs only one borrower has repaid since the start of the deprecation process despite being charged ~40%, I suspect that the remaining liquidity represents forgotten or abandoned positions. Another round of pings on Compound Twitter and Discord might be worth doing at this time just in case anyone will see it.\ncc @monet-supply @pauljlei\\nThanks, @OneTrueKirk. We plan on putting up an on-chain proposal this Sunday 10/9/2022 that 1) sets FEI reserve factor to 100% and 2) pauses FEI supply. We also provided notice in the Tribe governance forums 6.\\nThank you @pauljlei, we look forward to the outcome of the proposal and will endeavor to spread the word on our end."
  },
  {
    "number_of_comments": 14,
    "postid": "2688da56-d75c-403a-99b4-af5fd756a887",
    "posturl": "https://www.comp.xyz/t/risk-parameter-updates-2022-11-23/3802",
    "combinedcontent": "\nSimple Summary\nA proposal to adjust fourteen (14) risk parameters (borrow caps) for fourteen (14) Compound V2 assets.\n\nAbstract\nGauntlet’s simulation engine has ingested the latest market and liquidity data. These recommendations are Gauntlet’s regular parameter recommendations as part of Dynamic Risk Parameters 10.\n\nMotivation\nThis set of parameter updates seeks to maintain the overall risk tolerance of the protocol while making risk trade-offs between specific assets. Gauntlet has published a blog post 9 on our parameter recommendation methodology to provide more context to the community.\nOur parameter recommendations are driven by an optimization function that balances 3 core metrics: insolvencies, liquidations, and borrow usage. Our parameter recommendations seek to optimize for this objective function. Gauntlet’s agent-based simulations use a wide array of varied input data that changes on a daily basis (including but not limited to user positions, asset volatility, asset correlation, asset collateral usage, DEX/CEX liquidity, trading volume, expected market impact of trades, liquidator behavior). Our simulations tease out complex relationships between these inputs that cannot be simply expressed as heuristics. As such, the charts and tables shown below may help understand why some of the param recs have been made but should not be taken as the only reason for recommendation. Our individual collateral pages on the dashboard 13 cover other key statistics and outputs from our simulations that can help with understanding other interesting inputs and results related to our simulations.\nTop 30 borrowers’ aggregate positions & borrow usages\n\n3654×1070 420 KB\n\nTop 30 borrowers’ entire supply\n\n3642×1064 338 KB\n\nTop 30 borrowers’ entire borrows\n\n3646×1026 281 KB\n\n\n\n\n\nParameter\nCurrent Value\nRecommended Value\n\n\n\n\nUSDC Borrow Cap\nNo Limit\n500,000,000\n\n\nDAI Borrow Cap\nNo Limit\n400,000,000\n\n\nUSDT Borrow Cap\nNo Limit\n300,000,000\n\n\nTUSD Borrow Cap\nNo Limit\n10,000,000\n\n\nETH Borrow Cap\n100,000\nNo Change\n\n\nWBTC Borrow Cap\nNo Limit\n1,250\n\n\nBAT Borrow Cap\nNo Limit\n900,000\n\n\nUNI Borrow Cap\n11,250,000\n550,000\n\n\nCOMP Borrow Cap\n150,000\n18,000\n\n\nLINK Borrow Cap\nNo Limit\n45,000\n\n\nSUSHI Borrow Cap\nNo Limit\n750,000\n\n\nZRX Borrow Cap\nNo Limit\n1,000,000\n\n\nAAVE Borrow Cap\n66,000\n12,000\n\n\nYFI Borrow Cap\n1,500\n20\n\n\nMKR Borrow Cap\n5,000\n300\n\n\n\nSetting borrow caps help avoid high-risk attack vectors while sacrificing little capital efficiency and allowing for a threshold of organic borrow demand. Additionally, these conservative borrow caps still would not have prevented users from borrowing based on organic user borrow behavior in the past month. Given that the vast majority of borrows on Compound (> 96%) are consistently stablecoins, Gauntlet recommends setting borrow caps for these non-stablecoin assets given the risk/revenue tradeoffs. As organic demand for borrowing grows, the community can reassess borrow caps and raise accordingly depending on the market risk considerations.\n\nNext Steps\n\nInitiate a Compound Proposal\n\nBy approving this proposal, you agree that any services provided by Gauntlet shall be governed by the terms of service available at gauntlet.network/tos.\n\nQuick Links\nAnalytics Dashboard 6\nRisk Dashboard 13\nGauntlet Parameter Recommendation Methodology 9\nGauntlet Model Methodology 3\nGauntlet launched an insolvency refund for Compound that contains a portion of our payment stream that can be clawed back in the event of insolvencies due to market risk. Since our last recommendation there have been no new insolvencies in Compound, Gauntlet’s Insolvency Refund vault is still live and can be seen here 0x7667095Caa12b79fCa489ff6E2198Ca01fDAe057 4\\nHello @pauljlei, thank you for this proposal. We at Morpho Labs think that this proposal could drastically reduce the relevance of the protocol and thus raise new risks. We explain why and propose an alternative solution.\n\nObservations\n\n\nBorrowing use case limitations\nAnyone relying on the borrow functions of the Compound protocol can experience reverts for extended periods of time, if the borrowing cap is reached. This could happen before but was very unlikely in practice due to the interest rate model. This is not convenient and will (very) likely prevent many use cases for sustainable borrowing activity from being built on top of the Compound protocol.\n\nReduced borrow volume from protocols\nIn the case of Morpho, the biggest borrower and second biggest supplier of the Compound protocol, the introduction of the borrow caps would push Morpho Governance to deactivate the peer-to-peer matching engine of Morpho-Compound. This would highly reduce the interest in Morpho-Compound compared to Morpho-AaveV2. All the volume will very likely migrate to AaveV2 (via Morpho-AaveV2) if this proposal passes. This is what happened with the $ETH market a few weeks ago. This alone would significantly lower the borrowed volume on Compound. A lower borrowed volume means fewer interests generated, leading to worse rates for Compound suppliers, which would then likely diminish the total supply.\n\nAgainst “real” borrowers\nMoreover, as this proposal on Compound III ETH 3 suggests, $COMP emissions will also likely stop on the Compound protocol, reducing even further the borrowing demand of the protocol. Note that many of the largest borrowers are still using Compound only for farming $COMP. That’s the case for this account 4, this account, and this account, for example, gathering around $110M of total borrowed volume. This emphasizes the importance of preserving “real” borrowers of the protocol like Morpho-Compound.\n\nHuge change for Compound\nThe proposed changes are very limiting, and while we generally trust Gauntlet’s work, here we feel a lack of explanation and data for such a big limitation. The decision seems rushed and leaves very little time for discussion.\n\n\nSolution\nEven though we understand the concerns raised by Gauntlet, we believe that introducing the borrow cap can drastically reduce the relevance and the competitiveness of the Compound protocol.\nWe think the concerns could be addressed by having a improved interest rate model: integrating a liquidity dimension to the interest rates curve, such that the borrow rate spikes when the total borrow volume reaches a set parameter. This parameter would be an estimated safe maximum borrow amount, taking into account asset liquidity, trading volume, DEX/CEX liquidity etc. The induced spread could be taken as a reserve factor, helping cover any potential issues (to quantify). Ideally, this should apply only to new borrowers, such that long term ones are not impacted. An exception can be made for governance tokens where governance attacks should be prevented.\nThis would be in the same vein as the protocol’s current interest model where rates are adapted to take into account new liquidity and new borrowing demand. Indeed the interest rate model currently works as a measure against illiquidity, just as this proposal would make the interest rate model prevent too high total debt.\nHere are some of the important advantages of this approach:\n\nRetain sustainable borrowers: This prevents the activity of the Compound protocol from falling apart, especially with $COMP rewards emissions likely turning off soon. It keeps the full relevance of Compound, while not compromising on safety.\nStrengthen the reserves of the Compound protocol when market utilization is high. This reserve can be re-used to resorb bad debt.\n\nAlternatively, and as a short term solution, the current interest rates model’s parameter could be changed, along with liquidations parameters. These solutions have shown efficient for the moment on Compound, while not being limiting hard caps.\\nThese parameters can limit borrowing of protocols, limit collateral and supply to Compound\nWe are convinced that there can be other ways to tackle this risk, more conservative interest rate curve  or adjusting LTV\nDefiEdge\n\n  \n      \n\n      app.defiedge.io\n  \n\n  \n    \n\nDefiEdge 5\n\n  Asset management protocol built on uniswap v3.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\\nI agree with previous comments and think that the same goals can be accomplished through integrating a liquidity dimension to the interest rate curve.\nFundamentally, I think the idea of introducing borrowing caps is problematic and might cause volume to migrate to Aave.  It’s a huge change and needs to be thought about very carefully.\nPart of the problem is that Gauntlet’s proposals don’t get a lot of push-back because their simulations are somewhat of a black-box, but I think this change is an atypical one and could cause a lot of inadvertent damage.\\nDon’t throw the baby out with the bathwater.\\nThanks all for the thoughts. While we appreciate the strategic considerations and partnership with ecosystem participants like Morpho, from a risk perspective, setting borrow caps immediately helps protect the protocol from tail risks.\nSetting borrow caps allow the community to granularly protect against a wide variety of market risks and attack vectors, including but not limited to:\n\nInsolvency risk from liquidation cascades\nPrice manipulation “Mango squeeze” exploits: allowing attackers to borrow a significant amount of assets reduces the attackers’ cost basis for performing such attacks. In addition, it directly reduces their maximum profit (because the borrow cap limits how many assets from the protocol can be drained).\nRisk of high utilization (coming from demand to short assets) impeding atomic liquidations: setting borrow caps can help prevent 100% utilization of an asset\nRisk from shorting assets: if a user puts on a short position on Compound of significant size relative to the circulating supply of the asset being shorted, then this can pose outsized risk that leads to insolvency. Borrow caps can immediately limit the protocol’s exposure to this risk.\n\n\n\n\n PaulFrambot:\n\nWe at Morpho Labs think that this proposal could drastically reduce the relevance of the protocol and thus raise new risks.\n\n\nThe poster claims that the relevance of the protocol can be “drastically” reduced without providing any quantification of the opportunity cost. While the opportunity cost is not quantified, the benefits of setting borrow caps are incredibly straightforward, as they set a clear limit to the amount of borrow exposure that poses risk to the protocol.\n\n\n\n PaulFrambot:\n\nBorrowing use case limitations\n\n\nThe borrow caps proposed above allow for organic borrowing demand (e.g., the USDC, DAI, and USDT borrow caps are 2x the current amount of existing borrow on Compound).\n\n\n\n PaulFrambot:\n\nReduced borrow volume from protocols\n\n\nThis impact has not been quantified by the poster. In addition, as mentioned above, these borrow caps still allow for organic borrowing demand. If the pure existence of borrow caps prevents protocols like Morpho from integrating with Compound, then the Compound community needs to make a decision: are we willing to completely forgo the use of borrow caps in its entirety? Or do we use borrow caps (that have been deliberately built into the Compound system to mitigate risk), and have ecosystem partners integrate with Compound as our mechanisms have intended? From a risk-reward perspective, the picture is very clear. Setting borrow caps allow the community to precisely control the security threshold and help protect against long-tail risks.\n\n\n\n PaulFrambot:\n\nThis alone would significantly lower the borrowed volume on Compound. A lower borrowed volume means fewer interests generated, leading to worse rates for Compound suppliers, which would then likely diminish the total supply.\n\n\nWe caution the community against accepting broad conclusions like these without quantification. If Morpho can provide numbers on the expected impact, that would be helpful.\n\n\n\n PaulFrambot:\n\nWe think the concerns could be addressed by having a improved interest rate model: integrating a liquidity dimension to the interest rates curve, such that the borrow rate spikes when the total borrow volume reaches a set parameter.\n\n\nWhile potentially an interesting idea, it is unclear 1) who in the community would develop this, 2) whether this will work in practice, and 3) how long it would take to develop/test/implement this. On the other hand, setting borrow caps immediately can mitigate risk.\n\n\n\n PaulFrambot:\n\nAlternatively, and as a short term solution, the current interest rates model’s parameter could be changed, along with liquidations parameters. These solutions have shown efficient for the moment on Compound, while not being limiting hard caps.\n\n\nInterest rate curves do not completely protect the protocol from the risks outlined above. Especially for attack vectors that require borrowing an asset for only a short amount of time, high interest rates do not prevent such attacks if the expected profit of the attack (which we have seen in DeFi can be hundreds of millions of dollars) outweigh the interest cost. In addition, liquidations parameters also do not fully protect against some attacks and also come at the cost of capital efficiency for the protocol. What’s more, we have observed that market conditions have significantly changed since the FTX fallout. Market conditions are dynamic, and therefore, risk adjustments for the protocol should be dynamic. Solutions that have “shown efficient” in the past do not translate to changing market risks.\n\n\n\n 0x_vd:\n\nThese parameters can limit borrowing of protocols, limit collateral and supply to Compound\n\n\nSetting borrow caps do not prevent users from lending or supplying collateral on Compound.\n\nTL/DR: Setting borrow caps immediately helps mitigate tail risk and attack vectors while allowing for a threshold of organic borrowing growth.\\nThanks @pauljlei for your detailed answer.\nNow, borrow caps certainly have benefits. For your fourth point for example, it makes sense to keep some borrow caps on these assets. But that’s not the case at all for all assets: around 1% of USDCs are on Compound, and 0.3% of USDTs. And for the two first points, of course, capping the of borrow reduces the total funds at risk, but it lacks a counterbalance from a utilization and adoption point of view (eg. an empty pool has no risk at all).\nOn the form of this proposal, we regret several things. First, no explanations about the values of the borrow caps and assets chosen are given, aside from the fact that Gauntlet used “user positions, asset volatility, asset correlation, asset collateral usage, DEX/CEX liquidity, trading volume, expected market impact of trades, liquidator behavior” to determine them. It would also have facilitated the conversations. Here the community must blindly trust you and for changes as important as these, we find that it is not normal. That’s not what DeFi has been made for. On our side, contrary to what you said, we provided some data about the potential losses for Compound: Morpho, the one of Compound’s biggest borrower (the first in your chart above), and one of the rare non-recursive borrower, will have to leave the markets if some borrow caps are set everywhere.\nSecond, the time given to the community to discuss before the beginning of the vote was really short. We understand that there is sometimes the need for quick adjustments when it comes to risk parameters, but this should not be at the price of having to fully rely on the work of one entity. We would have liked to provide more data about the topic, but the time constraints were too short to have a deep conversation.\nIn short, we understand the benefits of setting borrow caps on Compound, but we question the fact that they should be set on all market, potentially forgetting other aspects than risk. The little time given for the community to debate on this topic does not help.\\nI completely agree with your points, @PaulFrambot – that borrow caps should be measured against the utility of the protocol, and that discussion about this important trade-off should occur before important changes are proposed.\nThat being said, Proposal 135 11 strikes in my mind the right balance between limiting utility and risk, and I plan to vote for the proposal.\nThis proposal implements borrowing caps on collateral assets (which historically have very little borrowing demand) and not stablecoin assets (which are the core borrowing demand for the protocol). Given the bad debt incurred by Aave through CRV (a minor collateral asset) borrowing recently, limiting this risk in Compound v2 makes sense. If there is borrowing demand for one of these collateral assets, governance can easily increase or remove the cap later.\nIt should be noted that Compound III was designed specifically to avoid these types of risks.\\n\n\n\n PaulFrambot:\n\nthe introduction of the borrow caps would push Morpho Governance to deactivate the peer-to-peer matching engine of Morpho-Compound.\n\n\nI don’t follow why this would be the case @PaulFrambot , can you elaborate?\n\n\n\n rleshner:\n\nnot stablecoin assets (which are the core borrowing demand for the protocol).\n\n\nThis compromise makes a lot of sense. As a stablecoin lender, we don’t see imposing a cap on max stablecoin total borrow as sensible, while otherwise don’t have much of a dog in this fight.\n\n\n\n PaulFrambot:\n\nWe think the concerns could be addressed by having a improved interest rate model: integrating a liquidity dimension to the interest rates curve, such that the borrow rate spikes when the total borrow volume reaches a set parameter. This parameter would be an estimated safe maximum borrow amount, taking into account asset liquidity, trading volume, DEX/CEX liquidity etc. The induced spread could be taken as a reserve factor, helping cover any potential issues (to quantify). Ideally, this should apply only to new borrowers, such that long term ones are not impacted. An exception can be made for governance tokens where governance attacks should be prevented.\n\n\nThe idea of applying a higher rate to marginal new borrows is very interesting.\\nI agree with Robert on this point.  One of the key points that caused me to change my mind was that this is a short-term solution and can be adjusted if needed.  @PaulFrambot also makes good points.\\nSorry for the slow answer; I have been away.\n@OneTrueKirk, in the current version of the Morpho-Compound, to make sure funds matched P2P are liquid, the protocol must be able to borrow on the pool. The lending pool should not have reached the borrow caps to offer this guarantee. Even if this is only a hedge case, not sure if the governance would be comfortable with this. Hope it is clearer.\n@rleshner, as you mention, Proposal 135 3 only sets borrow caps for assets that are not stablecoins, which balances the risk-utility trade-off in favor of setting borrow caps. Our point was mostly referring to stablecoins.\nIt looks like the proposal passed, but I am glad we could have this healthy debate. Morpho Labs continues to believe that the Compound protocol should not only consider risk for upcoming proposals but also the utility constraints of the protocol.\\nGauntlet Has 126,105.9 votes. Morpho didn’t vote against with $COMP.\nAlthough I support Morpho psychologically, I feel sad for this phenomenon.\nIs the governance result necessarily correct?\\n“The notion of “governance rights” as a narrative for why a token should be valuable is pathological.”\n  \n\n      twitter.com\n  \n\n  \n    \nvitalik.eth 5\n@VitalikButerin\n\n\n  The notion of \"governance rights\" as a narrative for why a token should be valuable is pathological. You're literally saying \"I'm buying $X because later on someone might buy it from me and a bunch of other people to twist the protocol toward their special interests\"\n\n\n\n  12:36 PM - 29 Nov 2022\n\n    \n      \n        \n      \n      2.8K\n    \n\n    \n      \n        \n      \n      410\n    \n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\\nI think the proposed changes are sensible and should help reduce remaining risk to Compound v2 from long tail assets. If we see organic borrowing demand pick up for some of these assets, it could be useful to have an established procedure in place to periodically update the borrow cap parameters, to continue providing maximum utility to borrowers and suppliers while limiting risk of manipulation.\\nThank you for the support @ClairvoyantLabs. Yet, although we think another path could have been explored before going straight for this solution, I think this result is fair. We don’t hold sufficiently many $COMP to weight in the balance. We only express opinion on the forum as contributors of Morpho, a major Compound user, to enlighten community’s opinion on non-trivial consequences that this proposal has. In the end, $COMP holders are the one deciding what they think is best for the protocol and we of course respect their decision."
  },
  {
    "number_of_comments": 11,
    "postid": "1f7ca6df-83c7-4a3f-ad27-6a53f2ce2ffd",
    "posturl": "https://www.comp.xyz/t/compound-contributor-grants/756",
    "combinedcontent": "@jmo @tarun @peteris, and Rei Chiang\nOur first proposal, Reduce COMP emissions by 20% 27, focused on providing a healthy amount of COMP for governance to utilize. Previously, the protocol was spending 0.5 COMP / block on liquidity incentives to borrowers and lenders. However, after our proposal (and proposal 10), the protocol’s Comptroller contract now holds a treasury that grows by 0.148 COMP / block and spends 0.352 COMP / block on liquidity incentives. But how exactly can governance spend this treasury? The answer to this question is a bit more complicated.\nCurrently, the Comptroller doesn’t have a particularly clean way for governance to grant COMP to a specific address. This means that the barrier to making payments is higher than it needs to be, especially for less technical proposal authors. Moreover, it would be convenient for Governance to be able to stream payments to participants, akin to a subscription. For instance, Governance may vote to pay for development of new features via quarterly payments.\nRobert’s post 18 a few months back describes a plethora of new ways the community can contribute to Compound. The post details how COMP grants will enable a number of improvements for both smart contract changes and risk management. Recent events, such as the Dai liquidations, have increased the urgency for a COMP grant feature, as the community has proposed a number of items that require such grants. These items that have come up since Robert’s post include:\n\nReimbursing those liquidated during the Thanksgiving liquidation event, as proposed by MrHen 28\nPaying for the development of new features and risk monitoring tools (c.f. Robert’s post on Dai Market Risk 8)\nRetrospective airdrop to early COMP users (akin to the Uniswap airdrop), proposed by Ali Yahya 135\n\nAs part of our work on adding vesting to the protocol 15, we added these features in a pull request. Given the recent increase in urgency for a simple mechanism for governance to make payments, we will split this pull request into three:\n\nProposal to add one-time and streaming grants (PR 20)\nProposal to add vesting within the protocol (PR 9)\nProposal to set the vesting parameter based on community feedback\n\nWe wrote a high-level specification 21 that goes through the technical components added. Furthermore, we commissioned an audit 6 of both the grant and vesting related changes by Quantstamp. There are no major issues with the changes, in spite of the relatively large surface area touched by the vesting changes. Given that the vesting changes affect economic behaviors within the protocol, especially for those building on top of the protocol, we are taking a more cautious approach and testing these changes on testnet over the coming weeks. We request that community members who integrate Compound within their products, such as wallets and exchanges, try out vesting and provide feedback, should this change cause any disruption to your product.\nWe are going to test out this feature by paying Gauntlet for the development costs associated with this change. This grant will cover:\n\nDevelopment costs for CP030\nAudit costs\nDevelopment costs for the future vesting proposal (which we have already implemented and is linked above)\n\nWe hope this will be the first of many COMP grants to community contributors.\nYou can expect to see this proposal on the Compound governance portal 47 shortly.\\nI believe you previously linked somewhere on estimated gas costs with this upgrade. Could you share that again?\\n-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA256\nThe current proposal (grants only) doesn’t affect gas consumption, but the gas\nestimates for vesting can be found below\n\n  \n      github.com/compound-finance/compound-protocol\n  \n  \n    \n  \n    \n  \n\n  \n    \n      Added fixed period vesting + Comptroller reductions\n    \n\n    \n      compound-finance:master ← GauntletNetworks:gauntlet/vesting\n    \n\n    \n      \n        opened \n        \n          \n        \n        Oct 14, 2020\n      \n      \n\n      \n        \n          \n          Pet3ris\n        \n      \n\n      \n        \n          +20241\n          -18309\n        \n      \n    \n\n  \n\n\n  \n  \n    \n    \n  \n  \n\n\n-----BEGIN PGP SIGNATURE-----\niQIyBAEBCAAdFiEEEJAuhmVduSEVN8uPKIPJySxksQ8FAl/L8I8ACgkQKIPJySxk\nsQ+Enw/2K69sSM2wYiZoycS90mmisdOthcdkG2JYY4A5T01MV5Y6x+nW1jhFwJtH\nYRVq3Zu8REjHHL5i/vYVcw7VF9rIaCCc05wiNvXRR69HHOOv4sElj9cpCykwVQUg\nY19vW9wni5y6GEZ8fLJuxG6vUlClNHi+3AdOJ1vfpBho+gbDSNf1Ck31HES/kngi\nM6EtBPJ5RrdwfkpLeQFtbOy8+NxedezP+ZXUuNngLZYsU/6gKtPXgRnsPlOfizkU\n95Oox56TDMTSz1NUFzNowc4RmNOVr41qIhjjkRZeRUtgClXzDPVkc1X4CfUMltrh\n+rC+j9PFgWAQb0E/EVT2wGhllyKdCTlhCZuKIsYntagstSY4GJzqdh2ldGHS9oIc\nchxcIcgOiyfFa8Yn0mOuOIePW/ZYzy7nJxkQU0Pht0p6/7UjFGpCATOxdPtLjUuS\nraCUzPwrkXcG6tmG8HTkknQsB9ucxYKXtLyH2QLCSxB+6w+Kn9iNqKEe5E6bW6XO\nK8jt41fuBIZxacxTzoyUweylF8MLVPwiPhrvDJCo6ZTFaImN7cY194QMzB9Ad4kQ\nEiWW6XomnsyCvt8aMIvoDTwQIncaA8NVgW1Dd5uDx1hwEE7xkwyBXSFRBz4DqL+d\n7wrltCgJPu7BaWr0mGRrk24fTSv21LwRHXusCMEXwYrZQsNIAw==\n=q2hp\n-----END PGP SIGNATURE-----\\nThe proposal is now live: https://compound.finance/governance/proposals/30 46! Looking forward to continuing the discussion, please post any questions here, we’ll work to answer them as soon as we can.\\nIn the spirit of community governance, could you please provide a breakdown of where the 1000 COMP grant will be going and how you got to that number? When a significant sum of community funds are involved, I believe the community should demand transparency.\\nI can definitely provide more info - the two main costs were:\n\nAudit - ~$50k\nDevelopment - ~2 FTE equivalents, for 3 months\n\nThese changes included:\n\nExtensive edits of the Comptroller.sol contract to make room for the new functionality.\nAdding vesting, the grant functions and other core functionality\nResponding to the audit and development community feedback\n\nYou can see the details in the PR’s linked above.\\nThere is a mistake in the code being implemented in Proposition 30, which is currently being voted in as the mainnet comptroller implementation, and can be found at 0x7d47d3f06a9c10576bc5dc87cefbf3288f96ea04\nOverview:\nLines 1317-1320 of Comptroller.sol do not achieve their intended purpose, due to a missing else-statement that should surround the code on line 1321 of Comptroller.sol.\nIntended Effect:\nThe comment states that if compSpeed is 0, the lastContributorBlock storage slot should be released so that gas can be saved.\nActual Effect:\nSince line 1321 of Comptroller.sol sets lastContributorBlock to getBlockNumber() regardless of whether compSpeed is 0 or not, no gas savings are achieved. Deleting a storage slot and then immediately filling it again does not yield a net gas refund. As such, lines 1317-1320 currently do nothing useful even if compSpeed is 0; in fact, they slightly increase the gas cost of the function in all cases, due to the ‘compSpeed == 0’ comparison.\nSolution:\nChange this:\n    if (compSpeed == 0) {\n        // release storage\n        delete lastContributorBlock[contributor];\n    }\n    lastContributorBlock[contributor] = getBlockNumber();\n\nTo this:\n    if (compSpeed == 0) {\n        // release storage\n        delete lastContributorBlock[contributor];\n    } else {\n        lastContributorBlock[contributor] = getBlockNumber();\n    }\\n@jvaqa, great catch!\nIndeed, if we were to set up and then release a contributor reward, this would leave an extra storage slot in play and waste gas.\nFor the benefit of governance, important to add that the recommended fix is preferrable but non-urgent:\n\nConsequence is low (one off storage slot paid for executing a _setContributorCompSpeed proposal)\nIt would require at least 2 additional patches to come to a situation where a contributor would both be receiving and then stop receiving a streaming grant. In practice we probably won’t see this in a while.\nEven if that happened, the above fix could still be used to retroactively reset the storage slot by calling _setContributorCompSpeed again for that contributor and releasing the storage slot.\n\nGovernance can decide to just leave this in for the moment to be fixed in the upcoming vesting patch (or the first accepted Comptroller change thereafter).\nFor reference, this is the line in question: https://github.com/compound-finance/compound-protocol/blob/3244eed6beefd589f6e9ef640b822c6c615e2433/contracts/Comptroller.sol#L1320 8.\\nThis observation seems to be correct to me and should have been caught prior to the proposal during community reviews. @jvaqa thank you for you time reviewing this new patch! This is what the community is all about.\nI believe that this highlights a major issue in the process taken here. For such extensive code changes, a significant amount of time should be given for the community to review the changes prior to proposing. I am aware that these contracts were developed in the open and constantly had some community feedback; however, many people in the community do not have the time to constantly review code. For future proposals involving extensive protocol changes, please post on comp.xyz with the proposal intentions linking a stable PR and allow for community feedback at least a week before proposing.\\nGiven some comments we’ve received, we wanted to clear up a few things regarding the initial grant payment. Firstly, the 1000 COMP payment covers the development expenses and the audit for the entire set of changes:\n\ngrantComp (PR #79)\nVesting (PR #71)\nDistribution improvements\n\nWe plan to see the remaining proposals through and do not plan to request any additional grant.\nAlso @arr00 - as far as communication processes go, I think this 1 week policy is a great rule of thumb for future proposals. The changes have been discussed on the recent developer calls and the code has been available for review on Github for a few weeks now 2, but posting on the forums more frequently in the future is an obvious improvement to community involvement. We had a strong bias towards pushing this one out because we wanted to enable the community to decide how to compensate users affected by the recent DAI liquidation incident.\nHopefully, the existence of the grant function will help spearhead the discussion about how grants should function and the procedure around them. From our perspective, we wanted to set the following precedents:\n\nPeople who take on larger development projects, like those requiring audits, can be compensated for those changes\nNot all work needs to be done speculatively. Currently, anyone contributing needs to make changes on the off-chance those are later accepted by the protocol. In this case, we’ve completed all of this work ahead of time. However, hopefully by requesting payment before all of the changes are deployed, we can take a small step in direction that is more appealing to contributors.\n\nAlso - great catch by @jvaqa! We’ll send a bug bounty for identifying the issue in _setContributorCompSpeed.\\nThank you @jmo , here is my address for the bug bounty reward:\n0xEcb66CC9e366aF7351DbAA52Fa0b290200eC7532\nI like @arr00 's suggestion for a week-long “open bug bounty” on this forum prior to code changes, I will certainly keep an eye out for future posts and help however I can.\\nNow that queued proposal has been executed, we’ve sent @jvaqa the bug bounty. Thanks again to everyone for the feedback!"
  },
  {
    "number_of_comments": 19,
    "postid": "5b9e7c78-b1e9-4bc1-b21b-bfb262956d64",
    "posturl": "https://www.comp.xyz/t/dynamic-risk-parameters/2223",
    "combinedcontent": "\nSummary\nA proposal for continuous market risk management to optimize yield, capital efficiency, and mitigate depositor losses.\n\nBackground\nFor almost two years now Gauntlet has formally and informally worked for Compound to perform market risk assessments 45, contribute to treasury management 40, optimize incentives 17, calibrate risk parameters 29, and upgrade 29 the protocol 11. During that time Gauntlet has been able to refine our core models and agents specifically for autonomous interest rate protocol’s like Compound.\nAs the protocol continues to decentralize to the community our position is that dynamic risk parameters are a vital component to growth. Most protocol upgrades and maintenance impact market risk of the protocol. For example, the seize function 7 and liquidator behavior. Or the introduction of Chainlink Price Feeds 6 which has and will continue to facilitate the onboarding of new assets. How should the community reason about initial borrow caps? When should collateral factors be raised or lowered? How do individual assets and their parameterizations affect insolvency risk?\n\nProposal\nIn the following sections, we will outline the case and goals for dynamic risk parameters. The initial proposed scope has target metrics Gauntlet aims to improve. Those metrics are:\n\nRisk-adjusted yield for Depositors\nCapital efficiency for Borrowers\nMitigate Depositor losses\n\nGauntlet will improve the metrics above while controlling for protocol insolvency risk.\nIllustrated in the governance example below are the benefits from a previous parametrization initiated and executed by Gauntlet. Additionally, we describe two initial areas for optimization that have been identified.\n\nCompound Proposal 039\nThe ZRX, BAT, and WBTC Parameter Update 29 governance proposal sought to change collateral factors with a primary focus on lowering WBTC. See the full thread 14 for details.\nTo measure impact of this change we can look at the total WBTC liquidated on both the Aave and Compound protocols during the weeks of April 18-24, 2021 and May 17-23, 2021. The total liquidity available on the Compound and Aave was similar during these weeks. In addition, the volume of WBTC liquidated on Compound was less than 10% of the volume on Aave in April, which was closer to our parameter change. From there we mapped Compound positions from 2021-02-21 against the subsequent price movements. If users held unchanged positions, which is not uncommon, from before the collateral factor update through single-day price drops of 20% in April and 41% drop in May, then there would have been ~$7M to $9M additional collateral available for liquidation on each occasion.\n\n1188×814 31 KB\n\n\nCapital Efficiency for New Assets\nCurrently, the collateral factors for AAVE, LINK, MKR, SUSHI, and YFI are conservative. As the supply of these assets grows, ensuring any individual asset does not contribute outsized risk to the protocol is key. Existing simulation outputs suggest increasing collateral factors for all five assets by approximately 15% is optimal. Doing so would allow users to borrow an additional $31M in assets.\nMaking early capital efficiency improvements like this are possible because Gauntlet runs daily off-chain simulations. Informed by market data (liquidity, slippage, etc.) we will adjust collateral factors lower or higher as needed.\n\nReserve Factor Support\nGauntlet will also support reserve factor 16 parameterization which is a key lever in driving revenue and growth (increasing yields/reducing interest paid). Previous conversation surrounding Reserve Factor Standardization 8 has been had but no further analysis has been performed into the optimal settings to track default probabilities. Gauntlet concurs that:\n\n\n\nReserve Factor Standardization\n\n\nChanging Reserve Factors does not immediately change the risk of the protocol (compared to changing a Collateral Factor), which should make them flexible levers to consider adjusting.\n\n\nWhile a secondary parameter for risk, the reserve factor is a primary parameter for revenue and growth of the protocol. For example, when changing borrow caps 12 consideration should also be given for the new optimal reserve factor.\n\nExpectations\n\n\nRisk Parameter Updates\n\nCoverage of all markets except Legacy (e.g., WBTC) and Deprecated (e.g, SAI, REP)\nSupported Risk Parameters: Collateral Factor, Close Factor, Borrow Cap, Reserve Factor, and Liquidation Incentive\nMarket conditions will determine the frequency of updates. For that reason, no SLA will be preset.\n\n\n\nCommunications\n\nRisk parameter change steps:\n\nForum post (e.g.,Reduce COMP emissions by 20% 9)\nCommunity discussion and revision\nOff chain polling\nOn chain vote\nPost-mortem\n\n\nQuarterly, Gauntlet will poll the community to determine the preferred risk tolerance of the community. The outcome of this vote will determine the risk and capital efficiency tradeoffs Gauntlet will target.\nMonthly forum posts and participation on community calls with explanations of risk parameter changes and any anomalies observed including but not limited to:\n\nDiscord Developer & Twitter Spaces Community Calls\n\n\nRisk Dashboard (refer to the next section)\nQuarterly Risk Reviews will provide a detailed retrospective on market risk.\n\n\n\nOut of Scope\n\nProtocol development work, (e.g. Solidity changes that improve risk/reward)\nFormalized mechanism design outside of the supported parameters.\nIn line with keeping the scope small, Gauntlet will not look to manage the following at the outset:\n\nEnabling or disabling a currency for borrowing\nSetting interest rate strategies\nOptimizing COMP emissions\n\n\n\n\n\n\nRisk Dashboard\nAs part of this engagement, Gauntlet will build a Risk Dashboard and API for the community to provide key insights into risk and capital efficiency.\n\n749×931 121 KB\n\n\n760×550 67.5 KB\n\n\n759×778 71.4 KB\n\n\n754×1109 180 KB\n\nPlease note, all numbers are for illustrative purposes only and do not reflect the current or possible future state of Compound.\nThe dashboard focuses on both the system-level risk in Compound and the market risk on an individual collateral level. Our goal is to help convey our methodology to the community and provide visibility into why we are making specific parameter recommendations.\nThe two key metrics are Value at Risk (VaR) and Borrow Usage.\nValue at Risk conveys capital at risk due to insolvencies and liquidations when markets are under duress (i.e., Black Thursday). The current VaR in the system breaks down by collateral type. We currently compute VaR (based on a measure of protocol insolvency) at the 95th percentile of our simulation runs assuming peak volatility in the past year. We do this using Compound’s current parameters as well as after modifying the parameters to the Gauntlet Recommendations.\nBorrow Usage provides information about how aggressively depositors of collateral borrow against their supply. Defined on a per Asset level as:\n\nwhere U is the utilization ratio of each user:\n\nWe aggregate this to a system level by taking a weighted sum of all the assets used as collateral.\nTo show Gauntlet’s impact, we measure these using the current system parameters and expected results (based on our simulations) if Compound were to implement the parameter recommendations suggested.\n\nCost\nGauntlet charges a service fee that seeks to be commensurate with the value we add to protocols. Gauntlet also wants to provide a strong signal of our alignment with the protocol. Using our prior COMP Contributor Grants  29 proposal we propose a service fee using the Contributor Comp Speed grant functionality. At the start of every quarter for one year Gauntlet will create a proposal to update the service fee payment in accordance with the forumla below.\nThe formula to calculate Gauntlet’s service fee has four components:\n\nAn asset multiplier to track risk management complexity\nA proxy for capital efficiency\nA marginal base fee\nVWAP (Volume Weighted Average Price) of COMP\n\nThe asset multiplier calculation is log(Number of Assets, 10)*. New assets on the protocol add complexity to risk management. While the market risk optimization problem does not grow linearly, consideration should be taken when onboarding assets.\nThe most straightforward proxy for capital efficiency is the total borrowed for risk-managed assets. Capital efficiency is realized by borrowing demand. The total borrowed amount is calculated as the 30-day average and rounded down to the nearest $1B.\nGauntlet’s risk management marginal base fee is derived from a conservative estimation of the impact from dynamic risk parameters.\n\n\n\n\nMarginal Base Fee\nTotal Borrow\n\n\n\n\n10 bps\n$0 - $5B\n\n\n5 bps\n$6B - $10B\n\n\n2.5 bps\n$11B - $15B\n\n\n1.25 bps\n$16B - $20B\n\n\n\nThe VWAP of COMP for the previous 30-days. Whether the price should be fixed or calculated quarterly, different communities have different opinions on how this aligns incentives. We will defer to the preference of the community but will default to calculating quarterly.\n\n*Gauntlet quarterly service fee denominated in COMP (table above calculated at $464)\n\n\n694×165 23.4 KB\n\nGrowth and drawdown examples\n\n*Log value is the minimum of the tier range except in the “<= 10” column, where it is 10. For example Column “21-25” returns log(21,10)\n** When Total Borrow < $3b, there is no basis point fee. The formula is log(Number of Assets,10) * $1,200,000 / 4 )\n\nAbout Gauntlet\nGauntlet 21 is a simulation platform for market risk management and protocol optimization. Our prior work includes assessments for Compound 45,  MakerDAO 12, Liquity 4, and Aave 2. Gauntlet’s continuous parameter optimization work includes  Balancer 2,  SushiSwap, Benqi 2, Aave, and  Acala 2.\nThanks to @tarun, @wfu, @shaan, @jmo and many others 6 for assistance on this proposal.\\n@inkymaze thank you for sharing this proposal on the Community Developer call this morning; a standard process to monitor & tune the risk parameters of the protocol is a long time coming (and one that I think the community should take!).\nA few preliminary questions:\n\nWith respect to the metrics that Gauntlet intends to optimize (risk adjusted yield, capital efficiency for borrowers, and mitigating depositor losses), how will these be tracked? Will they be on the risk dashboard that Gauntlet plans to create, to monitor them over time?\n\nProposal 49 7 updated the liquidation mechanics to reduce the “cascade risk” of certain markets; has this functionality been included in the simulation models yet, or will it be prior to beginning dynamic risk parameterization?\nHow do you envision setting an acceptable “VaR” relative to reserves? How do you plan to include the community in this decision?\nHow are the proposed fees being set? Have community members weighed in on these?\nDoes Gauntlet plan to hold or dispose, vote or delegate its proposed COMP payment?\n\\n\n\n\n rleshner:\n\nWith respect to the metrics that Gauntlet intends to optimize (risk adjusted yield, capital efficiency for borrowers, and mitigating depositor losses), how will these be tracked? Will they be on the risk dashboard that Gauntlet plans to create, to monitor them over time?\n\n\nInitially the dashboard will track capital efficiency via Borrow Usage and risk via Value At Risk. From our initial user studies, these were the key metrics users wanted to see.\nIn addition to the dashboard, we will follow the outlined communications plan to capture anything that isn’t yet represented (i.e. Risk adjusted yield).\nThere are additional features on our roadmap like a visualization of users with undercollateralized loans and collateral liquidation ‘depth’. We plan on iterating on this dashboard over time and adding new features that we identify through continued community engagement.\n\n\n\n rleshner:\n\nProposal 49  updated the liquidation mechanics to reduce the “cascade risk” of certain markets; has this functionality been included in the simulation models yet, or will it be prior to beginning dynamic risk parameterization?\n\n\nProposal 49 routes a fraction of liquidation incentive from liquidators to the reserves. This change effectively allocates 5.2% of liquidation to liquidators and 2.8% to the reserves, which increases the protocol’s ability to recover from insolvency by growing the backstop liquidity, but reduces the incentive for liquidators. We will update the effective liquidator incentive to 5.2% in the simulation to accommodate the change.\nOur current simulation is mainly focused on modeling insolvency risk in one day. Considering the average liquidation size relative to the sizes of reserves, the 2.8% of liquidations added to the reserves in a day will likely not have an immediate impact in such a short time frame. However, tracking the amount of reserves over time and forecasting the growth rate of the reserves due to parameter changes can definitely help community members to better understand the protocol’s liquidity backstop. Forecasting the reserve growth rate is not in our initial scope, but we will evaluate how to support this in Q1 2022.\n\n\n\n rleshner:\n\nHow do you envision setting an acceptable “VaR” relative to reserves? How do you plan to include the community in this decision?\n\n\nOur primary goal for simulations is to standardize VaR across all assets, to ensure no subset of assets adds disproportionate risk to the protocol. We will target a similar system-level VaR to the current risk parameters as the moderate risk level recommendation. Additionally, we will provide aggressive/conservative risk level recommendations by targeting x% of capital efficiency increase/decrease.\nAs our philosophy 2 is avoiding non-quantitative decisions, we don’t decide on what “acceptable” VaR relative to reserves is. We will facilitate the community’s decision-making process by creating quarterly off-chain polling for the community to decide this high-level objective. Community members can check our risk dashboard to get an estimate of VaR relative to reserves to understand the protocol’s ability to recover from insolvency events.\n\n\n\n rleshner:\n\nHow are the proposed fees being set? Have community members weighed in on these?\n\n\nAs mentioned above, Gauntlet seeks to charge commensurate to the value provided. This means measuring our work against the target metrics, communication objectives, and deliverables like the Risk Dashboard. We encourage the community to evaluate these items proactively and on a regular cadence ahead of our proposed quarterly fee update.\nWe have sought consultation from various community members, not solely our investors, and we encourage all others to use this thread to weigh in on the proposed fees.\n\n\n\n rleshner:\n\nDoes Gauntlet plan to hold or dispose, vote or delegate its proposed COMP payment?\n\n\nGauntlet is a firm believer in Compound’s mission and growth. As such, we plan on holding COMP tokens and self delegating for governance votes. Depending on Gauntlet’s cash flow needs, including but not limited to tax payments and operational expenses, we may need to sell tokens at a future date. For reference, we have not sold any of the COMP initially granted to us in Dec. 2020 via CP030 13.\\nOverall I think this is a good step forward for Compound & Gauntlet. The protocol needs teams to analyze the protocol and provide input to improve the market further. Compound is a protocol of risk management. Without community members and users maintaining and improving the protocol, we won’t thrive.\nAt a high level, the proposal says Gauntlet will recommend collateral factors, borrow caps, reserve factors, and liquidation incentives based on their propriety simulations and a quarterly poll to determine the community’s risk tolerance.\nTheir key metric is Value at Risk or VaR.\n\n\n\n inkymaze:\n\nThe current VaR in the system breaks down by collateral type. We currently compute VaR (based on a measure of protocol insolvency) at the 95th percentile of our simulation runs assuming peak volatility in the past year.\n\n\nThe current Compound market is ~$8.5B borrowed, and there are 15 coins (excluding deprecated markets). That means we would pay Gauntlet 3,614 COMP or ~$1.67m for the quarter.\nThis isn’t a small amount of money to pay for an uncertain return. That being said, I think Gauntlet has demonstrated good intent. I will vote for this quarter, but I think the first quarter is a trial. I expect Gauntlet to deliver on all of the points below to secure a second quarter.\n\n\nBuild and maintain a dashboard that provides valuable insight and data.\n\n\nRecommend collateral factors and provide sufficient evidence to back their recommendations.\n\n\nProactively support setting initial collateral for new assets.\n\n\nRegularly interact with the community.\n\n\nQuestions\n\n\nWhat happens with new markets? Today we have 15, but if you get paid and then new markets get added, will you provide support for those?\n\n\nDo you have experience in recommending borrow caps & reserve factors? I didn’t see them mentioned in the latest Compound report.\n\n\nDoes Compound have a point person at Gauntlet that community members can ask questions to?\n\n\nDo you have more info on how your Value at Risk statistic works? VaR isn’t mentioned in your most recent reports.\n\n\nWhat data are using as inputs for market liquidity and historical prices to calculate volatility?\n\n\nLast point: The protocol can fund more than one team to work on this. If you read this and think your team could do risk analysis and parameter recommendation, please submit a proposal. There is enough work and money to go around!\\n\n\n\n getty:\n\nWhat happens with new markets? Today we have 15, but if you get paid and then new markets get added, will you provide support for those?\n\n\nNew markets are immediately supported. Gauntlet’s pricing is only adjusted quarterly.\n\n\n\n getty:\n\nDo you have experience in recommending borrow caps & reserve factors? I didn’t see them mentioned in the latest Compound report.\n\n\nGauntlet provides Acala (Karura) debt ceiling recommendations. The debt ceiling recommendation may not be directly applicable to borrow caps, but we will modify our methodology to account for the difference. We are also preparing to recommend borrow caps for Benqi and Aave once supported 1.\nFor reserve factors, we have formulated the optimization problem in our more recent Aave Market Risk Assessment (see Appendix C.6 7). The function design will hold well for Compound.\n\n\n\n getty:\n\n\nDoes Compound have a point person at Gauntlet that community members can ask questions to?\n\n\n\nThe community should consider me the default but can expect communications from many people on the Gauntlet team across Product and Data Science.\n\n\n\n getty:\n\nDo you have more info on how your Value at Risk statistic works? VaR isn’t mentioned in your most recent reports.\n\n\nIn our previous reports (both Aave and Compound), we focused on market risk and protocol resiliency. For example, in Aave, we used the key metrics of asset insolvency or safety module slashing to benchmark simulation runs. These numbers become difficult to compare to key metrics for yield and user borrow behavior metrics. In addition, the metrics can often be skewed for various market conditions. Our VaR metric will be a conflation of liquidation loss and insolvency metrics that can be quantified as some percentage of users’ capital that can make it easier to compare to upside (yield) metrics as well as scaled for tail market events.\n\n\n\n getty:\n\nWhat data are using as inputs for market liquidity and historical prices to calculate volatility?\n\n\nGauntlet ingests historical price and liquidity data for CEXes from Amberdata and for DEXes from onchain data.\\nIs there a possibility of open-sourcing all of the risk monitoring tools, frameworks, and dashboards? So that in the next 1-2 years this can lead to a more decentralized risk team?\nThe goal of the Compound DAO should be to fund open-source teams and units that can benefit the protocol in the event any one person or company ceases to exist. I would even vote for the commission rate to be increased if the end goal is to develop open-source risk tools and frameworks that can work for any decentralized money market built on Ethereum.\\nGauntlet will continue to formalize an Asset Onboarding Framework 4 with the community and quantitatively clarify our methodology 4 on request, as we have done previously.\nTo answer the question directly, Gauntlet currently has no plans to open-source our proprietary risk models and simulation SDK.\\nThe updated VWAP and Total Borrow below will be used for the governance proposal.\n30-day VWAP price of $COMP = $441.9327798033424\n30-day average Total Borrow = $8.12707524543612E9\nUsing these inputs Gauntlet will target a setContributorCompSpeed of 3,794 COMP per quarter.\nShould a proposal pass, Gauntlet’s top priorities will be:\n\nUpdate our simulations to account for Proposal 49’s new liquidation economics.\nGauge the risk appetite of the community and update risk parameters accordingly to the level set.\nBenchmark the target metrics we aim to improve.\n\\nI think the proposed paid amount is very large and questions some things in my mind.\nWould it be better if we just keep this amount for safety (like reserves) so we can use in case something bad happens. as this would be nearly 6,7~ million $ in comp / year  (using your average comp price)\nHow will the protocol pay this amount to Gauntlet when there will be no more COMP left, the amount requested in the proposal would be significant part of the  “reserves” that currently the protocol generates from interest.\nI understand the proposal itself is to avoid bad things to happen at all.\nWill gauntlet take any responsibility in case their proposed changes does not protect the protocol?\nLike returning part of the COMP that they got as payment ?\\nThanks to the Gauntlet team for all the work they do on Compound. Was curious though:\nSo if proprietary, how are we as a community able to audit and understand the risk models underlying key protocol variables?  Also, is there a third party that reviews the risk models/sims and dashboard to make sure they are bug free and/or verifies the efficacy of methods used?\\nI’m a big fan of the work @inkymaze is doing at Gauntlet — it’s important work, and almost nobody else is doing it. Given the size of Compound protocol (~$1.8 billion circulated market capitalization as of this writing), it strikes me as reasonable to pay vendors a lot of money to make sure things don’t break, particularly when the cost of breakage is potentially in the hundreds of millions if not billions of dollars.\nHaving said that, I do have several comments on the proposal.\nPrice & Fees\n\n\nIf we extrapolate the proposed quarterly fee, the protocol will be paying ~15k COMP to Gauntlet per year. At current prices, that’s nearly $5 million per year. If the price doubles in the next year due to good protocol performance, that fee swells to nearly $10 million per year. Put another way, the protocol would be diluting all COMP tokenholders approximately 0.56% per year to pay for the service. My sense is the fee is in the upper-range of what the protocol should be willing to pay. All at once, beggars can’t be choosers; it is good business for vendors to charge what the protocol is willing to pay! (Note: as a technical matter, Gauntlet will re-adjust fees every quarter, so the “final bill” will depend on a variety of variables at four points in time).\n\n\nWhile this is by no means a popular view, I generally believe it’s a good idea to pay vendors in stable currencies like USDC or DAI. This achieves two things. First, it allows the protocol to know exactly what it’s paying for the service. The current fee structure is as complicated as it is volatile: the final bill can either be very low or very high, and the protocol will only find out what it’s paying after the fact. Second, paying in stable currencies decreases dilution for existing tokenholders. If the protocol pays every vendor millions of dollars worth of COMP, dilution may spiral out of control. I believe it’s prudent to keep an eye on dilution. Establishing standards by which vendors are paid is a path towards doing so. (Note: converting COMP that is sitting in the treasury to USDC/DAI may be complicated from an operations/tax perspective, so perhaps this point is an aspirational one).\n\n\nThe Process\n\n\nJudging by the relatively few responses to this thread, my concern is most tokenholders don’t fully understand what’s being proposed here. That’s not a diss to @inkymaze’s writing. Rather, I believe it’s a sign that the needs of the protocol have advanced beyond the capability of the average tokenholder. In short, I no longer think it makes sense for tokenholders to vote on technical proposals such as this one. Rather, it strikes me as reasonable to bring together a small group of experts who can review proposals such as this one on behalf of tokenholders. We can call them the “Risk Committee” or “Vendor Due Diligence Group” or perhaps another fancy name. Either way, they’d have way more experience and ability reviewing proposals like this one than a broad base of busy and un-expert tokenholders.\n\n\nWhen companies make large purchases, they typically run a bidding process to allow multiple vendors to compete for the work. This accomplishes several things: it allows (i) companies to receive the best price, (ii) vendors to compete with each other to offer the best product/service, and (iii) vendors who are new to the scene to have a fair shot at selling their product/service to the company. Perhaps Compound protocol can do the same? It would be fairly easy to set up a bidding process for other vendors to come in and bid for the work Gauntlet is proposing. It’s a big win for Compound protocol to let the best vendor win!\n\n\nAll in all, I am in support of Gauntlet’s proposal. But I strongly believe we need to get more sophisticated on evaluating proposals like this one going forward.\\n\n\n\n blck:\n\nWould it be better if we just keep this amount for safety (like reserves) so we can use in case something bad happens. as this would be nearly 6,7~ million $ in comp / year (using your average comp price)\n\n\nDenominating the contract in USD is very similar to paying it in USD from the protocol perspective. Paying it in USD means that 100% of the COMP is sold on the market, paying it in COMP allows contributors/vendors to divest over time as needed. Also, while everyone here is bullish Compound and DeFi, this week’s price action expose the risk Gauntlet and future contributors face taking payment in COMP.\n\n\n\n blck:\n\nHow will the protocol pay this amount to Gauntlet when there will be no more COMP left, the amount requested in the proposal would be significant part of the “reserves” that currently the protocol generates from interest.\n\n\n@sukernik and @monet-supply have also raised similar concerns. The alignment of incentives is something we are keen to iterate on with this community. For example, the marginal bps pricing structure was not implemented in our formula for Aave but something we intend to incorporate at renewal following community feedback.\nOne separate thought experiment is what if Compound does not charge reserves at all? Would all contributor grants stop? Reserves and other revenue could be sacrificed for growth, this wouldn’t mean that Gauntlet, Getty, or others should provide any more or less value.\n\n\n\n blck:\n\nWill gauntlet take any responsibility in case their proposed changes does not protect the protocol?\nLike returning part of the COMP that they got as payment ?\n\n\nIt is our strong preference to do so but there are significant regulatory concerns in structuring an engagement this way.\\n\n\n\n jthrack:\n\nSo if proprietary, how are we as a community able to audit and understand the risk models underlying key protocol variables?\n\n\nOur Risk Dashboard gives insight into our methodology. In it, you can see the direct outputs of our simulation models and how different risk parameters behave (in simulation) under different volatility levels. We provide a variety of different metrics including, how system collateralization ratio, amount of collateral liquidated, and net insolvent debt % are affected by tweaking specific parameters.\n\n\n\n jthrack:\n\nAlso, is there a third party that reviews the risk models/sims and dashboard to make sure they are bug free and/or verifies the efficacy of methods used?\n\n\nNot at this time. Our data sources are not proprietary, as mentioned above, and we encourage others to replicate or model optimal risk parameters. We have 3 and will provide model inputs for any future proposals including metrics like supply, ADV, supply ratio, volatility, and slippage intensity and power.\\n\n\n\n sukernik:\n\nI generally believe it’s a good idea to pay vendors in stable currencies like USDC or DAI.\n\n\nI’m glad you brought this up! The reason I disagree with this approach is that it is misaligned with the role of COMP as a governance token. Vendors and contributors should be willing to be rewarded with governance rights (COMP), because that is what the protocol can directly offer for their contributions.\nIf recipients want to trade those rights on the open market for stablecoins, or ETH, or a fiat currency, they are welcome to do so. Importantly, the protocol bears no control and no liability related to that decision. I don’t see it as incumbent on the protocol or its governors to shoulder any burdens or risks associated with trading of the COMP token on behalf of prospective recipients.\\nThanks for the reply @allthecolors. It’s less about the protocol making trading decisions on behalf of vendors and more about the protocol making a sound financial decision for itself.\nFor example, at some point in the future, it’s very likely that COMP will find itself undervalued by the market due to various market forces (e.g., bear market). At this point in time, issuing COMP to vendors would be a financially disastrous decision for the protocol: in effect, the protocol would be issuing tokens for less than their intrinsic value. It would be the equivalent of selling something you know is worth one dollar for 80 cents.\nWhile we are on the topic, it’s worth pointing out that vendors who are interested in aligning incentives with Compound could always purchase COMP with the stablecoins they are paid in. To me, using your hard-earned cash to purchase something is the strongest signal there is!\\nI strongly support this proposal even though I’d like to see an open-source non-proprietary risk model developed for Compound. Since we don’t have any such model, using Gaunlet’s proprietary solution is currently our best approach to managing market risk.\nI agree with @sukernik’s point on paying vendors in stablecoins, although Compound’s treasury is composed of COMP and I am against the treasury selling COMP for stablecoins. Compound has a great need for a treasury board to manage and grow its capital. Debt leveraging, investing, and yield farming are very effective tools the treasury could use to grow its capital and maintain its cash stablecoin flow.\nOn a new note, I hope that Gauntlet will incorporate regulatory risk, a protocol’s governance risk, smart contract risk, and other types of risks into their analysis and dashboard.\\nIn case anyone’s interested, I did a podcast with @inkymaze and @getty, where we discussed specific aspects of this proposal + some of the pushback/concerns mentioned here, and open questions/aspects to review in future quarters.\nLink here: A discussion with Gauntlet about risk monitoring for Compound, featuring Nick Cannon, Getty Hill, and Derek Hsue by I Pledge Allegiance 28\\n\n\n\n sukernik:\n\nJudging by the relatively few responses to this thread, my concern is most tokenholders don’t fully understand what’s being proposed here. That’s not a diss to @inkymaze’s writing. Rather, I believe it’s a sign that the needs of the protocol have advanced beyond the capability of the average tokenholder. In short, I no longer think it makes sense for tokenholders to vote on technical proposals such as this one.\n\n\nAverage tokenholder defined by “avg voting power”? There should be an obligation to the community and all users of the protocol that someone with purchased above average voting power should at least read a summary of potential changes to the protocol. I wouldn’t say it’s a matter of non-understanding but a lack of interest. Your suggestion definitely makes sense.\n\n\n\n inkymaze:\n\nGauntlet charges a service fee that seeks to be commensurate with the value we add to protocols.\n\n\nAlthough it is a fair price for your work, from protocol perspective its too expensive.\\n@sukernik and @dabar90 above have commented on the complexity of the proposal and the lack of participation from the community. I agree that the subject matter is complex. However, I think that there’s room to clarify and give it a better structure.\nAt the most basic level, I think it’s important to clearly specify the inputs, outputs and the objective of the simulation, so people have a feel for what the Gauntlet “Blackbox” could be doing. Here’s my attempt towards that.   @inkymaze and Gauntlet team, please provide corrections if required.\nObjective of Risk Simulation (in the near term)\nObjective is to enhance Capital efficiency while controlling risk. Capital efficiency is measured by Borrow Usage and risk is measured by Value-At-Risk (VAR).\nThat is, the objective is to find the right balance between Borrow Usage and VAR. This will be done at both system level and at each asset level.\nThe input and output data that is used to achieve the above objective can be categorized into four parts as follows:\n\nPrimary output metrics: VAR and Borrow Usage\nSimulation statistics (secondary output metrics of the simulation)\nProtocol Risk Parameters (primary inputs or variables to the simulation)\nMarket statistics (other market data used as inputs to the simulation)\n\nEach category above is elaborated below.\nPrimary output metrics:\n\nVAR: \t\tCurrent and recommended values\nBorrow Usage: \tCurrent and recommended values\n\nThe above outputs will be provided at both system and asset level.\nGauntlet will arrive at the recommended (optimal) values for VAR and Borrow Usage by means of simulations.\nSimulations will use the following as inputs:\n•\tProtocol Risk Parameters: These values will be tweaked (or simulated) in thousands of simulations\n•\tMarket statistics: These are mostly static inputs based on market statistics, except for Volatility which will be simulated in a range.\nSimulation statistics (outputs)\n\nNet Insolvent debt %\nAmount of Collateral Liquidated\n% of Users Liquidated\nSystem collateralization ratio\n\n(It would be great if Gauntlet can provide a good definition and math formula for item 1 above)\nProtocol Risk Parameters (inputs)\n\nCollateral Factor\nClose Factor\nBorrow Cap\nReserve Factor (Fee – portion of the borrower interest that goes to the protocol)\nLiquidation Incentive (discount factor on collateral given to liquidator - currently 5.2%)\nFraction of the liquidation incentive (2.8%) that goes to the reserves.\n\nMarket statistics (other inputs)\n\nVolatility (Peak volatility of the last year)\nAverage Daily Trading Volume\nAverage Collateralization Ratio\nCollateral Usage\nAsset Utilization Rate\n\nObjective of Risk Simulation (Longer term)\nGauntlet wants to optimize the following metrics too in the longer term, but these are not in scope presently.\n\nRisk-adjusted yield for Depositors\nCapital efficiency for Borrowers (This seems same as Borrow Usage)\nMitigate Depositor losses\n\\n\n\n\n Tyler Loewen:\n\nOn a new note, I hope that Gauntlet will incorporate regulatory risk , a protocol’s governance risk, smart contract risk, and other types of risks into their analysis and dashboard.\n\n\nGood point, full stack protocol risk management can hardly be too expensive. Is possible to calculate cost of security and governance risk management?\nGiven that Gauntlet worked risk assessment on several platforms with a similar business model, it would be good to see additional recommendations on how to increase capital efficiency. For example, how do protocol reserves better affect the stabilization of protocol solvency? Is it more useful to isolate a part of the liquidation market from MEV influence? (i.e. [Bprotocol BAMM model], both v1 and v2 model(https://bprotocol.org/ 9))\nIs “Buffer” between LTV and liquidation treshold positively affects risk management because it gives users the opportunity to avoid liquidations?\n@getty  mentioned in the podcast that Compound has a more cautious approach to introducing change but that doesn’t just mean giving recommendations for setting a few risk parameters given historical prices and volatility."
  },
  {
    "number_of_comments": 13,
    "postid": "d76be7a6-fa11-4c26-bd61-73b9153f9b06",
    "posturl": "https://www.comp.xyz/t/update-cusdt-interest-rate-model/170",
    "combinedcontent": "Following Dharma’s lead, I believe that all stable-coins should have their interest rate model updated to match. DAI has already been updated to the latest and greatest interest rate model but USDC and USDT lag behind.\nAs USDC is a legacy cToken, upgrading the interest rate model for it will require a new contract, but USDT can have a new interest rate model using the JumpRateModelV2 built by Dharma. I propose setting USDT interest rate model to a new JumpRateModelV2 deployed with the same parameters as the current DAI interest rate model: base rate of 0%, 4% at kink, kink at 80% utilization, and 25% at 100% utilization.\nUpdate: Interest rate model deployed here 45.\\nThe DAI model seems to have had the desired effect of the utilization dropping back to 80% adding a larger buffer and reducing risk. Makes sense to upgrade all tokens that can be upgraded to the latest and align the parameters. I’m also wondering why the reserve factors are so different between market?\\n(Unsurprisingly) we are very supportive.\\nIf the community is able to verify that the model matches the repo, and behaves as expected, migrating cUSDT to this model would be a no-brainer.\nGoing forward, both DAI and USDT rates will be able to be set through governance without deploying additional contracts; and could be made to match with a single proposal. That’s a nice improvement!\\n“another option” we could use the already deployed DAI’s interest rate model address for USDT aswell so there is no new contract in this case, since we want them to be the “same”, though it has advantages and disadvantages too, if we would like them to be seperated in the future it would require a new contract deployed.\\nThat’s also mostly correct; the only “wonkiness” is that the interest rate model contract uses an assumed reserve factor in order to derive the supply interest rate, from the borrowing interest rate.\nIn this case, cDAI and cUSDT have different reserve factors.\nFor correctness, the interest rate model shouldn’t have this property at all (the cToken, which knows its reserve factor, utilization, etc can derive the supply interest rate).\\nThe assumed reserve factor is currently only used when calculating base rate for DAI. The supply interest rate function has the reserve factor as a parameter in the header:\nfunction getSupplyRate(uint cash, uint borrows, uint reserves, uint reserveFactorMantissa) public view returns (uint)\nMy issue with tying together USDT and DAI is that I don’t know if we want USDT interest rate to change with MakerDAO actions. I think keeping them separate for now with equal parameters is the way to go.\\nProposal live here: https://compound.finance/governance/proposals/20 28\\nWhat steps have been taken to ensure that the deployed contract matches the rate model in the repo? Any way to compare the bytecode, or point to community review?\nExcited to see this as a proposal @arr00 \\nPersonally, I’ve ran the saddle verify method which compares bytecode and it all checks out. As I have deployed it, this doesn’t mean much coming from me. Someone else should check too! I’m thinking I’ll write a medium post on doing this just to make sure the community knows how it’s done.\nAnother important parameter to check now is that the owner was set to the Timelock correctly. You can see the owner in the read contract tab on etherscan.\\nWe think some of the changes are good but the parameter choices seem somewhat arbitrary. While we are supportive of this particular proposal as a form of dollar-denominated rate equalization, we think that future rate changes should de minimis be chosen in a data driven manner and ideally via simulation. Given the rapid changes in rates due to yield farming powered by Dai and COMP, data driven rates will provide significantly more leeway for the protocol to react to adverse farming events.\nBelow, we propose a few simple models for a data-driven IR model:\n\n\nBase Rate: Should be the risk-free rate of the asset e.g. for DAI, the base rate can be either DSR or stability fee rate. For ETH, the base rate can be the ETH 2.0 staking interest rate.\n\n\nKinks at x% utilization: this number should depend on the suppliers’ supply asset distribution. For example, using  1 - top supplier supply %   to define the Kink. The intuition for this heuristic is as follows: when the asset’s liquidity is too low such that the top holder can’t withdraw all his supplied assets, the system will try to aggressively push the interest rate up. If the asset supply is more centralized, the utilization to trigger crossing the kink will be lower. If we apply this simple methodology to the three stablecoins in Compound we find that:\n\n\n\n\nThe top USDT supplier is supplying roughly 18% of the cUSDT 7, so USDT kinks at 82%\nDAI kinks at 74% 7\nUSDC kinks at 72% 6\n\n\n\nBorrow rate at kink: Have this match the empirical highest APR for supplying assets in the other platform, e.g. Curve or Uniswap LP return. This rate should be risk adjusted based on current yield farming behavior (e.g. force be more conservative when there is a multiple sigma event due to farming)\n\n\nBorrow rate at 100% utilization: not sure whether there is a good way to come up with a reasonable number, since the goal for this is just to disincentivize borrowing demand. It could be 3x or 5x of the borrow interest rate at kink.\n\n\nGiven these methodological constraints, we believe that a 0% base rate and a kink at 80% utilization would be a safe, conservative route [0]. Based on USDC’s 73.4% utilization, we project that the current equilibrium point for USDT is around 10% borrow interest rate [1].\n— Gauntlet Team\n[0] We note that the implied 4% borrow rate at the kink might be too low and will effectively push the utilization to a neighborhood of the kink. Validating this conclusion with simulation is future work that we aim to perform.\n[1] We’re doing a back-of-the-envelope calculation to rescale USDT’s behavior to that of USDC, where we have a much higher empirical utilization (partially due to Dai creation).\\nThanks for your contribution @arr00 \nBefore voting 20th proposal, I’d like to check how to work this code.(actually, who is calling this contract)\nI’m not a smart contract developer so I just depends on its rationale/discussion and evidence to cast a voting on specific proposal. I’d like to understand its working process thoroughly as possible as I can.\nI understand and agree the intention to achieve with this proposal. (I think that making standard for stable coins is good idea)\nBut I don’t understand how to apply it (technically)\nFor technical side, I thought that there is some PR to replace the existing interest rate model contract address to this new one.\nWhich code will actually set up this new interest rate model contract address?\nand I’m not sure how many dev review this contract code.\nDoes it some evidence(some comments for codes reviews) this code was taking enough code review?\\nThe new contract was deployed by arr00, using the source from compound’s github “JumpRateModelV2”, the contract admin is the Compound’s Timelock address “0x6d903f6003cca6255D85CcA4D3B5E5146dC33925”\nI did verify the deployed contract bytecode and the source from compound’s github that anyone can do.\n\nnpx saddle match 0xFB564da37B41b2F6B6EDcc3e56FbF523bD9F2012 JumpRateModelV2 0 40000000000000000 1090000000000000000 800000000000000000 0x6d903f6003cca6255D85CcA4D3B5E5146dC33925 -n mainnet\nUsing network mainnet https://mainnet-eth.compound.finance 5\nMatching contract at 0xFB564da37B41b2F6B6EDcc3e56FbF523bD9F2012 to JumpRateModelV2 with args [0,“40000000000000000”,“1090000000000000000”,“800000000000000000”,“0x6d903f6003cca6255D85CcA4D3B5E5146dC33925”]\n Successfully matched JumpRateModelV2 to 0xFB564da37B41b2F6B6EDcc3e56FbF523bD9F2012 with args [0,“40000000000000000”,“1090000000000000000”,“800000000000000000”,“0x6d903f6003cca6255D85CcA4D3B5E5146dC33925”]\n\nThe proposal contains the necessery contract interactions that replaces the cUSDT interestratemodel address to 0xFB564da37B41b2F6B6EDcc3e56FbF523bD9F2012, when its ready to be executed anyone can do it.\\nCongratulations on the successful proposal!\nScreen Shot 2020-08-19 at 10.50.22 AM2138×392 41.8 KB"
  },
  {
    "number_of_comments": 9,
    "postid": "3860dcd7-4285-4533-8565-99ccd7ef5921",
    "posturl": "https://www.comp.xyz/t/compound-need-to-increase-community-engagement-and-more/3221",
    "combinedcontent": "Hello Compound Community, please allow us to describe the current state of Compound and we would like to share some ideas.\nAnalysis of social media operations , according to the 3 media listed on the official website:\nFirst of all, congratulations as Compound Treasury receives S&P Credit Rating. It is the first institutional decentralized finance (DeFi) offering to be rated by a major credit rating agency. The announcement of this great news is also Compound’s first medium of this year. Its last post was published 186 days ago.\nMaybe it was Medium’s restriction on the content which led to the low frequency of Compound’s posting at intervals of half a year. But it also shows that Compound Labs give very little attention to its marketing channels or materials. If it were Medium’s restriction, Compound can consider switching to Substack or Mirror, but we see no such changes, so maybe we should not blame Medium.\nPerhaps for security reasons, Compound has not set up an official Telegram channel, which also gives many fake Compound accounts the opportunities to fish for newbies. Users really need to improve their security awareness and cannot submit mnemonic phrases to those scam accounts.\nSuspicion of SCAM on telegram 2\nCompound Discord Channel hosts bi-weekly developer conferences, which is the only place where the community gets to know about Compound’s updates and progress. The community has no way of knowing what features, user need or use cases Compound is working on, or when they will complete in the future, nor any estimated time of completion.\nWe would very much like to see an roadmap which is transparent about its progress. \nThe following is roadmap examples of Filecoin 8 and Looksrare 2.\nIf Compound had been more open with its roadmap and process, the Community would have been able to give comments and share ideas, rather than seeing it spending time immersed in CompoundGateWay and then switched to the multi-chain strategy at a very belated stage. All of its lagged transition makes Compound falling behind the market, which directly demises its impact among defi protocols, as well as the faith of the Compound Community.\nWe often see spam messages in Discord channel left unattended for hours before some staff delete it. Most of the time, the number of discussions and speakers is equal to the total number of addresses participating in such proposal’s voting. The Compound Community is currently in a very quiet state, more actions are needed to engage the Community.\nIn the meanwhile, Compound Twitter stats are not optimistic either. Compound Labs tweets about 1 to 5 times per month. The net increase in its followers has decreased. Compared to growth data, tweet frequency and followers of other competing protocols, Compound has fell way behind.\n\nfollowers890×645 48.4 KB\n\n\nfollowers2890×638 50 KB\n\nTo sum up, we think there is much relativity between Compound’s inactive community, intransparent develop progress, below-average media influence and Compound’s overall user growth rate.\nCompound’s market occupancy rate in lending protocols decreased from 40.82% to 10.37%.\n\ntoken11982×594 168 KB\n\n\ntoken21984×633 99.3 KB\n\nSpecial thanks to Gauntlet’s efforts for this.\nLending | Markets | Token Terminal 1\nThe decline in its market occupancy directly leads to the decline of its protocol revenue, which indirectly reduces the attractiveness of COMP in the market. Buying interest declined. So please do not attribute it to the decline of the whole crypto market.\nCompound Forks \nForks - DefiLlama 2\nCurve Forks (like Saddle and Ellipsis) have brought some benefits (aka airdrop) to holders of Curve and veCRV, while Compound Fork never really bring benefits to holders of COMP. Moreover, due hack accidents related to Compound Forks’ modified code, Compound’s branding was tarnished, and COMP holders took the hit as well.\nRekt - Voltage Finance - REKT\nRekt - Agave DAO, Hundred Finance - REKT\nRekt - Fei Rari - REKT 2\nhttps://thedefiant.io/compound-finance-fork-easyfi-loses-over-60m-in-admin-key-hack/\nThis Compound Finance Fork Just Froze $1M in Ethereum Tokens - CoinDesk\nDeFi Lending Protocol Ola Finance Exploited for $3.6M\nDeFi Project Cream Finance Exploited in $25M Flash Loan Exploit - Decrypt\nShould Compound Forks allocate 3-10% tokens to COMP holders?\nCompound Forks and other lending protocols are getting VC attentions\nPolychain Capital Portfolio:\nAurigami(Compound fork in Aurora)\nSolend\nUMEE\nParaFi Capital Portfolio:\nBastion(Compound fork in Aurora)\nBurrow\nJet Protocol\nRobot Ventures Portfolio:\nMoonwell\nJet Protocol\nParadigm Portfolio:\nEuler\nJet Protocol\nIt is quite common for VCs to invest in multiple lending protocols. Considering Compound’s large COMP reserves, is Compound interested in getting new strategic investors? We know Compound probably doesn’t need financing, but what about strategic cooperation?\nAs the whole crypto market surges, more and more funds emerge in the years of 2021 and 2022. Many of them have large AUMs and are quite insightful. Maybe they have a lot of interest in Compound but in need of an opportunity to participate.\nNew funds might bring new perspectives and synergies, and can also provide financial support for some of Compound’s expenditures, such as Certora Formal Verification, increasing mmunefi’s bounty, recruiting developers and community operators.\n\nmessari12281×867 144 KB\n\nMessari - Bitcoin & crypto price, news, charts, and research 1\nCompound | Leaderboard\nThis reminds us of Sushi’s proposal for a Strategic Raise. We believe Sushi would have been in a much better place if such proposal wasn’t withdrawn.\n[Withdrawn] Sushi Phantom Troupe - Strategic Raise - Sushinomics - SushiSwap\nCONCLUSION\nDoes Compound need to increase user engagement in social media and improve its operation of Twitter and Discord?\nShould Compound create progress board for the community and increase its development transparency?\nShould we ask Compound Forks for token distribution?\nShould Compound consider bringing in new funds?\nCLAIRVOYANTLABS HAS NOT DESIGNED A SPECIFIC PLAN TO IMPROVE SOCIAL MEDIA. THIS IS JUST AN IDEA, IF MOST PEOPLE AGREE, WE SHALL DISCUSS THE DETAILS IN THE COMMUNITY TOGETHER.\nCLAIRVOYANTLABS DOES NOT HAVE PERMISSION TO CREATE DEVELOPMENT PROGRESS BOARD OR ACCESS TO ANY UNDISCLOSED INFORMATION, IT DEPENDS ON COMPOUND LABS WHETHER THEY ARE WILLING TO OPEN IT TO THE COMMUNITY.\nCLAIRVOYANTLABS HAS NO INTEREST IN ANY COMPOUND FORKS AND HAS NOT INITIATED ANY PROPOSALS IN ANY FORK COMMUNITY.\nCLAIRVOYANTLABS HAS NOT DISCUSSED FINANCING PLAN WITH COMPOUND LABS. IT IS JUST AN IDEA FOR THE COMMUNITY TO DISCUSS, NO FURTHER FUNDING DETAILS OR ANY POTENTIAL INSTITUTIONS. IT IS UP TO COMPOUND LABS AND THE COMMUNITY TO MULL ON.\nCLAIRVOYANTLABS IS NOT ASSOCIATED WITH COMPOUND LABS IN ANY WAY, OTHER THAN BEING A LONG-TERM HODL AND A TRUE HEAVY USER.\nDoes Compound need to increase user engagement in social media and improve its operation of Twitter and Discord?\n YES NO14votersResults\nShould Compound create progress board for the community and increase its development transparency?\n YES NO14votersResults\nShould we ask Compound Forks for token distribution?\n YES NO14votersResults\nShould Compound consider bringing in new funds?\n YES NO11votersResults\\nDon’t waste your time, just read old posts and you will realize that they don’t want community. Check initial token distribution and issuance model. Check “accidentally” bug that become famous as best airdrop after Uniswap. Check how “masterminds” tried control community with populistic tricks. Why Compound  VCs dont want deploy protocol on scaling solution?\nWhy they state that Coinbase oracle is better than Chainlink?\\nThanks for your replies and community members who voted, I have closed voting.\nThere is always someone who will be a month late, but the state of the market will not give him enough time.\\nbtw,  you did a great analysis\\n\n  \n      \n\n      compound.substack.com\n  \n\n  \n    \n\nDeFi State of the Union Recap, Treasury now supports international wires 8\n\n  Compound Digest - May 26, 2022\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\\nSo far, no portion of revenues on Compound goes to COMP holders.\\n\nimage680×522 87.6 KB\n\ngive me more than governance\n\n  \n      \n\n      The DeFi Edge – 27 May 22\n  \n\n  \n    \n\nGetting a piece of the token upside 6\n\n  Some DeFi protocols are able to generate millions per day in fees. Whenever I'm analyzing an investment, I'm asking myself, \"Are they sharing a piece of that with us?\"\n\nThat's value accrual.\n\nToday I'll be Covering:\n• Value Accrual. How are protocols...\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\\n\nimage1715×1175 160 KB\n\nDevelopment activity is positively correlated with Compound.\\nThe Role of Governance in DAOs\n  \n      \n\n      Medium – 24 May 22\n  \n\n  \n    \n\nThe Role of Governance in DAOs 8\n\n  One doesn’t have to be in DeFi very long to come across the word “governance.” People commonly wave their hands and say, “that’s up to…\n\n  \n    Reading time: 6 min read\n  \n\n  \n\n  \n    \n    \n  \n\n  \n\n\\nCompound governance is disorganized\nhttps://www.comp.xyz/t/compound-governance-is-disorganized\n\n  \n\n      docs.google.com\n  \n\n  \n    \n\nCOMP Gov Concerns (Jul/Aug 2021) 9\n\nCOMP Summer Cleaning: Time to get organized  Compound has been the guinea pig for decentralized, on-chain governance in DeFi. As such, it should be no surprise that things have not necessarily gone as smoothly as we may have hoped. There have been a...\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nCompound still hasn’t improved much on this."
  },
  {
    "number_of_comments": 9,
    "postid": "a665598b-ae22-4083-962c-7508fa4b8990",
    "posturl": "https://www.comp.xyz/t/compound-v3-usdc-comet-risk-parameter-updates-2022-12-28/3946",
    "combinedcontent": "\nSimple Summary\nA proposal to adjust one (1) risk parameter for two (2) Compound V3 assets.\nProposal:\n\n\n\n\nParameter\nCurrent Value\nRecommended Value\n\n\n\n\nCOMP Liquidation Factor\n88%\n83%\n\n\nWBTC Liquidation Factor\n95%\n93%\n\n\n\n\nAbstract\nUpdates to the Liquidation Factors allow the Compound protocol to ensure it’s solvency during extreme market conditions. By decreasing these parameters, this increases the incentive for liquidators to take collateral off the protocol’s books as it accumulates.\nGauntlet’s simulation engine has ingested the latest market and liquidity data. These parameter updates are a continuation of Gauntlet’s regular parameter recommendations as part of Dynamic Risk Parameters 2.\n\nMotivation\nIn certain extreme market conditions - Compound III’s liquidation mechanism 3 can be stuck with insolvent collateral. If liquidators can profit from swapping after acquiring collateral from Compound III at a discount, they will acquire the assets, and the protocol can avoid holding the collaterals over time. Thus, liquidity and liquidation factor are two key exogenous variables for successful liquidations (liquidators call absorb, buying from Compound III and swapping them out). While we cannot directly control the former, increasing liquidation incentives can reduce protocol insolvency risk. Gauntlet’s agent-based simulations use a wide array of varied input data that changes on a daily basis (including but not limited to user positions, asset volatility, asset correlation, asset collateral usage, DEX/CEX liquidity, trading volume, expected market impact of trades, liquidator behavior). Our simulations tease out complex relationships between these inputs that cannot be simply expressed as heuristics.\nBased on Gauntlet’s simulation results as of 2022-12-27, decreasing COMP liquidation factor from 88% to 83% reduces Value at Risk (VaR) for COMP from $5.5m to $5m, a ~9% reduction. Decreasing WBTC liquidation factor from 95% to 93% reduces its VaR from $5.9m to $4.9m, a ~17% reduction. Given the current supply amount of COMP and WBTC in the market, and the implied VaR size (38.5% and 8.7% respectively for COMP and WBTC), we want to proactively propose reducing liquidation factor as a prevention for potential disastrous market events.\n\n2372×1024 252 KB\n\nWe’d note that given that these parameter changes are for the liquidation incentive, there is not a meaningful impact on capital efficiency.\nThe below heatmap shows the average % net insolvent COMP under different market scenarios. When the market has 22x volatility, decreasing liquidation factor from 88% to 83% drops the insolvency percentage from 11.86% to 9.73%, and when the market has 16.75x volatility, from 3.47% to 2.99%.\n\n1184×974 45.3 KB\n\nThe below heatmap shows the average % net insolvent WBTC under different market scenarios. When the market has 22x volatility, decreasing liquidation factor from 95% to 93% drops the insolvency percentage from 2.04% to 1.33%, and when the market has 16.75x volatility, from 0.29% to 0.11%.\n\n1186×948 71.4 KB\n\n\nOne other option\nAs mentioned, these changes do increase the penalty paid by users when they are liquidated. This is to increase the premium paid to liquidators who take on the risk of selling their collateral, and can offset costs like slippage. This is not positive for the borrow user experience, but is likely justified by the reduction in VaR. Looking at COMP as the example:\n\nCurrently there is a 12% penalty paid when useres are liquidated\n6% of that goes to Compound reserves, 6% of that goes to liquidators\nThis proposal changes that to 17%, 8.5% and 8.5% respectively\n\nAnother option here would be to reduce the StoreFrontPriceFactor (SFPF) which determines the split between Compound and liquidators. This would allow the protocol to lower VaR without increasing the liquidation penalty as much (or even at all)l.\nThis would end up negatively affecting the protocol reserve growth over time, so there are real tradeoffs here. We have not suggested any changes to this parameter yet, but if people are worried about the liquidation penalty becoming to high, we could investigate the options we have with SFPF.\n\nNext Steps\nWe welcome community feedback here. We plan to go to proposal on-chain on 1/2/23 or as soon as we are able to incorporate any blocking feedback\n\nNotes\nWe’d note that VaR here refers to the amount of collateral that has been absorbed by Comet but not purchased by liquidators. In other words, the VaR calculation refers to the value of absorbed collateral that is sitting on Comet’s balance sheet and is subject to pricing risk. This definition of VaR is slightly different from the definition of VaR for Compound V2 in order to align specifically with Comet’s new liquidation mechanism.\nGauntlet launched an insolvency refund for Compound that contains a portion of our payment stream that can be clawed back in the event of insolvencies due to market risk. Since our last recommendation there have been no new insolvencies in Compound, Gauntlet’s Insolvency Refund vault is still live and can be seen here 0x7667095Caa12b79fCa489ff6E2198Ca01fDAe057 2\nBy approving this proposal, you agree that any services provided by Gauntlet shall be governed by the terms of service available at gauntlet.network/tos.\n\\nCome posso parlare in privato\\nThere is a trade-off between user experience (from borrowers) and protocol risk inherent in the liquidation factor. Borrowers want the lowest possible liquidation factor (“penalty”) in case they get liquidated, and risk managers want a high liquidation factor; to incentivize liquidators and provide a protocol reserve cushion.\nInstinctually, it does not feel like the WBTC liquidation factor should be reduced (more penalty), and whether COMP should be reduced (after it was just reduced from -7% to -12%) is another question.\nUsing WBTC as an example, in order for the protocol to incurr losses (bad debt, e.g. a decrease in reserves) through the liquidation process, the price of WBTC has to decline:\n\nApproximately ~23% (77% max LTV)\n\nBeyond the risk cushion that a user currently has (if a user is at 99% risk, WBTC might have to decline only 24%, if they are at 80% risk WBTC would have to decline ~39% in price)\n\nBefore a liquidator is incentivized to step in and liquidate the user (this is key)\n\nCurrently, there is $67M of WBTC collateral spread across many users, who would get liquidated at different price points. The core question is whether the 5% incentive (split equally between the liquidator and reserves) is enough incentive to encourage liquidation. Simplified, we can say, “will a liquidator want WBTC, at a 2.5% discount to market price, before WBTC has fallen 23-39%”?\nOn-chain (and liquidators can use both on-chain and off-chain liquidity) there is massive liquidity for WBTC; DefiLlama’s liquidity tool 2 currently indicates that $10M of WBTC can be instantly liquidated with ~1% slippage. Would a liquidator be willing to accept WBTC, at a 2.5% discount, knowing that it can instantly be sold at a 1% discount, in almost any size? Very likely. If they had probably hours for these stars to align (WBTC has to fall a long way before the protocol incurrs losses), would they be willing to accept a 2.5% discount? Far more likely.\nI am concerned that this proposal is overly conservative, and comes at the cost of user experience, when the protocol is still in its infancy and hasn’t scaled yet. As a community, I think we should be taking steps with the parameters and incentives to encourage the migration of assets to the newer protocol.\\nI agree with Robert on this point.  I’m also curious what Chaos Labs thinks, if anything.\\nThanks, @rleshner, for your feedback to help move community discussion forward. We agree there are important tradeoffs for the community. From a risk management perspective, we aim to provide the community with more information on the risk side. Our simulations show a meaningful reduction in Value at Risk (VaR) from these parameter changes.\n\n\n\n rleshner:\n\nBorrowers want the lowest possible liquidation factor (“penalty”) in case they get liquidated, and risk managers want a high liquidation factor; to incentivize liquidators and provide a protocol reserve cushion.\n\n\nTo clarify, higher liquidation bonus does not always reduce risk, because a bonus that is too high can push accounts closer to liquidation as well as increase the chances of liquidation cascades in certain tail events. As a result, we do not always recommend increasing liquidation bonus. Below is an example of simulation results that we conducted in our original Compound research. 2  After the liquidation bonus reaches a certain point (around 5% in this specific example), the likelihood of insolvency (bad debt) increases (of course, for V3, it is not entirely apples-to-apples).\n\n0_jcc-5gdIwM1IWXp1700×620 34.3 KB\n\n\n\n\n rleshner:\n\nOn-chain (and liquidators can use both on-chain and off-chain liquidity) there is massive liquidity for WBTC; DefiLlama’s liquidity tool  currently indicates that $10M of WBTC can be instantly liquidated with ~1% slippage.\n\n\nThanks for providing this data here. For context, we do not measure slippage in a static way (we do not only consider current liquidity levels). This is because our simulations aim to measure risk in market stress periods (which can happen rapidly, as we have seen during Black Thursday). In tail market events, liquidity conditions can change dramatically. For example, in stress periods, there would be massive liquidations in the whole DeFi ecosystem, and liquidity can evaporate rapidly. Our model takes a more conservative assumption in case of such liquidity squeezes, and the Dashboard provides transparency for the results of the simulation under different volatility settings.\nGiven the risks of WBTC insolvency (making up a significant portion of VaR), we could either adjust liquidation penalty, liquidation threshold, or storefront price factor to reduce VaR.\n\nWe assume that Liquidation penalty is not as influential as liquidation threshold as a driver for user migration to V3.\nAdjusting LT can either encourage risky behaviors or hinder user experience.\nGiven market conditions and recent volatility, our top priority is insolvency risk. Thus, we are relatively reluctant to recommend adjusting the storefront price factor right now, but it is a top consideration should reserves grow or if market conditions improve.\n\nOf course, there are strategic considerations here as well. For example, the community may decide to heavily focus on migration to v3 to shrink the size of v2 and thus de-risk v2 (which may certainly be a practical strategy). There are always revenue/risk considerations, and we hope that the above provides some perspective on the risk side. As always, we would appreciate the community’s feedback here as they consider the tradeoffs.\\nPaul’s explanation makes sense to me. Just a quick question about the Net Insolvent Value in the original post:\nvolatility scalar means the volatility of the underlying asset. I suppose the iv of BTC can be a good alternative indicator. Current iv is about 40-50%, then at 500% vol scalar the iv is 200-250%. We have seen during extreme market conditions iv does not exceed 500%, which corresponds to a vol scalar of around 1000%. If that’s the case, then it rarely makes sense to consider those areas above the line vol scalar=1000%. We can see that even if the market explodes the current liquidation factor should be fine.\nIn case you define vol scalar in a different way (instead of defining it based on iv), would you mind share your methodology for it?\nThanks!\\n\n\n\n pauljlei:\n\nTo clarify, higher liquidation bonus does not always reduce risk, because a bonus that is too high can push accounts closer to liquidation as well as increase the chances of liquidation cascades in certain tail events.\n\n\n@pauljlei, thanks for highlighting the above. However, this dynamic - a bonus that is too high can push accounts closer to liquidation - is not quite intuitive from a cause and effect perspective. I looked through the document you linked (Compound Market Risk Assessment) and the only explanation it provides is the following:\n“If the liquidation incentive is too high, a borrower may be unwilling to borrow assets from Compound in the first place, or she may open a borrowing position and maintain a high collateral factor.”\nIs the above user behavior an assumption of the Agent Based Simulations, or has it been observed in the real world? Unwillingness to open borrow positions is understandable when confronted with high liquidation incentive. However, the tendency to maintain a high collateral factor (if people indeed behave that way) is not intuitive. Is there a rational explanation for it?\\nI agree with RogerS’s concern. My concern about volatility also coincides with a point in his previous posts (Compound DAO vendor management capability & Gauntlet - #5 by pauljlei 1).\\nHi @RogerS - great question. A higher liquidation bonus takes more collateral out of the user position, leading to a lower health score, which increases the chances that the user position gets liquidatable again in a future point in time. As such, a higher liquidation bonus can increase the chances of a liquidation cascade (due to triggering more liquidations and the liquidations being larger). When liquidation bonuses are very high, it can directly lead to insolvency when the liquidation bonus is greater than the capital buffer itself.\\n@RapidsCapital - Our volatility scalar is used to multiply our estimated volatility for the tokens instead of IVs. For context, if our estimated WBTC volatility is 15%, a 3000% vol scalar would correspond to 450%."
  },
  {
    "number_of_comments": 9,
    "postid": "c60dae86-83df-42d0-9e4e-fcec8624d720",
    "posturl": "https://www.comp.xyz/t/migrate-gfx-labs-and-gauntlet-comp-streams-over-to-sablier/2785",
    "combinedcontent": "\nTL;DR\nReplace the current COMP streams which GFX Labs 9 and Gauntlet 6 are recipients of with Sablier 11 streams.\nHere 12 is an introduction to Sablier, and the reasons its adoption within Compound matters.\n\nProposal\nCompound’s current token-vesting solution essentially relies on an endless COMP stream which can only be ended by a Compound governance proposal. The streaming solution itself is largely undocumented (Sablier docs 6). Additionally, the current COMP streams are based on block number rather than timestamp. This isn’t ideal as block numbers tend to deviate from timestamps in the long run. Sablier enables term based payment streams that pay linearly with time and are cancelable before the end of term by payee or payer.\nWe propose to end the current streams GFX Labs and Gauntlet are receiving, and create new streams in Sablier to replace them at the same stream rate. The end date for those streams would be 1 year from the start date.\\nThanks for this. I’ll throw some newbie comments/questions in as someone following this with half an eye.\n\n\nOverall I like this because I think Sablier is a well established DeFi lego, and I hope that Compound can draw from the strengths of other protocols to a larger degree, to better focus and excel at what Compound does.\n\n\nHow does Sablier actually achieve using timestamps? Some sort of oracle solution?\n\n\nSlightly off-topic, but has Open Zeppelin started their work for Compound so that this (if done promptly) will be included?\n\n\\nSmall point of clarification:\nGFX Labs is not receiving compensation from the protocol. The existing streaming grant is to me. That said, I have a proposal that will go up in January for GFX Labs to step up it’s involvement in the protocol and receive compensation for those efforts. That will also include ending my streaming grant.\nBack on topic, I think using Sablier is worth exploring. I’ve been thinking through a few different compensation methods, and I think Sablier could be helpful.\\n\n\n\n TragedyStruck:\n\nHow does Sablier actually achieve using timestamps? Some sort of oracle solution?\n\n\nWe’re simply using block.timestamp to track time. This is a more reliable method than block numbers, which are hard to predict accurately.\nAs explained in this StackExchange Q&A 6, block.timestamp is safe to use if the time-dependent event can vary by a short amount of time.\\n\n\n\n TragedyStruck:\n\nSlightly off-topic, but has Open Zeppelin started their work for Compound so that this (if done promptly) will be included?\n\n\nStrongly supportive of this proposal. Agree that we should also include OpenZeppelin on this migration.\\nThanks for bringing this forward, @maxdesalle . At Gauntlet we welcome the community feedback and look forward to working together. We agree that Sablier can potentially reflect Compound’s engagement with Gauntlet more accurately than the existing COMP streams. Not sure how time sensitive this is given that Gauntlet began its engagement just ~3 months ago, but we look forward to finding solutions together here.\\nThere are no fees associated with Sablier, so it is definitely worth it to stream. Pay only the gas fees and stream worldwide 24/7/365 ahead of time.\\nI’m working on generating the numbers for creating the Sablier streams and will post them here later this week. Planning to propose next Monday.\\nBelow are the figures that will be used to migrate existing contributor comp streams to Sablier streams. The calculations used to derive the numbers are shown briefly. Sablier requires 4 inputs to create a new stream: toAddress, start time, end time, and total amount. The total amount must be divisible by the duration—this requirement is satisfied in the rounding step where the total_comp number is rounded to the nearest valid input.\nPlease review the below information and comment if anything seems to be wrong or confusing.\n\nGetty\nstart date 7/13/21. End date 7/13/22 + 1 at 12am gmt (1641920400 - 1657756800)\nCOMP_PER_DAY: 1.3589\nAddress: 0x9b68c14e936104e9a7a24c712beecdc220002984\nTotal Comp: 249075045833329227600\nDuration: 15836400\n183.291666666666667 days * COMP_PER_DAY = 249.075045833333334\nRounded: 249.075045833329227600 COMP\nComp Per Second: 15836400\n\nGauntlet\nstart date 9/27/21. End date 9/27/22 + 1 at 12am gmt (1641920400 - 1664323200)\nCOMP_PER_DAY: 41.2532534247\nAddress: 0xd20c9667bf0047f313228f9fe11f8b9f8dc29bba\nTotal comp: 10696624835912820075600\nDuration: 22402800\n259.291666666666667 days * COMP_PER_DAY = 10696.624835912837514 COMP\nRounded: 10696.624835912820075600 COMP\nComp Per Second: 477468210934027\n\nOZ\nstart date 12/21/21. End date 12/21/22 + 1 at 12am gmt (1641920400 - 1671667200)\nCOMP_PER_DAY:  41.3543551305\nAddress: 0x57c970568668087c05352456a3f59b58b0330066\nTotal Comp: 14237959851805055889600\nDuration: 29746800\n344.291666666666667 days * COMP_PER_DAY = 14237.959851805062514 COMP\nRounded: 14237.959851805055889600 COMP\nComp Per Second: 478638369565972\\nThis is now live in proposal 79 21. Go vote!\nNote:\nSomething to reiterate on the topic. Streams on sablier can be canceled at any point by governance or the recipient. If a stream needs to be repriced, it can be canceled and restarted."
  },
  {
    "number_of_comments": 56,
    "postid": "ed91c5cf-ec21-4254-875a-e54f29585a08",
    "posturl": "https://www.comp.xyz/t/building-a-medianizer/1031",
    "combinedcontent": "Compound NEEDS to improve the current oracle infrastructure as a matter of safety and growth. I think it is evident to users that the current solution is not ideal and not close to ideal. The event that occurred in November with Dai liquidations was preventable. Little has been done to prevent the situation from happening again with Dai or any other markets. In addition to preventing the November event from transpiring again, I think it is also in the protocol’s best interest to improve the oracle situation so Compound can safely support more assets.\nBased on the conversation that has been had on the forum and Discord, the community is uncertain what the next steps are to improving the situation. I have seen discussion of adding Chainlink as an oracle provider, Makerdao’s oracle, additional exchanges, and decentralized exchanges as price feeds. There is no consensus on what is to be trusted.\nWhile I am confident in using Compound and store assets on the platform, I think the protocol would greatly benefit from investing in better oracle infrastructure in the long-run. While the Open Price Feed built by the Compound team does a good job getting prices from exchanges onto the blockchain, it has no logic to average the prices or weight them by significance. We are fortunate enough to have many high-quality exchanges, both on and off-chain, that we should be utilizing.\nI think the best next step is to build a medianizer (following Makerdao’s lead) that can handle multiple price feeds. To make the task less daunting, I suggest the first implementation tries to utilize Coinbase Pro, Okex, and Uniswap prices. The medianzier will need to consider when the last price of each of these sources is made so that it omits stale prices. The solution should also allow more price feeds to be added on at a later date.\nUnfortunately, I lack the skills (learning) necessary to build this idea, but the community will likely reward who is ever successful if someone/group were to build it. Although I lack the technical skills, I know a good amount about incentives, liquidations, and exchanges, and I am willing to lend a hand in any way I can to make this improvement happen. I will also send personally $5k of COMP token to whoever successfully implements this, and if you are unsuccessful but can get to a full proposal, I will send $1k of COMP.\nAll questions, comments, and concerns are welcomed and encouraged. If you are an individual who is interested in building this and are unsure if the protocol will compensate and how much they might compensate you, please post those questions here and DM me if you want more certainty. While I am just 1 person and sub 200 votes, I would strongly advocate for a 1000 COMP 3 ($220k) grant to whoever implements this successfully by building and implementing a long-lasting solution.\\nIncreasing the number of price references is also important, but it is not a measure against instantaneous spikes and drops.\nCan’t Compound add a time lag before liquidation?\\nI am not sure I understand your point. Are you saying that the protocol should have more sources to get price info from? If so, that is part of the plan here. We need first to build the system to handle multiple sources, but we can add more sources once we have that.\\nAdding some additional thoughts about how to structure this:\nI think there are two routes to building this medianizer.\n\n\nModify the Open Price Feed to become just a tool to get prices from exchanges and post those prices to a separate contract that is the medianizer.\n\n\nModify the Open Price Feed to have a medianizer built-in and modify the medianizer to accept onchain prices.\n\n\nThere very well could be a better third option. If you have a different idea please feel free to share it.\\nTo recap the proposal, are you suggesting that instead of arguing about which price source to switch to, we’d first build the infrastructure that would allow us to add new price sources to be medianized, and then worry about which price source (Maker, Chainlink, …) should be added?\\nCorrect, we need to first build the infrastructure to support more price sources and then we can talk about adding many more.\\nI definitely appreciate you stepping up @getty and taking initiative here, putting your own COMP on the line no less!\nThat said, this seems like a lot of development time, effort, and complication that could be avoided by using Chainlink, which we know will vastly improve the oracle situation with minimal development consideration or risk to the protocol.\nAgain, I applaud you taking initiative on this much-needed oracle improvement and think this is a step in the right direction, however I’m not convinced the juice is worth the squeeze when there are better/simpler solutions available.\\nI have looked into this.\nIn order to remain truly decentralized I believe we can fetch plenty of source on chain.\nFor example I can fetch prices from the chain all day for free by using read-only calls to many DEX. We can fetch many of them at no cost and implement the appropriate correlation\nWhy not let the community decide what source (on chain) they consider trustworthy and which correlation factor they consider safe. Best example was the Dai case. A value that high above average source at the time and over time should have been correlated as a rejected value. Especially for Dai. I’m not expecting it to change that much so it should have a low factor.\nThe sources can be updated over time if one becomes untrusted by the community. Or new feeds are born and approved.\nThis would cover the reading part at no cost.\nNow as for state changing calls to record the prices. I would rather have a Comp incentive to do so for the Compound Protocol and by the community. This is, in my opinion the best way to achieve a long term solution that rely on a common decision.\nThis would also turn Compound into a real decent community maintained price feed.\\nI think this is the right approach. We know the current oracle system is not the protocol’s ideal state. Having the ability to modularly add new oracle sources is a step in the right direction.\nIt’d be great if we get feedback from protocol devs on feasibility and cost/benefit.\\nWell I know this is something I can do. It’s just a matter of integration to the protocol and community response.\\n\n\n\n MasterofNonce:\n\nThat said, this seems like a lot of development time, effort, and complication that could be avoided by using Chainlink, which we know will vastly improve the oracle situation with minimal development consideration or risk to the protocol.\n\n\nI think using any one source is not the right decision. As well, I am certain governance would not vote in favor of switching to Chainlink. That being said, I do think we could use Chainlink as part of the medianizer (I don’t know how this would work), and the community would be more likely to use Chainlink if it was part of a larger system.\n\n\n\n madeindreams:\n\nWhy not let the community decide what source (on chain) they consider trustworthy and which correlation factor they consider safe.\n\n\nI like the medianizer idea a bit more because it is less complex and already proven to be safe, but I think this idea is positive. If you were to build this and out and implement this successfully, I would be thrilled to reward you for it.\\n\n\n\n getty:\n\nI think using any one source is not the right decision. As well, I am certain governance would not vote in favor of switching to Chainlink. That being said, I do think we could use Chainlink as part of the medianizer (I don’t know how this would work), and the community would be more likely to use Chainlink if it was part of a larger system.\n\n\nChainlink is not really one source, as the price is calculated as an average from several sources. For example, the DAI/USD price is delivered by 7 different sources (DAI/USD 1). Why not use a ready and proven solution instead of building a new one?\nWhy would “governance” not vote for switching to Chainlink? It would be nice to hear the arguments of the guys with the largest voting power. This time BEFORE the proposal is created.\\nthe question is why WOULD you use chainlink? you need to pay link and trust on top. chainlink is quite susceptible to human error. we could just add more price feeds to the open oracle.\\nI don’t think you have to pay LINK, if you use the oracle prices.\n\n\nchainlink is quite susceptible to human error.\n\n\nWhat exactly do you mean by this?\\nThere is a litany of reasons listed in this thread 4. However it boils down to the fact that Chainlink provides an immediate and comprehensive oracle solution that doesn’t require all this time and development, which could present unnecessary complications/risk within the protocol.\nJust pulling from a few more exchanges will not provide adequate market coverage, and you don’t have to trust any single entity but a decentralized network of Chainlink nodes.\nRegardless, I definitely agree with @cryptix in that it would be nice to hear an argument from those with the largest voting power.\\nReposting my thoughts from Discord:\nIt seems like there is not only a critical flaw in the system (the false liquidation attack vector is still open) but that this same flaw is also hamstringing developments and improvements to the protocol. To me it seems like fixing both of these (and addressing victims of the previous attack) should be priority number 1. If the push is coming from Coinbase as investors then they are shooting themselves (and every other COMP holder) in the foot here. Let me know if I’m missing something though.\nFor clarification it seems to me that this breaks down to 2 issues which would both be solved with a different oracle mechanism (third party or custom built). The false liquidation attack vector and the hard coupling to a single centralized entity bottlenecking development.\\nHi guys. Want to jump in quickly. I do think having medianizer will help with security significantly, both by adding more data sources and reducing systematic risk from relying on a small number of tech infrastructures. I’m happy to take on the work to implement the code and go through audit to make this happen.\nAlso, Band Protocol has open oracle implementation already live https://open-oracle.bandprotocol.com/ 8 and we are excited to join as a reporter if the community agrees to it \\nI’m a fan of Chainlink and I think it should be included as one of the sources. Though, having a single oracle categorically exposes the protocol to single point of failure risk – no matter how many feeds are aggregated into Chainlink, there’s still only a single feed that reaches Compound. This recent issue would be addressed but the oracle problem as a whole will remain. It’s important that we take a wholesome approach in addressing the oracle issue, not just a reaction to the events during the Thanksgiving week.\nThat’s why I think Getty’s proposal is the right approach: this proposal allows the protocol to have multiple sources of oracles and that is much needed.\\nThis! We could implement chainlink into the open oracle/medianizer and also add more dex/cex then we could be well off.\\nWhile multiple oracles may sound like a good idea, mixing high quality and low quality data is like mixing wine and vinegar…it just ends up worse.\nLet’s say we mixed 1) Chainlink, whose nodes source from an array of premium data providers who take volume/liquidity into account 2) an oracle that fetches prices from a preselected array of exchange APIs, and 3) a single data source or exchange API.\nIt’s not hard to imagine a highly volatile scenario where Chainlink is reporting a volume-adjusted market-wide price of $500, the pre-selected exchange array is reporting $400 due it covering only small percent of total market volume, and the single source reporting a price of $0 due an outage caused by the volatility. This scenario would result in a median of $400, or even worse…a mean of $300.\nAgain, I see how it sounds like a good idea on the surface, but mixing sources of various quality, calculation methods etc. is not ideal and can threaten the integrity of the overall system.\\nUPDATE: 1/27/2020\nOn the goverance call today, I introduced building a medianizer and received a warm reception. @sorawit and the Band Protocol Team have stepped up and taken on the role of developing a solution. They believe they can build a system that meets the guidelines I posted. On today’s call, the Band Team said they hope to present to the community at the next governance call.\nThe team at Band has developed their own oracle system that “allows developers to query any data including real-world events, sports, weather, random numbers and more.” You can learn more about their work here 2.\nI am thrilled there is a strong team interested in executing this upgrade, and I think it would be a great example of governance working if this succeeds. In addition, this medianizer has the potential to be a tool for the whole DeFi community to utilize. Unlike other solutions, this would be a totally open source and free way for developers to get price info.\\nJust to clarify, Chainlink has numerous independent nodes and data sources per oracle feed, such as 21 nodes on the ETH/USD price feed currently and soon 31 in their OCR launch. It’s already decentralized, so one node or even several nodes going down will not cause a failure. Also, not as well known, but Chainlink has two separate node clients running at the same time, so if there were a problem in the core Chainlink software client then it would just failover to the previous version. With its OCR launch appearing to be very soon, it will actually have 3 client versions, for extra redundancy and failover protection. Where would this point of failure you described be if there are numerous independent nodes, data sources, and software client versions?\\nYou are right. From a technical point of view Chainlink alone would be the perfect solution. The problem is, that for unknown reasons, the original team decided not to use Chainlink and the sentiment against Chainlink seems to be unchanged. So, I think to argue technically for Chainlink doesn’t make sense, as this is a political decision.\\nI would like to raise the community attention to the fact that coinbase oracle api is sent with different delays to different IP addresses.\nThe delay is of full minutes (not only few seconds), and probably stem from a sub-optimal REST API system on coinbase side.\nThis give unfair advantage to certain liquidators who get the price feed update before others, and this advantage can amount to millions of $ (e.g., in the $90M liquidation event).\nWe first realized this issue while working on the B.Protocol integration with Compound, and @blck also confirmed it after we raised the issue at the discord channel.\nMoreover @blck pointed out to us that different IP addresses have different delay.\nCentralized exchanges are notorious for giving different service classes to different players, with 0 transparency.\nAnd when designing new price feed, this should be taken into account. Relying on off-chain could bring CeFi dirty tricks to DeFi.\\n\n\n\n yaronvel:\n\nAnd when designing new price feed, this should be taken into account. Relying on off-chain could bring CeFi dirty tricks to DeFi.\n\n\nThank you for pointing this out. I would love to see some test results that prove this if you have them available. I am not surprised that this is happening, unfortunately.\nPart of the big picture with building the medianizer is adding many more price feeds, and in particular onchain sources, so that not any one source can exert pressure on an asset’s price.\\nHere is a snapshot from two synced servers, courtesy of @blck\nserver 1\nimage1325×242 5.58 KB\nserver 2\nimage1326×154 4.79 KB\nhere is also a code snippet I used.\n\n\n\n getty:\n\nPart of the big picture with building the medianizer is adding many more price feeds, and in particular onchain sources, so that not any one source can exert pressure on an asset’s price.\n\n\nThe issue here is not price manipulation, but rather that knowing even epsilon change in price could worth millions in certain scenarios.\nA practical approach is to demand a functional websocket service from coinbase (and okex) with no delays.\nOn the smart contract level, one can think of other mitigations as well, e.g., some build in delay between the price update and the liqudiation.\\nBut if there is actually governance here then that shouldn’t matter. It should be the best solution as voted on by the community. The idea that the team’s wanting or not wanting something for political reasons is still a thing, either means that:\n\nGovernance does not matter because a few people /entities control the majority of votes\nWe have not put it to a vote to actually find out.\n\\n\n\n\n yaronvel:\n\nCentralized exchanges are notorious for giving different service classes to different players, with 0 transparency. And when designing new price feed, this should be taken into account. Relying on off-chain could bring CeFi dirty tricks to DeFi.\n\n\nThanks for sharing this @yaronvel - this a good example of the disparity in data quality/transparency we are going to continue running into while constructing a patchwork of various price feeds. I’m concerned people underestimate the impact one bad apple can have on the bunch.\nWe could build the best medianizer in the world, but it is nonetheless beholden to the quality of data it receives. Chainlink, whose nodes source from top aggregators, will always provide the best data. And just think of how much time/energy we could save to put towards things like Compound Chain, adding new collateral etc.\n\n\n\n HopeisNotaStrategy:\n\nBut if there is actually governance here then that shouldn’t matter. It should be the best solution as voted on by the community. The idea that the team’s wanting or not wanting something for political reasons is still a thing, either means that:\n\nGovernance does not matter because a few people /entities control the majority of votes\nWe have not put it to a vote to actually find out.\n\n\n\nGood point. A quick look at the leaderboard shows that 5 entities have over 50% of the voting weight, meaning together can effectively pass/reject any proposal. Meanwhile I have no idea what they think about the oracle situation, which is no doubt odd. It would be nice to have a clearer understanding of their intentions, reasoning etc. to help better guide our conversation. If it is political, I’d like to know why.\\nThis thread is about building a medianizer.\nIf you have questions, comments, ideas about the medianizer, you are welcome to post them here. At this point, we have an initial consensus that the community would like this idea developed and are no longer debating the merits of developing a medianizer in this channel. If you disagree with this development, then I suggest you try to enact a change yourself or vote against this when given the opportunity.\nI will happily debate transitioning the Open Oracle Feed to a Chainlink service, but this is no longer the place to have that debate.\\nI’m not very tech-savvy, so please let me know.\nHow long does it take to clear a compound after it reaches the clearing price?\nAnd is it possible to avoid the clearing with the data feedback using “CHAINKINK” or “BAND” that will be adopted in this proposal?\nI checked the status of Chainlink’s data feedback, and it seems to be about every 10 minutes.\nSorry for asking so many questions.\nChainlink   datafeed\n2021-01-29 (1)_LI1136×640 35 KB\\nWhen discussing a major protocol change, there shouldn’t be a point at which discussing its merits is over, even if there is ‘initial consensus.’\nWe are already uncovering issues with data quality, and if this approach doesn’t end up solving the problems the community thinks it will then it’s a lot of wasted time, development effort, or worse.\nOut of respect I will dial it back and allow this thread to be more development focused, but I will be keeping eye on the progress here and continuing to voice any new concerns.\\nChainlink updates on price deviations, such as every .5% change in price will trigger an update. So the volatility of the asset over the 24 hour period will determine how much the price feed is updated.\\nIs there a list of data sources that will be used for this? I’m not sure how such a medianizer will ensure market coverage if it’s taking the median value from a mixture of oracle price feeds that track many exchanges directly with raw exchange data from single exchanges. Not all data sources are created equal, sources with more market coverage should be weighted higher. Decentralization for pricing data is obviously important, but I don’t think it should come at the expense of market coverage or data quality in general. Market coverage is the primary issue that this solution should seek to solve.\\nGreat question, picking which data sources are safe to add will be interesting once the medianizer is built. For now, the goal is to develop a system that can take input from both offchain and onchain sources, and it will be up to the community to decide what sources are safe to add. Once we know how the medianizer is going to be structured, we can begin to discuss which sources the community wants to add in addition to Coinbase. I will likely start a separate forum thread once it is time to start that conversation. I will say I am partial to have many sources rather than just a handful, here 3 is an interesting article to read related to the topic.\\n\n\n\n getty:\n\nAt this point, we have an initial consensus that the community would like this idea developed\n\n\nWho exactly is “the community”? I couldn’t see any discussion here in the so called “Compound community forum”.\\nI still don’t understand how this medianizer solves the issue of market coverage if it’s still going to be weighting every data source equally. Raw data from a single exchange is of a very different quality/utility than a decentralized oracle network that pulls from multiple data aggregators who track all exchanges by weighing each by real volume. Not every oracle network is of the same quality either so that’s why I think the medianizer shouldn’t just be strictly a “median picker” because that doesn’t account for different data qualities from different sources.\nI can give an example, if we used a medianizer that pulls from five sources: Coinbase, OKEx, Uniswap, Chainlink price feeds, Band price feeds, and then volume/liquidity of an asset consolidates to an exchange that isn’t Coinbase, OKEx, or Uniswap (let’s say it goes to Binance), then those three exchanges being used in the medianizer can become manipulated for very little cost and end up controlling the median value because those three exchanges are being weighted equally to the two price feeds that aggregate from a much broader collection of exchanges including Binance (the median of five sources takes the third value in an ordered list). This may or may not be an issue for a highly liquid assets like ETH, but for something less liquid like ZRX or for a new less liquid collateral as Compound scales, it could cause a abnormal deviation which could allow for an undercollateralized loan to be created on or for false liquidations to occur.\nThe medianizer can be built, but we still need to discuss its merits and determine if it will end up solving the problem we think it is before a full integration. I worry that it is simply increasing the attack surface of Compound by mixing lower quality raw data with higher quality refined data.\\nAdding more oracles requires increased gas usage. Compound’s current OPF oracles are extremely gas efficient since they just parrot back a stored answer. Other on-chain oracles are not as efficient and can cost a lot more gas.\nGiven this, a possible solution would be to have an intermediary contract that periodically does the gas expensive tasks of collecting prices from a variety of sources, then distilling that prices down to a single trusted price that it stores. Compound would just fetch this stored price. This could use the same fetching API as the current OPF oracles, so that the ctoken code does not need to change. This would still maintain the gas efficiency of the current setup.\\nAgree, that we can react to event. Aggressive liquidation with 8% premium for liquidator is great incentive for manipulator, and users are not protected.\n\n\n\n getty:\n\nWhile the Open Price Feed built by the Compound team does a good job getting prices from exchanges onto the blockchain\n\n\nWe cant get any report or explanation from centralize exchange when event like “DAI  liquidation” happens. I wish I could investigate an “unforeseen event” on the blockchain.\nI dont think that is good job, I think we stuck with Coinbase.\n\n\n\n getty:\n\nI think there are two routes to building this medianizer.\n\nModify the Open Price Feed to become just a tool to get prices from exchanges and post those prices to a separate contract that is the medianizer.\nModify the Open Price Feed to have a medianizer built-in and modify the medianizer to accept onchain prices.\n\n\n\nInteresting suggestions\\n\n\n\n megamind44:\n\nI still don’t understand how this medianizer solves the issue of market coverage if it’s still going to be weighting every data source equally.\n\n\nCurrently, Compound Finance is using one source (Coinbase) for getting prices. The medianizer will be a huge upgrade and allow the protocol to consider adding more price feeds safely. It will be up to governance to only add high-quality sources for price data.\n\n\n\n megamind44:\n\nmedianizer can become manipulated for very little cost\n\n\nI think you are underestimating how much it costs to move markets of the assets on Compound. As a market maker, I can tell you the market is fairly efficient and has matured significantly over the last year.\nCompound Finance should not be adding any assets to the platform that are only traded at a few exchanges and have limited liquidity. We should be only adding high-quality ERC20s.\\nHi all,\nFirst post here. I built Maker’s Oracles so I think I can provide a bit of perspective here.\nMaker uses a Medianizer which takes the median of a set of Feeds. Each Feed queries a wide sample of exchanges (both on-chain and offchain) to compose it’s own price. This gives us good market-coverage without relying on any single entity. A core piece of Maker’s collateral onboarding process is determining the Data Model (the set of sources) for an asset.\nSo I think the Medianizer model works great, but I’m a bit concerned by the implementation being discussed here. I want to caution against mixing individual sources like Uniswap and Coinbase along with aggregated sources like Chainlink, Band, or Maker Oracles because you end up overweighting some sources over others which facilitates Oracle manipulation. On top of that, many of the aggregators have another layer of obscurity where they use data provider services rather than exchange data itself. So you end up with this really opaque mess where there are hidden vulnerabilities for exploiting the Oracle, but it’s not clear where they are.\nMy suggestion is to keep things simple.\nUsing signed exchange data is very gas-efficient and negates the need for trusted parties. However, not enough exchanges currently sign their price data, making it very difficult to build a robust manipulation-resistant Oracle. Doing the hybrid model (Open Oracle + aggregator sources) doesn’t fix anything and just creates hidden risks. So in my opinion, the Compound community has three options.\n\n\nConvince more exchanges to sign price data. In the interim period before sufficient exchanges are onboarded the Compound Protocol might be vulnerable.\n\n\nReplace the Compound Open Oracle with an aggregated Oracle service like Chainlink, Maker, or Band. This is the easiest solution, but reduces Compound’s flexibility by adding an external dependency. This reduced flexibility is why Maker created its own Oracles rather than going with something like Chainlink.\n\n\nFork one of the Oracle protocols and onboard your own set of trusted actors (Feeds) via Compound Governance. This is a lot more work, since you have additional infra to maintain and support, and have the governance overhead of onboarding “Feeds”. The benefit here is that Compound’s community would have maximum flexibility to integrate whatever asset they want with their own Data Model and don’t have to rely on an external org for deliverables.\n\n\nI’ll throw Maker’s medianizer 4 in the ring that’s been in use in Multi-Collateral Dai since launch as an option for both (2) and (3). Here’s a dashboard 5 showing the medianized Oracle prices for Maker.\\nNice to meet you, and thank you for all your advice for Compound.\nAs an alternative approach to the oracle, how about the idea of holding off on liquidation for a minute when the price reaches a price that will result in liquidation?\\nThank you, Nik, for chiming in and sharing your thoughts. You bring up good points that will certainly be taken into account while developing better oracle infrastructure for Compound.\nThe immediate goal I have is to get the medianizer built so we can begin to discuss what feeds should be added. In tandem, I am working on getting more exchanges to adopt what Coinbase and Okex have done for signing prices. I don’t foresee Compound adding an oracle aggregate (Chainlink, Band, etc) in the near future.\nWhat I know for certain, whatever is built will not be perfect, and that is okay. The main goal is to improve the current system from one source. I would like to avoid getting caught up in trying to build the perfect oracle solution and instead focus on using resources efficiently.\\n\n\n\n getty:\n\nThe medianizer will be a huge upgrade and allow the protocol to consider adding more price feeds safely. It will be up to governance to only add high-quality sources for price data.\n\n\nThe core issue here though is that a medianizer always weighs each input equally, actually increasing the risk of data manipulation attacks by assuming each source is of a equal quality at all times. This is an issue no matter what collection of sources are used:\n\n\nWeighing raw exchange data equally with decentralized price feeds that aggregate data from a multitude of exchanges is an issue as it overweighs a select few exchanges. This lowers the cost of manipulation by reducing the number of exchanges a malicious actor has to manipulate to affect the final median value, especially if volume/liquidity consolidates away from those select few exchanges.\n\n\nWeighing data raw from different exchanges equally presents an issue as every exchange has a different level of liquidity and volume (different cost of manipulation). This overweighs exchanges with less volume/liquidity and underweighs those with higher volume/liquidity. Additionally, if volume consolidates to a small number of exchanges, then a malicious actor only has to manipulate the low volume/liquidity exchanges to affect the median value.\n\n\nWeighing different price feed oracle solutions equally is an issue each price feed generates its data in different ways. Some price feeds fetch raw data from a predefined selection of exchanges weighing each equally and thus not generating proper market coverage, while other price feeds fetch from multiple data aggregation firms who have full time data quality teams and monitoring tools in order to generate proper market coverage by weighing each exchange by its real volume/liquidity. Additionally, some price feeds make the mistake of pulling from both raw exchanges and data aggregators, weighing each equally and thus overweighing a select few exchanges.\n\n\nEssentially, such a medianizer design is vulnerable to these types of issues regardless of what inputs are used. As a result, I do not think this medianizer design will be tamper-resistant enough against data manipulation in order to properly secure a $5.6Bn market, particularly when there already exists decentralized price feed solutions that can be integrated into Compound today like Chainlink that provides proper market coverage.\n\n\n\n getty:\n\nI think you are underestimating how much it costs to move markets of the assets on Compound.\n\n\nWe have already seen from the DAI Liquidation event 1 that you don’t need to move the entire market to manipulate a price oracle that operates without proper market coverage. Even a highly liquid exchange like Coinbase can deviate from the market wide price, so if the medianizer is overweighing a select few exchanges, then only those exchanges needs to be manipulated (not the whole market) to affect the oracle. While lower liquidity assets are more vulnerable to these issues I’ve listed, the lack of market coverage is actually an issue for all assets on the Compound protocol.\\nYou present five issues:\n\n\n\n megamind44:\n\nThe core issue here though is that a medianizer always weighs each input equally\n\n\n\n\n\n megamind44:\n\nWeighing raw exchange data equally with decentralized price feeds that aggregate data from a multitude of exchanges is an issue as it overweighs a select few exchanges.\n\n\n\n\n\n megamind44:\n\nWeighing data raw from different exchanges equally presents an issue as every exchange has a different level of liquidity and volume (different cost of manipulation).\n\n\n\n\n\n megamind44:\n\nWeighing different price feed oracle solutions equally is an issue each price feed generates its data in different ways.\n\n\n\n\n\n megamind44:\n\nWe have already seen from the DAI Liquidation event that you don’t need to move the entire market to manipulate a price oracle that operates without proper market coverage.\n\n\n\n\nThe median of a set of numbers is much safer than a mean. If we have a handful of exchanges reporting data, but one or two are reporting bad data or are down (an unlikely event), we don’t have to worry because the other exchanges will still create a safe median. If we were using a mean then you would be right.\n\n\nI am planning on advocating to add more exchanges and onchain exchanges and not to add aggregate sources like Chainlink because I agree this would be a problem. Although, it would be a small problem in the scale of things.\n\n\nYes, some exchanges have more liquidity than others. In the big picture, all the top 20 exchanges have plenty of liquidity to produce a real market. The bigger concern I have is about uptime/downtime, but if we have 10 plus sources and one or two go down, we don’t need to worry.\n\n\nFor now, I am not considering adding any oracle solutions, so this is not a concern of mine.\n\n\nI already cite the Dai Liquidation event in November as a failure of the current system. It is well documented that if we have been sourcing prices from multiple exchanges that this wouldn’t have happened, or at least not at the same scale.\n\n\\nOn no1, would your plan to add more high exchanges need them to adopt open oracle and  sign their price feed data?\\n\n\n\n getty:\n\nThe median of a set of numbers is much safer than a mean.\n\n\nI am not talking about taking a simple mean, but how it is more optimal to create a reference price where sources are weighted differently according to their difference in quality (like what proven price feed solutions already do). If a source went down and the previous data they gave becomes stale, they could have a weight of zero. If we look into how professional data aggregation firms generate their indices, you’ll see that they don’t take a simple median or a simple mean, because that does not generate proper market coverage, they generate a volume weighted average price.\n\n\n\n getty:\n\nThe bigger concern I have is about uptime/downtime, but if we have 10 plus sources and one or two go down, we don’t need to worry.\n\n\nTaking the median from multiple exchanges means you are more protected from downtime yes, but it does not mean you are safe from manipulation. This medianizer cannot produce safe values if volume/liquidity consolidates to a few exchanges, especially if it moves to new exchanges that aren’t being tracked. While the exchanges added to the medianizer may provide enough market coverage initially, there is no guarantee it will stay that way going into the future.\nWhen taking a properly weighted average, adding more exchanges means you track more of the market, but when taking a median value, it means a malicious actor only has to manipulate 51% of the exchanges to affect the final value, even if 80%+ of the volume consolidates to one or two exchanges (like how many DeFi tokens are vastly more liquid on Uniswap). Adding more exchanges does not mean additional tamper-resistance against manipulation in this current design, particularly as Compound scales up and adds more collateral to stay competitive to grow in TVL.\nDowntime is really not the issue here, data manipulation attacks are.\n\n\n\n getty:\n\nFor now, I am not considering adding any oracle solutions, so this is not a concern of mine.\n\n\nI did not get the impression from others in this thread that only values from exchanges would be used, but price feeds as well. It is up to governance to decide if this is the case of not, but regardless of what sources are used, a simple median is not enough in my opinion. Some questions to consider:\n\nWho will be responsible for ensuring the medianizer has proper market coverage at all times (automated systems and alerts for this)?\nWhen volume consolidates to a few exchanges, particularly exchanges not being tracked, what is the process for updating the collection of sources to keep market coverage (will this be fast enough during extreme market events)?\nHow will we convince more exchanges to sign their data (seems to hit a stall on this front)?\nWhy should the Compound community spend precious resources (COMP) building a new price feed solution that cannot adequately guarantee market coverage, when there already exists price feed solutions that have proven their ability to generate and maintain market coverage for other large scale money markets (like how Aave and Cream use Chainlink feeds without issue)?\n\\nThanks for weighing in @nikkunkel. You bring up many good points. I share your concern regarding mixing different data types as well as your concern with us being able to actually get enough exchanges to commit to signing data in the first place.\n\n\n\n nikkunkel:\n\nmany of the aggregators have another layer of obscurity where they use data provider services rather than exchange data itself. So you end up with this really opaque mess where there are hidden vulnerabilities for exploiting the Oracle, but it’s not clear where they are.\n\n\nData aggregators ensure full market coverage as they are monitored by a variety of full-time data experts who can quickly respond to volume shifts across exchanges, data irregularities etc. Combining many high-quality aggregators ensures there is no single source of truth, and allows for the utilization of multiple data aggregations methodologies which increases the overall tamper-resistance of the system.\nPulling directly from exchange APIs, on the other hand, leaves these data-centric concerns in the hands of Compound governance, which may be slow/unable to react to certain market conditions. It will also present challenges when it comes to adding more collateral types as they will need to be individually assessed, voted in, and continually monitored for proper market coverage.\nPersonally I don’t think forking an existing oracle project is the way to go, as it would be too expensive and laborious to keep updated and we wouldn’t benefit from the shared cost model of simply using a feed from an existing oracle project.\nThis all leads to me supporting option #2 (using an aggregated oracle service). The simplest solution is almost always the correct one.\\nCorrect me if I’m wrong, but I think we kinda all agree that relying on one source is not the ideal oracle implementation. Current Compound open oracle uses Coinbase (reporter 0xfceadafab14d46e20144f48824d0c09b1a03f2bc) for price data (must be within ~20% of Uniswap as another layer of sanity check).\nGiven that the open oracle 1 already has the standard offchain signing and OpenOraclePriceData already supports posting prices from multiple reporters to the store, modifying the smart contract to do medianizer from multiple sources should not be hard. We can start from doing weighted median and see how it goes.\nSo here’s what I think makes sense:\n\nWe write a new oracle medianizer that allows many prices from many reporters to be submitted within one transaction. Valid data points will be recorded on-chain.\nGovernance should be able to add more reporters and adjust the weight of each reporter. The medianizer will use weighted median 1 as the asset price for Comptroller calculation.\nOn chain price data (such as KP3R’s Uniquote or ChainLink or BANK) can also become a “reporter”. In that case, no signatures are needed, but rather the smart contracts need to implementation a certain TBD interface.\n\nNow the question of who will decide what a “reporter” is, whether a primary source, an aggregator, an oracle solution, an anon guy, etc is pretty irrelevant to the implementation I think.\nEach solution has pros and cons as discussed by several of you, but that can be decided by COMP governance later. My personal opinion is that the combination of all reporter types, while makes it hard to reason where the exact price data value comes from, is the probably the hardest to manipulate by an attacker in practice. And Compound market does not really need the exact, most correct price – just need to be within a certain threshold to make sure loans are liquidated before becoming insolvent.\nLet me know what you guys think. We gotta start somewhere and if many of you think this is the right start, I can start the implementation right away!\\nI very much agree with your suggestions.\nEspecially the following part\n\nBlockquote\nCompound market does not really need the exact, most correct price – just need to be within a certain threshold to make sure loans are liquidated before becoming insolvent.\n\nAll we need to do is prevent liquidation!\\n\n\n\n sorawit:\n\nCompound market does not really need the exact, most correct price – just need to be within a certain threshold to make sure loans are liquidated before becoming insolvent.\n\n\nCompromising on data quality is what got us in trouble in the first place, and I fear this mindset is a slippery slope that will present more issues as Compound continues to scale and add new collateral.\nThere just isn’t a great reason for a multi-billion dollar protocol to compromise on data quality/accuracy, especially when we are seeing other money markets like Aave operating perfectly because they utilize feeds that don’t.\\n@TennisBowling lets try and get some snapshots up?\\n\n\n\n sorawit:\n\nSo here’s what I think makes sense:\n\nWe write a new oracle medianizer that allows many prices from many reporters to be submitted within one transaction. Valid data points will be recorded on-chain.\nGovernance should be able to add more reporters and adjust the weight of each reporter. The medianizer will use weighted median as the asset price for Comptroller calculation.\nOn chain price data (such as KP3R’s Uniquote or ChainLink or BANK) can also become a “reporter”. In that case, no signatures are needed, but rather the smart contracts need to implementation a certain TBD interface.\n\nNow the question of who will decide what a “reporter” is, whether a primary source, an aggregator, an oracle solution, an anon guy, etc is pretty irrelevant to the implementation I think.\n\n\nThis sounds like a great start! I know the community is looking forward to seeing your/Band’s proof of concept on Wednesday’s governance call.\nAs the medainzier slowly comes to life, I’ll start a new thread about the next steps. Generally speaking, going from just using Coinbase to using Coinbase and Okex will be a huge improvement, but I have higher hopes than that.\nI’ll start a separate forum thread after Wednesday’s call if it goes well to begin the discussion.\\nLooking forward to discussing this in the next thread!\\nUPDATE: 2/10/2020\nThe Band Protocol team (@sorawit) presented their proof of concept for the medianzer on today’s governance call. Overall the POC received a warm reception.\nNext steps:\n\n\nMake the POC into an efficient contract ready for audit.\n\n\nRun some simulations to analyze gas costs.\n\n\nMake the contract upgradable\n\n\nResearch integrating oracles (like Uniswap) directly into the medianizer to save on gas.\n\n\nAdditionally, be on the look out for a new forum post beginning the discussion of reports/price feeds.\\nThank you for the update.\nCould you link the Band Protocol team’s statement?\\nI don’t have a written statement to post, and unfortunately, the last goverance call does not have a recording I can link due to technical issues.\nI can link Band’s Github 17 for the medaizner.\\nAlthough I’m nowhere near technical enough to actively build this I just have to say I listened to the Community Call with @getty and also Band protocol representatives, and I have to say this is a great effort, and you did a great job presenting it! I’m absolutely sold on this being required to keep growing the Compound protocol. Thanks for doing this!"
  },
  {
    "number_of_comments": 23,
    "postid": "6b8e71f1-d0ee-48f4-b13e-56d14eb10de8",
    "posturl": "https://www.comp.xyz/t/frax-listing-proposal/1540",
    "combinedcontent": "I am proposing adding FRAX to Compound.  I’m a member of the Frax community.  I’m hoping to gather feedback, especially around proposed parameters.  FRAX’s capital efficiency and lending AMO should make FRAX the least expensive stablecoin to borrow on Compound.  As time passes and FRAX proves itself as a useful asset on Compound, I hope FRAX will gradually be treated more like a fully collateralized fiat stablecoin on Compound.\nSummary\nFRAX is a redeemable stablecoin with a dynamic collateral ratio that adjusts based on market demand. Frax is unique among recent stablecoin designs in that each FRAX is always redeemable for $1.00 worth of assets from the Frax protocol. This has kept the price of FRAX in a relatively tight band around $1.00, making Frax an ideal asset to borrow and lend against on Compound. Frax is already integrating with Compound and is currently the 17th largest cUSDC holder (~$30m) and growing, demonstrating a strong synergy between the communities.  As the FRAX supply increases, additional collateral will be deposited into Compound.  The Frax community has also chosen to hold all COMP rewards received to be an active participant in Compound governance.\nv2 of the Frax protocol recently launched and introduces a lending module that can be built directly into Compound. This would enable the Frax protocol to supply large amounts of FRAX to Compound on demand. FRAX could become the lowest borrowing cost stablecoin on Compound, which would be quite attractive for Compound borrowers.  The first step towards this integration is the addition of FRAX as an asset on Compound.\nFRAX Stability\nSince launching in December 2020, FRAX has held a tight band around the $1.00 price target.  Below is a chart showing FRAX price history since launch:\n\nEach FRAX is collateralized by approximately $0.86 USDC (including interest bearing equivalents) and $0.14 of the Frax governance token, FXS. When the price of FRAX is at or above $1.00, the protocol gradually lowers the collateralization ratio of USDC to FXS. When the price of FRAX is below $1.00, the protocol gradually increases the ratio. FRAX can always be minted or redeemed by the protocol for $1.00 of assets, which counterbalances significant price deviations from the $1.00 target. Frax emphasizes a highly autonomous approach with no active management of the price stability function.  More information about the mechanisms behind FRAX can be found in the links below.\nFRAX is not an algorithmic stablecoin.  There are algorithmic aspects of FRAX (the protocol uses FXS as a tool to stabilize the price of FRAX) but it is better understood as a capital efficient collateralized stablecoin.  Given that the Compound collateral factor of FRAX is likely to be well below the current collateralization ratio of 86.5%, FRAX as a collateral asset on Compound would effectively be over collateralized by a healthy margin.  It is also worth noting that there is also approximately $60m of Uniswap, Sushiswap and FRAX-3Pool LP tokens locked within the Frax protocol. A significant portion of this is locked for greater than 2 years. This provides guaranteed liquidity for FRAX and provides significant benefits to the stability of the protocol in times of volatility.\nLending AMO\nFrax has built a lending contract (AMO) that will integrate the Frax protocol directly into Compound to supply FRAX on demand for borrowing.  This would be somewhat analogous to the Fed discount window, where borrowers could borrow directly from the FRAX protocol via Compoound. A direct integration with Compound will also make supplying FRAX more attractive to other market participants because there is a guarantee of FRAX liquidity directly from the protocol, avoiding the pitfalls of high utilization rates in DeFi money markets.\nMisc. Project Info\nThere is approximately $102m of FRAX liquidity currently on Uniswap.  The main venue for FRAX is slowly shifting to a Curve FRAX-3pool; the stableswap curve is so much more capital efficient at holding the $1.00 price target.  FRAX volume was averaging about $9m volume in March - Coingecko is currently under reporting the volume as it does not include the Curve pool or Sushiswap.  Given that FRAX can always be minted and redeemed at the protocol, daily volume is less of a factor in my opinion.  Frax has undergone extensive code reviews and was audited by Certik with additional audits upcoming.\nInput on Proposed Parameters\nI’ve spoken with the Frax team and they are happy to help any with oracle / contract deployment needs.  I’d appreciate some Compound community input on the Collateral Factor and Reserve Factor. Given that each FRAX is backed by approximately $0.865 of USDC, cUSDC, aUSDC and yUSDC (obviously on chain), it would make sense to me that FRAX’s Collateral Factor and Reserve Factor would look more like DAI than USDT - perhaps a CF of 60% and RF of 15% to start.  I also understand if a more conservative approach is desired as things get off the ground - please let me know what you think!\nThank you for your time and participation.  Please let me know if you have any questions or concerns.  As I receive input, I’m happy to revise the proposal to incorporate it.  Reference links are below.\nReferences\n\nProject: https://frax.finance  4\n\nWhitepaper: Introduction - Frax ¤ Finance 3\n\nTwitter: https://twitter.com/fraxfinance 1 \n\nCodebase: frax.finance · GitHub  1\n\nDocumentation: https://docs.frax.finance/  1\n\nApp: https://app.frax.finance/  2\n\nAudit: https://certik-public-assets.s3.amazonaws.com/REP-Frax-06-11-20.pdf 1\n\nFRAX token contract: https://etherscan.io/token/0x853d955acef822db058eb8505911ed77f175b99e 2\n\nTelegram: Telegram: Contact @fraxfinance 1\n\nDiscord: https://discord.gg/eKXUxaKJXT  2\n\n\\nThanks for creating this proposal here @metalface. Really important for FRAX to get on Compound as we are the most capital efficient stablecoin on the market. As everyone can see, our peg is perfect and rock solid just like DAI, USDC, etc. I’m also confident that due to the capital efficient design of FRAX, it will be the cheapest/most affordable stablecoin to borrow which would increase Compound usage by a lot.\nAdditionally, it’s important to note that the Frax Protocol itself is one of the largest lenders on Compound with its idle collateral. So there’s incredible synergy here. Frax Protocol lends over $30m on Compound to earn yield with its unused collateral. We also earn COMP tokens for this lending that we have never sold and do not plan to sell. We want to be an active part of Compound governance as important players in the DeFi space.\nVery excited to build with you all if the proposal is accepted for a vote on chain!\\nQuick note - I’m not sure if the current oracle soln. can be easily adapted to support FRAX: Compound | Docs - Open Price Feed 11\\nHey guys! I’m bumping this up to get more insights from the Compound community as well as now that the recent Chainlink oracle proposal has been passed through COMP governance, this should address @jmo’s concern about FRAX not having an open oracle feed. FRAX has a Chainlink feed in both FRAX-ETH and FRAX-USD which can be leveraged if COMP governance votes to include FRAX into the protocol.\nI also want to update on our progress. Since this proposal was created, FRAX has not just grown but has thrived, becoming the premier decentralized stablecoin that has successfully developed new algorithmic stability mechanisms. We’ve never broken our peg in over 7 months of operation now. Market data is on our side. We’re also ranked as the 11th largest stablecoin by Defipulse with performance on par with all other stablecoins ranked in the list.\n\nScreen Shot 2021-07-14 at 3.59.07 PM893×777 66.1 KB\n\\nAnother reference point > Aave - Open Source Liquidity Protocol 13\\nThanks @inkymaze - it’s exciting to see FRAX supported by Aave.  Since the original post, Frax has more than tripled in supply, completed an audit with Trail of Bits and also has a FRAX / USD Chainlink price feed which should address @jmo concerns:\n\n  \n      \n\n      data.chain.link\n  \n\n  \n    \n\nFRAX / USD price today | Chainlink 3\n\n  Monitor the FRAX / USD Data Feed for accurate, up-to-date market data powered by Chainlink’s decentralized oracle network on undefined.\n\n\n  \n\n  \n    \n    \n  \n\n  \n\n\nIt’s nice to see the interest in the newer generation of stablecoins that are actually stable (shoutout to FEI for their recent success and having an awesome community overall).  Let me know if you have any questions, would love to move this forward.\\nGently bumping this.  Frax has been live and stable for nearly a year and has over 1bn FRAX circulating.  Frax lending AMOs are currently supplying over $100m of liquidity to other lending platforms.  If Compound adds FRAX, there will be liquidity.  Frax has chainlink price feeds.  If Gauntlet or other’s see any issues here, I’d be happy to try and find the right person to address them - otherwise would love to push this forward.\\nGauntlet is recommending that FRAX be added as a collateral asset on Compound. FRAX is an exciting new stablecoin that will provide more stablecoin liquidity to the Compound protocol in addition to DAI, USDC, and USDT. By convention, assets are usually listed on Compound with a collateral factor of 0, but given the liquidity and volatility profile of FRAX, the Gauntlet Platform expects to ramp up the FRAX collateral factor post-listing.\n\nMarket Risk\nFRAX is currently listed on exchanges such as Uniswap, Sushiswap, Solarbeam (on Moonriver), Pangolin (on Avalanche), and more. The liquidity for FRAX has been steadily increasing, with the 90, 60, and 30 day ADVs being $22M, $30M, and $43M respectively.\n\n1192×734 17.6 KB\n\nNote that while the ADV of FRAX is increasing, it is still substantially lower than DAI (roughly 10X lower) and USDC (roughly 50X lower).\nFRAX has a 36% annualized volatility over the past month, which is more volatile than DAI (4%) or USDC (2%).\n\nDecentralization\nThe top 10 FRAX token positions are listed below:\n\n1980×1048 285 KB\n\nAs seen in the chart below, the top 10 positions make up 87.6% of the total FRAX supply. However, the majority of those tokens are either locked in Curve’s FRAX3CRV pool, other exchanges, and bridges.\n\n1360×840 90.2 KB\n\nIf FRAX is trading below $1 on exchanges, users are incentivized to burn FRAX in exchange for $1 of USDC and FXS. Conversely, if FRAX is trading above $1 on exchanges, users are incentivized to mint FRAX by depositing USDC and burning FXS, then selling FRAX on the exchange. This way the supplies of FRAX and FXS are controlled while maintaining a peg to the USD. While the FRAX protocol is decentralized, the collateralization mechanism relies on USDC as collateral, which is centrally backed.\n\nAnalyzing USDC and FRAX Pegs to USD\nGiven that FRAX is heavily dependent on USDC, Gauntlet’s platform will carefully analyze how Collateral Factor changes to both USDC and FRAX may affect their respective USD pegs.\\nCompletely agree that FRAX would be a great addition to Compound and looking forward to seeing the proposal soon!\\nLooking forward to seeing the proposal. FRAX has grown substantially over the lifetime of this thread, and the market has shown its durability. We support the addition of FRAX as an asset on Compound.\\nAgree with the above, Blockchain at UCLA supports this proposal to add FRAX as an asset to Compound\\nBlockchain@Columbia supports this proposal to add FRAX to compound given appropriate risk parameters are added. We think that it is important to add more stablecoins on the decentralization spectrum to Compound to help support their usage in the ecosystem and give users the widest variety of assets to borrow/lend on Compound.\\nHi guys, I’m with the Frax team and I’ve gone ahead and deployed the necessary contracts as well as added the necessary tests and configs to push this proposal onchain.\nContracts:\ncFRAX: CErc20Delegator | 0x1ddaAfEe72Ffc6Cd60368f17cEB376692F69515F 3\nNew Uniswap Anchored View which includes cFRAX: UniswapAnchoredView | 0x15dE940e76Ad5771C49bb82D5083C9148Af9fDD0 2\ncFRAX Validator Proxy: ValidatorProxy | 0x5B24EA09De554175Dd5431533Aad223c7C3583E4 2\ncFRAX was deployed to match other stables:\nUnderlying: 0x853d955acef822db058eb8505911ed77f175b99e\nComptroller: 0x3d9819210a31b4961b30ef54be2aed79b9c9cd3b\nInterestRateModel (matches USDC): 0xd8ec56013ea119e7181d231e5048f90fbbe753c0 2\nInitialExchangeRate(matches USDC): 2000000000000000000\nName: Compound Frax\nSymbol: cFRAX\nDecimals: 8\nAdmin: 0x6d903f6003cca6255d85cca4d3b5e5146dc33925\nImplementation: 0x24aa720906378bb8364228bddb8cabbc1f6fe1ba\nWe used the same Implementation/CErc20Delegate as cLINK and cWBTC2, which I believe is standard, but can change this in the proposal if it’s necessary.\nThe new Uniswap Anchored View has all the same configs as the existing one, 0x046728da7cb8272284238bd3e47909823d63a58d, with the addition of cFRAX. A diff of the config arrays, pulled from on-chain, can be found here: https://www.diffchecker.com/zBUL6nRJ 3\nThe cFRAX Validator Proxy was copied from USDP’s recently deployed one. Aggregator is set to the one used by Chainlink FRAX/USD: 0x61eb091ea16a32ea5b880d0b3d09d518c340d750 2, and the Validator is set to the new Uniswap Anchored View: 0x15de940e76ad5771c49bb82d5083c9148af9fdd0 2. Ownership has been offered to 0x21f73d42eb58ba49ddb685dc29d3bf5c0f0373ca 2, like the other Validator Proxies.\nSimulations:\nI adapted @TylerEther’s tests which he made for USDP, which simulates the proposal, and it passes as expected.\nYou can see the tests here 4, and you can see a  paste of the successful output here 4\nPull Requests:\nI’ve opened three PRs in the Compound github to add the necessary configs and assets for if the proposal passes. They include the simulation, mainnet config updates for all the new contracts, and a shiny new cFRAX token icon.\nCompound-protocol: https://github.com/compound-finance/compound-protocol/pull/183 3\nCompound-config: https://github.com/compound-finance/compound-config/pull/51 2\nCompound-assets: https://github.com/compound-finance/compound-components/pull/59 5\nHopefully this is what we need to push the proposal on-chain!\\nThanks, @corddry . As an update to the Compound Community, Gauntlet has been working on this listing with the FRAX team and has analyzed the economic risks of listing FRAX on Compound. We plan on putting up an on-chain proposal with an initial CF of 0% and reserve factor of 15%. On the smart contract and technical risk side, we would welcome analysis from the Community and any other interested parties, as Gauntlet is not an auditing firm.\\n@corddry good job managing to figure all of that out; it’s definitely not easy. Like my first deployment, you also made some similar mistakes .\n\nInitial exchange rate is incorrect\nTo quote @mistertom from my LINK market addition thread:\n\nThe standard cToken initial exchange rate for Compound is 0.02 scaled . The goal is to have 50 cTokens = 1 underlyingToken when the market is first deployed. In our case for cLINK , the math is:\n\n0.02 * 10^(18 + underlyingDecimals - cTokenDecimals)\n= 0.02 * 10^(18+18-8)\n= 0.02 * 10^28 … floating-point representation …\n= 2e26 … unsigned scaled integer representation … (value used for constructor)\n\n\n\nLike LINK, FRAX also uses 18 decimal places, so the initial exchange rate should be 2e26.\n\nStablecoins have been using a different implementation\nThe stablecoin implementation is 0xa035b9e130F2B1AedC733eEFb1C67Ba4c503491F 4.\n\nNew stablecoins have been using an improved interest rate model\nThis improved interest rate model is 0xFB564da37B41b2F6B6EDcc3e56FbF523bD9F2012 4.\nGood job so far!\nThanks @pauljlei for your help! How about a 25% reserve factor? This has become the standard for all new deployments.\\nThanks for the feedback! cFRAX has been redeployed with the updated parameters alongside an updated UAV to match.\ncFRAX: CErc20Delegator | 0xe7373A0D692F60400AF4A5ac6dfB927840414F86 8\nUAV: UniswapAnchoredView | 0xa469ddb19f903f4de66fdae32eb0d5a87c3826b3 5\nThe ValidatorProxy has been updated to point to the new UAV, and I’ve updated the tests and pull requests. Test results are successful and can be found here 5\nNew cFRAX parameters:\n    \"underlying\": \"0x853d955aCEf822Db058eb8505911ED77F175b99e\",\n    \"comptroller\": \"0x3d9819210a31b4961b30ef54be2aed79b9c9cd3b\",\n    \"interestRateModel\": \"0xFB564da37B41b2F6B6EDcc3e56FbF523bD9F2012\",\n    \"initialExchangeRateMantissa\": \"2.0e26\",\n    \"name\": \"Compound Frax\",\n    \"symbol\": \"cFRAX\",\n    \"decimals\": \"8\",\n    \"admin\": \"0x6d903f6003cca6255d85cca4d3b5e5146dc33925\",\n    \"implementation\": \"0xa035b9e130F2B1AedC733eEFb1C67Ba4c503491F\",\n    \"becomeImplementationData\": \"0x\"\n\nLet me know if I missed anything!\\nHi @TylerEther  , thanks very much for all your help here and for your thoughts. There are tradeoffs to be made whether we list with a 25% versus 15% Reserve Factor. Listing with a higher reserve factor increases protocol revenue from the start. In addition, there might be less user friction in decreasing the reserve factor down the line as opposed to increasing the reserve factor. On the other hand, listing with a lower reserve factor incentivizes suppliers with a higher deposit APY. Incentivizing supply here can be particularly useful given that users won’t be able to use FRAX as a collateral asset from the start. Although most new assets on Compound have listed at a Reserve Factor of 25% (LINK, AAVE, SUSHI, YFI, etc), most stablecoins (with the exception of USDP) currently have a Reserve Factor of 7.5% (and DAI is 15%), therefore, 15% is closer to a steady state value that most other stablecoins have. For these reasons, we will initially list FRAX with a 15% Reserve Factor but our platform will continuously ingest on-chain data to track how market conditions and user positions evolve.\\nAs an update, we plan on posting an on-chain proposal to list FRAX the week of February 21. We want to give this 2-weeks long notice period in order to provide enough time to @cylon Open Zeppelin and Community developers to assess and become comfortable with any smart contract / technical risks.\\n0xe7373A0D692F60400AF4A5ac6dfB927840414F86\nLooks good!\\nHi everyone, the OpenZeppelin team has been working on a security recommendation for how to handle asset listings that the community needs to consider before moving to list FRAX.\nWe’ve learned in our ongoing audit of the Compound protocol that the security risks for listing new assets are greater than most of the community might be aware. We’ve also found that a prior Critical issue related to an asset listing already existed and we’ve already worked with the Compound Multisig and the team behind the affected asset to ensure it is patched without incident. We’ll be releasing the complete Compound Audit Report and more details on the bug that was patched in the coming weeks.\nGiven what we’ve now learned, we believe that asset listing proposals such as this should receive more security attention going forward. We recommend that the community hold off on listing new assets for now as we work to put together a more detailed policy to ensure asset listings do not cause integration issues that could endanger the protocol. One option is to have OpenZeppelin audit each listing proposal although we need to consider the impact this might have on our existing backlog of protocol changes and the priority to assign to each one.\nI will follow up early next week with more detailed guidance on a path forward for the FRAX proposal. I understand that it may be frustrating to hold off on listing new assets right now so I ask for your patience as we work to create security processes that will protect Compound while also minimizing proposal wait times as much as possible.\nIf any of you want to learn more or share thoughts on securing asset listings, please don’t hesitate to reach out to me.\\nHey @cylon thanks for sharing your thoughts. I’d love to learn more about the thinking behind your recommendation. What is the best way to do so / reach you?\\nHi @josh\nWe’ve developed an asset listing process at this repository 4 that walks through the process of preparing for an asset listing. The main thing is to provide information requested in the Checklist 8 and then get community validation, have a market analysis from Gauntlet and then perform testing before moving forward.\nIt looks like you’ve already provided some of the info and gotten simulations and market analysis, so I’d recommend starting by checking what you’re missing from the Checklist. You can reach out to me on Telegram at @cyloncat if you have any questions on this. More on info this process is also described in our April Security Updates\\nThank you for the reply and for sharing the checklist. I will run through things on my end and assess where it stands vs. the checklist standards\\nHey guys-- picking up on this thread with some updates to fill in some pieces of the checklist that might not have been addressed fully before the audit.\nToken contract: Frax Finance: FRAX Token | Address 0x853d955acef822db058eb8505911ed77f175b99e | Etherscan 3\nToken Distribution: As a stablecoin integrated deeply throughout DeFi, all of Frax’s top holders are smart contracts. ~700M resides in a curve pool against 3crv and another ~150M resides in the new FraxBP. The rest of the holders with >2% of the supply are FPI-FRAX on Curve, UniV3 FRAX-USDC, UniV2 FXS-FRAX, and the Olympus DAO Treasury.\nPermissioned Functionality: The Frax stablecoin has no blacklist, is not pausable, has no external mint perms, etc. The Frax protocol has no ability to change user balances or prevent transactions from happening. The only privileged roles belong to the protocol itself to allow the Frax mint/redeem process and Frax’s signature AMOs. There are no mint permissions outside the protocol. This makes Frax essentially identical to DAI in terms of functionality of the token contract.\nUpgadability: The Frax contract has no proxy patterns and the implementation cannot be changed. The only parts of the contract which can be updated are minor parameters regarding the mint/redeem system such as oracles."
  }
]